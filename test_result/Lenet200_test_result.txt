Namespace(batch_size=60, dataset='mnist', epochs=40, lr=0.0012, model_arch='Lenet200_50', prune_iters=19, prune_per_conv=1, prune_per_linear=0.2, prune_per_out=0.1, test_iters=3, test_type='test_accu', testname='Lenet200_test', validation_ratio=0, weight_decay=0)
Learning start!
------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :        266200 (266200 | 0)          100.00
fc1.weight   :        235200 (235200 | 0)          100.00
fc2.weight   :         30000 (30000 | 0)           100.00
fcout.weight :          1000 (1000 | 0)            100.00
------------------------------------------------------------
Learning start! [Test_Iter : (1/3), Prune_iter : (1/19), Remaining weight : 100.0 %]
[epoch : 1] (l_loss: 0.20552) (t_loss: 0.14771) (accu: 0.9548)
[epoch : 2] (l_loss: 0.09078) (t_loss: 0.08666) (accu: 0.9711)
[epoch : 3] (l_loss: 0.06603) (t_loss: 0.08588) (accu: 0.9760)
[epoch : 4] (l_loss: 0.05023) (t_loss: 0.09412) (accu: 0.9740)
[epoch : 5] (l_loss: 0.04264) (t_loss: 0.09177) (accu: 0.9716)
[epoch : 6] (l_loss: 0.03834) (t_loss: 0.08313) (accu: 0.9778)
[epoch : 7] (l_loss: 0.03267) (t_loss: 0.08448) (accu: 0.9773)
[epoch : 8] (l_loss: 0.02853) (t_loss: 0.10487) (accu: 0.9745)
[epoch : 9] (l_loss: 0.02319) (t_loss: 0.10711) (accu: 0.9753)
[epoch : 10] (l_loss: 0.02352) (t_loss: 0.12562) (accu: 0.9725)
[epoch : 11] (l_loss: 0.02351) (t_loss: 0.10467) (accu: 0.9772)
[epoch : 12] (l_loss: 0.01870) (t_loss: 0.11704) (accu: 0.9753)
[epoch : 13] (l_loss: 0.02002) (t_loss: 0.11053) (accu: 0.9756)
[epoch : 14] (l_loss: 0.01856) (t_loss: 0.14033) (accu: 0.9729)
[epoch : 15] (l_loss: 0.01698) (t_loss: 0.10449) (accu: 0.9807)
[epoch : 16] (l_loss: 0.01674) (t_loss: 0.10813) (accu: 0.9771)
[epoch : 17] (l_loss: 0.01715) (t_loss: 0.14651) (accu: 0.9746)
[epoch : 18] (l_loss: 0.01398) (t_loss: 0.13648) (accu: 0.9776)
[epoch : 19] (l_loss: 0.01837) (t_loss: 0.10584) (accu: 0.9804)
[epoch : 20] (l_loss: 0.01261) (t_loss: 0.14293) (accu: 0.9783)
[epoch : 21] (l_loss: 0.01426) (t_loss: 0.12348) (accu: 0.9799)
[epoch : 22] (l_loss: 0.01496) (t_loss: 0.14264) (accu: 0.9787)
[epoch : 23] (l_loss: 0.01390) (t_loss: 0.12789) (accu: 0.9797)
[epoch : 24] (l_loss: 0.01287) (t_loss: 0.15370) (accu: 0.9786)
[epoch : 25] (l_loss: 0.01278) (t_loss: 0.15155) (accu: 0.9763)
[epoch : 26] (l_loss: 0.01388) (t_loss: 0.13209) (accu: 0.9815)
[epoch : 27] (l_loss: 0.01234) (t_loss: 0.15591) (accu: 0.9788)
[epoch : 28] (l_loss: 0.01192) (t_loss: 0.15825) (accu: 0.9774)
[epoch : 29] (l_loss: 0.01265) (t_loss: 0.16475) (accu: 0.9806)
[epoch : 30] (l_loss: 0.01535) (t_loss: 0.16597) (accu: 0.9795)
[epoch : 31] (l_loss: 0.00801) (t_loss: 0.18178) (accu: 0.9784)
[epoch : 32] (l_loss: 0.01354) (t_loss: 0.14722) (accu: 0.9805)
[epoch : 33] (l_loss: 0.00984) (t_loss: 0.15367) (accu: 0.9800)
[epoch : 34] (l_loss: 0.01297) (t_loss: 0.16895) (accu: 0.9796)
[epoch : 35] (l_loss: 0.00964) (t_loss: 0.18421) (accu: 0.9773)
[epoch : 36] (l_loss: 0.01291) (t_loss: 0.18272) (accu: 0.9789)
[epoch : 37] (l_loss: 0.00983) (t_loss: 0.15152) (accu: 0.9819)
[epoch : 38] (l_loss: 0.01260) (t_loss: 0.17516) (accu: 0.9787)
[epoch : 39] (l_loss: 0.00853) (t_loss: 0.16563) (accu: 0.9801)
[epoch : 40] (l_loss: 0.01295) (t_loss: 0.17737) (accu: 0.9804)
Finish! (Best accu: 0.9819) (Time taken(sec) : 662.85) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (213060 | 53140)         80.04
fc1.weight   :      235200 (188160 | 47040)         80.00
fc2.weight   :        30000 (24000 | 6000)          80.00
fcout.weight :          1000 (900 | 100)            90.00
------------------------------------------------------------
Learning start! [Test_Iter : (1/3), Prune_iter : (2/19), Remaining weight : 80.04 %]
[epoch : 1] (l_loss: 0.18977) (t_loss: 0.13557) (accu: 0.9589)
[epoch : 2] (l_loss: 0.08213) (t_loss: 0.07660) (accu: 0.9754)
[epoch : 3] (l_loss: 0.05784) (t_loss: 0.09893) (accu: 0.9692)
[epoch : 4] (l_loss: 0.04555) (t_loss: 0.07667) (accu: 0.9771)
[epoch : 5] (l_loss: 0.03827) (t_loss: 0.08887) (accu: 0.9761)
[epoch : 6] (l_loss: 0.03048) (t_loss: 0.08873) (accu: 0.9774)
[epoch : 7] (l_loss: 0.02788) (t_loss: 0.08456) (accu: 0.9779)
[epoch : 8] (l_loss: 0.02564) (t_loss: 0.09439) (accu: 0.9772)
[epoch : 9] (l_loss: 0.02299) (t_loss: 0.10046) (accu: 0.9767)
[epoch : 10] (l_loss: 0.01879) (t_loss: 0.11966) (accu: 0.9753)
[epoch : 11] (l_loss: 0.02000) (t_loss: 0.10441) (accu: 0.9781)
[epoch : 12] (l_loss: 0.01513) (t_loss: 0.10052) (accu: 0.9788)
[epoch : 13] (l_loss: 0.01715) (t_loss: 0.09553) (accu: 0.9807)
[epoch : 14] (l_loss: 0.01724) (t_loss: 0.11910) (accu: 0.9775)
[epoch : 15] (l_loss: 0.01522) (t_loss: 0.09469) (accu: 0.9811)
[epoch : 16] (l_loss: 0.01089) (t_loss: 0.12023) (accu: 0.9783)
[epoch : 17] (l_loss: 0.01603) (t_loss: 0.13493) (accu: 0.9771)
[epoch : 18] (l_loss: 0.01430) (t_loss: 0.11465) (accu: 0.9809)
[epoch : 19] (l_loss: 0.01481) (t_loss: 0.12751) (accu: 0.9785)
[epoch : 20] (l_loss: 0.00928) (t_loss: 0.11127) (accu: 0.9801)
[epoch : 21] (l_loss: 0.01391) (t_loss: 0.13589) (accu: 0.9775)
[epoch : 22] (l_loss: 0.01109) (t_loss: 0.11989) (accu: 0.9797)
[epoch : 23] (l_loss: 0.01151) (t_loss: 0.14700) (accu: 0.9790)
[epoch : 24] (l_loss: 0.01418) (t_loss: 0.13583) (accu: 0.9809)
[epoch : 25] (l_loss: 0.00922) (t_loss: 0.17197) (accu: 0.9754)
[epoch : 26] (l_loss: 0.01381) (t_loss: 0.14528) (accu: 0.9802)
[epoch : 27] (l_loss: 0.01040) (t_loss: 0.14152) (accu: 0.9799)
[epoch : 28] (l_loss: 0.00961) (t_loss: 0.16845) (accu: 0.9790)
[epoch : 29] (l_loss: 0.00867) (t_loss: 0.14047) (accu: 0.9798)
[epoch : 30] (l_loss: 0.01084) (t_loss: 0.16672) (accu: 0.9782)
[epoch : 31] (l_loss: 0.00923) (t_loss: 0.16946) (accu: 0.9796)
[epoch : 32] (l_loss: 0.01522) (t_loss: 0.13780) (accu: 0.9802)
[epoch : 33] (l_loss: 0.00641) (t_loss: 0.15102) (accu: 0.9805)
[epoch : 34] (l_loss: 0.00788) (t_loss: 0.12919) (accu: 0.9827)
[epoch : 35] (l_loss: 0.01134) (t_loss: 0.20998) (accu: 0.9782)
[epoch : 36] (l_loss: 0.00858) (t_loss: 0.14761) (accu: 0.9802)
[epoch : 37] (l_loss: 0.01343) (t_loss: 0.17188) (accu: 0.9791)
[epoch : 38] (l_loss: 0.00680) (t_loss: 0.17938) (accu: 0.9803)
[epoch : 39] (l_loss: 0.00891) (t_loss: 0.17995) (accu: 0.9808)
[epoch : 40] (l_loss: 0.00726) (t_loss: 0.20889) (accu: 0.9783)
Finish! (Best accu: 0.9827) (Time taken(sec) : 663.23) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (170538 | 95662)         64.06
fc1.weight   :      235200 (150528 | 84672)         64.00
fc2.weight   :       30000 (19200 | 10800)          64.00
fcout.weight :          1000 (810 | 190)            81.00
------------------------------------------------------------
Learning start! [Test_Iter : (1/3), Prune_iter : (3/19), Remaining weight : 64.06 %]
[epoch : 1] (l_loss: 0.18226) (t_loss: 0.11042) (accu: 0.9655)
[epoch : 2] (l_loss: 0.07461) (t_loss: 0.07340) (accu: 0.9769)
[epoch : 3] (l_loss: 0.05208) (t_loss: 0.07832) (accu: 0.9760)
[epoch : 4] (l_loss: 0.03988) (t_loss: 0.07836) (accu: 0.9768)
[epoch : 5] (l_loss: 0.03345) (t_loss: 0.08143) (accu: 0.9766)
[epoch : 6] (l_loss: 0.02671) (t_loss: 0.07245) (accu: 0.9800)
[epoch : 7] (l_loss: 0.02151) (t_loss: 0.09722) (accu: 0.9785)
[epoch : 8] (l_loss: 0.02219) (t_loss: 0.08183) (accu: 0.9785)
[epoch : 9] (l_loss: 0.01845) (t_loss: 0.09316) (accu: 0.9786)
[epoch : 10] (l_loss: 0.01873) (t_loss: 0.09216) (accu: 0.9766)
[epoch : 11] (l_loss: 0.01681) (t_loss: 0.09836) (accu: 0.9797)
[epoch : 12] (l_loss: 0.01493) (t_loss: 0.09426) (accu: 0.9795)
[epoch : 13] (l_loss: 0.01147) (t_loss: 0.12609) (accu: 0.9770)
[epoch : 14] (l_loss: 0.01564) (t_loss: 0.10877) (accu: 0.9798)
[epoch : 15] (l_loss: 0.01107) (t_loss: 0.11428) (accu: 0.9796)
[epoch : 16] (l_loss: 0.01515) (t_loss: 0.10425) (accu: 0.9803)
[epoch : 17] (l_loss: 0.01268) (t_loss: 0.12286) (accu: 0.9775)
[epoch : 18] (l_loss: 0.01136) (t_loss: 0.11292) (accu: 0.9799)
[epoch : 19] (l_loss: 0.01163) (t_loss: 0.13427) (accu: 0.9772)
[epoch : 20] (l_loss: 0.01112) (t_loss: 0.12092) (accu: 0.9801)
[epoch : 21] (l_loss: 0.01171) (t_loss: 0.11216) (accu: 0.9803)
[epoch : 22] (l_loss: 0.00598) (t_loss: 0.12259) (accu: 0.9800)
[epoch : 23] (l_loss: 0.00994) (t_loss: 0.14657) (accu: 0.9762)
[epoch : 24] (l_loss: 0.01161) (t_loss: 0.12993) (accu: 0.9785)
[epoch : 25] (l_loss: 0.00945) (t_loss: 0.13123) (accu: 0.9805)
[epoch : 26] (l_loss: 0.00676) (t_loss: 0.15260) (accu: 0.9792)
[epoch : 27] (l_loss: 0.01176) (t_loss: 0.13656) (accu: 0.9807)
[epoch : 28] (l_loss: 0.00862) (t_loss: 0.14380) (accu: 0.9805)
[epoch : 29] (l_loss: 0.01277) (t_loss: 0.15886) (accu: 0.9777)
[epoch : 30] (l_loss: 0.00738) (t_loss: 0.13081) (accu: 0.9808)
[epoch : 31] (l_loss: 0.00740) (t_loss: 0.15015) (accu: 0.9813)
[epoch : 32] (l_loss: 0.00737) (t_loss: 0.14480) (accu: 0.9802)
[epoch : 33] (l_loss: 0.00928) (t_loss: 0.15095) (accu: 0.9794)
[epoch : 34] (l_loss: 0.00817) (t_loss: 0.16462) (accu: 0.9811)
[epoch : 35] (l_loss: 0.00851) (t_loss: 0.15751) (accu: 0.9807)
[epoch : 36] (l_loss: 0.00580) (t_loss: 0.15604) (accu: 0.9815)
[epoch : 37] (l_loss: 0.01358) (t_loss: 0.16699) (accu: 0.9790)
[epoch : 38] (l_loss: 0.00782) (t_loss: 0.17930) (accu: 0.9795)
[epoch : 39] (l_loss: 0.00518) (t_loss: 0.15352) (accu: 0.9817)
[epoch : 40] (l_loss: 0.01050) (t_loss: 0.15836) (accu: 0.9797)
Finish! (Best accu: 0.9817) (Time taken(sec) : 676.38) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (136511 | 129689)        51.28
fc1.weight   :      235200 (120422 | 114778)        51.20
fc2.weight   :       30000 (15360 | 14640)          51.20
fcout.weight :          1000 (729 | 271)            72.90
------------------------------------------------------------
Learning start! [Test_Iter : (1/3), Prune_iter : (4/19), Remaining weight : 51.28 %]
[epoch : 1] (l_loss: 0.17708) (t_loss: 0.10029) (accu: 0.9684)
[epoch : 2] (l_loss: 0.06735) (t_loss: 0.07719) (accu: 0.9754)
[epoch : 3] (l_loss: 0.04433) (t_loss: 0.08112) (accu: 0.9755)
[epoch : 4] (l_loss: 0.03399) (t_loss: 0.08949) (accu: 0.9767)
[epoch : 5] (l_loss: 0.02763) (t_loss: 0.07737) (accu: 0.9778)
[epoch : 6] (l_loss: 0.02407) (t_loss: 0.08978) (accu: 0.9780)
[epoch : 7] (l_loss: 0.01831) (t_loss: 0.08608) (accu: 0.9772)
[epoch : 8] (l_loss: 0.01880) (t_loss: 0.09742) (accu: 0.9747)
[epoch : 9] (l_loss: 0.01714) (t_loss: 0.09081) (accu: 0.9771)
[epoch : 10] (l_loss: 0.01277) (t_loss: 0.08912) (accu: 0.9806)
[epoch : 11] (l_loss: 0.01305) (t_loss: 0.08410) (accu: 0.9817)
[epoch : 12] (l_loss: 0.01443) (t_loss: 0.10602) (accu: 0.9792)
[epoch : 13] (l_loss: 0.01505) (t_loss: 0.10164) (accu: 0.9799)
[epoch : 14] (l_loss: 0.00912) (t_loss: 0.09390) (accu: 0.9818)
[epoch : 15] (l_loss: 0.00907) (t_loss: 0.10917) (accu: 0.9812)
[epoch : 16] (l_loss: 0.01466) (t_loss: 0.13607) (accu: 0.9769)
[epoch : 17] (l_loss: 0.00859) (t_loss: 0.10409) (accu: 0.9811)
[epoch : 18] (l_loss: 0.00914) (t_loss: 0.11022) (accu: 0.9795)
[epoch : 19] (l_loss: 0.00838) (t_loss: 0.12038) (accu: 0.9815)
[epoch : 20] (l_loss: 0.00912) (t_loss: 0.11382) (accu: 0.9827)
[epoch : 21] (l_loss: 0.00879) (t_loss: 0.13585) (accu: 0.9791)
[epoch : 22] (l_loss: 0.00938) (t_loss: 0.11908) (accu: 0.9817)
[epoch : 23] (l_loss: 0.00892) (t_loss: 0.11245) (accu: 0.9813)
[epoch : 24] (l_loss: 0.00622) (t_loss: 0.12593) (accu: 0.9801)
[epoch : 25] (l_loss: 0.01024) (t_loss: 0.11790) (accu: 0.9811)
[epoch : 26] (l_loss: 0.00745) (t_loss: 0.11857) (accu: 0.9811)
[epoch : 27] (l_loss: 0.00678) (t_loss: 0.12866) (accu: 0.9801)
[epoch : 28] (l_loss: 0.00756) (t_loss: 0.14989) (accu: 0.9785)
[epoch : 29] (l_loss: 0.00759) (t_loss: 0.15849) (accu: 0.9794)
[epoch : 30] (l_loss: 0.00829) (t_loss: 0.15737) (accu: 0.9810)
[epoch : 31] (l_loss: 0.00764) (t_loss: 0.17956) (accu: 0.9770)
[epoch : 32] (l_loss: 0.00742) (t_loss: 0.14108) (accu: 0.9811)
[epoch : 33] (l_loss: 0.00557) (t_loss: 0.17403) (accu: 0.9776)
[epoch : 34] (l_loss: 0.00848) (t_loss: 0.14895) (accu: 0.9820)
[epoch : 35] (l_loss: 0.00636) (t_loss: 0.15778) (accu: 0.9806)
[epoch : 36] (l_loss: 0.00698) (t_loss: 0.15169) (accu: 0.9808)
[epoch : 37] (l_loss: 0.00918) (t_loss: 0.15561) (accu: 0.9809)
[epoch : 38] (l_loss: 0.00575) (t_loss: 0.17465) (accu: 0.9812)
[epoch : 39] (l_loss: 0.00853) (t_loss: 0.21978) (accu: 0.9781)
[epoch : 40] (l_loss: 0.00724) (t_loss: 0.17555) (accu: 0.9798)
Finish! (Best accu: 0.9827) (Time taken(sec) : 687.22) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (109282 | 156918)        41.05
fc1.weight   :      235200 (96338 | 138862)         40.96
fc2.weight   :       30000 (12288 | 17712)          40.96
fcout.weight :          1000 (656 | 344)            65.60
------------------------------------------------------------
Learning start! [Test_Iter : (1/3), Prune_iter : (5/19), Remaining weight : 41.05 %]
[epoch : 1] (l_loss: 0.16895) (t_loss: 0.08344) (accu: 0.9748)
[epoch : 2] (l_loss: 0.06018) (t_loss: 0.07474) (accu: 0.9770)
[epoch : 3] (l_loss: 0.03854) (t_loss: 0.07679) (accu: 0.9777)
[epoch : 4] (l_loss: 0.02852) (t_loss: 0.08424) (accu: 0.9777)
[epoch : 5] (l_loss: 0.02185) (t_loss: 0.08167) (accu: 0.9762)
[epoch : 6] (l_loss: 0.01924) (t_loss: 0.09709) (accu: 0.9766)
[epoch : 7] (l_loss: 0.01592) (t_loss: 0.07782) (accu: 0.9825)
[epoch : 8] (l_loss: 0.01298) (t_loss: 0.09950) (accu: 0.9785)
[epoch : 9] (l_loss: 0.01215) (t_loss: 0.09112) (accu: 0.9815)
[epoch : 10] (l_loss: 0.01302) (t_loss: 0.11559) (accu: 0.9774)
[epoch : 11] (l_loss: 0.01341) (t_loss: 0.12489) (accu: 0.9755)
[epoch : 12] (l_loss: 0.00932) (t_loss: 0.09788) (accu: 0.9807)
[epoch : 13] (l_loss: 0.00940) (t_loss: 0.11404) (accu: 0.9799)
[epoch : 14] (l_loss: 0.01087) (t_loss: 0.12463) (accu: 0.9788)
[epoch : 15] (l_loss: 0.00841) (t_loss: 0.11446) (accu: 0.9811)
[epoch : 16] (l_loss: 0.00755) (t_loss: 0.11785) (accu: 0.9800)
[epoch : 17] (l_loss: 0.00849) (t_loss: 0.10414) (accu: 0.9825)
[epoch : 18] (l_loss: 0.00942) (t_loss: 0.11632) (accu: 0.9802)
[epoch : 19] (l_loss: 0.00844) (t_loss: 0.11765) (accu: 0.9807)
[epoch : 20] (l_loss: 0.00773) (t_loss: 0.12230) (accu: 0.9813)
[epoch : 21] (l_loss: 0.00893) (t_loss: 0.13705) (accu: 0.9787)
[epoch : 22] (l_loss: 0.00563) (t_loss: 0.14603) (accu: 0.9809)
[epoch : 23] (l_loss: 0.00712) (t_loss: 0.15578) (accu: 0.9801)
[epoch : 24] (l_loss: 0.00715) (t_loss: 0.12855) (accu: 0.9820)
[epoch : 25] (l_loss: 0.00635) (t_loss: 0.15761) (accu: 0.9801)
[epoch : 26] (l_loss: 0.00659) (t_loss: 0.12598) (accu: 0.9829)
[epoch : 27] (l_loss: 0.00662) (t_loss: 0.13824) (accu: 0.9824)
[epoch : 28] (l_loss: 0.00546) (t_loss: 0.14078) (accu: 0.9828)
[epoch : 29] (l_loss: 0.00967) (t_loss: 0.13387) (accu: 0.9822)
[epoch : 30] (l_loss: 0.00467) (t_loss: 0.14143) (accu: 0.9825)
[epoch : 31] (l_loss: 0.00301) (t_loss: 0.15699) (accu: 0.9809)
[epoch : 32] (l_loss: 0.00664) (t_loss: 0.16543) (accu: 0.9811)
[epoch : 33] (l_loss: 0.00953) (t_loss: 0.15717) (accu: 0.9815)
[epoch : 34] (l_loss: 0.00392) (t_loss: 0.17583) (accu: 0.9795)
[epoch : 35] (l_loss: 0.00624) (t_loss: 0.15823) (accu: 0.9798)
[epoch : 36] (l_loss: 0.00613) (t_loss: 0.17758) (accu: 0.9795)
[epoch : 37] (l_loss: 0.00628) (t_loss: 0.17469) (accu: 0.9820)
[epoch : 38] (l_loss: 0.00296) (t_loss: 0.17000) (accu: 0.9805)
[epoch : 39] (l_loss: 0.00969) (t_loss: 0.18923) (accu: 0.9805)
[epoch : 40] (l_loss: 0.00810) (t_loss: 0.16151) (accu: 0.9818)
Finish! (Best accu: 0.9829) (Time taken(sec) : 692.47) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (87490 | 178710)         32.87
fc1.weight   :      235200 (77070 | 158130)         32.77
fc2.weight   :        30000 (9830 | 20170)          32.77
fcout.weight :          1000 (590 | 410)            59.00
------------------------------------------------------------
Learning start! [Test_Iter : (1/3), Prune_iter : (6/19), Remaining weight : 32.87 %]
[epoch : 1] (l_loss: 0.16295) (t_loss: 0.07519) (accu: 0.9770)
[epoch : 2] (l_loss: 0.05395) (t_loss: 0.06813) (accu: 0.9798)
[epoch : 3] (l_loss: 0.03289) (t_loss: 0.06292) (accu: 0.9807)
[epoch : 4] (l_loss: 0.02244) (t_loss: 0.06444) (accu: 0.9802)
[epoch : 5] (l_loss: 0.01593) (t_loss: 0.07321) (accu: 0.9810)
[epoch : 6] (l_loss: 0.01517) (t_loss: 0.07239) (accu: 0.9821)
[epoch : 7] (l_loss: 0.01171) (t_loss: 0.08109) (accu: 0.9808)
[epoch : 8] (l_loss: 0.01162) (t_loss: 0.10036) (accu: 0.9775)
[epoch : 9] (l_loss: 0.01032) (t_loss: 0.09046) (accu: 0.9807)
[epoch : 10] (l_loss: 0.00684) (t_loss: 0.08911) (accu: 0.9808)
[epoch : 11] (l_loss: 0.01079) (t_loss: 0.09178) (accu: 0.9822)
[epoch : 12] (l_loss: 0.00796) (t_loss: 0.10237) (accu: 0.9816)
[epoch : 13] (l_loss: 0.00803) (t_loss: 0.11279) (accu: 0.9798)
[epoch : 14] (l_loss: 0.00947) (t_loss: 0.11366) (accu: 0.9824)
[epoch : 15] (l_loss: 0.00561) (t_loss: 0.11131) (accu: 0.9828)
[epoch : 16] (l_loss: 0.00587) (t_loss: 0.11621) (accu: 0.9823)
[epoch : 17] (l_loss: 0.00903) (t_loss: 0.13385) (accu: 0.9811)
[epoch : 18] (l_loss: 0.00725) (t_loss: 0.11013) (accu: 0.9825)
[epoch : 19] (l_loss: 0.00705) (t_loss: 0.13081) (accu: 0.9802)
[epoch : 20] (l_loss: 0.00482) (t_loss: 0.11990) (accu: 0.9821)
[epoch : 21] (l_loss: 0.00689) (t_loss: 0.12677) (accu: 0.9823)
[epoch : 22] (l_loss: 0.00565) (t_loss: 0.13595) (accu: 0.9814)
[epoch : 23] (l_loss: 0.00609) (t_loss: 0.13460) (accu: 0.9840)
[epoch : 24] (l_loss: 0.00449) (t_loss: 0.15251) (accu: 0.9795)
[epoch : 25] (l_loss: 0.00659) (t_loss: 0.16037) (accu: 0.9795)
[epoch : 26] (l_loss: 0.00785) (t_loss: 0.14238) (accu: 0.9799)
[epoch : 27] (l_loss: 0.00512) (t_loss: 0.11567) (accu: 0.9834)
[epoch : 28] (l_loss: 0.00460) (t_loss: 0.13349) (accu: 0.9810)
[epoch : 29] (l_loss: 0.00511) (t_loss: 0.13404) (accu: 0.9812)
[epoch : 30] (l_loss: 0.00424) (t_loss: 0.15808) (accu: 0.9789)
[epoch : 31] (l_loss: 0.00757) (t_loss: 0.14264) (accu: 0.9824)
[epoch : 32] (l_loss: 0.00494) (t_loss: 0.14063) (accu: 0.9832)
[epoch : 33] (l_loss: 0.00346) (t_loss: 0.15278) (accu: 0.9818)
[epoch : 34] (l_loss: 0.00555) (t_loss: 0.15288) (accu: 0.9794)
[epoch : 35] (l_loss: 0.00650) (t_loss: 0.16070) (accu: 0.9805)
[epoch : 36] (l_loss: 0.00266) (t_loss: 0.14762) (accu: 0.9817)
[epoch : 37] (l_loss: 0.00148) (t_loss: 0.16068) (accu: 0.9806)
[epoch : 38] (l_loss: 0.00834) (t_loss: 0.17063) (accu: 0.9798)
[epoch : 39] (l_loss: 0.00550) (t_loss: 0.15149) (accu: 0.9823)
[epoch : 40] (l_loss: 0.00243) (t_loss: 0.16664) (accu: 0.9819)
Finish! (Best accu: 0.9840) (Time taken(sec) : 687.57) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (70051 | 196149)         26.32
fc1.weight   :      235200 (61656 | 173544)         26.21
fc2.weight   :        30000 (7864 | 22136)          26.21
fcout.weight :          1000 (531 | 469)            53.10
------------------------------------------------------------
Learning start! [Test_Iter : (1/3), Prune_iter : (7/19), Remaining weight : 26.32 %]
[epoch : 1] (l_loss: 0.15868) (t_loss: 0.08250) (accu: 0.9746)
[epoch : 2] (l_loss: 0.04784) (t_loss: 0.06391) (accu: 0.9803)
[epoch : 3] (l_loss: 0.02689) (t_loss: 0.06404) (accu: 0.9804)
[epoch : 4] (l_loss: 0.01897) (t_loss: 0.06830) (accu: 0.9814)
[epoch : 5] (l_loss: 0.01226) (t_loss: 0.07204) (accu: 0.9800)
[epoch : 6] (l_loss: 0.01104) (t_loss: 0.07473) (accu: 0.9815)
[epoch : 7] (l_loss: 0.00950) (t_loss: 0.07567) (accu: 0.9826)
[epoch : 8] (l_loss: 0.00786) (t_loss: 0.08085) (accu: 0.9822)
[epoch : 9] (l_loss: 0.00811) (t_loss: 0.09438) (accu: 0.9798)
[epoch : 10] (l_loss: 0.00903) (t_loss: 0.11342) (accu: 0.9794)
[epoch : 11] (l_loss: 0.00612) (t_loss: 0.10268) (accu: 0.9821)
[epoch : 12] (l_loss: 0.00650) (t_loss: 0.12123) (accu: 0.9782)
[epoch : 13] (l_loss: 0.00627) (t_loss: 0.11070) (accu: 0.9811)
[epoch : 14] (l_loss: 0.00471) (t_loss: 0.10266) (accu: 0.9828)
[epoch : 15] (l_loss: 0.00655) (t_loss: 0.09620) (accu: 0.9840)
[epoch : 16] (l_loss: 0.00695) (t_loss: 0.10982) (accu: 0.9829)
[epoch : 17] (l_loss: 0.00585) (t_loss: 0.11222) (accu: 0.9825)
[epoch : 18] (l_loss: 0.00735) (t_loss: 0.11217) (accu: 0.9821)
[epoch : 19] (l_loss: 0.00361) (t_loss: 0.10685) (accu: 0.9834)
[epoch : 20] (l_loss: 0.00438) (t_loss: 0.12479) (accu: 0.9818)
[epoch : 21] (l_loss: 0.00535) (t_loss: 0.15121) (accu: 0.9773)
[epoch : 22] (l_loss: 0.00298) (t_loss: 0.13259) (accu: 0.9811)
[epoch : 23] (l_loss: 0.00733) (t_loss: 0.12713) (accu: 0.9814)
[epoch : 24] (l_loss: 0.00483) (t_loss: 0.11793) (accu: 0.9834)
[epoch : 25] (l_loss: 0.00414) (t_loss: 0.12900) (accu: 0.9829)
[epoch : 26] (l_loss: 0.00555) (t_loss: 0.15111) (accu: 0.9791)
[epoch : 27] (l_loss: 0.00486) (t_loss: 0.13151) (accu: 0.9835)
[epoch : 28] (l_loss: 0.00396) (t_loss: 0.14647) (accu: 0.9799)
[epoch : 29] (l_loss: 0.00627) (t_loss: 0.13995) (accu: 0.9817)
[epoch : 30] (l_loss: 0.00376) (t_loss: 0.12605) (accu: 0.9838)
[epoch : 31] (l_loss: 0.00333) (t_loss: 0.14260) (accu: 0.9826)
[epoch : 32] (l_loss: 0.00342) (t_loss: 0.17168) (accu: 0.9797)
[epoch : 33] (l_loss: 0.00339) (t_loss: 0.13970) (accu: 0.9822)
[epoch : 34] (l_loss: 0.00297) (t_loss: 0.14121) (accu: 0.9827)
[epoch : 35] (l_loss: 0.00440) (t_loss: 0.14905) (accu: 0.9814)
[epoch : 36] (l_loss: 0.00388) (t_loss: 0.15845) (accu: 0.9827)
[epoch : 37] (l_loss: 0.00605) (t_loss: 0.16187) (accu: 0.9818)
[epoch : 38] (l_loss: 0.00263) (t_loss: 0.17603) (accu: 0.9802)
[epoch : 39] (l_loss: 0.00332) (t_loss: 0.15133) (accu: 0.9836)
[epoch : 40] (l_loss: 0.00351) (t_loss: 0.16709) (accu: 0.9815)
Finish! (Best accu: 0.9840) (Time taken(sec) : 699.77) 


