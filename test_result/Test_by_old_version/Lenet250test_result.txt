model_type: Lenet_250_75
lr: 0.0012
epochs: 50
batch_size: 60
weight_decay: 0.0012
prune_per_c: 1
prune_per_f: 0.2
prune_per_o: 0.1
test_iter: 5
prune_iter: 21
trainset: Dataset MNIST
    Number of datapoints: 60000
    Root location: ../MNIST_data/
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=(0.1307,), std=(0.3081,))
           )
valset: empty
testset: Dataset MNIST
    Number of datapoints: 10000
    Root location: ../MNIST_data/
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=(0.1307,), std=(0.3081,))
           )
train_loader: <torch.utils.data.dataloader.DataLoader object at 0x7fa5fb854c50>
val_loader: empty
test_loader: <torch.utils.data.dataloader.DataLoader object at 0x7fa5fd2a6ad0> 


Model structure
 Lenet_250_75(
  (fc1): Linear(in_features=784, out_features=250, bias=True)
  (fc2): Linear(in_features=250, out_features=75, bias=True)
  (fcout): Linear(in_features=75, out_features=10, bias=True)
)
===================================================================== 

Test_Iter (1/5)
------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :        215500 (215500 | 0)          100.00
fc1.weight   :        196000 (196000 | 0)          100.00
fc2.weight   :         18750 (18750 | 0)           100.00
fcout.weight :           750 (750 | 0)             100.00
------------------------------------------------------------
[Prune_iter : (1/21), Remaining weight : 100.0 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.72266) (accu: 0.0790)
[epoch : 1] (l_loss: 0.21148) (t_loss: 0.14389) (accu: 0.9524)
[epoch : 2] (l_loss: 0.10670) (t_loss: 0.10498) (accu: 0.9674)
[epoch : 3] (l_loss: 0.09213) (t_loss: 0.14336) (accu: 0.9543)
[epoch : 4] (l_loss: 0.08580) (t_loss: 0.10490) (accu: 0.9645)
[epoch : 5] (l_loss: 0.08178) (t_loss: 0.09441) (accu: 0.9689)
[epoch : 6] (l_loss: 0.07760) (t_loss: 0.08698) (accu: 0.9715)
[epoch : 7] (l_loss: 0.07407) (t_loss: 0.08032) (accu: 0.9762)
[epoch : 8] (l_loss: 0.07087) (t_loss: 0.08353) (accu: 0.9728)
[epoch : 9] (l_loss: 0.06838) (t_loss: 0.07514) (accu: 0.9764)
[epoch : 10] (l_loss: 0.06613) (t_loss: 0.07860) (accu: 0.9753)
[epoch : 11] (l_loss: 0.06610) (t_loss: 0.08748) (accu: 0.9719)
[epoch : 12] (l_loss: 0.06486) (t_loss: 0.08324) (accu: 0.9753)
[epoch : 13] (l_loss: 0.06262) (t_loss: 0.09013) (accu: 0.9718)
[epoch : 14] (l_loss: 0.06175) (t_loss: 0.06831) (accu: 0.9787)
[epoch : 15] (l_loss: 0.06221) (t_loss: 0.08558) (accu: 0.9730)
[epoch : 16] (l_loss: 0.06001) (t_loss: 0.07168) (accu: 0.9769)
[epoch : 17] (l_loss: 0.05966) (t_loss: 0.07257) (accu: 0.9773)
[epoch : 18] (l_loss: 0.06205) (t_loss: 0.08188) (accu: 0.9741)
[epoch : 19] (l_loss: 0.05933) (t_loss: 0.07012) (accu: 0.9781)
[epoch : 20] (l_loss: 0.05916) (t_loss: 0.07306) (accu: 0.9777)
[epoch : 21] (l_loss: 0.05826) (t_loss: 0.07405) (accu: 0.9767)
[epoch : 22] (l_loss: 0.05843) (t_loss: 0.09314) (accu: 0.9685)
[epoch : 23] (l_loss: 0.05966) (t_loss: 0.06772) (accu: 0.9798)
[epoch : 24] (l_loss: 0.05708) (t_loss: 0.07677) (accu: 0.9760)
[epoch : 25] (l_loss: 0.05787) (t_loss: 0.07712) (accu: 0.9755)
[epoch : 26] (l_loss: 0.05605) (t_loss: 0.09345) (accu: 0.9701)
[epoch : 27] (l_loss: 0.05826) (t_loss: 0.08307) (accu: 0.9746)
[epoch : 28] (l_loss: 0.05688) (t_loss: 0.08091) (accu: 0.9757)
[epoch : 29] (l_loss: 0.05749) (t_loss: 0.07691) (accu: 0.9756)
[epoch : 30] (l_loss: 0.05771) (t_loss: 0.07467) (accu: 0.9760)
[epoch : 31] (l_loss: 0.05716) (t_loss: 0.07639) (accu: 0.9750)
[epoch : 32] (l_loss: 0.05769) (t_loss: 0.07478) (accu: 0.9759)
[epoch : 33] (l_loss: 0.05424) (t_loss: 0.06905) (accu: 0.9793)
[epoch : 34] (l_loss: 0.05755) (t_loss: 0.06681) (accu: 0.9769)
[epoch : 35] (l_loss: 0.05634) (t_loss: 0.06992) (accu: 0.9771)
[epoch : 36] (l_loss: 0.05592) (t_loss: 0.06577) (accu: 0.9805)
[epoch : 37] (l_loss: 0.05621) (t_loss: 0.07605) (accu: 0.9767)
[epoch : 38] (l_loss: 0.05691) (t_loss: 0.07389) (accu: 0.9772)
[epoch : 39] (l_loss: 0.05482) (t_loss: 0.07040) (accu: 0.9779)
[epoch : 40] (l_loss: 0.05774) (t_loss: 0.07882) (accu: 0.9752)
[epoch : 41] (l_loss: 0.05605) (t_loss: 0.07116) (accu: 0.9767)
[epoch : 42] (l_loss: 0.05826) (t_loss: 0.07735) (accu: 0.9750)
[epoch : 43] (l_loss: 0.05496) (t_loss: 0.06983) (accu: 0.9779)
[epoch : 44] (l_loss: 0.05547) (t_loss: 0.07126) (accu: 0.9776)
[epoch : 45] (l_loss: 0.05554) (t_loss: 0.07013) (accu: 0.9787)
[epoch : 46] (l_loss: 0.05680) (t_loss: 0.08935) (accu: 0.9701)
[epoch : 47] (l_loss: 0.05510) (t_loss: 0.07393) (accu: 0.9750)
[epoch : 48] (l_loss: 0.05769) (t_loss: 0.08488) (accu: 0.9749)
[epoch : 49] (l_loss: 0.05525) (t_loss: 0.06375) (accu: 0.9805)
[epoch : 50] (l_loss: 0.05388) (t_loss: 0.07797) (accu: 0.9735)
Finish! (Best accu: 0.9805) (Time taken(sec) : 554.65) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (172475 | 43025)         80.03
fc1.weight   :      196000 (156800 | 39200)         80.00
fc2.weight   :        18750 (15000 | 3750)          80.00
fcout.weight :           750 (675 | 75)             90.00
------------------------------------------------------------
[Prune_iter : (2/21), Remaining weight : 80.03 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.53934) (accu: 0.1087)
[epoch : 1] (l_loss: 0.21079) (t_loss: 0.14649) (accu: 0.9519)
[epoch : 2] (l_loss: 0.10644) (t_loss: 0.11144) (accu: 0.9641)
[epoch : 3] (l_loss: 0.09044) (t_loss: 0.09574) (accu: 0.9718)
[epoch : 4] (l_loss: 0.08307) (t_loss: 0.09251) (accu: 0.9706)
[epoch : 5] (l_loss: 0.07765) (t_loss: 0.08859) (accu: 0.9731)
[epoch : 6] (l_loss: 0.07354) (t_loss: 0.08031) (accu: 0.9756)
[epoch : 7] (l_loss: 0.06964) (t_loss: 0.09116) (accu: 0.9715)
[epoch : 8] (l_loss: 0.06663) (t_loss: 0.08204) (accu: 0.9742)
[epoch : 9] (l_loss: 0.06532) (t_loss: 0.07120) (accu: 0.9796)
[epoch : 10] (l_loss: 0.06392) (t_loss: 0.08193) (accu: 0.9723)
[epoch : 11] (l_loss: 0.06172) (t_loss: 0.08995) (accu: 0.9726)
[epoch : 12] (l_loss: 0.06178) (t_loss: 0.07505) (accu: 0.9753)
[epoch : 13] (l_loss: 0.06015) (t_loss: 0.07340) (accu: 0.9782)
[epoch : 14] (l_loss: 0.06052) (t_loss: 0.07828) (accu: 0.9767)
[epoch : 15] (l_loss: 0.05782) (t_loss: 0.06638) (accu: 0.9792)
[epoch : 16] (l_loss: 0.05946) (t_loss: 0.07441) (accu: 0.9768)
[epoch : 17] (l_loss: 0.05746) (t_loss: 0.07319) (accu: 0.9769)
[epoch : 18] (l_loss: 0.05761) (t_loss: 0.06689) (accu: 0.9789)
[epoch : 19] (l_loss: 0.05489) (t_loss: 0.07452) (accu: 0.9766)
[epoch : 20] (l_loss: 0.05637) (t_loss: 0.08090) (accu: 0.9752)
[epoch : 21] (l_loss: 0.05573) (t_loss: 0.07452) (accu: 0.9778)
[epoch : 22] (l_loss: 0.05748) (t_loss: 0.07894) (accu: 0.9749)
[epoch : 23] (l_loss: 0.05517) (t_loss: 0.07849) (accu: 0.9763)
[epoch : 24] (l_loss: 0.05730) (t_loss: 0.09394) (accu: 0.9700)
[epoch : 25] (l_loss: 0.05490) (t_loss: 0.07753) (accu: 0.9759)
[epoch : 26] (l_loss: 0.05589) (t_loss: 0.08292) (accu: 0.9744)
[epoch : 27] (l_loss: 0.05337) (t_loss: 0.07978) (accu: 0.9749)
[epoch : 28] (l_loss: 0.05615) (t_loss: 0.07213) (accu: 0.9783)
[epoch : 29] (l_loss: 0.05457) (t_loss: 0.06900) (accu: 0.9773)
[epoch : 30] (l_loss: 0.05485) (t_loss: 0.09550) (accu: 0.9712)
[epoch : 31] (l_loss: 0.05439) (t_loss: 0.07073) (accu: 0.9774)
[epoch : 32] (l_loss: 0.05253) (t_loss: 0.08955) (accu: 0.9716)
[epoch : 33] (l_loss: 0.05478) (t_loss: 0.08983) (accu: 0.9714)
[epoch : 34] (l_loss: 0.05422) (t_loss: 0.09768) (accu: 0.9694)
[epoch : 35] (l_loss: 0.05564) (t_loss: 0.08055) (accu: 0.9739)
[epoch : 36] (l_loss: 0.05382) (t_loss: 0.08199) (accu: 0.9755)
[epoch : 37] (l_loss: 0.05284) (t_loss: 0.08011) (accu: 0.9753)
[epoch : 38] (l_loss: 0.05288) (t_loss: 0.07446) (accu: 0.9758)
[epoch : 39] (l_loss: 0.05456) (t_loss: 0.07294) (accu: 0.9770)
[epoch : 40] (l_loss: 0.05289) (t_loss: 0.07651) (accu: 0.9763)
[epoch : 41] (l_loss: 0.05458) (t_loss: 0.07850) (accu: 0.9756)
[epoch : 42] (l_loss: 0.05399) (t_loss: 0.09086) (accu: 0.9718)
[epoch : 43] (l_loss: 0.05257) (t_loss: 0.07423) (accu: 0.9767)
[epoch : 44] (l_loss: 0.05289) (t_loss: 0.08236) (accu: 0.9766)
[epoch : 45] (l_loss: 0.05166) (t_loss: 0.08123) (accu: 0.9759)
[epoch : 46] (l_loss: 0.05324) (t_loss: 0.08107) (accu: 0.9760)
[epoch : 47] (l_loss: 0.05223) (t_loss: 0.07785) (accu: 0.9755)
[epoch : 48] (l_loss: 0.05474) (t_loss: 0.07455) (accu: 0.9773)
[epoch : 49] (l_loss: 0.05262) (t_loss: 0.07963) (accu: 0.9743)
[epoch : 50] (l_loss: 0.05297) (t_loss: 0.07734) (accu: 0.9771)
Finish! (Best accu: 0.9796) (Time taken(sec) : 560.00) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (138048 | 77452)         64.06
fc1.weight   :      196000 (125440 | 70560)         64.00
fc2.weight   :        18750 (12000 | 6750)          64.00
fcout.weight :          750 (608 | 142)             81.07
------------------------------------------------------------
[Prune_iter : (3/21), Remaining weight : 64.06 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.47125) (accu: 0.0998)
[epoch : 1] (l_loss: 0.21876) (t_loss: 0.12307) (accu: 0.9634)
[epoch : 2] (l_loss: 0.10325) (t_loss: 0.09056) (accu: 0.9722)
[epoch : 3] (l_loss: 0.08955) (t_loss: 0.09265) (accu: 0.9710)
[epoch : 4] (l_loss: 0.08094) (t_loss: 0.09142) (accu: 0.9722)
[epoch : 5] (l_loss: 0.07351) (t_loss: 0.08288) (accu: 0.9749)
[epoch : 6] (l_loss: 0.06891) (t_loss: 0.09211) (accu: 0.9704)
[epoch : 7] (l_loss: 0.06756) (t_loss: 0.08849) (accu: 0.9727)
[epoch : 8] (l_loss: 0.06478) (t_loss: 0.08227) (accu: 0.9739)
[epoch : 9] (l_loss: 0.06376) (t_loss: 0.07382) (accu: 0.9769)
[epoch : 10] (l_loss: 0.06044) (t_loss: 0.08796) (accu: 0.9732)
[epoch : 11] (l_loss: 0.06030) (t_loss: 0.07710) (accu: 0.9767)
[epoch : 12] (l_loss: 0.05808) (t_loss: 0.08286) (accu: 0.9734)
[epoch : 13] (l_loss: 0.05892) (t_loss: 0.07701) (accu: 0.9776)
[epoch : 14] (l_loss: 0.05687) (t_loss: 0.07073) (accu: 0.9778)
[epoch : 15] (l_loss: 0.05468) (t_loss: 0.07710) (accu: 0.9765)
[epoch : 16] (l_loss: 0.05652) (t_loss: 0.06941) (accu: 0.9781)
[epoch : 17] (l_loss: 0.05551) (t_loss: 0.07864) (accu: 0.9750)
[epoch : 18] (l_loss: 0.05338) (t_loss: 0.08993) (accu: 0.9712)
[epoch : 19] (l_loss: 0.05392) (t_loss: 0.08273) (accu: 0.9734)
[epoch : 20] (l_loss: 0.05382) (t_loss: 0.06681) (accu: 0.9787)
[epoch : 21] (l_loss: 0.05339) (t_loss: 0.08198) (accu: 0.9752)
[epoch : 22] (l_loss: 0.05417) (t_loss: 0.07009) (accu: 0.9775)
[epoch : 23] (l_loss: 0.05120) (t_loss: 0.06671) (accu: 0.9791)
[epoch : 24] (l_loss: 0.05299) (t_loss: 0.07824) (accu: 0.9754)
[epoch : 25] (l_loss: 0.05418) (t_loss: 0.07259) (accu: 0.9767)
[epoch : 26] (l_loss: 0.05203) (t_loss: 0.07758) (accu: 0.9751)
[epoch : 27] (l_loss: 0.05055) (t_loss: 0.08995) (accu: 0.9723)
[epoch : 28] (l_loss: 0.05274) (t_loss: 0.06925) (accu: 0.9792)
[epoch : 29] (l_loss: 0.05085) (t_loss: 0.07351) (accu: 0.9777)
[epoch : 30] (l_loss: 0.05117) (t_loss: 0.07294) (accu: 0.9773)
[epoch : 31] (l_loss: 0.05121) (t_loss: 0.07556) (accu: 0.9759)
[epoch : 32] (l_loss: 0.05141) (t_loss: 0.07527) (accu: 0.9759)
[epoch : 33] (l_loss: 0.05040) (t_loss: 0.07656) (accu: 0.9765)
[epoch : 34] (l_loss: 0.05188) (t_loss: 0.07181) (accu: 0.9783)
[epoch : 35] (l_loss: 0.04989) (t_loss: 0.07873) (accu: 0.9746)
[epoch : 36] (l_loss: 0.05115) (t_loss: 0.07229) (accu: 0.9767)
[epoch : 37] (l_loss: 0.05138) (t_loss: 0.07741) (accu: 0.9756)
[epoch : 38] (l_loss: 0.05065) (t_loss: 0.06876) (accu: 0.9769)
[epoch : 39] (l_loss: 0.05020) (t_loss: 0.07359) (accu: 0.9771)
[epoch : 40] (l_loss: 0.04993) (t_loss: 0.07819) (accu: 0.9764)
[epoch : 41] (l_loss: 0.04869) (t_loss: 0.06991) (accu: 0.9788)
[epoch : 42] (l_loss: 0.04982) (t_loss: 0.06882) (accu: 0.9797)
[epoch : 43] (l_loss: 0.04948) (t_loss: 0.07178) (accu: 0.9778)
[epoch : 44] (l_loss: 0.05008) (t_loss: 0.07053) (accu: 0.9785)
[epoch : 45] (l_loss: 0.05027) (t_loss: 0.07405) (accu: 0.9763)
[epoch : 46] (l_loss: 0.04928) (t_loss: 0.06972) (accu: 0.9779)
[epoch : 47] (l_loss: 0.05023) (t_loss: 0.07377) (accu: 0.9775)
[epoch : 48] (l_loss: 0.04923) (t_loss: 0.07987) (accu: 0.9757)
[epoch : 49] (l_loss: 0.04948) (t_loss: 0.07458) (accu: 0.9763)
[epoch : 50] (l_loss: 0.04843) (t_loss: 0.08507) (accu: 0.9723)
Finish! (Best accu: 0.9797) (Time taken(sec) : 560.07) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (110499 | 105001)        51.28
fc1.weight   :      196000 (100352 | 95648)         51.20
fc2.weight   :        18750 (9600 | 9150)           51.20
fcout.weight :          750 (547 | 203)             72.93
------------------------------------------------------------
[Prune_iter : (4/21), Remaining weight : 51.28 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.39143) (accu: 0.0904)
[epoch : 1] (l_loss: 0.21734) (t_loss: 0.11819) (accu: 0.9647)
[epoch : 2] (l_loss: 0.10334) (t_loss: 0.10830) (accu: 0.9652)
[epoch : 3] (l_loss: 0.08749) (t_loss: 0.08575) (accu: 0.9737)
[epoch : 4] (l_loss: 0.07809) (t_loss: 0.08448) (accu: 0.9743)
[epoch : 5] (l_loss: 0.07234) (t_loss: 0.11727) (accu: 0.9643)
[epoch : 6] (l_loss: 0.07049) (t_loss: 0.08717) (accu: 0.9735)
[epoch : 7] (l_loss: 0.06563) (t_loss: 0.08631) (accu: 0.9740)
[epoch : 8] (l_loss: 0.06351) (t_loss: 0.07654) (accu: 0.9762)
[epoch : 9] (l_loss: 0.06246) (t_loss: 0.08329) (accu: 0.9743)
[epoch : 10] (l_loss: 0.06112) (t_loss: 0.07325) (accu: 0.9772)
[epoch : 11] (l_loss: 0.05724) (t_loss: 0.08328) (accu: 0.9729)
[epoch : 12] (l_loss: 0.05936) (t_loss: 0.07761) (accu: 0.9754)
[epoch : 13] (l_loss: 0.05852) (t_loss: 0.07940) (accu: 0.9747)
[epoch : 14] (l_loss: 0.05761) (t_loss: 0.06574) (accu: 0.9796)
[epoch : 15] (l_loss: 0.05462) (t_loss: 0.07550) (accu: 0.9764)
[epoch : 16] (l_loss: 0.05505) (t_loss: 0.07548) (accu: 0.9759)
[epoch : 17] (l_loss: 0.05376) (t_loss: 0.07639) (accu: 0.9752)
[epoch : 18] (l_loss: 0.05407) (t_loss: 0.07412) (accu: 0.9771)
[epoch : 19] (l_loss: 0.05294) (t_loss: 0.06996) (accu: 0.9794)
[epoch : 20] (l_loss: 0.05302) (t_loss: 0.08112) (accu: 0.9754)
[epoch : 21] (l_loss: 0.05222) (t_loss: 0.06720) (accu: 0.9804)
[epoch : 22] (l_loss: 0.05212) (t_loss: 0.07325) (accu: 0.9763)
[epoch : 23] (l_loss: 0.05196) (t_loss: 0.08333) (accu: 0.9738)
[epoch : 24] (l_loss: 0.05146) (t_loss: 0.09668) (accu: 0.9687)
[epoch : 25] (l_loss: 0.05181) (t_loss: 0.06680) (accu: 0.9799)
[epoch : 26] (l_loss: 0.05148) (t_loss: 0.07341) (accu: 0.9793)
[epoch : 27] (l_loss: 0.05083) (t_loss: 0.07355) (accu: 0.9774)
[epoch : 28] (l_loss: 0.05126) (t_loss: 0.06833) (accu: 0.9788)
[epoch : 29] (l_loss: 0.05066) (t_loss: 0.07234) (accu: 0.9775)
[epoch : 30] (l_loss: 0.05014) (t_loss: 0.08565) (accu: 0.9733)
[epoch : 31] (l_loss: 0.05076) (t_loss: 0.07471) (accu: 0.9786)
[epoch : 32] (l_loss: 0.05046) (t_loss: 0.06977) (accu: 0.9784)
[epoch : 33] (l_loss: 0.05117) (t_loss: 0.07318) (accu: 0.9777)
[epoch : 34] (l_loss: 0.04926) (t_loss: 0.08045) (accu: 0.9742)
[epoch : 35] (l_loss: 0.04965) (t_loss: 0.07024) (accu: 0.9784)
[epoch : 36] (l_loss: 0.05053) (t_loss: 0.07248) (accu: 0.9755)
[epoch : 37] (l_loss: 0.05110) (t_loss: 0.07454) (accu: 0.9758)
[epoch : 38] (l_loss: 0.04819) (t_loss: 0.06769) (accu: 0.9774)
[epoch : 39] (l_loss: 0.05056) (t_loss: 0.06838) (accu: 0.9781)
[epoch : 40] (l_loss: 0.04991) (t_loss: 0.07497) (accu: 0.9765)
[epoch : 41] (l_loss: 0.04869) (t_loss: 0.06457) (accu: 0.9781)
[epoch : 42] (l_loss: 0.04854) (t_loss: 0.06578) (accu: 0.9797)
[epoch : 43] (l_loss: 0.05035) (t_loss: 0.06752) (accu: 0.9794)
[epoch : 44] (l_loss: 0.04939) (t_loss: 0.06635) (accu: 0.9793)
[epoch : 45] (l_loss: 0.04810) (t_loss: 0.06513) (accu: 0.9793)
[epoch : 46] (l_loss: 0.05055) (t_loss: 0.08067) (accu: 0.9756)
[epoch : 47] (l_loss: 0.05013) (t_loss: 0.07177) (accu: 0.9775)
[epoch : 48] (l_loss: 0.04909) (t_loss: 0.07538) (accu: 0.9759)
[epoch : 49] (l_loss: 0.04835) (t_loss: 0.06718) (accu: 0.9783)
[epoch : 50] (l_loss: 0.04955) (t_loss: 0.07849) (accu: 0.9732)
Finish! (Best accu: 0.9804) (Time taken(sec) : 567.82) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (88454 | 127046)         41.05
fc1.weight   :      196000 (80282 | 115718)         40.96
fc2.weight   :        18750 (7680 | 11070)          40.96
fcout.weight :          750 (492 | 258)             65.60
------------------------------------------------------------
[Prune_iter : (5/21), Remaining weight : 41.05 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.34813) (accu: 0.0961)
[epoch : 1] (l_loss: 0.22087) (t_loss: 0.13521) (accu: 0.9560)
[epoch : 2] (l_loss: 0.10444) (t_loss: 0.09736) (accu: 0.9706)
[epoch : 3] (l_loss: 0.08651) (t_loss: 0.09525) (accu: 0.9711)
[epoch : 4] (l_loss: 0.07825) (t_loss: 0.08709) (accu: 0.9728)
[epoch : 5] (l_loss: 0.07349) (t_loss: 0.08270) (accu: 0.9750)
[epoch : 6] (l_loss: 0.06823) (t_loss: 0.07880) (accu: 0.9763)
[epoch : 7] (l_loss: 0.06569) (t_loss: 0.07832) (accu: 0.9765)
[epoch : 8] (l_loss: 0.06145) (t_loss: 0.07784) (accu: 0.9772)
[epoch : 9] (l_loss: 0.06007) (t_loss: 0.07593) (accu: 0.9768)
[epoch : 10] (l_loss: 0.06001) (t_loss: 0.07947) (accu: 0.9754)
[epoch : 11] (l_loss: 0.05699) (t_loss: 0.07600) (accu: 0.9765)
[epoch : 12] (l_loss: 0.05702) (t_loss: 0.08352) (accu: 0.9749)
[epoch : 13] (l_loss: 0.05499) (t_loss: 0.07578) (accu: 0.9769)
[epoch : 14] (l_loss: 0.05487) (t_loss: 0.07765) (accu: 0.9753)
[epoch : 15] (l_loss: 0.05420) (t_loss: 0.07682) (accu: 0.9759)
[epoch : 16] (l_loss: 0.05490) (t_loss: 0.07514) (accu: 0.9773)
[epoch : 17] (l_loss: 0.05386) (t_loss: 0.07747) (accu: 0.9755)
[epoch : 18] (l_loss: 0.05352) (t_loss: 0.07196) (accu: 0.9771)
[epoch : 19] (l_loss: 0.05309) (t_loss: 0.06908) (accu: 0.9789)
[epoch : 20] (l_loss: 0.05219) (t_loss: 0.06248) (accu: 0.9805)
[epoch : 21] (l_loss: 0.05177) (t_loss: 0.07493) (accu: 0.9781)
[epoch : 22] (l_loss: 0.05198) (t_loss: 0.07492) (accu: 0.9771)
[epoch : 23] (l_loss: 0.05027) (t_loss: 0.06477) (accu: 0.9800)
[epoch : 24] (l_loss: 0.05214) (t_loss: 0.06938) (accu: 0.9794)
[epoch : 25] (l_loss: 0.04998) (t_loss: 0.06754) (accu: 0.9792)
[epoch : 26] (l_loss: 0.05034) (t_loss: 0.08260) (accu: 0.9738)
[epoch : 27] (l_loss: 0.05109) (t_loss: 0.06387) (accu: 0.9790)
[epoch : 28] (l_loss: 0.04990) (t_loss: 0.06755) (accu: 0.9812)
[epoch : 29] (l_loss: 0.04954) (t_loss: 0.06701) (accu: 0.9799)
[epoch : 30] (l_loss: 0.04908) (t_loss: 0.07302) (accu: 0.9781)
[epoch : 31] (l_loss: 0.05003) (t_loss: 0.07099) (accu: 0.9778)
[epoch : 32] (l_loss: 0.04916) (t_loss: 0.06979) (accu: 0.9801)
[epoch : 33] (l_loss: 0.04941) (t_loss: 0.07564) (accu: 0.9772)
[epoch : 34] (l_loss: 0.04960) (t_loss: 0.06880) (accu: 0.9786)
[epoch : 35] (l_loss: 0.04978) (t_loss: 0.07405) (accu: 0.9780)
[epoch : 36] (l_loss: 0.04935) (t_loss: 0.07407) (accu: 0.9772)
[epoch : 37] (l_loss: 0.05076) (t_loss: 0.08293) (accu: 0.9732)
[epoch : 38] (l_loss: 0.04889) (t_loss: 0.06948) (accu: 0.9777)
[epoch : 39] (l_loss: 0.04911) (t_loss: 0.07625) (accu: 0.9766)
[epoch : 40] (l_loss: 0.04942) (t_loss: 0.06790) (accu: 0.9788)
[epoch : 41] (l_loss: 0.04632) (t_loss: 0.06835) (accu: 0.9780)
[epoch : 42] (l_loss: 0.05073) (t_loss: 0.06537) (accu: 0.9792)
[epoch : 43] (l_loss: 0.04980) (t_loss: 0.07204) (accu: 0.9767)
[epoch : 44] (l_loss: 0.04785) (t_loss: 0.07457) (accu: 0.9771)
[epoch : 45] (l_loss: 0.04915) (t_loss: 0.07649) (accu: 0.9773)
[epoch : 46] (l_loss: 0.04722) (t_loss: 0.07500) (accu: 0.9769)
[epoch : 47] (l_loss: 0.04921) (t_loss: 0.07628) (accu: 0.9772)
[epoch : 48] (l_loss: 0.04854) (t_loss: 0.06781) (accu: 0.9789)
[epoch : 49] (l_loss: 0.04663) (t_loss: 0.07227) (accu: 0.9790)
[epoch : 50] (l_loss: 0.04846) (t_loss: 0.07453) (accu: 0.9763)
Finish! (Best accu: 0.9812) (Time taken(sec) : 573.17) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (70812 | 144688)         32.86
fc1.weight   :      196000 (64225 | 131775)         32.77
fc2.weight   :        18750 (6144 | 12606)          32.77
fcout.weight :          750 (443 | 307)             59.07
------------------------------------------------------------
[Prune_iter : (6/21), Remaining weight : 32.86 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.36749) (accu: 0.1141)
[epoch : 1] (l_loss: 0.22743) (t_loss: 0.11385) (accu: 0.9666)
[epoch : 2] (l_loss: 0.10756) (t_loss: 0.09875) (accu: 0.9707)
[epoch : 3] (l_loss: 0.08713) (t_loss: 0.08707) (accu: 0.9731)
[epoch : 4] (l_loss: 0.07752) (t_loss: 0.08144) (accu: 0.9756)
[epoch : 5] (l_loss: 0.07098) (t_loss: 0.08285) (accu: 0.9756)
[epoch : 6] (l_loss: 0.06898) (t_loss: 0.08538) (accu: 0.9718)
[epoch : 7] (l_loss: 0.06371) (t_loss: 0.08834) (accu: 0.9710)
[epoch : 8] (l_loss: 0.06234) (t_loss: 0.07457) (accu: 0.9761)
[epoch : 9] (l_loss: 0.06085) (t_loss: 0.07761) (accu: 0.9753)
[epoch : 10] (l_loss: 0.05862) (t_loss: 0.10193) (accu: 0.9677)
[epoch : 11] (l_loss: 0.05711) (t_loss: 0.08650) (accu: 0.9733)
[epoch : 12] (l_loss: 0.05667) (t_loss: 0.08324) (accu: 0.9750)
[epoch : 13] (l_loss: 0.05505) (t_loss: 0.07710) (accu: 0.9765)
[epoch : 14] (l_loss: 0.05332) (t_loss: 0.08078) (accu: 0.9746)
[epoch : 15] (l_loss: 0.05362) (t_loss: 0.08294) (accu: 0.9741)
[epoch : 16] (l_loss: 0.05490) (t_loss: 0.06722) (accu: 0.9794)
[epoch : 17] (l_loss: 0.05375) (t_loss: 0.07997) (accu: 0.9748)
[epoch : 18] (l_loss: 0.05240) (t_loss: 0.07496) (accu: 0.9761)
[epoch : 19] (l_loss: 0.05216) (t_loss: 0.07441) (accu: 0.9772)
[epoch : 20] (l_loss: 0.05135) (t_loss: 0.06919) (accu: 0.9785)
[epoch : 21] (l_loss: 0.05223) (t_loss: 0.07472) (accu: 0.9771)
[epoch : 22] (l_loss: 0.05120) (t_loss: 0.07647) (accu: 0.9772)
[epoch : 23] (l_loss: 0.05191) (t_loss: 0.07627) (accu: 0.9771)
[epoch : 24] (l_loss: 0.05088) (t_loss: 0.06962) (accu: 0.9778)
[epoch : 25] (l_loss: 0.05073) (t_loss: 0.07460) (accu: 0.9779)
[epoch : 26] (l_loss: 0.05157) (t_loss: 0.07155) (accu: 0.9773)
[epoch : 27] (l_loss: 0.05135) (t_loss: 0.07631) (accu: 0.9752)
[epoch : 28] (l_loss: 0.05164) (t_loss: 0.08368) (accu: 0.9750)
[epoch : 29] (l_loss: 0.05084) (t_loss: 0.07537) (accu: 0.9779)
[epoch : 30] (l_loss: 0.05070) (t_loss: 0.07045) (accu: 0.9782)
[epoch : 31] (l_loss: 0.05017) (t_loss: 0.08670) (accu: 0.9745)
[epoch : 32] (l_loss: 0.04979) (t_loss: 0.07260) (accu: 0.9785)
[epoch : 33] (l_loss: 0.05140) (t_loss: 0.07098) (accu: 0.9779)
[epoch : 34] (l_loss: 0.05018) (t_loss: 0.07486) (accu: 0.9767)
[epoch : 35] (l_loss: 0.04968) (t_loss: 0.07069) (accu: 0.9788)
[epoch : 36] (l_loss: 0.04992) (t_loss: 0.07464) (accu: 0.9763)
[epoch : 37] (l_loss: 0.05031) (t_loss: 0.07517) (accu: 0.9773)
[epoch : 38] (l_loss: 0.05065) (t_loss: 0.07934) (accu: 0.9752)
[epoch : 39] (l_loss: 0.04878) (t_loss: 0.07129) (accu: 0.9775)
[epoch : 40] (l_loss: 0.04971) (t_loss: 0.07846) (accu: 0.9763)
[epoch : 41] (l_loss: 0.04734) (t_loss: 0.07367) (accu: 0.9781)
[epoch : 42] (l_loss: 0.05030) (t_loss: 0.07238) (accu: 0.9787)
[epoch : 43] (l_loss: 0.04882) (t_loss: 0.07563) (accu: 0.9772)
[epoch : 44] (l_loss: 0.04936) (t_loss: 0.07446) (accu: 0.9771)
[epoch : 45] (l_loss: 0.04891) (t_loss: 0.06989) (accu: 0.9788)
[epoch : 46] (l_loss: 0.04957) (t_loss: 0.07494) (accu: 0.9751)
[epoch : 47] (l_loss: 0.04926) (t_loss: 0.07977) (accu: 0.9751)
[epoch : 48] (l_loss: 0.04908) (t_loss: 0.07253) (accu: 0.9792)
[epoch : 49] (l_loss: 0.04985) (t_loss: 0.07669) (accu: 0.9768)
[epoch : 50] (l_loss: 0.04926) (t_loss: 0.06615) (accu: 0.9795)
Finish! (Best accu: 0.9795) (Time taken(sec) : 600.19) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (56694 | 158806)         26.31
fc1.weight   :      196000 (51380 | 144620)         26.21
fc2.weight   :        18750 (4915 | 13835)          26.21
fcout.weight :          750 (399 | 351)             53.20
------------------------------------------------------------
[Prune_iter : (7/21), Remaining weight : 26.31 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.36339) (accu: 0.0916)
[epoch : 1] (l_loss: 0.23422) (t_loss: 0.13603) (accu: 0.9571)
[epoch : 2] (l_loss: 0.10650) (t_loss: 0.10581) (accu: 0.9674)
[epoch : 3] (l_loss: 0.08634) (t_loss: 0.10986) (accu: 0.9675)
[epoch : 4] (l_loss: 0.07675) (t_loss: 0.09034) (accu: 0.9723)
[epoch : 5] (l_loss: 0.06981) (t_loss: 0.09140) (accu: 0.9713)
[epoch : 6] (l_loss: 0.06604) (t_loss: 0.08499) (accu: 0.9724)
[epoch : 7] (l_loss: 0.06332) (t_loss: 0.08099) (accu: 0.9724)
[epoch : 8] (l_loss: 0.06172) (t_loss: 0.08047) (accu: 0.9747)
[epoch : 9] (l_loss: 0.05812) (t_loss: 0.07941) (accu: 0.9745)
[epoch : 10] (l_loss: 0.05733) (t_loss: 0.07830) (accu: 0.9745)
[epoch : 11] (l_loss: 0.05668) (t_loss: 0.07637) (accu: 0.9766)
[epoch : 12] (l_loss: 0.05489) (t_loss: 0.07662) (accu: 0.9749)
[epoch : 13] (l_loss: 0.05472) (t_loss: 0.07601) (accu: 0.9769)
[epoch : 14] (l_loss: 0.05386) (t_loss: 0.07485) (accu: 0.9770)
[epoch : 15] (l_loss: 0.05299) (t_loss: 0.07137) (accu: 0.9773)
[epoch : 16] (l_loss: 0.05237) (t_loss: 0.08130) (accu: 0.9739)
[epoch : 17] (l_loss: 0.05134) (t_loss: 0.07636) (accu: 0.9759)
[epoch : 18] (l_loss: 0.05132) (t_loss: 0.07172) (accu: 0.9787)
[epoch : 19] (l_loss: 0.05010) (t_loss: 0.07868) (accu: 0.9766)
[epoch : 20] (l_loss: 0.04981) (t_loss: 0.07501) (accu: 0.9767)
[epoch : 21] (l_loss: 0.05148) (t_loss: 0.07045) (accu: 0.9774)
[epoch : 22] (l_loss: 0.05170) (t_loss: 0.06691) (accu: 0.9796)
[epoch : 23] (l_loss: 0.04978) (t_loss: 0.07293) (accu: 0.9766)
[epoch : 24] (l_loss: 0.05104) (t_loss: 0.06834) (accu: 0.9785)
[epoch : 25] (l_loss: 0.04953) (t_loss: 0.08050) (accu: 0.9728)
[epoch : 26] (l_loss: 0.05074) (t_loss: 0.07584) (accu: 0.9756)
[epoch : 27] (l_loss: 0.05016) (t_loss: 0.07075) (accu: 0.9778)
[epoch : 28] (l_loss: 0.04997) (t_loss: 0.07254) (accu: 0.9768)
[epoch : 29] (l_loss: 0.04863) (t_loss: 0.07727) (accu: 0.9764)
[epoch : 30] (l_loss: 0.05032) (t_loss: 0.07152) (accu: 0.9773)
[epoch : 31] (l_loss: 0.04989) (t_loss: 0.07765) (accu: 0.9749)
[epoch : 32] (l_loss: 0.04812) (t_loss: 0.08090) (accu: 0.9740)
[epoch : 33] (l_loss: 0.04988) (t_loss: 0.07391) (accu: 0.9764)
[epoch : 34] (l_loss: 0.04874) (t_loss: 0.07067) (accu: 0.9779)
[epoch : 35] (l_loss: 0.04911) (t_loss: 0.07603) (accu: 0.9755)
[epoch : 36] (l_loss: 0.04842) (t_loss: 0.08098) (accu: 0.9756)
[epoch : 37] (l_loss: 0.04920) (t_loss: 0.08391) (accu: 0.9743)
[epoch : 38] (l_loss: 0.04829) (t_loss: 0.07706) (accu: 0.9751)
[epoch : 39] (l_loss: 0.04909) (t_loss: 0.07353) (accu: 0.9782)
[epoch : 40] (l_loss: 0.04772) (t_loss: 0.06903) (accu: 0.9783)
[epoch : 41] (l_loss: 0.04927) (t_loss: 0.07623) (accu: 0.9760)
[epoch : 42] (l_loss: 0.04783) (t_loss: 0.07525) (accu: 0.9761)
[epoch : 43] (l_loss: 0.04764) (t_loss: 0.07286) (accu: 0.9761)
[epoch : 44] (l_loss: 0.04839) (t_loss: 0.07410) (accu: 0.9771)
[epoch : 45] (l_loss: 0.04869) (t_loss: 0.06961) (accu: 0.9783)
[epoch : 46] (l_loss: 0.04662) (t_loss: 0.07335) (accu: 0.9760)
[epoch : 47] (l_loss: 0.04886) (t_loss: 0.06535) (accu: 0.9792)
[epoch : 48] (l_loss: 0.04762) (t_loss: 0.07595) (accu: 0.9755)
[epoch : 49] (l_loss: 0.04816) (t_loss: 0.06809) (accu: 0.9786)
[epoch : 50] (l_loss: 0.04845) (t_loss: 0.08203) (accu: 0.9746)
Finish! (Best accu: 0.9796) (Time taken(sec) : 600.83) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (45395 | 170105)         21.06
fc1.weight   :      196000 (41104 | 154896)         20.97
fc2.weight   :        18750 (3932 | 14818)          20.97
fcout.weight :          750 (359 | 391)             47.87
------------------------------------------------------------
[Prune_iter : (8/21), Remaining weight : 21.06 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.34797) (accu: 0.0865)
[epoch : 1] (l_loss: 0.24594) (t_loss: 0.13037) (accu: 0.9596)
[epoch : 2] (l_loss: 0.11057) (t_loss: 0.11328) (accu: 0.9643)
[epoch : 3] (l_loss: 0.08861) (t_loss: 0.09859) (accu: 0.9698)
[epoch : 4] (l_loss: 0.07909) (t_loss: 0.08835) (accu: 0.9724)
[epoch : 5] (l_loss: 0.07150) (t_loss: 0.07485) (accu: 0.9768)
[epoch : 6] (l_loss: 0.06756) (t_loss: 0.08732) (accu: 0.9721)
[epoch : 7] (l_loss: 0.06448) (t_loss: 0.08607) (accu: 0.9738)
[epoch : 8] (l_loss: 0.06207) (t_loss: 0.09314) (accu: 0.9721)
[epoch : 9] (l_loss: 0.06155) (t_loss: 0.07546) (accu: 0.9764)
[epoch : 10] (l_loss: 0.05889) (t_loss: 0.08048) (accu: 0.9757)
[epoch : 11] (l_loss: 0.05650) (t_loss: 0.08110) (accu: 0.9747)
[epoch : 12] (l_loss: 0.05558) (t_loss: 0.07247) (accu: 0.9766)
[epoch : 13] (l_loss: 0.05537) (t_loss: 0.09107) (accu: 0.9712)
[epoch : 14] (l_loss: 0.05472) (t_loss: 0.07766) (accu: 0.9748)
[epoch : 15] (l_loss: 0.05291) (t_loss: 0.07726) (accu: 0.9757)
[epoch : 16] (l_loss: 0.05319) (t_loss: 0.07125) (accu: 0.9769)
[epoch : 17] (l_loss: 0.05317) (t_loss: 0.08694) (accu: 0.9742)
[epoch : 18] (l_loss: 0.05242) (t_loss: 0.08271) (accu: 0.9734)
[epoch : 19] (l_loss: 0.05089) (t_loss: 0.07120) (accu: 0.9777)
[epoch : 20] (l_loss: 0.05091) (t_loss: 0.07803) (accu: 0.9751)
[epoch : 21] (l_loss: 0.05111) (t_loss: 0.08204) (accu: 0.9738)
[epoch : 22] (l_loss: 0.05058) (t_loss: 0.07196) (accu: 0.9779)
[epoch : 23] (l_loss: 0.05109) (t_loss: 0.07165) (accu: 0.9770)
[epoch : 24] (l_loss: 0.05088) (t_loss: 0.06869) (accu: 0.9777)
[epoch : 25] (l_loss: 0.04916) (t_loss: 0.08287) (accu: 0.9717)
[epoch : 26] (l_loss: 0.05206) (t_loss: 0.07972) (accu: 0.9735)
[epoch : 27] (l_loss: 0.04801) (t_loss: 0.07530) (accu: 0.9762)
[epoch : 28] (l_loss: 0.05007) (t_loss: 0.06990) (accu: 0.9777)
[epoch : 29] (l_loss: 0.04981) (t_loss: 0.07420) (accu: 0.9753)
[epoch : 30] (l_loss: 0.04989) (t_loss: 0.07023) (accu: 0.9788)
[epoch : 31] (l_loss: 0.04986) (t_loss: 0.09794) (accu: 0.9702)
[epoch : 32] (l_loss: 0.04864) (t_loss: 0.07763) (accu: 0.9774)
[epoch : 33] (l_loss: 0.04999) (t_loss: 0.07641) (accu: 0.9755)
[epoch : 34] (l_loss: 0.04870) (t_loss: 0.07035) (accu: 0.9773)
[epoch : 35] (l_loss: 0.04971) (t_loss: 0.07237) (accu: 0.9768)
[epoch : 36] (l_loss: 0.04862) (t_loss: 0.07781) (accu: 0.9761)
[epoch : 37] (l_loss: 0.04867) (t_loss: 0.07265) (accu: 0.9763)
[epoch : 38] (l_loss: 0.04781) (t_loss: 0.07191) (accu: 0.9778)
[epoch : 39] (l_loss: 0.04852) (t_loss: 0.07740) (accu: 0.9759)
[epoch : 40] (l_loss: 0.04840) (t_loss: 0.06528) (accu: 0.9777)
[epoch : 41] (l_loss: 0.04894) (t_loss: 0.07554) (accu: 0.9767)
[epoch : 42] (l_loss: 0.04838) (t_loss: 0.06899) (accu: 0.9783)
[epoch : 43] (l_loss: 0.04887) (t_loss: 0.08239) (accu: 0.9747)
[epoch : 44] (l_loss: 0.04782) (t_loss: 0.07053) (accu: 0.9774)
[epoch : 45] (l_loss: 0.04802) (t_loss: 0.06896) (accu: 0.9784)
[epoch : 46] (l_loss: 0.04909) (t_loss: 0.07525) (accu: 0.9760)
[epoch : 47] (l_loss: 0.04868) (t_loss: 0.07304) (accu: 0.9770)
[epoch : 48] (l_loss: 0.04795) (t_loss: 0.07175) (accu: 0.9769)
[epoch : 49] (l_loss: 0.04817) (t_loss: 0.07591) (accu: 0.9775)
[epoch : 50] (l_loss: 0.04828) (t_loss: 0.07690) (accu: 0.9761)
Finish! (Best accu: 0.9788) (Time taken(sec) : 624.34) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (36352 | 179148)         16.87
fc1.weight   :      196000 (32883 | 163117)         16.78
fc2.weight   :        18750 (3146 | 15604)          16.78
fcout.weight :          750 (323 | 427)             43.07
------------------------------------------------------------
[Prune_iter : (9/21), Remaining weight : 16.87 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.34051) (accu: 0.0794)
[epoch : 1] (l_loss: 0.25337) (t_loss: 0.14341) (accu: 0.9576)
[epoch : 2] (l_loss: 0.11077) (t_loss: 0.09682) (accu: 0.9705)
[epoch : 3] (l_loss: 0.09041) (t_loss: 0.10320) (accu: 0.9669)
[epoch : 4] (l_loss: 0.07870) (t_loss: 0.08553) (accu: 0.9730)
[epoch : 5] (l_loss: 0.07053) (t_loss: 0.08256) (accu: 0.9724)
[epoch : 6] (l_loss: 0.06704) (t_loss: 0.08651) (accu: 0.9719)
[epoch : 7] (l_loss: 0.06210) (t_loss: 0.09258) (accu: 0.9715)
[epoch : 8] (l_loss: 0.06158) (t_loss: 0.07287) (accu: 0.9764)
[epoch : 9] (l_loss: 0.05860) (t_loss: 0.08645) (accu: 0.9733)
[epoch : 10] (l_loss: 0.05896) (t_loss: 0.07449) (accu: 0.9781)
[epoch : 11] (l_loss: 0.05547) (t_loss: 0.08168) (accu: 0.9747)
[epoch : 12] (l_loss: 0.05629) (t_loss: 0.07538) (accu: 0.9756)
[epoch : 13] (l_loss: 0.05398) (t_loss: 0.07956) (accu: 0.9764)
[epoch : 14] (l_loss: 0.05411) (t_loss: 0.07618) (accu: 0.9771)
[epoch : 15] (l_loss: 0.05428) (t_loss: 0.08081) (accu: 0.9763)
[epoch : 16] (l_loss: 0.05311) (t_loss: 0.08472) (accu: 0.9738)
[epoch : 17] (l_loss: 0.05215) (t_loss: 0.07162) (accu: 0.9783)
[epoch : 18] (l_loss: 0.05169) (t_loss: 0.07278) (accu: 0.9772)
[epoch : 19] (l_loss: 0.05213) (t_loss: 0.07851) (accu: 0.9760)
[epoch : 20] (l_loss: 0.05121) (t_loss: 0.07020) (accu: 0.9774)
[epoch : 21] (l_loss: 0.05012) (t_loss: 0.07852) (accu: 0.9750)
[epoch : 22] (l_loss: 0.05237) (t_loss: 0.06901) (accu: 0.9802)
[epoch : 23] (l_loss: 0.05074) (t_loss: 0.07483) (accu: 0.9770)
[epoch : 24] (l_loss: 0.04994) (t_loss: 0.07196) (accu: 0.9784)
[epoch : 25] (l_loss: 0.04940) (t_loss: 0.07207) (accu: 0.9769)
[epoch : 26] (l_loss: 0.04941) (t_loss: 0.07421) (accu: 0.9755)
[epoch : 27] (l_loss: 0.04877) (t_loss: 0.08604) (accu: 0.9731)
[epoch : 28] (l_loss: 0.04808) (t_loss: 0.06935) (accu: 0.9781)
[epoch : 29] (l_loss: 0.05013) (t_loss: 0.08110) (accu: 0.9750)
[epoch : 30] (l_loss: 0.04797) (t_loss: 0.07014) (accu: 0.9764)
[epoch : 31] (l_loss: 0.04780) (t_loss: 0.07098) (accu: 0.9778)
[epoch : 32] (l_loss: 0.04954) (t_loss: 0.06624) (accu: 0.9794)
[epoch : 33] (l_loss: 0.04895) (t_loss: 0.07332) (accu: 0.9783)
[epoch : 34] (l_loss: 0.04837) (t_loss: 0.06766) (accu: 0.9781)
[epoch : 35] (l_loss: 0.04827) (t_loss: 0.07050) (accu: 0.9788)
[epoch : 36] (l_loss: 0.04827) (t_loss: 0.07683) (accu: 0.9766)
[epoch : 37] (l_loss: 0.04860) (t_loss: 0.08161) (accu: 0.9743)
[epoch : 38] (l_loss: 0.04852) (t_loss: 0.07181) (accu: 0.9771)
[epoch : 39] (l_loss: 0.04874) (t_loss: 0.07641) (accu: 0.9772)
[epoch : 40] (l_loss: 0.04834) (t_loss: 0.07107) (accu: 0.9795)
[epoch : 41] (l_loss: 0.04785) (t_loss: 0.06664) (accu: 0.9797)
[epoch : 42] (l_loss: 0.04752) (t_loss: 0.07129) (accu: 0.9782)
[epoch : 43] (l_loss: 0.04807) (t_loss: 0.06906) (accu: 0.9770)
[epoch : 44] (l_loss: 0.04676) (t_loss: 0.07136) (accu: 0.9781)
[epoch : 45] (l_loss: 0.04934) (t_loss: 0.08089) (accu: 0.9758)
[epoch : 46] (l_loss: 0.04667) (t_loss: 0.06955) (accu: 0.9787)
[epoch : 47] (l_loss: 0.04687) (t_loss: 0.07776) (accu: 0.9743)
[epoch : 48] (l_loss: 0.04813) (t_loss: 0.06462) (accu: 0.9804)
[epoch : 49] (l_loss: 0.04729) (t_loss: 0.07932) (accu: 0.9743)
[epoch : 50] (l_loss: 0.04612) (t_loss: 0.07927) (accu: 0.9744)
Finish! (Best accu: 0.9804) (Time taken(sec) : 627.81) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (29115 | 186385)         13.51
fc1.weight   :      196000 (26307 | 169693)         13.42
fc2.weight   :        18750 (2517 | 16233)          13.42
fcout.weight :          750 (291 | 459)             38.80
------------------------------------------------------------
[Prune_iter : (10/21), Remaining weight : 13.51 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29809) (accu: 0.1214)
[epoch : 1] (l_loss: 0.26524) (t_loss: 0.12675) (accu: 0.9624)
[epoch : 2] (l_loss: 0.11829) (t_loss: 0.10780) (accu: 0.9672)
[epoch : 3] (l_loss: 0.09311) (t_loss: 0.08684) (accu: 0.9742)
[epoch : 4] (l_loss: 0.08124) (t_loss: 0.09449) (accu: 0.9706)
[epoch : 5] (l_loss: 0.07258) (t_loss: 0.08164) (accu: 0.9748)
[epoch : 6] (l_loss: 0.06849) (t_loss: 0.08265) (accu: 0.9743)
[epoch : 7] (l_loss: 0.06458) (t_loss: 0.07500) (accu: 0.9770)
[epoch : 8] (l_loss: 0.06188) (t_loss: 0.08191) (accu: 0.9735)
[epoch : 9] (l_loss: 0.05994) (t_loss: 0.07820) (accu: 0.9745)
[epoch : 10] (l_loss: 0.05758) (t_loss: 0.07791) (accu: 0.9770)
[epoch : 11] (l_loss: 0.05789) (t_loss: 0.08194) (accu: 0.9749)
[epoch : 12] (l_loss: 0.05547) (t_loss: 0.07509) (accu: 0.9769)
[epoch : 13] (l_loss: 0.05555) (t_loss: 0.08520) (accu: 0.9742)
[epoch : 14] (l_loss: 0.05378) (t_loss: 0.07334) (accu: 0.9777)
[epoch : 15] (l_loss: 0.05419) (t_loss: 0.08574) (accu: 0.9747)
[epoch : 16] (l_loss: 0.05249) (t_loss: 0.07056) (accu: 0.9771)
[epoch : 17] (l_loss: 0.05250) (t_loss: 0.06855) (accu: 0.9789)
[epoch : 18] (l_loss: 0.05133) (t_loss: 0.07454) (accu: 0.9764)
[epoch : 19] (l_loss: 0.05098) (t_loss: 0.06643) (accu: 0.9785)
[epoch : 20] (l_loss: 0.05100) (t_loss: 0.06754) (accu: 0.9789)
[epoch : 21] (l_loss: 0.05123) (t_loss: 0.07138) (accu: 0.9783)
[epoch : 22] (l_loss: 0.05055) (t_loss: 0.07822) (accu: 0.9769)
[epoch : 23] (l_loss: 0.04919) (t_loss: 0.06996) (accu: 0.9787)
[epoch : 24] (l_loss: 0.05106) (t_loss: 0.07564) (accu: 0.9771)
[epoch : 25] (l_loss: 0.05005) (t_loss: 0.07331) (accu: 0.9777)
[epoch : 26] (l_loss: 0.05021) (t_loss: 0.06724) (accu: 0.9793)
[epoch : 27] (l_loss: 0.04893) (t_loss: 0.08424) (accu: 0.9753)
[epoch : 28] (l_loss: 0.05028) (t_loss: 0.07266) (accu: 0.9774)
[epoch : 29] (l_loss: 0.04877) (t_loss: 0.07101) (accu: 0.9783)
[epoch : 30] (l_loss: 0.04928) (t_loss: 0.07089) (accu: 0.9786)
[epoch : 31] (l_loss: 0.04925) (t_loss: 0.06409) (accu: 0.9797)
[epoch : 32] (l_loss: 0.04864) (t_loss: 0.06788) (accu: 0.9788)
[epoch : 33] (l_loss: 0.04835) (t_loss: 0.07836) (accu: 0.9752)
[epoch : 34] (l_loss: 0.04845) (t_loss: 0.07331) (accu: 0.9771)
[epoch : 35] (l_loss: 0.04976) (t_loss: 0.07942) (accu: 0.9761)
[epoch : 36] (l_loss: 0.04831) (t_loss: 0.07750) (accu: 0.9772)
[epoch : 37] (l_loss: 0.04846) (t_loss: 0.07373) (accu: 0.9784)
[epoch : 38] (l_loss: 0.04717) (t_loss: 0.07196) (accu: 0.9795)
[epoch : 39] (l_loss: 0.04964) (t_loss: 0.07060) (accu: 0.9789)
[epoch : 40] (l_loss: 0.04763) (t_loss: 0.06967) (accu: 0.9790)
[epoch : 41] (l_loss: 0.04825) (t_loss: 0.07286) (accu: 0.9779)
[epoch : 42] (l_loss: 0.04701) (t_loss: 0.07691) (accu: 0.9749)
[epoch : 43] (l_loss: 0.04842) (t_loss: 0.07455) (accu: 0.9770)
[epoch : 44] (l_loss: 0.04758) (t_loss: 0.06858) (accu: 0.9787)
[epoch : 45] (l_loss: 0.04722) (t_loss: 0.08250) (accu: 0.9761)
[epoch : 46] (l_loss: 0.04826) (t_loss: 0.07195) (accu: 0.9792)
[epoch : 47] (l_loss: 0.04875) (t_loss: 0.08256) (accu: 0.9757)
[epoch : 48] (l_loss: 0.04703) (t_loss: 0.06931) (accu: 0.9783)
[epoch : 49] (l_loss: 0.04654) (t_loss: 0.07319) (accu: 0.9762)
[epoch : 50] (l_loss: 0.04790) (t_loss: 0.06784) (accu: 0.9786)
Finish! (Best accu: 0.9797) (Time taken(sec) : 608.73) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (23320 | 192180)         10.82
fc1.weight   :      196000 (21045 | 174955)         10.74
fc2.weight   :        18750 (2013 | 16737)          10.74
fcout.weight :          750 (262 | 488)             34.93
------------------------------------------------------------
[Prune_iter : (11/21), Remaining weight : 10.82 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.27847) (accu: 0.1444)
[epoch : 1] (l_loss: 0.27077) (t_loss: 0.14044) (accu: 0.9578)
[epoch : 2] (l_loss: 0.12100) (t_loss: 0.11253) (accu: 0.9670)
[epoch : 3] (l_loss: 0.09263) (t_loss: 0.09600) (accu: 0.9708)
[epoch : 4] (l_loss: 0.08114) (t_loss: 0.09293) (accu: 0.9735)
[epoch : 5] (l_loss: 0.07351) (t_loss: 0.10452) (accu: 0.9672)
[epoch : 6] (l_loss: 0.06723) (t_loss: 0.08266) (accu: 0.9741)
[epoch : 7] (l_loss: 0.06503) (t_loss: 0.07915) (accu: 0.9738)
[epoch : 8] (l_loss: 0.06050) (t_loss: 0.08542) (accu: 0.9730)
[epoch : 9] (l_loss: 0.05999) (t_loss: 0.08577) (accu: 0.9743)
[epoch : 10] (l_loss: 0.05805) (t_loss: 0.08181) (accu: 0.9753)
[epoch : 11] (l_loss: 0.05684) (t_loss: 0.08992) (accu: 0.9723)
[epoch : 12] (l_loss: 0.05494) (t_loss: 0.07737) (accu: 0.9753)
[epoch : 13] (l_loss: 0.05449) (t_loss: 0.07359) (accu: 0.9774)
[epoch : 14] (l_loss: 0.05433) (t_loss: 0.07795) (accu: 0.9761)
[epoch : 15] (l_loss: 0.05344) (t_loss: 0.08101) (accu: 0.9742)
[epoch : 16] (l_loss: 0.05340) (t_loss: 0.07060) (accu: 0.9786)
[epoch : 17] (l_loss: 0.05194) (t_loss: 0.07038) (accu: 0.9781)
[epoch : 18] (l_loss: 0.05147) (t_loss: 0.08323) (accu: 0.9734)
[epoch : 19] (l_loss: 0.05233) (t_loss: 0.07806) (accu: 0.9764)
[epoch : 20] (l_loss: 0.05009) (t_loss: 0.07936) (accu: 0.9746)
[epoch : 21] (l_loss: 0.05090) (t_loss: 0.07814) (accu: 0.9769)
[epoch : 22] (l_loss: 0.05001) (t_loss: 0.07392) (accu: 0.9756)
[epoch : 23] (l_loss: 0.05085) (t_loss: 0.07261) (accu: 0.9786)
[epoch : 24] (l_loss: 0.04929) (t_loss: 0.06896) (accu: 0.9785)
[epoch : 25] (l_loss: 0.05065) (t_loss: 0.07261) (accu: 0.9761)
[epoch : 26] (l_loss: 0.04986) (t_loss: 0.07335) (accu: 0.9785)
[epoch : 27] (l_loss: 0.04967) (t_loss: 0.06733) (accu: 0.9780)
[epoch : 28] (l_loss: 0.04862) (t_loss: 0.07604) (accu: 0.9759)
[epoch : 29] (l_loss: 0.05013) (t_loss: 0.07358) (accu: 0.9784)
[epoch : 30] (l_loss: 0.04779) (t_loss: 0.07375) (accu: 0.9759)
[epoch : 31] (l_loss: 0.04947) (t_loss: 0.06610) (accu: 0.9810)
[epoch : 32] (l_loss: 0.04821) (t_loss: 0.07565) (accu: 0.9772)
[epoch : 33] (l_loss: 0.04969) (t_loss: 0.06754) (accu: 0.9791)
[epoch : 34] (l_loss: 0.04795) (t_loss: 0.07523) (accu: 0.9771)
[epoch : 35] (l_loss: 0.04822) (t_loss: 0.06913) (accu: 0.9795)
[epoch : 36] (l_loss: 0.04910) (t_loss: 0.07152) (accu: 0.9784)
[epoch : 37] (l_loss: 0.04745) (t_loss: 0.08308) (accu: 0.9720)
[epoch : 38] (l_loss: 0.04834) (t_loss: 0.07825) (accu: 0.9752)
[epoch : 39] (l_loss: 0.04785) (t_loss: 0.07785) (accu: 0.9763)
[epoch : 40] (l_loss: 0.04771) (t_loss: 0.07660) (accu: 0.9769)
[epoch : 41] (l_loss: 0.04836) (t_loss: 0.06976) (accu: 0.9795)
[epoch : 42] (l_loss: 0.04688) (t_loss: 0.06987) (accu: 0.9777)
[epoch : 43] (l_loss: 0.04699) (t_loss: 0.06883) (accu: 0.9784)
[epoch : 44] (l_loss: 0.04806) (t_loss: 0.07311) (accu: 0.9770)
[epoch : 45] (l_loss: 0.04854) (t_loss: 0.07288) (accu: 0.9757)
[epoch : 46] (l_loss: 0.04722) (t_loss: 0.06721) (accu: 0.9789)
[epoch : 47] (l_loss: 0.04754) (t_loss: 0.07269) (accu: 0.9771)
[epoch : 48] (l_loss: 0.04737) (t_loss: 0.06986) (accu: 0.9779)
[epoch : 49] (l_loss: 0.04826) (t_loss: 0.07264) (accu: 0.9770)
[epoch : 50] (l_loss: 0.04693) (t_loss: 0.06797) (accu: 0.9783)
Finish! (Best accu: 0.9810) (Time taken(sec) : 631.72) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (18682 | 196818)          8.67
fc1.weight   :      196000 (16836 | 179164)          8.59
fc2.weight   :        18750 (1611 | 17139)           8.59
fcout.weight :          750 (235 | 515)             31.33
------------------------------------------------------------
[Prune_iter : (12/21), Remaining weight : 8.67 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.28018) (accu: 0.1350)
[epoch : 1] (l_loss: 0.27432) (t_loss: 0.14013) (accu: 0.9580)
[epoch : 2] (l_loss: 0.11990) (t_loss: 0.10792) (accu: 0.9661)
[epoch : 3] (l_loss: 0.09435) (t_loss: 0.09978) (accu: 0.9695)
[epoch : 4] (l_loss: 0.07929) (t_loss: 0.08304) (accu: 0.9729)
[epoch : 5] (l_loss: 0.07178) (t_loss: 0.08920) (accu: 0.9720)
[epoch : 6] (l_loss: 0.06617) (t_loss: 0.09202) (accu: 0.9686)
[epoch : 7] (l_loss: 0.06332) (t_loss: 0.08851) (accu: 0.9722)
[epoch : 8] (l_loss: 0.06079) (t_loss: 0.07886) (accu: 0.9747)
[epoch : 9] (l_loss: 0.05732) (t_loss: 0.07284) (accu: 0.9759)
[epoch : 10] (l_loss: 0.05556) (t_loss: 0.08056) (accu: 0.9746)
[epoch : 11] (l_loss: 0.05476) (t_loss: 0.08057) (accu: 0.9749)
[epoch : 12] (l_loss: 0.05357) (t_loss: 0.08178) (accu: 0.9756)
[epoch : 13] (l_loss: 0.05256) (t_loss: 0.09375) (accu: 0.9703)
[epoch : 14] (l_loss: 0.05129) (t_loss: 0.07605) (accu: 0.9777)
[epoch : 15] (l_loss: 0.05066) (t_loss: 0.08289) (accu: 0.9738)
[epoch : 16] (l_loss: 0.05051) (t_loss: 0.07817) (accu: 0.9747)
[epoch : 17] (l_loss: 0.05045) (t_loss: 0.07483) (accu: 0.9763)
[epoch : 18] (l_loss: 0.05016) (t_loss: 0.07343) (accu: 0.9758)
[epoch : 19] (l_loss: 0.04972) (t_loss: 0.06887) (accu: 0.9785)
[epoch : 20] (l_loss: 0.04843) (t_loss: 0.06803) (accu: 0.9794)
[epoch : 21] (l_loss: 0.04799) (t_loss: 0.06838) (accu: 0.9781)
[epoch : 22] (l_loss: 0.04873) (t_loss: 0.07346) (accu: 0.9773)
[epoch : 23] (l_loss: 0.04961) (t_loss: 0.06960) (accu: 0.9782)
[epoch : 24] (l_loss: 0.04854) (t_loss: 0.06996) (accu: 0.9779)
[epoch : 25] (l_loss: 0.04733) (t_loss: 0.07786) (accu: 0.9757)
[epoch : 26] (l_loss: 0.04739) (t_loss: 0.07880) (accu: 0.9747)
[epoch : 27] (l_loss: 0.04760) (t_loss: 0.07491) (accu: 0.9772)
[epoch : 28] (l_loss: 0.04887) (t_loss: 0.07566) (accu: 0.9754)
[epoch : 29] (l_loss: 0.04734) (t_loss: 0.07921) (accu: 0.9750)
[epoch : 30] (l_loss: 0.04720) (t_loss: 0.07495) (accu: 0.9744)
[epoch : 31] (l_loss: 0.04807) (t_loss: 0.07272) (accu: 0.9782)
[epoch : 32] (l_loss: 0.04677) (t_loss: 0.07783) (accu: 0.9765)
[epoch : 33] (l_loss: 0.04686) (t_loss: 0.07654) (accu: 0.9750)
[epoch : 34] (l_loss: 0.04760) (t_loss: 0.07072) (accu: 0.9763)
[epoch : 35] (l_loss: 0.04620) (t_loss: 0.07050) (accu: 0.9786)
[epoch : 36] (l_loss: 0.04683) (t_loss: 0.07346) (accu: 0.9775)
[epoch : 37] (l_loss: 0.04647) (t_loss: 0.06786) (accu: 0.9789)
[epoch : 38] (l_loss: 0.04574) (t_loss: 0.07475) (accu: 0.9754)
[epoch : 39] (l_loss: 0.04739) (t_loss: 0.06902) (accu: 0.9779)
[epoch : 40] (l_loss: 0.04663) (t_loss: 0.07239) (accu: 0.9775)
[epoch : 41] (l_loss: 0.04602) (t_loss: 0.07303) (accu: 0.9768)
[epoch : 42] (l_loss: 0.04645) (t_loss: 0.07309) (accu: 0.9777)
[epoch : 43] (l_loss: 0.04483) (t_loss: 0.07129) (accu: 0.9786)
[epoch : 44] (l_loss: 0.04666) (t_loss: 0.08551) (accu: 0.9730)
[epoch : 45] (l_loss: 0.04586) (t_loss: 0.06692) (accu: 0.9790)
[epoch : 46] (l_loss: 0.04617) (t_loss: 0.07126) (accu: 0.9773)
[epoch : 47] (l_loss: 0.04759) (t_loss: 0.07340) (accu: 0.9771)
[epoch : 48] (l_loss: 0.04631) (t_loss: 0.07854) (accu: 0.9750)
[epoch : 49] (l_loss: 0.04467) (t_loss: 0.06746) (accu: 0.9788)
[epoch : 50] (l_loss: 0.04601) (t_loss: 0.07190) (accu: 0.9769)
Finish! (Best accu: 0.9794) (Time taken(sec) : 608.68) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (14969 | 200531)          6.95
fc1.weight   :      196000 (13469 | 182531)          6.87
fc2.weight   :        18750 (1288 | 17462)           6.87
fcout.weight :          750 (212 | 538)             28.27
------------------------------------------------------------
[Prune_iter : (13/21), Remaining weight : 6.95 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.28820) (accu: 0.1436)
[epoch : 1] (l_loss: 0.28137) (t_loss: 0.14038) (accu: 0.9575)
[epoch : 2] (l_loss: 0.11541) (t_loss: 0.10086) (accu: 0.9692)
[epoch : 3] (l_loss: 0.09179) (t_loss: 0.09551) (accu: 0.9702)
[epoch : 4] (l_loss: 0.07817) (t_loss: 0.09113) (accu: 0.9731)
[epoch : 5] (l_loss: 0.07034) (t_loss: 0.08506) (accu: 0.9733)
[epoch : 6] (l_loss: 0.06504) (t_loss: 0.08943) (accu: 0.9729)
[epoch : 7] (l_loss: 0.06128) (t_loss: 0.08132) (accu: 0.9755)
[epoch : 8] (l_loss: 0.05897) (t_loss: 0.09321) (accu: 0.9712)
[epoch : 9] (l_loss: 0.05703) (t_loss: 0.07687) (accu: 0.9760)
[epoch : 10] (l_loss: 0.05425) (t_loss: 0.07774) (accu: 0.9760)
[epoch : 11] (l_loss: 0.05261) (t_loss: 0.07844) (accu: 0.9757)
[epoch : 12] (l_loss: 0.05152) (t_loss: 0.07664) (accu: 0.9770)
[epoch : 13] (l_loss: 0.05114) (t_loss: 0.07739) (accu: 0.9766)
[epoch : 14] (l_loss: 0.05004) (t_loss: 0.07337) (accu: 0.9765)
[epoch : 15] (l_loss: 0.05047) (t_loss: 0.07608) (accu: 0.9763)
[epoch : 16] (l_loss: 0.04793) (t_loss: 0.07738) (accu: 0.9749)
[epoch : 17] (l_loss: 0.04760) (t_loss: 0.06670) (accu: 0.9781)
[epoch : 18] (l_loss: 0.04819) (t_loss: 0.06867) (accu: 0.9774)
[epoch : 19] (l_loss: 0.04830) (t_loss: 0.07828) (accu: 0.9753)
[epoch : 20] (l_loss: 0.04725) (t_loss: 0.08247) (accu: 0.9738)
[epoch : 21] (l_loss: 0.04735) (t_loss: 0.07720) (accu: 0.9758)
[epoch : 22] (l_loss: 0.04502) (t_loss: 0.07694) (accu: 0.9767)
[epoch : 23] (l_loss: 0.04712) (t_loss: 0.07712) (accu: 0.9748)
[epoch : 24] (l_loss: 0.04610) (t_loss: 0.07073) (accu: 0.9792)
[epoch : 25] (l_loss: 0.04617) (t_loss: 0.07488) (accu: 0.9775)
[epoch : 26] (l_loss: 0.04555) (t_loss: 0.07057) (accu: 0.9766)
[epoch : 27] (l_loss: 0.04440) (t_loss: 0.07977) (accu: 0.9748)
[epoch : 28] (l_loss: 0.04592) (t_loss: 0.07245) (accu: 0.9774)
[epoch : 29] (l_loss: 0.04568) (t_loss: 0.07183) (accu: 0.9780)
[epoch : 30] (l_loss: 0.04521) (t_loss: 0.07110) (accu: 0.9763)
[epoch : 31] (l_loss: 0.04573) (t_loss: 0.07857) (accu: 0.9762)
[epoch : 32] (l_loss: 0.04492) (t_loss: 0.07539) (accu: 0.9764)
[epoch : 33] (l_loss: 0.04462) (t_loss: 0.07640) (accu: 0.9776)
[epoch : 34] (l_loss: 0.04524) (t_loss: 0.07811) (accu: 0.9765)
[epoch : 35] (l_loss: 0.04426) (t_loss: 0.07574) (accu: 0.9764)
[epoch : 36] (l_loss: 0.04407) (t_loss: 0.07916) (accu: 0.9748)
[epoch : 37] (l_loss: 0.04455) (t_loss: 0.07499) (accu: 0.9765)
[epoch : 38] (l_loss: 0.04426) (t_loss: 0.07667) (accu: 0.9766)
[epoch : 39] (l_loss: 0.04441) (t_loss: 0.07063) (accu: 0.9775)
[epoch : 40] (l_loss: 0.04520) (t_loss: 0.06890) (accu: 0.9779)
[epoch : 41] (l_loss: 0.04277) (t_loss: 0.07313) (accu: 0.9779)
[epoch : 42] (l_loss: 0.04496) (t_loss: 0.07176) (accu: 0.9771)
[epoch : 43] (l_loss: 0.04452) (t_loss: 0.07486) (accu: 0.9760)
[epoch : 44] (l_loss: 0.04383) (t_loss: 0.07145) (accu: 0.9773)
[epoch : 45] (l_loss: 0.04462) (t_loss: 0.07732) (accu: 0.9747)
[epoch : 46] (l_loss: 0.04434) (t_loss: 0.07090) (accu: 0.9762)
[epoch : 47] (l_loss: 0.04391) (t_loss: 0.07213) (accu: 0.9776)
[epoch : 48] (l_loss: 0.04320) (t_loss: 0.07238) (accu: 0.9772)
[epoch : 49] (l_loss: 0.04355) (t_loss: 0.07007) (accu: 0.9789)
[epoch : 50] (l_loss: 0.04460) (t_loss: 0.07709) (accu: 0.9779)
Finish! (Best accu: 0.9792) (Time taken(sec) : 598.22) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (11997 | 203503)          5.57
fc1.weight   :      196000 (10775 | 185225)          5.50
fc2.weight   :        18750 (1031 | 17719)           5.50
fcout.weight :          750 (191 | 559)             25.47
------------------------------------------------------------
[Prune_iter : (14/21), Remaining weight : 5.57 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.26896) (accu: 0.1641)
[epoch : 1] (l_loss: 0.28491) (t_loss: 0.14095) (accu: 0.9581)
[epoch : 2] (l_loss: 0.11202) (t_loss: 0.10013) (accu: 0.9701)
[epoch : 3] (l_loss: 0.08627) (t_loss: 0.09019) (accu: 0.9724)
[epoch : 4] (l_loss: 0.07210) (t_loss: 0.09064) (accu: 0.9720)
[epoch : 5] (l_loss: 0.06313) (t_loss: 0.07557) (accu: 0.9774)
[epoch : 6] (l_loss: 0.05820) (t_loss: 0.07652) (accu: 0.9763)
[epoch : 7] (l_loss: 0.05421) (t_loss: 0.07432) (accu: 0.9760)
[epoch : 8] (l_loss: 0.05147) (t_loss: 0.07520) (accu: 0.9767)
[epoch : 9] (l_loss: 0.04970) (t_loss: 0.07187) (accu: 0.9782)
[epoch : 10] (l_loss: 0.04744) (t_loss: 0.08330) (accu: 0.9740)
[epoch : 11] (l_loss: 0.04682) (t_loss: 0.07692) (accu: 0.9762)
[epoch : 12] (l_loss: 0.04620) (t_loss: 0.07901) (accu: 0.9765)
[epoch : 13] (l_loss: 0.04543) (t_loss: 0.06780) (accu: 0.9782)
[epoch : 14] (l_loss: 0.04466) (t_loss: 0.07444) (accu: 0.9768)
[epoch : 15] (l_loss: 0.04478) (t_loss: 0.07127) (accu: 0.9768)
[epoch : 16] (l_loss: 0.04422) (t_loss: 0.07172) (accu: 0.9777)
[epoch : 17] (l_loss: 0.04341) (t_loss: 0.07321) (accu: 0.9771)
[epoch : 18] (l_loss: 0.04358) (t_loss: 0.06530) (accu: 0.9800)
[epoch : 19] (l_loss: 0.04330) (t_loss: 0.06930) (accu: 0.9776)
[epoch : 20] (l_loss: 0.04303) (t_loss: 0.08080) (accu: 0.9745)
[epoch : 21] (l_loss: 0.04286) (t_loss: 0.07303) (accu: 0.9774)
[epoch : 22] (l_loss: 0.04163) (t_loss: 0.06575) (accu: 0.9783)
[epoch : 23] (l_loss: 0.04189) (t_loss: 0.06947) (accu: 0.9765)
[epoch : 24] (l_loss: 0.04164) (t_loss: 0.06798) (accu: 0.9792)
[epoch : 25] (l_loss: 0.04196) (t_loss: 0.06998) (accu: 0.9788)
[epoch : 26] (l_loss: 0.04137) (t_loss: 0.06905) (accu: 0.9771)
[epoch : 27] (l_loss: 0.04203) (t_loss: 0.06610) (accu: 0.9799)
[epoch : 28] (l_loss: 0.04134) (t_loss: 0.06964) (accu: 0.9793)
[epoch : 29] (l_loss: 0.04141) (t_loss: 0.06536) (accu: 0.9789)
[epoch : 30] (l_loss: 0.04183) (t_loss: 0.07063) (accu: 0.9786)
[epoch : 31] (l_loss: 0.04165) (t_loss: 0.07207) (accu: 0.9770)
[epoch : 32] (l_loss: 0.04208) (t_loss: 0.06702) (accu: 0.9787)
[epoch : 33] (l_loss: 0.04135) (t_loss: 0.07063) (accu: 0.9783)
[epoch : 34] (l_loss: 0.04156) (t_loss: 0.06909) (accu: 0.9794)
[epoch : 35] (l_loss: 0.04050) (t_loss: 0.07120) (accu: 0.9782)
[epoch : 36] (l_loss: 0.04147) (t_loss: 0.07441) (accu: 0.9767)
[epoch : 37] (l_loss: 0.04131) (t_loss: 0.06900) (accu: 0.9785)
[epoch : 38] (l_loss: 0.04140) (t_loss: 0.07031) (accu: 0.9775)
[epoch : 39] (l_loss: 0.04053) (t_loss: 0.07001) (accu: 0.9785)
[epoch : 40] (l_loss: 0.04100) (t_loss: 0.06509) (accu: 0.9789)
[epoch : 41] (l_loss: 0.04140) (t_loss: 0.06795) (accu: 0.9788)
[epoch : 42] (l_loss: 0.04121) (t_loss: 0.07027) (accu: 0.9778)
[epoch : 43] (l_loss: 0.04069) (t_loss: 0.07692) (accu: 0.9763)
[epoch : 44] (l_loss: 0.04167) (t_loss: 0.06827) (accu: 0.9785)
[epoch : 45] (l_loss: 0.04119) (t_loss: 0.07167) (accu: 0.9776)
[epoch : 46] (l_loss: 0.04048) (t_loss: 0.06550) (accu: 0.9791)
[epoch : 47] (l_loss: 0.04131) (t_loss: 0.07132) (accu: 0.9782)
[epoch : 48] (l_loss: 0.04109) (t_loss: 0.07252) (accu: 0.9771)
[epoch : 49] (l_loss: 0.04151) (t_loss: 0.06789) (accu: 0.9778)
[epoch : 50] (l_loss: 0.04083) (t_loss: 0.07230) (accu: 0.9785)
Finish! (Best accu: 0.9800) (Time taken(sec) : 613.72) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (9617 | 205883)          4.46
fc1.weight   :       196000 (8620 | 187380)          4.40
fc2.weight   :        18750 (825 | 17925)            4.40
fcout.weight :          750 (172 | 578)             22.93
------------------------------------------------------------
[Prune_iter : (15/21), Remaining weight : 4.46 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.28926) (accu: 0.1350)
[epoch : 1] (l_loss: 0.29955) (t_loss: 0.13203) (accu: 0.9612)
[epoch : 2] (l_loss: 0.11116) (t_loss: 0.09633) (accu: 0.9711)
[epoch : 3] (l_loss: 0.08207) (t_loss: 0.08754) (accu: 0.9731)
[epoch : 4] (l_loss: 0.06806) (t_loss: 0.08464) (accu: 0.9723)
[epoch : 5] (l_loss: 0.05884) (t_loss: 0.08895) (accu: 0.9716)
[epoch : 6] (l_loss: 0.05456) (t_loss: 0.07710) (accu: 0.9770)
[epoch : 7] (l_loss: 0.05112) (t_loss: 0.06998) (accu: 0.9787)
[epoch : 8] (l_loss: 0.04832) (t_loss: 0.07008) (accu: 0.9789)
[epoch : 9] (l_loss: 0.04633) (t_loss: 0.07167) (accu: 0.9787)
[epoch : 10] (l_loss: 0.04433) (t_loss: 0.07010) (accu: 0.9788)
[epoch : 11] (l_loss: 0.04382) (t_loss: 0.07370) (accu: 0.9774)
[epoch : 12] (l_loss: 0.04260) (t_loss: 0.07050) (accu: 0.9781)
[epoch : 13] (l_loss: 0.04239) (t_loss: 0.07389) (accu: 0.9777)
[epoch : 14] (l_loss: 0.04162) (t_loss: 0.06843) (accu: 0.9797)
[epoch : 15] (l_loss: 0.04034) (t_loss: 0.06745) (accu: 0.9784)
[epoch : 16] (l_loss: 0.04085) (t_loss: 0.06984) (accu: 0.9776)
[epoch : 17] (l_loss: 0.04028) (t_loss: 0.07254) (accu: 0.9776)
[epoch : 18] (l_loss: 0.04065) (t_loss: 0.07320) (accu: 0.9771)
[epoch : 19] (l_loss: 0.03988) (t_loss: 0.07276) (accu: 0.9783)
[epoch : 20] (l_loss: 0.04044) (t_loss: 0.06862) (accu: 0.9785)
[epoch : 21] (l_loss: 0.03984) (t_loss: 0.07163) (accu: 0.9798)
[epoch : 22] (l_loss: 0.03946) (t_loss: 0.07014) (accu: 0.9781)
[epoch : 23] (l_loss: 0.03970) (t_loss: 0.06842) (accu: 0.9791)
[epoch : 24] (l_loss: 0.03957) (t_loss: 0.06869) (accu: 0.9790)
[epoch : 25] (l_loss: 0.03897) (t_loss: 0.06766) (accu: 0.9795)
[epoch : 26] (l_loss: 0.04032) (t_loss: 0.07324) (accu: 0.9772)
[epoch : 27] (l_loss: 0.03930) (t_loss: 0.06820) (accu: 0.9798)
[epoch : 28] (l_loss: 0.03868) (t_loss: 0.06614) (accu: 0.9797)
[epoch : 29] (l_loss: 0.04003) (t_loss: 0.07037) (accu: 0.9790)
[epoch : 30] (l_loss: 0.03836) (t_loss: 0.07391) (accu: 0.9774)
[epoch : 31] (l_loss: 0.03962) (t_loss: 0.06574) (accu: 0.9812)
[epoch : 32] (l_loss: 0.03893) (t_loss: 0.06770) (accu: 0.9782)
[epoch : 33] (l_loss: 0.03866) (t_loss: 0.06924) (accu: 0.9789)
[epoch : 34] (l_loss: 0.03844) (t_loss: 0.06904) (accu: 0.9777)
[epoch : 35] (l_loss: 0.03768) (t_loss: 0.06988) (accu: 0.9772)
[epoch : 36] (l_loss: 0.03814) (t_loss: 0.06596) (accu: 0.9794)
[epoch : 37] (l_loss: 0.03809) (t_loss: 0.06840) (accu: 0.9788)
[epoch : 38] (l_loss: 0.03883) (t_loss: 0.06658) (accu: 0.9791)
[epoch : 39] (l_loss: 0.03870) (t_loss: 0.06637) (accu: 0.9809)
[epoch : 40] (l_loss: 0.03774) (t_loss: 0.07128) (accu: 0.9769)
[epoch : 41] (l_loss: 0.03917) (t_loss: 0.07016) (accu: 0.9782)
[epoch : 42] (l_loss: 0.03810) (t_loss: 0.06320) (accu: 0.9807)
[epoch : 43] (l_loss: 0.03777) (t_loss: 0.07017) (accu: 0.9788)
[epoch : 44] (l_loss: 0.03795) (t_loss: 0.06689) (accu: 0.9797)
[epoch : 45] (l_loss: 0.03738) (t_loss: 0.06808) (accu: 0.9781)
[epoch : 46] (l_loss: 0.03786) (t_loss: 0.07086) (accu: 0.9792)
[epoch : 47] (l_loss: 0.03867) (t_loss: 0.07084) (accu: 0.9772)
[epoch : 48] (l_loss: 0.03848) (t_loss: 0.07030) (accu: 0.9788)
[epoch : 49] (l_loss: 0.03832) (t_loss: 0.06368) (accu: 0.9805)
[epoch : 50] (l_loss: 0.03840) (t_loss: 0.06898) (accu: 0.9785)
Finish! (Best accu: 0.9812) (Time taken(sec) : 626.09) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (7710 | 207790)          3.58
fc1.weight   :       196000 (6896 | 189104)          3.52
fc2.weight   :        18750 (660 | 18090)            3.52
fcout.weight :          750 (154 | 596)             20.53
------------------------------------------------------------
[Prune_iter : (16/21), Remaining weight : 3.58 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.28426) (accu: 0.1206)
[epoch : 1] (l_loss: 0.32233) (t_loss: 0.14253) (accu: 0.9560)
[epoch : 2] (l_loss: 0.11002) (t_loss: 0.09624) (accu: 0.9712)
[epoch : 3] (l_loss: 0.07935) (t_loss: 0.08668) (accu: 0.9732)
[epoch : 4] (l_loss: 0.06488) (t_loss: 0.08386) (accu: 0.9736)
[epoch : 5] (l_loss: 0.05622) (t_loss: 0.07348) (accu: 0.9773)
[epoch : 6] (l_loss: 0.05192) (t_loss: 0.07449) (accu: 0.9772)
[epoch : 7] (l_loss: 0.04770) (t_loss: 0.07575) (accu: 0.9759)
[epoch : 8] (l_loss: 0.04672) (t_loss: 0.06858) (accu: 0.9778)
[epoch : 9] (l_loss: 0.04369) (t_loss: 0.06805) (accu: 0.9782)
[epoch : 10] (l_loss: 0.04268) (t_loss: 0.07879) (accu: 0.9755)
[epoch : 11] (l_loss: 0.04232) (t_loss: 0.06551) (accu: 0.9806)
[epoch : 12] (l_loss: 0.04098) (t_loss: 0.06403) (accu: 0.9803)
[epoch : 13] (l_loss: 0.04073) (t_loss: 0.06826) (accu: 0.9794)
[epoch : 14] (l_loss: 0.04000) (t_loss: 0.06967) (accu: 0.9787)
[epoch : 15] (l_loss: 0.03932) (t_loss: 0.07081) (accu: 0.9784)
[epoch : 16] (l_loss: 0.04005) (t_loss: 0.06730) (accu: 0.9790)
[epoch : 17] (l_loss: 0.03884) (t_loss: 0.06903) (accu: 0.9788)
[epoch : 18] (l_loss: 0.03925) (t_loss: 0.06563) (accu: 0.9797)
[epoch : 19] (l_loss: 0.03952) (t_loss: 0.06543) (accu: 0.9803)
[epoch : 20] (l_loss: 0.03832) (t_loss: 0.06357) (accu: 0.9803)
[epoch : 21] (l_loss: 0.03846) (t_loss: 0.06794) (accu: 0.9798)
[epoch : 22] (l_loss: 0.03807) (t_loss: 0.07063) (accu: 0.9785)
[epoch : 23] (l_loss: 0.03833) (t_loss: 0.06699) (accu: 0.9791)
[epoch : 24] (l_loss: 0.03807) (t_loss: 0.06680) (accu: 0.9784)
[epoch : 25] (l_loss: 0.03826) (t_loss: 0.06894) (accu: 0.9798)
[epoch : 26] (l_loss: 0.03831) (t_loss: 0.07276) (accu: 0.9775)
[epoch : 27] (l_loss: 0.03800) (t_loss: 0.06462) (accu: 0.9810)
[epoch : 28] (l_loss: 0.03834) (t_loss: 0.06957) (accu: 0.9775)
[epoch : 29] (l_loss: 0.03784) (t_loss: 0.07090) (accu: 0.9780)
[epoch : 30] (l_loss: 0.03821) (t_loss: 0.06855) (accu: 0.9783)
[epoch : 31] (l_loss: 0.03721) (t_loss: 0.07050) (accu: 0.9779)
[epoch : 32] (l_loss: 0.03794) (t_loss: 0.06953) (accu: 0.9772)
[epoch : 33] (l_loss: 0.03868) (t_loss: 0.06711) (accu: 0.9787)
[epoch : 34] (l_loss: 0.03744) (t_loss: 0.06416) (accu: 0.9793)
[epoch : 35] (l_loss: 0.03793) (t_loss: 0.06750) (accu: 0.9798)
[epoch : 36] (l_loss: 0.03793) (t_loss: 0.06610) (accu: 0.9793)
[epoch : 37] (l_loss: 0.03746) (t_loss: 0.07394) (accu: 0.9783)
[epoch : 38] (l_loss: 0.03785) (t_loss: 0.06844) (accu: 0.9788)
[epoch : 39] (l_loss: 0.03746) (t_loss: 0.06709) (accu: 0.9797)
[epoch : 40] (l_loss: 0.03820) (t_loss: 0.06800) (accu: 0.9785)
[epoch : 41] (l_loss: 0.03806) (t_loss: 0.06620) (accu: 0.9793)
[epoch : 42] (l_loss: 0.03804) (t_loss: 0.06757) (accu: 0.9806)
[epoch : 43] (l_loss: 0.03739) (t_loss: 0.06706) (accu: 0.9797)
[epoch : 44] (l_loss: 0.03735) (t_loss: 0.06952) (accu: 0.9794)
[epoch : 45] (l_loss: 0.03799) (t_loss: 0.07065) (accu: 0.9782)
[epoch : 46] (l_loss: 0.03773) (t_loss: 0.07043) (accu: 0.9783)
[epoch : 47] (l_loss: 0.03796) (t_loss: 0.06610) (accu: 0.9805)
[epoch : 48] (l_loss: 0.03816) (t_loss: 0.06673) (accu: 0.9795)
[epoch : 49] (l_loss: 0.03802) (t_loss: 0.06786) (accu: 0.9793)
[epoch : 50] (l_loss: 0.03676) (t_loss: 0.07203) (accu: 0.9778)
Finish! (Best accu: 0.9810) (Time taken(sec) : 644.07) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (6184 | 209316)          2.87
fc1.weight   :       196000 (5517 | 190483)          2.81
fc2.weight   :        18750 (528 | 18222)            2.82
fcout.weight :          750 (139 | 611)             18.53
------------------------------------------------------------
[Prune_iter : (17/21), Remaining weight : 2.87 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.28128) (accu: 0.1251)
[epoch : 1] (l_loss: 0.33313) (t_loss: 0.13579) (accu: 0.9601)
[epoch : 2] (l_loss: 0.10893) (t_loss: 0.09771) (accu: 0.9704)
[epoch : 3] (l_loss: 0.07994) (t_loss: 0.08747) (accu: 0.9734)
[epoch : 4] (l_loss: 0.06668) (t_loss: 0.08022) (accu: 0.9745)
[epoch : 5] (l_loss: 0.05842) (t_loss: 0.07530) (accu: 0.9769)
[epoch : 6] (l_loss: 0.05356) (t_loss: 0.07200) (accu: 0.9780)
[epoch : 7] (l_loss: 0.05013) (t_loss: 0.07507) (accu: 0.9773)
[epoch : 8] (l_loss: 0.04760) (t_loss: 0.07333) (accu: 0.9765)
[epoch : 9] (l_loss: 0.04587) (t_loss: 0.07060) (accu: 0.9778)
[epoch : 10] (l_loss: 0.04467) (t_loss: 0.07071) (accu: 0.9769)
[epoch : 11] (l_loss: 0.04368) (t_loss: 0.06932) (accu: 0.9776)
[epoch : 12] (l_loss: 0.04290) (t_loss: 0.06818) (accu: 0.9796)
[epoch : 13] (l_loss: 0.04257) (t_loss: 0.07084) (accu: 0.9788)
[epoch : 14] (l_loss: 0.04223) (t_loss: 0.07037) (accu: 0.9787)
[epoch : 15] (l_loss: 0.04138) (t_loss: 0.06799) (accu: 0.9794)
[epoch : 16] (l_loss: 0.04173) (t_loss: 0.06840) (accu: 0.9787)
[epoch : 17] (l_loss: 0.04110) (t_loss: 0.06899) (accu: 0.9776)
[epoch : 18] (l_loss: 0.04106) (t_loss: 0.07522) (accu: 0.9775)
[epoch : 19] (l_loss: 0.03996) (t_loss: 0.07217) (accu: 0.9779)
[epoch : 20] (l_loss: 0.04058) (t_loss: 0.06775) (accu: 0.9796)
[epoch : 21] (l_loss: 0.04012) (t_loss: 0.07186) (accu: 0.9782)
[epoch : 22] (l_loss: 0.03933) (t_loss: 0.07061) (accu: 0.9781)
[epoch : 23] (l_loss: 0.04001) (t_loss: 0.06650) (accu: 0.9788)
[epoch : 24] (l_loss: 0.03968) (t_loss: 0.06834) (accu: 0.9781)
[epoch : 25] (l_loss: 0.03925) (t_loss: 0.06714) (accu: 0.9785)
[epoch : 26] (l_loss: 0.03947) (t_loss: 0.07003) (accu: 0.9791)
[epoch : 27] (l_loss: 0.03937) (t_loss: 0.07320) (accu: 0.9779)
[epoch : 28] (l_loss: 0.03912) (t_loss: 0.07433) (accu: 0.9779)
[epoch : 29] (l_loss: 0.03949) (t_loss: 0.06963) (accu: 0.9798)
[epoch : 30] (l_loss: 0.03870) (t_loss: 0.06512) (accu: 0.9798)
[epoch : 31] (l_loss: 0.03928) (t_loss: 0.06812) (accu: 0.9788)
[epoch : 32] (l_loss: 0.03910) (t_loss: 0.07094) (accu: 0.9776)
[epoch : 33] (l_loss: 0.03907) (t_loss: 0.06632) (accu: 0.9803)
[epoch : 34] (l_loss: 0.03874) (t_loss: 0.06937) (accu: 0.9782)
[epoch : 35] (l_loss: 0.03872) (t_loss: 0.06701) (accu: 0.9806)
[epoch : 36] (l_loss: 0.03902) (t_loss: 0.06821) (accu: 0.9790)
[epoch : 37] (l_loss: 0.03844) (t_loss: 0.07079) (accu: 0.9778)
[epoch : 38] (l_loss: 0.03838) (t_loss: 0.06630) (accu: 0.9792)
[epoch : 39] (l_loss: 0.03888) (t_loss: 0.06675) (accu: 0.9796)
[epoch : 40] (l_loss: 0.03901) (t_loss: 0.06909) (accu: 0.9793)
[epoch : 41] (l_loss: 0.03923) (t_loss: 0.06932) (accu: 0.9786)
[epoch : 42] (l_loss: 0.03954) (t_loss: 0.06635) (accu: 0.9796)
[epoch : 43] (l_loss: 0.03855) (t_loss: 0.06548) (accu: 0.9815)
[epoch : 44] (l_loss: 0.03841) (t_loss: 0.06807) (accu: 0.9782)
[epoch : 45] (l_loss: 0.03897) (t_loss: 0.06799) (accu: 0.9793)
[epoch : 46] (l_loss: 0.03843) (t_loss: 0.06953) (accu: 0.9787)
[epoch : 47] (l_loss: 0.03906) (t_loss: 0.06590) (accu: 0.9794)
[epoch : 48] (l_loss: 0.03836) (t_loss: 0.07130) (accu: 0.9789)
[epoch : 49] (l_loss: 0.03818) (t_loss: 0.06708) (accu: 0.9791)
[epoch : 50] (l_loss: 0.03862) (t_loss: 0.06880) (accu: 0.9801)
Finish! (Best accu: 0.9815) (Time taken(sec) : 646.18) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (4961 | 210539)          2.30
fc1.weight   :       196000 (4414 | 191586)          2.25
fc2.weight   :        18750 (422 | 18328)            2.25
fcout.weight :          750 (125 | 625)             16.67
------------------------------------------------------------
[Prune_iter : (18/21), Remaining weight : 2.3 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.28432) (accu: 0.1286)
[epoch : 1] (l_loss: 0.35615) (t_loss: 0.13506) (accu: 0.9624)
[epoch : 2] (l_loss: 0.11248) (t_loss: 0.10362) (accu: 0.9690)
[epoch : 3] (l_loss: 0.08295) (t_loss: 0.08653) (accu: 0.9744)
[epoch : 4] (l_loss: 0.07015) (t_loss: 0.08452) (accu: 0.9761)
[epoch : 5] (l_loss: 0.06311) (t_loss: 0.07880) (accu: 0.9753)
[epoch : 6] (l_loss: 0.05812) (t_loss: 0.07418) (accu: 0.9772)
[epoch : 7] (l_loss: 0.05425) (t_loss: 0.07150) (accu: 0.9785)
[epoch : 8] (l_loss: 0.05141) (t_loss: 0.07408) (accu: 0.9776)
[epoch : 9] (l_loss: 0.04940) (t_loss: 0.07484) (accu: 0.9781)
[epoch : 10] (l_loss: 0.04797) (t_loss: 0.07051) (accu: 0.9778)
[epoch : 11] (l_loss: 0.04702) (t_loss: 0.07234) (accu: 0.9779)
[epoch : 12] (l_loss: 0.04604) (t_loss: 0.07490) (accu: 0.9776)
[epoch : 13] (l_loss: 0.04538) (t_loss: 0.07146) (accu: 0.9785)
[epoch : 14] (l_loss: 0.04528) (t_loss: 0.06829) (accu: 0.9793)
[epoch : 15] (l_loss: 0.04452) (t_loss: 0.06844) (accu: 0.9797)
[epoch : 16] (l_loss: 0.04393) (t_loss: 0.07122) (accu: 0.9782)
[epoch : 17] (l_loss: 0.04354) (t_loss: 0.07552) (accu: 0.9779)
[epoch : 18] (l_loss: 0.04328) (t_loss: 0.07303) (accu: 0.9793)
[epoch : 19] (l_loss: 0.04283) (t_loss: 0.06944) (accu: 0.9794)
[epoch : 20] (l_loss: 0.04288) (t_loss: 0.06891) (accu: 0.9794)
[epoch : 21] (l_loss: 0.04240) (t_loss: 0.07406) (accu: 0.9777)
[epoch : 22] (l_loss: 0.04297) (t_loss: 0.07038) (accu: 0.9790)
[epoch : 23] (l_loss: 0.04225) (t_loss: 0.07079) (accu: 0.9783)
[epoch : 24] (l_loss: 0.04237) (t_loss: 0.06535) (accu: 0.9802)
[epoch : 25] (l_loss: 0.04223) (t_loss: 0.06750) (accu: 0.9804)
[epoch : 26] (l_loss: 0.04229) (t_loss: 0.07017) (accu: 0.9791)
[epoch : 27] (l_loss: 0.04186) (t_loss: 0.06852) (accu: 0.9780)
[epoch : 28] (l_loss: 0.04143) (t_loss: 0.06814) (accu: 0.9786)
[epoch : 29] (l_loss: 0.04190) (t_loss: 0.06831) (accu: 0.9793)
[epoch : 30] (l_loss: 0.04178) (t_loss: 0.07106) (accu: 0.9773)
[epoch : 31] (l_loss: 0.04152) (t_loss: 0.07220) (accu: 0.9776)
[epoch : 32] (l_loss: 0.04096) (t_loss: 0.06806) (accu: 0.9792)
[epoch : 33] (l_loss: 0.04178) (t_loss: 0.06894) (accu: 0.9794)
[epoch : 34] (l_loss: 0.04168) (t_loss: 0.06911) (accu: 0.9783)
[epoch : 35] (l_loss: 0.04122) (t_loss: 0.06999) (accu: 0.9788)
[epoch : 36] (l_loss: 0.04136) (t_loss: 0.07075) (accu: 0.9791)
[epoch : 37] (l_loss: 0.04150) (t_loss: 0.07192) (accu: 0.9785)
[epoch : 38] (l_loss: 0.04094) (t_loss: 0.07202) (accu: 0.9794)
[epoch : 39] (l_loss: 0.04115) (t_loss: 0.06970) (accu: 0.9791)
[epoch : 40] (l_loss: 0.04166) (t_loss: 0.06939) (accu: 0.9788)
[epoch : 41] (l_loss: 0.04121) (t_loss: 0.06705) (accu: 0.9793)
[epoch : 42] (l_loss: 0.04099) (t_loss: 0.06841) (accu: 0.9795)
[epoch : 43] (l_loss: 0.04071) (t_loss: 0.07030) (accu: 0.9791)
[epoch : 44] (l_loss: 0.04172) (t_loss: 0.07427) (accu: 0.9777)
[epoch : 45] (l_loss: 0.04170) (t_loss: 0.07191) (accu: 0.9780)
[epoch : 46] (l_loss: 0.04127) (t_loss: 0.07086) (accu: 0.9788)
[epoch : 47] (l_loss: 0.04144) (t_loss: 0.07296) (accu: 0.9785)
[epoch : 48] (l_loss: 0.04093) (t_loss: 0.07116) (accu: 0.9788)
[epoch : 49] (l_loss: 0.04087) (t_loss: 0.06917) (accu: 0.9793)
[epoch : 50] (l_loss: 0.04165) (t_loss: 0.06893) (accu: 0.9785)
Finish! (Best accu: 0.9804) (Time taken(sec) : 643.19) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (3982 | 211518)          1.85
fc1.weight   :       196000 (3531 | 192469)          1.80
fc2.weight   :        18750 (338 | 18412)            1.80
fcout.weight :          750 (113 | 637)             15.07
------------------------------------------------------------
[Prune_iter : (19/21), Remaining weight : 1.85 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.28656) (accu: 0.1375)
[epoch : 1] (l_loss: 0.37376) (t_loss: 0.14315) (accu: 0.9597)
[epoch : 2] (l_loss: 0.11380) (t_loss: 0.10143) (accu: 0.9706)
[epoch : 3] (l_loss: 0.08332) (t_loss: 0.08746) (accu: 0.9743)
[epoch : 4] (l_loss: 0.06902) (t_loss: 0.08062) (accu: 0.9770)
[epoch : 5] (l_loss: 0.06042) (t_loss: 0.08119) (accu: 0.9757)
[epoch : 6] (l_loss: 0.05616) (t_loss: 0.08311) (accu: 0.9741)
[epoch : 7] (l_loss: 0.05268) (t_loss: 0.07436) (accu: 0.9769)
[epoch : 8] (l_loss: 0.05055) (t_loss: 0.07369) (accu: 0.9782)
[epoch : 9] (l_loss: 0.05005) (t_loss: 0.07512) (accu: 0.9776)
[epoch : 10] (l_loss: 0.04783) (t_loss: 0.07201) (accu: 0.9782)
[epoch : 11] (l_loss: 0.04781) (t_loss: 0.07293) (accu: 0.9785)
[epoch : 12] (l_loss: 0.04748) (t_loss: 0.07254) (accu: 0.9780)
[epoch : 13] (l_loss: 0.04675) (t_loss: 0.07084) (accu: 0.9784)
[epoch : 14] (l_loss: 0.04623) (t_loss: 0.06997) (accu: 0.9777)
[epoch : 15] (l_loss: 0.04629) (t_loss: 0.07325) (accu: 0.9776)
[epoch : 16] (l_loss: 0.04606) (t_loss: 0.07544) (accu: 0.9772)
[epoch : 17] (l_loss: 0.04547) (t_loss: 0.07439) (accu: 0.9775)
[epoch : 18] (l_loss: 0.04525) (t_loss: 0.06979) (accu: 0.9779)
[epoch : 19] (l_loss: 0.04506) (t_loss: 0.07157) (accu: 0.9781)
[epoch : 20] (l_loss: 0.04542) (t_loss: 0.07007) (accu: 0.9789)
[epoch : 21] (l_loss: 0.04508) (t_loss: 0.07068) (accu: 0.9782)
[epoch : 22] (l_loss: 0.04465) (t_loss: 0.07310) (accu: 0.9777)
[epoch : 23] (l_loss: 0.04490) (t_loss: 0.07401) (accu: 0.9769)
[epoch : 24] (l_loss: 0.04463) (t_loss: 0.07235) (accu: 0.9773)
[epoch : 25] (l_loss: 0.04531) (t_loss: 0.07132) (accu: 0.9773)
[epoch : 26] (l_loss: 0.04508) (t_loss: 0.07196) (accu: 0.9781)
[epoch : 27] (l_loss: 0.04454) (t_loss: 0.06940) (accu: 0.9784)
[epoch : 28] (l_loss: 0.04510) (t_loss: 0.07161) (accu: 0.9786)
[epoch : 29] (l_loss: 0.04486) (t_loss: 0.07312) (accu: 0.9766)
[epoch : 30] (l_loss: 0.04471) (t_loss: 0.07097) (accu: 0.9789)
[epoch : 31] (l_loss: 0.04477) (t_loss: 0.07194) (accu: 0.9774)
[epoch : 32] (l_loss: 0.04381) (t_loss: 0.07440) (accu: 0.9773)
[epoch : 33] (l_loss: 0.04458) (t_loss: 0.07113) (accu: 0.9777)
[epoch : 34] (l_loss: 0.04458) (t_loss: 0.07385) (accu: 0.9777)
[epoch : 35] (l_loss: 0.04468) (t_loss: 0.07413) (accu: 0.9781)
[epoch : 36] (l_loss: 0.04457) (t_loss: 0.07144) (accu: 0.9781)
[epoch : 37] (l_loss: 0.04478) (t_loss: 0.07344) (accu: 0.9781)
[epoch : 38] (l_loss: 0.04503) (t_loss: 0.07041) (accu: 0.9786)
[epoch : 39] (l_loss: 0.04430) (t_loss: 0.07256) (accu: 0.9778)
[epoch : 40] (l_loss: 0.04458) (t_loss: 0.07350) (accu: 0.9764)
[epoch : 41] (l_loss: 0.04503) (t_loss: 0.07503) (accu: 0.9757)
[epoch : 42] (l_loss: 0.04447) (t_loss: 0.07013) (accu: 0.9789)
[epoch : 43] (l_loss: 0.04416) (t_loss: 0.07409) (accu: 0.9772)
[epoch : 44] (l_loss: 0.04460) (t_loss: 0.07367) (accu: 0.9774)
[epoch : 45] (l_loss: 0.04484) (t_loss: 0.07149) (accu: 0.9779)
[epoch : 46] (l_loss: 0.04454) (t_loss: 0.07311) (accu: 0.9770)
[epoch : 47] (l_loss: 0.04475) (t_loss: 0.07206) (accu: 0.9781)
[epoch : 48] (l_loss: 0.04493) (t_loss: 0.07497) (accu: 0.9759)
[epoch : 49] (l_loss: 0.04435) (t_loss: 0.06963) (accu: 0.9792)
[epoch : 50] (l_loss: 0.04446) (t_loss: 0.07392) (accu: 0.9777)
Finish! (Best accu: 0.9792) (Time taken(sec) : 636.17) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (3196 | 212304)          1.48
fc1.weight   :       196000 (2825 | 193175)          1.44
fc2.weight   :        18750 (270 | 18480)            1.44
fcout.weight :          750 (101 | 649)             13.47
------------------------------------------------------------
[Prune_iter : (20/21), Remaining weight : 1.48 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.28789) (accu: 0.1015)
[epoch : 1] (l_loss: 0.40452) (t_loss: 0.15245) (accu: 0.9558)
[epoch : 2] (l_loss: 0.12180) (t_loss: 0.10758) (accu: 0.9689)
[epoch : 3] (l_loss: 0.08967) (t_loss: 0.09589) (accu: 0.9712)
[epoch : 4] (l_loss: 0.07580) (t_loss: 0.08871) (accu: 0.9742)
[epoch : 5] (l_loss: 0.06751) (t_loss: 0.08431) (accu: 0.9759)
[epoch : 6] (l_loss: 0.06238) (t_loss: 0.08402) (accu: 0.9772)
[epoch : 7] (l_loss: 0.05943) (t_loss: 0.08185) (accu: 0.9767)
[epoch : 8] (l_loss: 0.05740) (t_loss: 0.07999) (accu: 0.9776)
[epoch : 9] (l_loss: 0.05611) (t_loss: 0.08023) (accu: 0.9765)
[epoch : 10] (l_loss: 0.05423) (t_loss: 0.07742) (accu: 0.9788)
[epoch : 11] (l_loss: 0.05356) (t_loss: 0.07823) (accu: 0.9779)
[epoch : 12] (l_loss: 0.05307) (t_loss: 0.08151) (accu: 0.9769)
[epoch : 13] (l_loss: 0.05210) (t_loss: 0.07755) (accu: 0.9778)
[epoch : 14] (l_loss: 0.05191) (t_loss: 0.07674) (accu: 0.9786)
[epoch : 15] (l_loss: 0.05164) (t_loss: 0.07389) (accu: 0.9789)
[epoch : 16] (l_loss: 0.05133) (t_loss: 0.07516) (accu: 0.9789)
[epoch : 17] (l_loss: 0.05133) (t_loss: 0.07984) (accu: 0.9775)
[epoch : 18] (l_loss: 0.05104) (t_loss: 0.07579) (accu: 0.9786)
[epoch : 19] (l_loss: 0.05059) (t_loss: 0.07585) (accu: 0.9771)
[epoch : 20] (l_loss: 0.05054) (t_loss: 0.07567) (accu: 0.9770)
[epoch : 21] (l_loss: 0.05057) (t_loss: 0.07724) (accu: 0.9770)
[epoch : 22] (l_loss: 0.04985) (t_loss: 0.07664) (accu: 0.9779)
[epoch : 23] (l_loss: 0.04987) (t_loss: 0.07704) (accu: 0.9774)
[epoch : 24] (l_loss: 0.04968) (t_loss: 0.07421) (accu: 0.9786)
[epoch : 25] (l_loss: 0.04934) (t_loss: 0.07416) (accu: 0.9774)
[epoch : 26] (l_loss: 0.04897) (t_loss: 0.07784) (accu: 0.9774)
[epoch : 27] (l_loss: 0.04928) (t_loss: 0.07368) (accu: 0.9789)
[epoch : 28] (l_loss: 0.04912) (t_loss: 0.07407) (accu: 0.9779)
[epoch : 29] (l_loss: 0.04914) (t_loss: 0.07554) (accu: 0.9776)
[epoch : 30] (l_loss: 0.04865) (t_loss: 0.08168) (accu: 0.9761)
[epoch : 31] (l_loss: 0.04907) (t_loss: 0.07610) (accu: 0.9780)
[epoch : 32] (l_loss: 0.04844) (t_loss: 0.07641) (accu: 0.9781)
[epoch : 33] (l_loss: 0.04882) (t_loss: 0.07323) (accu: 0.9787)
[epoch : 34] (l_loss: 0.04817) (t_loss: 0.07183) (accu: 0.9785)
[epoch : 35] (l_loss: 0.04834) (t_loss: 0.07202) (accu: 0.9792)
[epoch : 36] (l_loss: 0.04793) (t_loss: 0.07251) (accu: 0.9791)
[epoch : 37] (l_loss: 0.04748) (t_loss: 0.07350) (accu: 0.9785)
[epoch : 38] (l_loss: 0.04757) (t_loss: 0.07312) (accu: 0.9784)
[epoch : 39] (l_loss: 0.04792) (t_loss: 0.07247) (accu: 0.9779)
[epoch : 40] (l_loss: 0.04782) (t_loss: 0.07635) (accu: 0.9777)
[epoch : 41] (l_loss: 0.04742) (t_loss: 0.07168) (accu: 0.9780)
[epoch : 42] (l_loss: 0.04769) (t_loss: 0.07631) (accu: 0.9784)
[epoch : 43] (l_loss: 0.04776) (t_loss: 0.07514) (accu: 0.9782)
[epoch : 44] (l_loss: 0.04723) (t_loss: 0.07362) (accu: 0.9771)
[epoch : 45] (l_loss: 0.04746) (t_loss: 0.07527) (accu: 0.9780)
[epoch : 46] (l_loss: 0.04799) (t_loss: 0.07545) (accu: 0.9782)
[epoch : 47] (l_loss: 0.04738) (t_loss: 0.07849) (accu: 0.9772)
[epoch : 48] (l_loss: 0.04756) (t_loss: 0.07421) (accu: 0.9780)
[epoch : 49] (l_loss: 0.04747) (t_loss: 0.07472) (accu: 0.9783)
[epoch : 50] (l_loss: 0.04768) (t_loss: 0.07253) (accu: 0.9786)
Finish! (Best accu: 0.9792) (Time taken(sec) : 653.50) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (2567 | 212933)          1.19
fc1.weight   :       196000 (2260 | 193740)          1.15
fc2.weight   :        18750 (216 | 18534)            1.15
fcout.weight :           750 (91 | 659)             12.13
------------------------------------------------------------
[Prune_iter : (21/21), Remaining weight : 1.19 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29892) (accu: 0.0951)
[epoch : 1] (l_loss: 0.44664) (t_loss: 0.16370) (accu: 0.9552)
[epoch : 2] (l_loss: 0.13441) (t_loss: 0.12290) (accu: 0.9629)
[epoch : 3] (l_loss: 0.10252) (t_loss: 0.10229) (accu: 0.9698)
[epoch : 4] (l_loss: 0.08653) (t_loss: 0.09447) (accu: 0.9722)
[epoch : 5] (l_loss: 0.07687) (t_loss: 0.08935) (accu: 0.9731)
[epoch : 6] (l_loss: 0.07052) (t_loss: 0.09202) (accu: 0.9722)
[epoch : 7] (l_loss: 0.06641) (t_loss: 0.08516) (accu: 0.9740)
[epoch : 8] (l_loss: 0.06369) (t_loss: 0.08264) (accu: 0.9735)
[epoch : 9] (l_loss: 0.06123) (t_loss: 0.08215) (accu: 0.9747)
[epoch : 10] (l_loss: 0.05988) (t_loss: 0.08502) (accu: 0.9748)
[epoch : 11] (l_loss: 0.05862) (t_loss: 0.07962) (accu: 0.9752)
[epoch : 12] (l_loss: 0.05736) (t_loss: 0.07931) (accu: 0.9753)
[epoch : 13] (l_loss: 0.05651) (t_loss: 0.07984) (accu: 0.9742)
[epoch : 14] (l_loss: 0.05634) (t_loss: 0.08182) (accu: 0.9746)
[epoch : 15] (l_loss: 0.05581) (t_loss: 0.07978) (accu: 0.9748)
[epoch : 16] (l_loss: 0.05586) (t_loss: 0.08029) (accu: 0.9761)
[epoch : 17] (l_loss: 0.05534) (t_loss: 0.07837) (accu: 0.9757)
[epoch : 18] (l_loss: 0.05520) (t_loss: 0.07901) (accu: 0.9745)
[epoch : 19] (l_loss: 0.05501) (t_loss: 0.07872) (accu: 0.9740)
[epoch : 20] (l_loss: 0.05497) (t_loss: 0.07779) (accu: 0.9766)
[epoch : 21] (l_loss: 0.05440) (t_loss: 0.07659) (accu: 0.9765)
[epoch : 22] (l_loss: 0.05475) (t_loss: 0.07797) (accu: 0.9749)
[epoch : 23] (l_loss: 0.05420) (t_loss: 0.07990) (accu: 0.9756)
[epoch : 24] (l_loss: 0.05432) (t_loss: 0.08099) (accu: 0.9743)
[epoch : 25] (l_loss: 0.05443) (t_loss: 0.08263) (accu: 0.9756)
[epoch : 26] (l_loss: 0.05401) (t_loss: 0.07815) (accu: 0.9752)
[epoch : 27] (l_loss: 0.05428) (t_loss: 0.07963) (accu: 0.9759)
[epoch : 28] (l_loss: 0.05421) (t_loss: 0.07795) (accu: 0.9755)
[epoch : 29] (l_loss: 0.05435) (t_loss: 0.08274) (accu: 0.9734)
[epoch : 30] (l_loss: 0.05472) (t_loss: 0.07813) (accu: 0.9757)
[epoch : 31] (l_loss: 0.05441) (t_loss: 0.08010) (accu: 0.9745)
[epoch : 32] (l_loss: 0.05440) (t_loss: 0.07662) (accu: 0.9766)
[epoch : 33] (l_loss: 0.05425) (t_loss: 0.07958) (accu: 0.9756)
[epoch : 34] (l_loss: 0.05402) (t_loss: 0.07918) (accu: 0.9743)
[epoch : 35] (l_loss: 0.05431) (t_loss: 0.07882) (accu: 0.9751)
[epoch : 36] (l_loss: 0.05394) (t_loss: 0.07820) (accu: 0.9760)
[epoch : 37] (l_loss: 0.05419) (t_loss: 0.07860) (accu: 0.9762)
[epoch : 38] (l_loss: 0.05382) (t_loss: 0.07994) (accu: 0.9753)
[epoch : 39] (l_loss: 0.05412) (t_loss: 0.07983) (accu: 0.9734)
[epoch : 40] (l_loss: 0.05403) (t_loss: 0.07867) (accu: 0.9756)
[epoch : 41] (l_loss: 0.05426) (t_loss: 0.07977) (accu: 0.9744)
[epoch : 42] (l_loss: 0.05419) (t_loss: 0.07735) (accu: 0.9744)
[epoch : 43] (l_loss: 0.05404) (t_loss: 0.07622) (accu: 0.9753)
[epoch : 44] (l_loss: 0.05432) (t_loss: 0.07778) (accu: 0.9749)
[epoch : 45] (l_loss: 0.05364) (t_loss: 0.07760) (accu: 0.9760)
[epoch : 46] (l_loss: 0.05405) (t_loss: 0.07693) (accu: 0.9766)
[epoch : 47] (l_loss: 0.05396) (t_loss: 0.07683) (accu: 0.9767)
[epoch : 48] (l_loss: 0.05374) (t_loss: 0.07817) (accu: 0.9762)
[epoch : 49] (l_loss: 0.05394) (t_loss: 0.07638) (accu: 0.9744)
[epoch : 50] (l_loss: 0.05369) (t_loss: 0.07957) (accu: 0.9749)
Finish! (Best accu: 0.9767) (Time taken(sec) : 660.36) 


Maximum accuracy per weight remaining
Remaining weight 100.0 %  Epoch 48 Accu 0.9805
Remaining weight 80.03 %  Epoch 8 Accu 0.9796
Remaining weight 64.06 %  Epoch 41 Accu 0.9797
Remaining weight 51.28 %  Epoch 20 Accu 0.9804
Remaining weight 41.05 %  Epoch 27 Accu 0.9812
Remaining weight 32.86 %  Epoch 49 Accu 0.9795
Remaining weight 26.31 %  Epoch 21 Accu 0.9796
Remaining weight 21.06 %  Epoch 29 Accu 0.9788
Remaining weight 16.87 %  Epoch 47 Accu 0.9804
Remaining weight 13.51 %  Epoch 30 Accu 0.9797
Remaining weight 10.82 %  Epoch 30 Accu 0.9810
Remaining weight 8.67 %  Epoch 19 Accu 0.9794
Remaining weight 6.95 %  Epoch 23 Accu 0.9792
Remaining weight 5.57 %  Epoch 17 Accu 0.9800
Remaining weight 4.46 %  Epoch 30 Accu 0.9812
Remaining weight 3.58 %  Epoch 26 Accu 0.9810
Remaining weight 2.87 %  Epoch 42 Accu 0.9815
Remaining weight 2.3 %  Epoch 24 Accu 0.9804
Remaining weight 1.85 %  Epoch 48 Accu 0.9792
Remaining weight 1.48 %  Epoch 34 Accu 0.9792
Remaining weight 1.19 %  Epoch 46 Accu 0.9767
===================================================================== 

Test_Iter (2/5)
------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :        215500 (215500 | 0)          100.00
fc1.weight   :        196000 (196000 | 0)          100.00
fc2.weight   :         18750 (18750 | 0)           100.00
fcout.weight :           750 (750 | 0)             100.00
------------------------------------------------------------
[Prune_iter : (1/21), Remaining weight : 100.0 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.71525) (accu: 0.0808)
[epoch : 1] (l_loss: 0.50511) (t_loss: 0.16527) (accu: 0.9526)
[epoch : 2] (l_loss: 0.13553) (t_loss: 0.11429) (accu: 0.9669)
[epoch : 3] (l_loss: 0.09836) (t_loss: 0.09837) (accu: 0.9687)
[epoch : 4] (l_loss: 0.08099) (t_loss: 0.09096) (accu: 0.9719)
[epoch : 5] (l_loss: 0.07171) (t_loss: 0.08335) (accu: 0.9743)
[epoch : 6] (l_loss: 0.06589) (t_loss: 0.08151) (accu: 0.9756)
[epoch : 7] (l_loss: 0.06268) (t_loss: 0.07990) (accu: 0.9752)
[epoch : 8] (l_loss: 0.06016) (t_loss: 0.08456) (accu: 0.9732)
[epoch : 9] (l_loss: 0.05863) (t_loss: 0.07861) (accu: 0.9753)
[epoch : 10] (l_loss: 0.05782) (t_loss: 0.07733) (accu: 0.9763)
[epoch : 11] (l_loss: 0.05740) (t_loss: 0.07798) (accu: 0.9760)
[epoch : 12] (l_loss: 0.05638) (t_loss: 0.07710) (accu: 0.9770)
[epoch : 13] (l_loss: 0.05597) (t_loss: 0.07667) (accu: 0.9767)
[epoch : 14] (l_loss: 0.05558) (t_loss: 0.07796) (accu: 0.9753)
[epoch : 15] (l_loss: 0.05510) (t_loss: 0.07859) (accu: 0.9757)
[epoch : 16] (l_loss: 0.05549) (t_loss: 0.07867) (accu: 0.9752)
[epoch : 17] (l_loss: 0.05489) (t_loss: 0.07749) (accu: 0.9759)
[epoch : 18] (l_loss: 0.05434) (t_loss: 0.07670) (accu: 0.9749)
[epoch : 19] (l_loss: 0.05429) (t_loss: 0.07884) (accu: 0.9753)
[epoch : 20] (l_loss: 0.05429) (t_loss: 0.07831) (accu: 0.9754)
[epoch : 21] (l_loss: 0.05426) (t_loss: 0.07908) (accu: 0.9746)
[epoch : 22] (l_loss: 0.05425) (t_loss: 0.07928) (accu: 0.9745)
[epoch : 23] (l_loss: 0.05444) (t_loss: 0.07821) (accu: 0.9766)
[epoch : 24] (l_loss: 0.05438) (t_loss: 0.07575) (accu: 0.9761)
[epoch : 25] (l_loss: 0.05457) (t_loss: 0.07988) (accu: 0.9740)
[epoch : 26] (l_loss: 0.05424) (t_loss: 0.07819) (accu: 0.9757)
[epoch : 27] (l_loss: 0.05438) (t_loss: 0.07787) (accu: 0.9762)
[epoch : 28] (l_loss: 0.05434) (t_loss: 0.07922) (accu: 0.9755)
[epoch : 29] (l_loss: 0.05410) (t_loss: 0.07636) (accu: 0.9746)
[epoch : 30] (l_loss: 0.05378) (t_loss: 0.07743) (accu: 0.9756)
[epoch : 31] (l_loss: 0.05432) (t_loss: 0.08186) (accu: 0.9747)
[epoch : 32] (l_loss: 0.05417) (t_loss: 0.08182) (accu: 0.9746)
[epoch : 33] (l_loss: 0.05430) (t_loss: 0.07950) (accu: 0.9750)
[epoch : 34] (l_loss: 0.05355) (t_loss: 0.07834) (accu: 0.9750)
[epoch : 35] (l_loss: 0.05429) (t_loss: 0.07687) (accu: 0.9756)
[epoch : 36] (l_loss: 0.05361) (t_loss: 0.07640) (accu: 0.9760)
[epoch : 37] (l_loss: 0.05378) (t_loss: 0.07611) (accu: 0.9759)
[epoch : 38] (l_loss: 0.05373) (t_loss: 0.07704) (accu: 0.9753)
[epoch : 39] (l_loss: 0.05388) (t_loss: 0.07814) (accu: 0.9759)
[epoch : 40] (l_loss: 0.05391) (t_loss: 0.08001) (accu: 0.9749)
[epoch : 41] (l_loss: 0.05387) (t_loss: 0.07689) (accu: 0.9759)
[epoch : 42] (l_loss: 0.05401) (t_loss: 0.07442) (accu: 0.9773)
[epoch : 43] (l_loss: 0.05368) (t_loss: 0.07758) (accu: 0.9756)
[epoch : 44] (l_loss: 0.05386) (t_loss: 0.07888) (accu: 0.9755)
[epoch : 45] (l_loss: 0.05454) (t_loss: 0.07715) (accu: 0.9759)
[epoch : 46] (l_loss: 0.05398) (t_loss: 0.07888) (accu: 0.9749)
[epoch : 47] (l_loss: 0.05388) (t_loss: 0.07807) (accu: 0.9756)
[epoch : 48] (l_loss: 0.05414) (t_loss: 0.07663) (accu: 0.9756)
[epoch : 49] (l_loss: 0.05363) (t_loss: 0.07757) (accu: 0.9752)
[epoch : 50] (l_loss: 0.05397) (t_loss: 0.07737) (accu: 0.9761)
Finish! (Best accu: 0.9773) (Time taken(sec) : 660.48) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (172475 | 43025)         80.03
fc1.weight   :      196000 (156800 | 39200)         80.00
fc2.weight   :        18750 (15000 | 3750)          80.00
fcout.weight :           750 (675 | 75)             90.00
------------------------------------------------------------
[Prune_iter : (2/21), Remaining weight : 80.03 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.48462) (accu: 0.1011)
[epoch : 1] (l_loss: 0.49880) (t_loss: 0.16561) (accu: 0.9513)
[epoch : 2] (l_loss: 0.13688) (t_loss: 0.11729) (accu: 0.9634)
[epoch : 3] (l_loss: 0.10128) (t_loss: 0.09957) (accu: 0.9699)
[epoch : 4] (l_loss: 0.08508) (t_loss: 0.09751) (accu: 0.9699)
[epoch : 5] (l_loss: 0.07637) (t_loss: 0.08768) (accu: 0.9731)
[epoch : 6] (l_loss: 0.07081) (t_loss: 0.08764) (accu: 0.9732)
[epoch : 7] (l_loss: 0.06632) (t_loss: 0.08569) (accu: 0.9727)
[epoch : 8] (l_loss: 0.06365) (t_loss: 0.08163) (accu: 0.9748)
[epoch : 9] (l_loss: 0.06257) (t_loss: 0.08356) (accu: 0.9740)
[epoch : 10] (l_loss: 0.06109) (t_loss: 0.08307) (accu: 0.9743)
[epoch : 11] (l_loss: 0.06004) (t_loss: 0.08404) (accu: 0.9751)
[epoch : 12] (l_loss: 0.05960) (t_loss: 0.08365) (accu: 0.9746)
[epoch : 13] (l_loss: 0.05907) (t_loss: 0.08244) (accu: 0.9740)
[epoch : 14] (l_loss: 0.05836) (t_loss: 0.08010) (accu: 0.9748)
[epoch : 15] (l_loss: 0.05830) (t_loss: 0.08321) (accu: 0.9750)
[epoch : 16] (l_loss: 0.05776) (t_loss: 0.08239) (accu: 0.9749)
[epoch : 17] (l_loss: 0.05764) (t_loss: 0.08172) (accu: 0.9745)
[epoch : 18] (l_loss: 0.05771) (t_loss: 0.08019) (accu: 0.9749)
[epoch : 19] (l_loss: 0.05736) (t_loss: 0.07893) (accu: 0.9751)
[epoch : 20] (l_loss: 0.05725) (t_loss: 0.08016) (accu: 0.9759)
[epoch : 21] (l_loss: 0.05699) (t_loss: 0.08639) (accu: 0.9729)
[epoch : 22] (l_loss: 0.05701) (t_loss: 0.07924) (accu: 0.9747)
[epoch : 23] (l_loss: 0.05665) (t_loss: 0.08213) (accu: 0.9745)
[epoch : 24] (l_loss: 0.05680) (t_loss: 0.08048) (accu: 0.9749)
[epoch : 25] (l_loss: 0.05678) (t_loss: 0.07963) (accu: 0.9742)
[epoch : 26] (l_loss: 0.05668) (t_loss: 0.08114) (accu: 0.9747)
[epoch : 27] (l_loss: 0.05644) (t_loss: 0.08244) (accu: 0.9754)
[epoch : 28] (l_loss: 0.05635) (t_loss: 0.08167) (accu: 0.9739)
[epoch : 29] (l_loss: 0.05647) (t_loss: 0.08280) (accu: 0.9740)
[epoch : 30] (l_loss: 0.05665) (t_loss: 0.07916) (accu: 0.9744)
[epoch : 31] (l_loss: 0.05662) (t_loss: 0.07962) (accu: 0.9753)
[epoch : 32] (l_loss: 0.05670) (t_loss: 0.07880) (accu: 0.9760)
[epoch : 33] (l_loss: 0.05659) (t_loss: 0.07985) (accu: 0.9753)
[epoch : 34] (l_loss: 0.05623) (t_loss: 0.08048) (accu: 0.9754)
[epoch : 35] (l_loss: 0.05677) (t_loss: 0.07958) (accu: 0.9755)
[epoch : 36] (l_loss: 0.05640) (t_loss: 0.08005) (accu: 0.9748)
[epoch : 37] (l_loss: 0.05677) (t_loss: 0.07998) (accu: 0.9753)
[epoch : 38] (l_loss: 0.05621) (t_loss: 0.08091) (accu: 0.9750)
[epoch : 39] (l_loss: 0.05621) (t_loss: 0.08064) (accu: 0.9748)
[epoch : 40] (l_loss: 0.05666) (t_loss: 0.08155) (accu: 0.9745)
[epoch : 41] (l_loss: 0.05626) (t_loss: 0.08251) (accu: 0.9730)
[epoch : 42] (l_loss: 0.05641) (t_loss: 0.08157) (accu: 0.9754)
[epoch : 43] (l_loss: 0.05654) (t_loss: 0.08055) (accu: 0.9743)
[epoch : 44] (l_loss: 0.05651) (t_loss: 0.07860) (accu: 0.9743)
[epoch : 45] (l_loss: 0.05598) (t_loss: 0.08000) (accu: 0.9756)
[epoch : 46] (l_loss: 0.05613) (t_loss: 0.08105) (accu: 0.9746)
[epoch : 47] (l_loss: 0.05598) (t_loss: 0.08144) (accu: 0.9738)
[epoch : 48] (l_loss: 0.05623) (t_loss: 0.07930) (accu: 0.9755)
[epoch : 49] (l_loss: 0.05575) (t_loss: 0.08014) (accu: 0.9745)
[epoch : 50] (l_loss: 0.05620) (t_loss: 0.08009) (accu: 0.9746)
Finish! (Best accu: 0.9760) (Time taken(sec) : 647.25) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (138048 | 77452)         64.06
fc1.weight   :      196000 (125440 | 70560)         64.00
fc2.weight   :        18750 (12000 | 6750)          64.00
fcout.weight :          750 (608 | 142)             81.07
------------------------------------------------------------
[Prune_iter : (3/21), Remaining weight : 64.06 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.37860) (accu: 0.0974)
[epoch : 1] (l_loss: 0.48122) (t_loss: 0.15827) (accu: 0.9558)
[epoch : 2] (l_loss: 0.12833) (t_loss: 0.11327) (accu: 0.9660)
[epoch : 3] (l_loss: 0.09439) (t_loss: 0.09606) (accu: 0.9720)
[epoch : 4] (l_loss: 0.07749) (t_loss: 0.08580) (accu: 0.9747)
[epoch : 5] (l_loss: 0.06911) (t_loss: 0.08323) (accu: 0.9750)
[epoch : 6] (l_loss: 0.06377) (t_loss: 0.07939) (accu: 0.9759)
[epoch : 7] (l_loss: 0.06029) (t_loss: 0.07917) (accu: 0.9749)
[epoch : 8] (l_loss: 0.05777) (t_loss: 0.07971) (accu: 0.9755)
[epoch : 9] (l_loss: 0.05603) (t_loss: 0.07813) (accu: 0.9758)
[epoch : 10] (l_loss: 0.05539) (t_loss: 0.07718) (accu: 0.9764)
[epoch : 11] (l_loss: 0.05485) (t_loss: 0.07838) (accu: 0.9748)
[epoch : 12] (l_loss: 0.05347) (t_loss: 0.08088) (accu: 0.9756)
[epoch : 13] (l_loss: 0.05322) (t_loss: 0.08030) (accu: 0.9769)
[epoch : 14] (l_loss: 0.05311) (t_loss: 0.07447) (accu: 0.9763)
[epoch : 15] (l_loss: 0.05295) (t_loss: 0.07365) (accu: 0.9779)
[epoch : 16] (l_loss: 0.05249) (t_loss: 0.07802) (accu: 0.9768)
[epoch : 17] (l_loss: 0.05272) (t_loss: 0.07764) (accu: 0.9761)
[epoch : 18] (l_loss: 0.05255) (t_loss: 0.07651) (accu: 0.9765)
[epoch : 19] (l_loss: 0.05246) (t_loss: 0.07592) (accu: 0.9765)
[epoch : 20] (l_loss: 0.05247) (t_loss: 0.07714) (accu: 0.9770)
[epoch : 21] (l_loss: 0.05201) (t_loss: 0.07725) (accu: 0.9763)
[epoch : 22] (l_loss: 0.05253) (t_loss: 0.07484) (accu: 0.9763)
[epoch : 23] (l_loss: 0.05196) (t_loss: 0.07728) (accu: 0.9768)
[epoch : 24] (l_loss: 0.05193) (t_loss: 0.07485) (accu: 0.9761)
[epoch : 25] (l_loss: 0.05204) (t_loss: 0.07508) (accu: 0.9779)
[epoch : 26] (l_loss: 0.05199) (t_loss: 0.07855) (accu: 0.9760)
[epoch : 27] (l_loss: 0.05224) (t_loss: 0.07665) (accu: 0.9757)
[epoch : 28] (l_loss: 0.05211) (t_loss: 0.07826) (accu: 0.9760)
[epoch : 29] (l_loss: 0.05185) (t_loss: 0.07660) (accu: 0.9758)
[epoch : 30] (l_loss: 0.05154) (t_loss: 0.07665) (accu: 0.9776)
[epoch : 31] (l_loss: 0.05200) (t_loss: 0.07813) (accu: 0.9770)
[epoch : 32] (l_loss: 0.05175) (t_loss: 0.07632) (accu: 0.9756)
[epoch : 33] (l_loss: 0.05163) (t_loss: 0.07537) (accu: 0.9780)
[epoch : 34] (l_loss: 0.05191) (t_loss: 0.07673) (accu: 0.9764)
[epoch : 35] (l_loss: 0.05218) (t_loss: 0.07701) (accu: 0.9757)
[epoch : 36] (l_loss: 0.05165) (t_loss: 0.07618) (accu: 0.9766)
[epoch : 37] (l_loss: 0.05197) (t_loss: 0.07531) (accu: 0.9774)
[epoch : 38] (l_loss: 0.05182) (t_loss: 0.07905) (accu: 0.9760)
[epoch : 39] (l_loss: 0.05150) (t_loss: 0.07657) (accu: 0.9774)
[epoch : 40] (l_loss: 0.05174) (t_loss: 0.07454) (accu: 0.9770)
[epoch : 41] (l_loss: 0.05167) (t_loss: 0.07670) (accu: 0.9781)
[epoch : 42] (l_loss: 0.05201) (t_loss: 0.07985) (accu: 0.9748)
[epoch : 43] (l_loss: 0.05190) (t_loss: 0.07633) (accu: 0.9757)
[epoch : 44] (l_loss: 0.05167) (t_loss: 0.07664) (accu: 0.9770)
[epoch : 45] (l_loss: 0.05163) (t_loss: 0.07679) (accu: 0.9756)
[epoch : 46] (l_loss: 0.05208) (t_loss: 0.07563) (accu: 0.9767)
[epoch : 47] (l_loss: 0.05164) (t_loss: 0.07556) (accu: 0.9777)
[epoch : 48] (l_loss: 0.05198) (t_loss: 0.07691) (accu: 0.9768)
[epoch : 49] (l_loss: 0.05198) (t_loss: 0.07603) (accu: 0.9767)
[epoch : 50] (l_loss: 0.05203) (t_loss: 0.07837) (accu: 0.9760)
Finish! (Best accu: 0.9781) (Time taken(sec) : 646.23) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (110499 | 105001)        51.28
fc1.weight   :      196000 (100352 | 95648)         51.20
fc2.weight   :        18750 (9600 | 9150)           51.20
fcout.weight :          750 (547 | 203)             72.93
------------------------------------------------------------
[Prune_iter : (4/21), Remaining weight : 51.28 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.33947) (accu: 0.0978)
[epoch : 1] (l_loss: 0.48682) (t_loss: 0.15975) (accu: 0.9539)
[epoch : 2] (l_loss: 0.13265) (t_loss: 0.11585) (accu: 0.9653)
[epoch : 3] (l_loss: 0.09800) (t_loss: 0.10063) (accu: 0.9708)
[epoch : 4] (l_loss: 0.08279) (t_loss: 0.09189) (accu: 0.9710)
[epoch : 5] (l_loss: 0.07317) (t_loss: 0.08648) (accu: 0.9739)
[epoch : 6] (l_loss: 0.06681) (t_loss: 0.08531) (accu: 0.9744)
[epoch : 7] (l_loss: 0.06269) (t_loss: 0.08285) (accu: 0.9751)
[epoch : 8] (l_loss: 0.05969) (t_loss: 0.08118) (accu: 0.9771)
[epoch : 9] (l_loss: 0.05782) (t_loss: 0.07792) (accu: 0.9758)
[epoch : 10] (l_loss: 0.05634) (t_loss: 0.07941) (accu: 0.9762)
[epoch : 11] (l_loss: 0.05570) (t_loss: 0.07794) (accu: 0.9770)
[epoch : 12] (l_loss: 0.05495) (t_loss: 0.07744) (accu: 0.9768)
[epoch : 13] (l_loss: 0.05469) (t_loss: 0.07961) (accu: 0.9765)
[epoch : 14] (l_loss: 0.05406) (t_loss: 0.07822) (accu: 0.9764)
[epoch : 15] (l_loss: 0.05365) (t_loss: 0.07794) (accu: 0.9760)
[epoch : 16] (l_loss: 0.05378) (t_loss: 0.07704) (accu: 0.9766)
[epoch : 17] (l_loss: 0.05338) (t_loss: 0.07939) (accu: 0.9762)
[epoch : 18] (l_loss: 0.05319) (t_loss: 0.07715) (accu: 0.9773)
[epoch : 19] (l_loss: 0.05313) (t_loss: 0.07579) (accu: 0.9763)
[epoch : 20] (l_loss: 0.05330) (t_loss: 0.07783) (accu: 0.9767)
[epoch : 21] (l_loss: 0.05314) (t_loss: 0.07795) (accu: 0.9772)
[epoch : 22] (l_loss: 0.05285) (t_loss: 0.07762) (accu: 0.9780)
[epoch : 23] (l_loss: 0.05290) (t_loss: 0.07818) (accu: 0.9756)
[epoch : 24] (l_loss: 0.05288) (t_loss: 0.08027) (accu: 0.9765)
[epoch : 25] (l_loss: 0.05316) (t_loss: 0.07555) (accu: 0.9768)
[epoch : 26] (l_loss: 0.05295) (t_loss: 0.07820) (accu: 0.9764)
[epoch : 27] (l_loss: 0.05306) (t_loss: 0.07618) (accu: 0.9763)
[epoch : 28] (l_loss: 0.05260) (t_loss: 0.07775) (accu: 0.9766)
[epoch : 29] (l_loss: 0.05269) (t_loss: 0.07883) (accu: 0.9761)
[epoch : 30] (l_loss: 0.05213) (t_loss: 0.07813) (accu: 0.9764)
[epoch : 31] (l_loss: 0.05250) (t_loss: 0.07611) (accu: 0.9773)
[epoch : 32] (l_loss: 0.05238) (t_loss: 0.07727) (accu: 0.9757)
[epoch : 33] (l_loss: 0.05276) (t_loss: 0.07686) (accu: 0.9754)
[epoch : 34] (l_loss: 0.05235) (t_loss: 0.07766) (accu: 0.9756)
[epoch : 35] (l_loss: 0.05226) (t_loss: 0.07674) (accu: 0.9760)
[epoch : 36] (l_loss: 0.05265) (t_loss: 0.07683) (accu: 0.9768)
[epoch : 37] (l_loss: 0.05248) (t_loss: 0.07862) (accu: 0.9767)
[epoch : 38] (l_loss: 0.05267) (t_loss: 0.07705) (accu: 0.9775)
[epoch : 39] (l_loss: 0.05258) (t_loss: 0.07931) (accu: 0.9767)
[epoch : 40] (l_loss: 0.05257) (t_loss: 0.07592) (accu: 0.9783)
[epoch : 41] (l_loss: 0.05241) (t_loss: 0.07578) (accu: 0.9783)
[epoch : 42] (l_loss: 0.05259) (t_loss: 0.07689) (accu: 0.9768)
[epoch : 43] (l_loss: 0.05211) (t_loss: 0.07834) (accu: 0.9750)
[epoch : 44] (l_loss: 0.05180) (t_loss: 0.07677) (accu: 0.9765)
[epoch : 45] (l_loss: 0.05197) (t_loss: 0.07903) (accu: 0.9763)
[epoch : 46] (l_loss: 0.05163) (t_loss: 0.07781) (accu: 0.9763)
[epoch : 47] (l_loss: 0.05168) (t_loss: 0.08176) (accu: 0.9747)
[epoch : 48] (l_loss: 0.05151) (t_loss: 0.07487) (accu: 0.9768)
[epoch : 49] (l_loss: 0.05158) (t_loss: 0.07784) (accu: 0.9755)
[epoch : 50] (l_loss: 0.05167) (t_loss: 0.07576) (accu: 0.9764)
Finish! (Best accu: 0.9783) (Time taken(sec) : 666.72) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (88454 | 127046)         41.05
fc1.weight   :      196000 (80282 | 115718)         40.96
fc2.weight   :        18750 (7680 | 11070)          40.96
fcout.weight :          750 (492 | 258)             65.60
------------------------------------------------------------
[Prune_iter : (5/21), Remaining weight : 41.05 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.34346) (accu: 0.1106)
[epoch : 1] (l_loss: 0.48233) (t_loss: 0.16844) (accu: 0.9520)
[epoch : 2] (l_loss: 0.13935) (t_loss: 0.11949) (accu: 0.9630)
[epoch : 3] (l_loss: 0.10409) (t_loss: 0.10240) (accu: 0.9689)
[epoch : 4] (l_loss: 0.08813) (t_loss: 0.09901) (accu: 0.9704)
[epoch : 5] (l_loss: 0.07898) (t_loss: 0.09066) (accu: 0.9718)
[epoch : 6] (l_loss: 0.07297) (t_loss: 0.08836) (accu: 0.9733)
[epoch : 7] (l_loss: 0.06847) (t_loss: 0.08461) (accu: 0.9748)
[epoch : 8] (l_loss: 0.06541) (t_loss: 0.08720) (accu: 0.9741)
[epoch : 9] (l_loss: 0.06317) (t_loss: 0.08444) (accu: 0.9746)
[epoch : 10] (l_loss: 0.06227) (t_loss: 0.08372) (accu: 0.9755)
[epoch : 11] (l_loss: 0.06073) (t_loss: 0.08352) (accu: 0.9739)
[epoch : 12] (l_loss: 0.06027) (t_loss: 0.08372) (accu: 0.9740)
[epoch : 13] (l_loss: 0.05912) (t_loss: 0.08462) (accu: 0.9755)
[epoch : 14] (l_loss: 0.05911) (t_loss: 0.08218) (accu: 0.9755)
[epoch : 15] (l_loss: 0.05826) (t_loss: 0.08392) (accu: 0.9758)
[epoch : 16] (l_loss: 0.05835) (t_loss: 0.08480) (accu: 0.9732)
[epoch : 17] (l_loss: 0.05837) (t_loss: 0.08305) (accu: 0.9746)
[epoch : 18] (l_loss: 0.05756) (t_loss: 0.08449) (accu: 0.9748)
[epoch : 19] (l_loss: 0.05838) (t_loss: 0.08386) (accu: 0.9754)
[epoch : 20] (l_loss: 0.05799) (t_loss: 0.08206) (accu: 0.9752)
[epoch : 21] (l_loss: 0.05682) (t_loss: 0.08135) (accu: 0.9749)
[epoch : 22] (l_loss: 0.05678) (t_loss: 0.08241) (accu: 0.9748)
[epoch : 23] (l_loss: 0.05599) (t_loss: 0.08081) (accu: 0.9748)
[epoch : 24] (l_loss: 0.05508) (t_loss: 0.08119) (accu: 0.9758)
[epoch : 25] (l_loss: 0.05491) (t_loss: 0.08014) (accu: 0.9752)
[epoch : 26] (l_loss: 0.05558) (t_loss: 0.08083) (accu: 0.9756)
[epoch : 27] (l_loss: 0.05490) (t_loss: 0.08183) (accu: 0.9748)
[epoch : 28] (l_loss: 0.05449) (t_loss: 0.08262) (accu: 0.9757)
[epoch : 29] (l_loss: 0.05489) (t_loss: 0.08141) (accu: 0.9756)
[epoch : 30] (l_loss: 0.05488) (t_loss: 0.07801) (accu: 0.9756)
[epoch : 31] (l_loss: 0.05505) (t_loss: 0.07882) (accu: 0.9758)
[epoch : 32] (l_loss: 0.05475) (t_loss: 0.08110) (accu: 0.9757)
[epoch : 33] (l_loss: 0.05458) (t_loss: 0.07857) (accu: 0.9755)
[epoch : 34] (l_loss: 0.05453) (t_loss: 0.07878) (accu: 0.9766)
[epoch : 35] (l_loss: 0.05479) (t_loss: 0.07867) (accu: 0.9758)
[epoch : 36] (l_loss: 0.05476) (t_loss: 0.07950) (accu: 0.9765)
[epoch : 37] (l_loss: 0.05491) (t_loss: 0.08153) (accu: 0.9749)
[epoch : 38] (l_loss: 0.05522) (t_loss: 0.07995) (accu: 0.9760)
[epoch : 39] (l_loss: 0.05484) (t_loss: 0.07965) (accu: 0.9756)
[epoch : 40] (l_loss: 0.05465) (t_loss: 0.08323) (accu: 0.9748)
[epoch : 41] (l_loss: 0.05466) (t_loss: 0.08304) (accu: 0.9747)
[epoch : 42] (l_loss: 0.05468) (t_loss: 0.07926) (accu: 0.9762)
[epoch : 43] (l_loss: 0.05445) (t_loss: 0.08035) (accu: 0.9749)
[epoch : 44] (l_loss: 0.05469) (t_loss: 0.07869) (accu: 0.9754)
[epoch : 45] (l_loss: 0.05452) (t_loss: 0.08000) (accu: 0.9754)
[epoch : 46] (l_loss: 0.05487) (t_loss: 0.07706) (accu: 0.9771)
[epoch : 47] (l_loss: 0.05469) (t_loss: 0.08028) (accu: 0.9751)
[epoch : 48] (l_loss: 0.05462) (t_loss: 0.08094) (accu: 0.9737)
[epoch : 49] (l_loss: 0.05455) (t_loss: 0.07974) (accu: 0.9743)
[epoch : 50] (l_loss: 0.05505) (t_loss: 0.07983) (accu: 0.9759)
Finish! (Best accu: 0.9771) (Time taken(sec) : 645.97) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (70812 | 144688)         32.86
fc1.weight   :      196000 (64225 | 131775)         32.77
fc2.weight   :        18750 (6144 | 12606)          32.77
fcout.weight :          750 (443 | 307)             59.07
------------------------------------------------------------
[Prune_iter : (6/21), Remaining weight : 32.86 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.32476) (accu: 0.1197)
[epoch : 1] (l_loss: 0.48561) (t_loss: 0.17363) (accu: 0.9515)
[epoch : 2] (l_loss: 0.14369) (t_loss: 0.12200) (accu: 0.9647)
[epoch : 3] (l_loss: 0.10613) (t_loss: 0.10355) (accu: 0.9684)
[epoch : 4] (l_loss: 0.08934) (t_loss: 0.09635) (accu: 0.9697)
[epoch : 5] (l_loss: 0.07914) (t_loss: 0.09238) (accu: 0.9710)
[epoch : 6] (l_loss: 0.07267) (t_loss: 0.08795) (accu: 0.9729)
[epoch : 7] (l_loss: 0.06746) (t_loss: 0.08734) (accu: 0.9731)
[epoch : 8] (l_loss: 0.06303) (t_loss: 0.08210) (accu: 0.9744)
[epoch : 9] (l_loss: 0.06012) (t_loss: 0.08283) (accu: 0.9765)
[epoch : 10] (l_loss: 0.05800) (t_loss: 0.08064) (accu: 0.9762)
[epoch : 11] (l_loss: 0.05695) (t_loss: 0.07742) (accu: 0.9762)
[epoch : 12] (l_loss: 0.05537) (t_loss: 0.07717) (accu: 0.9763)
[epoch : 13] (l_loss: 0.05478) (t_loss: 0.07766) (accu: 0.9762)
[epoch : 14] (l_loss: 0.05422) (t_loss: 0.07913) (accu: 0.9751)
[epoch : 15] (l_loss: 0.05428) (t_loss: 0.07812) (accu: 0.9780)
[epoch : 16] (l_loss: 0.05422) (t_loss: 0.07691) (accu: 0.9759)
[epoch : 17] (l_loss: 0.05353) (t_loss: 0.07993) (accu: 0.9760)
[epoch : 18] (l_loss: 0.05336) (t_loss: 0.07530) (accu: 0.9764)
[epoch : 19] (l_loss: 0.05296) (t_loss: 0.07734) (accu: 0.9764)
[epoch : 20] (l_loss: 0.05318) (t_loss: 0.07653) (accu: 0.9763)
[epoch : 21] (l_loss: 0.05315) (t_loss: 0.07639) (accu: 0.9770)
[epoch : 22] (l_loss: 0.05310) (t_loss: 0.07546) (accu: 0.9775)
[epoch : 23] (l_loss: 0.05289) (t_loss: 0.08437) (accu: 0.9728)
[epoch : 24] (l_loss: 0.05292) (t_loss: 0.08526) (accu: 0.9744)
[epoch : 25] (l_loss: 0.05281) (t_loss: 0.07438) (accu: 0.9777)
[epoch : 26] (l_loss: 0.05270) (t_loss: 0.07578) (accu: 0.9769)
[epoch : 27] (l_loss: 0.05246) (t_loss: 0.07660) (accu: 0.9765)
[epoch : 28] (l_loss: 0.05307) (t_loss: 0.07788) (accu: 0.9757)
[epoch : 29] (l_loss: 0.05250) (t_loss: 0.07774) (accu: 0.9761)
[epoch : 30] (l_loss: 0.05284) (t_loss: 0.07767) (accu: 0.9767)
[epoch : 31] (l_loss: 0.05258) (t_loss: 0.07818) (accu: 0.9770)
[epoch : 32] (l_loss: 0.05275) (t_loss: 0.07538) (accu: 0.9781)
[epoch : 33] (l_loss: 0.05258) (t_loss: 0.07841) (accu: 0.9757)
[epoch : 34] (l_loss: 0.05289) (t_loss: 0.07535) (accu: 0.9766)
[epoch : 35] (l_loss: 0.05289) (t_loss: 0.07431) (accu: 0.9763)
[epoch : 36] (l_loss: 0.05217) (t_loss: 0.07983) (accu: 0.9766)
[epoch : 37] (l_loss: 0.05307) (t_loss: 0.07834) (accu: 0.9764)
[epoch : 38] (l_loss: 0.05246) (t_loss: 0.07609) (accu: 0.9771)
[epoch : 39] (l_loss: 0.05270) (t_loss: 0.07612) (accu: 0.9773)
[epoch : 40] (l_loss: 0.05277) (t_loss: 0.07742) (accu: 0.9760)
[epoch : 41] (l_loss: 0.05202) (t_loss: 0.07816) (accu: 0.9765)
[epoch : 42] (l_loss: 0.05272) (t_loss: 0.07818) (accu: 0.9765)
[epoch : 43] (l_loss: 0.05230) (t_loss: 0.07712) (accu: 0.9759)
[epoch : 44] (l_loss: 0.05264) (t_loss: 0.07555) (accu: 0.9773)
[epoch : 45] (l_loss: 0.05254) (t_loss: 0.08039) (accu: 0.9752)
[epoch : 46] (l_loss: 0.05266) (t_loss: 0.07774) (accu: 0.9755)
[epoch : 47] (l_loss: 0.05240) (t_loss: 0.07867) (accu: 0.9762)
[epoch : 48] (l_loss: 0.05247) (t_loss: 0.07673) (accu: 0.9764)
[epoch : 49] (l_loss: 0.05239) (t_loss: 0.07447) (accu: 0.9759)
[epoch : 50] (l_loss: 0.05269) (t_loss: 0.07848) (accu: 0.9753)
Finish! (Best accu: 0.9781) (Time taken(sec) : 675.74) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (56694 | 158806)         26.31
fc1.weight   :      196000 (51380 | 144620)         26.21
fc2.weight   :        18750 (4915 | 13835)          26.21
fcout.weight :          750 (399 | 351)             53.20
------------------------------------------------------------
[Prune_iter : (7/21), Remaining weight : 26.31 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29523) (accu: 0.1425)
[epoch : 1] (l_loss: 0.46456) (t_loss: 0.16500) (accu: 0.9518)
[epoch : 2] (l_loss: 0.13833) (t_loss: 0.12066) (accu: 0.9643)
[epoch : 3] (l_loss: 0.10589) (t_loss: 0.10525) (accu: 0.9674)
[epoch : 4] (l_loss: 0.08742) (t_loss: 0.09581) (accu: 0.9701)
[epoch : 5] (l_loss: 0.07590) (t_loss: 0.08573) (accu: 0.9744)
[epoch : 6] (l_loss: 0.06841) (t_loss: 0.08414) (accu: 0.9747)
[epoch : 7] (l_loss: 0.06357) (t_loss: 0.08081) (accu: 0.9754)
[epoch : 8] (l_loss: 0.06031) (t_loss: 0.08246) (accu: 0.9759)
[epoch : 9] (l_loss: 0.05893) (t_loss: 0.07801) (accu: 0.9764)
[epoch : 10] (l_loss: 0.05742) (t_loss: 0.07971) (accu: 0.9748)
[epoch : 11] (l_loss: 0.05623) (t_loss: 0.07970) (accu: 0.9752)
[epoch : 12] (l_loss: 0.05523) (t_loss: 0.07817) (accu: 0.9768)
[epoch : 13] (l_loss: 0.05514) (t_loss: 0.07875) (accu: 0.9775)
[epoch : 14] (l_loss: 0.05432) (t_loss: 0.07986) (accu: 0.9762)
[epoch : 15] (l_loss: 0.05445) (t_loss: 0.07703) (accu: 0.9766)
[epoch : 16] (l_loss: 0.05445) (t_loss: 0.07965) (accu: 0.9750)
[epoch : 17] (l_loss: 0.05393) (t_loss: 0.07801) (accu: 0.9778)
[epoch : 18] (l_loss: 0.05378) (t_loss: 0.07685) (accu: 0.9760)
[epoch : 19] (l_loss: 0.05349) (t_loss: 0.07821) (accu: 0.9763)
[epoch : 20] (l_loss: 0.05328) (t_loss: 0.07591) (accu: 0.9769)
[epoch : 21] (l_loss: 0.05324) (t_loss: 0.08020) (accu: 0.9744)
[epoch : 22] (l_loss: 0.05351) (t_loss: 0.07647) (accu: 0.9767)
[epoch : 23] (l_loss: 0.05315) (t_loss: 0.07684) (accu: 0.9752)
[epoch : 24] (l_loss: 0.05342) (t_loss: 0.08017) (accu: 0.9755)
[epoch : 25] (l_loss: 0.05291) (t_loss: 0.07879) (accu: 0.9772)
[epoch : 26] (l_loss: 0.05280) (t_loss: 0.07660) (accu: 0.9767)
[epoch : 27] (l_loss: 0.05317) (t_loss: 0.07783) (accu: 0.9765)
[epoch : 28] (l_loss: 0.05367) (t_loss: 0.07677) (accu: 0.9755)
[epoch : 29] (l_loss: 0.05297) (t_loss: 0.07976) (accu: 0.9758)
[epoch : 30] (l_loss: 0.05282) (t_loss: 0.07607) (accu: 0.9763)
[epoch : 31] (l_loss: 0.05306) (t_loss: 0.07728) (accu: 0.9761)
[epoch : 32] (l_loss: 0.05326) (t_loss: 0.08266) (accu: 0.9756)
[epoch : 33] (l_loss: 0.05320) (t_loss: 0.07881) (accu: 0.9753)
[epoch : 34] (l_loss: 0.05295) (t_loss: 0.07632) (accu: 0.9757)
[epoch : 35] (l_loss: 0.05308) (t_loss: 0.07953) (accu: 0.9743)
[epoch : 36] (l_loss: 0.05316) (t_loss: 0.08057) (accu: 0.9742)
[epoch : 37] (l_loss: 0.05282) (t_loss: 0.07682) (accu: 0.9761)
[epoch : 38] (l_loss: 0.05308) (t_loss: 0.07863) (accu: 0.9761)
[epoch : 39] (l_loss: 0.05285) (t_loss: 0.07763) (accu: 0.9768)
[epoch : 40] (l_loss: 0.05340) (t_loss: 0.07616) (accu: 0.9764)
[epoch : 41] (l_loss: 0.05284) (t_loss: 0.07708) (accu: 0.9751)
[epoch : 42] (l_loss: 0.05268) (t_loss: 0.07680) (accu: 0.9766)
[epoch : 43] (l_loss: 0.05313) (t_loss: 0.07876) (accu: 0.9764)
[epoch : 44] (l_loss: 0.05309) (t_loss: 0.07803) (accu: 0.9749)
[epoch : 45] (l_loss: 0.05290) (t_loss: 0.07720) (accu: 0.9761)
[epoch : 46] (l_loss: 0.05291) (t_loss: 0.07769) (accu: 0.9765)
[epoch : 47] (l_loss: 0.05288) (t_loss: 0.08013) (accu: 0.9754)
[epoch : 48] (l_loss: 0.05273) (t_loss: 0.07890) (accu: 0.9767)
[epoch : 49] (l_loss: 0.05266) (t_loss: 0.08164) (accu: 0.9738)
[epoch : 50] (l_loss: 0.05333) (t_loss: 0.07812) (accu: 0.9772)
Finish! (Best accu: 0.9778) (Time taken(sec) : 681.80) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (45395 | 170105)         21.06
fc1.weight   :      196000 (41104 | 154896)         20.97
fc2.weight   :        18750 (3932 | 14818)          20.97
fcout.weight :          750 (359 | 391)             47.87
------------------------------------------------------------
[Prune_iter : (8/21), Remaining weight : 21.06 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.31065) (accu: 0.1322)
[epoch : 1] (l_loss: 0.46428) (t_loss: 0.16388) (accu: 0.9541)
[epoch : 2] (l_loss: 0.13645) (t_loss: 0.11863) (accu: 0.9643)
[epoch : 3] (l_loss: 0.10317) (t_loss: 0.10475) (accu: 0.9686)
[epoch : 4] (l_loss: 0.08541) (t_loss: 0.09386) (accu: 0.9708)
[epoch : 5] (l_loss: 0.07445) (t_loss: 0.08918) (accu: 0.9720)
[epoch : 6] (l_loss: 0.06751) (t_loss: 0.08046) (accu: 0.9749)
[epoch : 7] (l_loss: 0.06223) (t_loss: 0.07891) (accu: 0.9773)
[epoch : 8] (l_loss: 0.05903) (t_loss: 0.07851) (accu: 0.9760)
[epoch : 9] (l_loss: 0.05702) (t_loss: 0.07497) (accu: 0.9769)
[epoch : 10] (l_loss: 0.05586) (t_loss: 0.07642) (accu: 0.9774)
[epoch : 11] (l_loss: 0.05517) (t_loss: 0.07923) (accu: 0.9758)
[epoch : 12] (l_loss: 0.05385) (t_loss: 0.07643) (accu: 0.9771)
[epoch : 13] (l_loss: 0.05306) (t_loss: 0.07574) (accu: 0.9756)
[epoch : 14] (l_loss: 0.05271) (t_loss: 0.07632) (accu: 0.9773)
[epoch : 15] (l_loss: 0.05258) (t_loss: 0.07621) (accu: 0.9769)
[epoch : 16] (l_loss: 0.05247) (t_loss: 0.07679) (accu: 0.9762)
[epoch : 17] (l_loss: 0.05251) (t_loss: 0.07648) (accu: 0.9762)
[epoch : 18] (l_loss: 0.05218) (t_loss: 0.07561) (accu: 0.9764)
[epoch : 19] (l_loss: 0.05198) (t_loss: 0.07580) (accu: 0.9769)
[epoch : 20] (l_loss: 0.05184) (t_loss: 0.07784) (accu: 0.9767)
[epoch : 21] (l_loss: 0.05213) (t_loss: 0.07705) (accu: 0.9750)
[epoch : 22] (l_loss: 0.05194) (t_loss: 0.07424) (accu: 0.9773)
[epoch : 23] (l_loss: 0.05130) (t_loss: 0.07659) (accu: 0.9764)
[epoch : 24] (l_loss: 0.05199) (t_loss: 0.07714) (accu: 0.9765)
[epoch : 25] (l_loss: 0.05183) (t_loss: 0.07437) (accu: 0.9761)
[epoch : 26] (l_loss: 0.05186) (t_loss: 0.07647) (accu: 0.9752)
[epoch : 27] (l_loss: 0.05161) (t_loss: 0.07577) (accu: 0.9765)
[epoch : 28] (l_loss: 0.05171) (t_loss: 0.07781) (accu: 0.9758)
[epoch : 29] (l_loss: 0.05148) (t_loss: 0.07452) (accu: 0.9770)
[epoch : 30] (l_loss: 0.05216) (t_loss: 0.07950) (accu: 0.9748)
[epoch : 31] (l_loss: 0.05107) (t_loss: 0.07806) (accu: 0.9751)
[epoch : 32] (l_loss: 0.05156) (t_loss: 0.07789) (accu: 0.9748)
[epoch : 33] (l_loss: 0.05163) (t_loss: 0.07681) (accu: 0.9766)
[epoch : 34] (l_loss: 0.05139) (t_loss: 0.07609) (accu: 0.9748)
[epoch : 35] (l_loss: 0.05126) (t_loss: 0.07658) (accu: 0.9765)
[epoch : 36] (l_loss: 0.05175) (t_loss: 0.07785) (accu: 0.9747)
[epoch : 37] (l_loss: 0.05167) (t_loss: 0.07639) (accu: 0.9777)
[epoch : 38] (l_loss: 0.05149) (t_loss: 0.07727) (accu: 0.9763)
[epoch : 39] (l_loss: 0.05166) (t_loss: 0.07747) (accu: 0.9757)
[epoch : 40] (l_loss: 0.05111) (t_loss: 0.07493) (accu: 0.9769)
[epoch : 41] (l_loss: 0.05149) (t_loss: 0.07435) (accu: 0.9767)
[epoch : 42] (l_loss: 0.05162) (t_loss: 0.07746) (accu: 0.9778)
[epoch : 43] (l_loss: 0.05138) (t_loss: 0.07503) (accu: 0.9780)
[epoch : 44] (l_loss: 0.05157) (t_loss: 0.07832) (accu: 0.9760)
[epoch : 45] (l_loss: 0.05148) (t_loss: 0.07622) (accu: 0.9754)
[epoch : 46] (l_loss: 0.05137) (t_loss: 0.07726) (accu: 0.9770)
[epoch : 47] (l_loss: 0.05172) (t_loss: 0.07607) (accu: 0.9769)
[epoch : 48] (l_loss: 0.05132) (t_loss: 0.07658) (accu: 0.9764)
[epoch : 49] (l_loss: 0.05137) (t_loss: 0.07812) (accu: 0.9755)
[epoch : 50] (l_loss: 0.05149) (t_loss: 0.07431) (accu: 0.9764)
Finish! (Best accu: 0.9780) (Time taken(sec) : 684.07) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (36352 | 179148)         16.87
fc1.weight   :      196000 (32883 | 163117)         16.78
fc2.weight   :        18750 (3146 | 15604)          16.78
fcout.weight :          750 (323 | 427)             43.07
------------------------------------------------------------
[Prune_iter : (9/21), Remaining weight : 16.87 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30928) (accu: 0.1603)
[epoch : 1] (l_loss: 0.45556) (t_loss: 0.15472) (accu: 0.9576)
[epoch : 2] (l_loss: 0.12985) (t_loss: 0.11395) (accu: 0.9679)
[epoch : 3] (l_loss: 0.09965) (t_loss: 0.10041) (accu: 0.9691)
[epoch : 4] (l_loss: 0.08565) (t_loss: 0.09353) (accu: 0.9714)
[epoch : 5] (l_loss: 0.07629) (t_loss: 0.08690) (accu: 0.9724)
[epoch : 6] (l_loss: 0.07020) (t_loss: 0.08568) (accu: 0.9738)
[epoch : 7] (l_loss: 0.06550) (t_loss: 0.08537) (accu: 0.9742)
[epoch : 8] (l_loss: 0.06198) (t_loss: 0.08533) (accu: 0.9728)
[epoch : 9] (l_loss: 0.06033) (t_loss: 0.08079) (accu: 0.9756)
[epoch : 10] (l_loss: 0.05780) (t_loss: 0.07817) (accu: 0.9758)
[epoch : 11] (l_loss: 0.05658) (t_loss: 0.07746) (accu: 0.9763)
[epoch : 12] (l_loss: 0.05551) (t_loss: 0.07624) (accu: 0.9778)
[epoch : 13] (l_loss: 0.05421) (t_loss: 0.07600) (accu: 0.9771)
[epoch : 14] (l_loss: 0.05371) (t_loss: 0.07775) (accu: 0.9762)
[epoch : 15] (l_loss: 0.05367) (t_loss: 0.07476) (accu: 0.9777)
[epoch : 16] (l_loss: 0.05349) (t_loss: 0.07806) (accu: 0.9770)
[epoch : 17] (l_loss: 0.05307) (t_loss: 0.07600) (accu: 0.9767)
[epoch : 18] (l_loss: 0.05282) (t_loss: 0.08077) (accu: 0.9748)
[epoch : 19] (l_loss: 0.05244) (t_loss: 0.07765) (accu: 0.9764)
[epoch : 20] (l_loss: 0.05224) (t_loss: 0.07682) (accu: 0.9770)
[epoch : 21] (l_loss: 0.05218) (t_loss: 0.07575) (accu: 0.9774)
[epoch : 22] (l_loss: 0.05221) (t_loss: 0.07588) (accu: 0.9764)
[epoch : 23] (l_loss: 0.05225) (t_loss: 0.07359) (accu: 0.9768)
[epoch : 24] (l_loss: 0.05230) (t_loss: 0.07888) (accu: 0.9754)
[epoch : 25] (l_loss: 0.05224) (t_loss: 0.07518) (accu: 0.9772)
[epoch : 26] (l_loss: 0.05190) (t_loss: 0.07758) (accu: 0.9758)
[epoch : 27] (l_loss: 0.05211) (t_loss: 0.07566) (accu: 0.9763)
[epoch : 28] (l_loss: 0.05206) (t_loss: 0.07702) (accu: 0.9766)
[epoch : 29] (l_loss: 0.05210) (t_loss: 0.07663) (accu: 0.9766)
[epoch : 30] (l_loss: 0.05140) (t_loss: 0.07669) (accu: 0.9772)
[epoch : 31] (l_loss: 0.05178) (t_loss: 0.07639) (accu: 0.9757)
[epoch : 32] (l_loss: 0.05206) (t_loss: 0.07701) (accu: 0.9758)
[epoch : 33] (l_loss: 0.05146) (t_loss: 0.07702) (accu: 0.9774)
[epoch : 34] (l_loss: 0.05199) (t_loss: 0.07659) (accu: 0.9763)
[epoch : 35] (l_loss: 0.05166) (t_loss: 0.07517) (accu: 0.9770)
[epoch : 36] (l_loss: 0.05162) (t_loss: 0.07609) (accu: 0.9755)
[epoch : 37] (l_loss: 0.05231) (t_loss: 0.07755) (accu: 0.9759)
[epoch : 38] (l_loss: 0.05187) (t_loss: 0.07532) (accu: 0.9768)
[epoch : 39] (l_loss: 0.05193) (t_loss: 0.08017) (accu: 0.9765)
[epoch : 40] (l_loss: 0.05198) (t_loss: 0.07739) (accu: 0.9755)
[epoch : 41] (l_loss: 0.05180) (t_loss: 0.07779) (accu: 0.9768)
[epoch : 42] (l_loss: 0.05162) (t_loss: 0.07624) (accu: 0.9756)
[epoch : 43] (l_loss: 0.05167) (t_loss: 0.07996) (accu: 0.9758)
[epoch : 44] (l_loss: 0.05187) (t_loss: 0.07463) (accu: 0.9776)
[epoch : 45] (l_loss: 0.05194) (t_loss: 0.07814) (accu: 0.9747)
[epoch : 46] (l_loss: 0.05161) (t_loss: 0.08010) (accu: 0.9766)
[epoch : 47] (l_loss: 0.05173) (t_loss: 0.07887) (accu: 0.9770)
[epoch : 48] (l_loss: 0.05173) (t_loss: 0.07664) (accu: 0.9768)
[epoch : 49] (l_loss: 0.05161) (t_loss: 0.07435) (accu: 0.9779)
[epoch : 50] (l_loss: 0.05156) (t_loss: 0.07622) (accu: 0.9760)
Finish! (Best accu: 0.9779) (Time taken(sec) : 682.64) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (29115 | 186385)         13.51
fc1.weight   :      196000 (26307 | 169693)         13.42
fc2.weight   :        18750 (2517 | 16233)          13.42
fcout.weight :          750 (291 | 459)             38.80
------------------------------------------------------------
[Prune_iter : (10/21), Remaining weight : 13.51 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30428) (accu: 0.1549)
[epoch : 1] (l_loss: 0.45914) (t_loss: 0.16215) (accu: 0.9552)
[epoch : 2] (l_loss: 0.13398) (t_loss: 0.11733) (accu: 0.9665)
[epoch : 3] (l_loss: 0.10179) (t_loss: 0.10418) (accu: 0.9695)
[epoch : 4] (l_loss: 0.08534) (t_loss: 0.09576) (accu: 0.9713)
[epoch : 5] (l_loss: 0.07594) (t_loss: 0.09555) (accu: 0.9719)
[epoch : 6] (l_loss: 0.06939) (t_loss: 0.08414) (accu: 0.9745)
[epoch : 7] (l_loss: 0.06437) (t_loss: 0.08342) (accu: 0.9746)
[epoch : 8] (l_loss: 0.06060) (t_loss: 0.08310) (accu: 0.9758)
[epoch : 9] (l_loss: 0.05824) (t_loss: 0.08030) (accu: 0.9755)
[epoch : 10] (l_loss: 0.05650) (t_loss: 0.07959) (accu: 0.9774)
[epoch : 11] (l_loss: 0.05522) (t_loss: 0.07940) (accu: 0.9750)
[epoch : 12] (l_loss: 0.05457) (t_loss: 0.07847) (accu: 0.9755)
[epoch : 13] (l_loss: 0.05377) (t_loss: 0.07722) (accu: 0.9763)
[epoch : 14] (l_loss: 0.05380) (t_loss: 0.07524) (accu: 0.9774)
[epoch : 15] (l_loss: 0.05299) (t_loss: 0.07680) (accu: 0.9767)
[epoch : 16] (l_loss: 0.05279) (t_loss: 0.07778) (accu: 0.9757)
[epoch : 17] (l_loss: 0.05269) (t_loss: 0.07590) (accu: 0.9770)
[epoch : 18] (l_loss: 0.05265) (t_loss: 0.07577) (accu: 0.9763)
[epoch : 19] (l_loss: 0.05199) (t_loss: 0.07439) (accu: 0.9775)
[epoch : 20] (l_loss: 0.05230) (t_loss: 0.07588) (accu: 0.9769)
[epoch : 21] (l_loss: 0.05209) (t_loss: 0.07628) (accu: 0.9770)
[epoch : 22] (l_loss: 0.05201) (t_loss: 0.07631) (accu: 0.9768)
[epoch : 23] (l_loss: 0.05209) (t_loss: 0.07563) (accu: 0.9765)
[epoch : 24] (l_loss: 0.05151) (t_loss: 0.07846) (accu: 0.9760)
[epoch : 25] (l_loss: 0.05171) (t_loss: 0.07849) (accu: 0.9767)
[epoch : 26] (l_loss: 0.05192) (t_loss: 0.07587) (accu: 0.9768)
[epoch : 27] (l_loss: 0.05179) (t_loss: 0.07489) (accu: 0.9760)
[epoch : 28] (l_loss: 0.05169) (t_loss: 0.07675) (accu: 0.9766)
[epoch : 29] (l_loss: 0.05187) (t_loss: 0.07404) (accu: 0.9775)
[epoch : 30] (l_loss: 0.05136) (t_loss: 0.07760) (accu: 0.9774)
[epoch : 31] (l_loss: 0.05159) (t_loss: 0.07789) (accu: 0.9763)
[epoch : 32] (l_loss: 0.05182) (t_loss: 0.07748) (accu: 0.9758)
[epoch : 33] (l_loss: 0.05181) (t_loss: 0.07547) (accu: 0.9778)
[epoch : 34] (l_loss: 0.05151) (t_loss: 0.07674) (accu: 0.9755)
[epoch : 35] (l_loss: 0.05161) (t_loss: 0.07604) (accu: 0.9757)
[epoch : 36] (l_loss: 0.05129) (t_loss: 0.07527) (accu: 0.9766)
[epoch : 37] (l_loss: 0.05162) (t_loss: 0.07654) (accu: 0.9777)
[epoch : 38] (l_loss: 0.05198) (t_loss: 0.07818) (accu: 0.9760)
[epoch : 39] (l_loss: 0.05157) (t_loss: 0.07789) (accu: 0.9761)
[epoch : 40] (l_loss: 0.05141) (t_loss: 0.07758) (accu: 0.9770)
[epoch : 41] (l_loss: 0.05178) (t_loss: 0.07592) (accu: 0.9770)
[epoch : 42] (l_loss: 0.05129) (t_loss: 0.08214) (accu: 0.9746)
[epoch : 43] (l_loss: 0.05136) (t_loss: 0.07739) (accu: 0.9755)
[epoch : 44] (l_loss: 0.05202) (t_loss: 0.07460) (accu: 0.9771)
[epoch : 45] (l_loss: 0.05137) (t_loss: 0.07456) (accu: 0.9772)
[epoch : 46] (l_loss: 0.05208) (t_loss: 0.07572) (accu: 0.9762)
[epoch : 47] (l_loss: 0.05139) (t_loss: 0.07549) (accu: 0.9769)
[epoch : 48] (l_loss: 0.05165) (t_loss: 0.07657) (accu: 0.9767)
[epoch : 49] (l_loss: 0.05189) (t_loss: 0.07602) (accu: 0.9769)
[epoch : 50] (l_loss: 0.05151) (t_loss: 0.07544) (accu: 0.9774)
Finish! (Best accu: 0.9778) (Time taken(sec) : 673.56) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (23320 | 192180)         10.82
fc1.weight   :      196000 (21045 | 174955)         10.74
fc2.weight   :        18750 (2013 | 16737)          10.74
fcout.weight :          750 (262 | 488)             34.93
------------------------------------------------------------
[Prune_iter : (11/21), Remaining weight : 10.82 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29666) (accu: 0.1342)
[epoch : 1] (l_loss: 0.45754) (t_loss: 0.16273) (accu: 0.9517)
[epoch : 2] (l_loss: 0.13606) (t_loss: 0.11709) (accu: 0.9647)
[epoch : 3] (l_loss: 0.10226) (t_loss: 0.10357) (accu: 0.9680)
[epoch : 4] (l_loss: 0.08639) (t_loss: 0.09229) (accu: 0.9725)
[epoch : 5] (l_loss: 0.07642) (t_loss: 0.09126) (accu: 0.9724)
[epoch : 6] (l_loss: 0.07046) (t_loss: 0.08426) (accu: 0.9754)
[epoch : 7] (l_loss: 0.06638) (t_loss: 0.08283) (accu: 0.9756)
[epoch : 8] (l_loss: 0.06296) (t_loss: 0.08224) (accu: 0.9757)
[epoch : 9] (l_loss: 0.06041) (t_loss: 0.08354) (accu: 0.9739)
[epoch : 10] (l_loss: 0.05894) (t_loss: 0.08535) (accu: 0.9750)
[epoch : 11] (l_loss: 0.05732) (t_loss: 0.08035) (accu: 0.9757)
[epoch : 12] (l_loss: 0.05619) (t_loss: 0.08048) (accu: 0.9767)
[epoch : 13] (l_loss: 0.05590) (t_loss: 0.07909) (accu: 0.9765)
[epoch : 14] (l_loss: 0.05508) (t_loss: 0.07923) (accu: 0.9756)
[epoch : 15] (l_loss: 0.05490) (t_loss: 0.07892) (accu: 0.9759)
[epoch : 16] (l_loss: 0.05456) (t_loss: 0.07761) (accu: 0.9762)
[epoch : 17] (l_loss: 0.05433) (t_loss: 0.08116) (accu: 0.9754)
[epoch : 18] (l_loss: 0.05441) (t_loss: 0.07762) (accu: 0.9774)
[epoch : 19] (l_loss: 0.05423) (t_loss: 0.07670) (accu: 0.9761)
[epoch : 20] (l_loss: 0.05375) (t_loss: 0.07514) (accu: 0.9776)
[epoch : 21] (l_loss: 0.05388) (t_loss: 0.07525) (accu: 0.9777)
[epoch : 22] (l_loss: 0.05357) (t_loss: 0.07988) (accu: 0.9764)
[epoch : 23] (l_loss: 0.05371) (t_loss: 0.08151) (accu: 0.9750)
[epoch : 24] (l_loss: 0.05354) (t_loss: 0.07902) (accu: 0.9769)
[epoch : 25] (l_loss: 0.05326) (t_loss: 0.07945) (accu: 0.9751)
[epoch : 26] (l_loss: 0.05353) (t_loss: 0.07727) (accu: 0.9768)
[epoch : 27] (l_loss: 0.05313) (t_loss: 0.07604) (accu: 0.9768)
[epoch : 28] (l_loss: 0.05335) (t_loss: 0.07947) (accu: 0.9755)
[epoch : 29] (l_loss: 0.05339) (t_loss: 0.07744) (accu: 0.9759)
[epoch : 30] (l_loss: 0.05314) (t_loss: 0.07874) (accu: 0.9759)
[epoch : 31] (l_loss: 0.05305) (t_loss: 0.07825) (accu: 0.9758)
[epoch : 32] (l_loss: 0.05313) (t_loss: 0.07583) (accu: 0.9769)
[epoch : 33] (l_loss: 0.05304) (t_loss: 0.07519) (accu: 0.9771)
[epoch : 34] (l_loss: 0.05296) (t_loss: 0.07808) (accu: 0.9764)
[epoch : 35] (l_loss: 0.05281) (t_loss: 0.07893) (accu: 0.9768)
[epoch : 36] (l_loss: 0.05330) (t_loss: 0.07665) (accu: 0.9761)
[epoch : 37] (l_loss: 0.05318) (t_loss: 0.07676) (accu: 0.9776)
[epoch : 38] (l_loss: 0.05312) (t_loss: 0.07719) (accu: 0.9765)
[epoch : 39] (l_loss: 0.05295) (t_loss: 0.07672) (accu: 0.9763)
[epoch : 40] (l_loss: 0.05343) (t_loss: 0.07700) (accu: 0.9756)
[epoch : 41] (l_loss: 0.05343) (t_loss: 0.07799) (accu: 0.9761)
[epoch : 42] (l_loss: 0.05285) (t_loss: 0.07716) (accu: 0.9759)
[epoch : 43] (l_loss: 0.05334) (t_loss: 0.07608) (accu: 0.9777)
[epoch : 44] (l_loss: 0.05298) (t_loss: 0.07862) (accu: 0.9769)
[epoch : 45] (l_loss: 0.05318) (t_loss: 0.07382) (accu: 0.9769)
[epoch : 46] (l_loss: 0.05318) (t_loss: 0.07821) (accu: 0.9759)
[epoch : 47] (l_loss: 0.05286) (t_loss: 0.07775) (accu: 0.9756)
[epoch : 48] (l_loss: 0.05322) (t_loss: 0.07729) (accu: 0.9767)
[epoch : 49] (l_loss: 0.05328) (t_loss: 0.08026) (accu: 0.9760)
[epoch : 50] (l_loss: 0.05301) (t_loss: 0.07701) (accu: 0.9779)
Finish! (Best accu: 0.9779) (Time taken(sec) : 687.76) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (18682 | 196818)          8.67
fc1.weight   :      196000 (16836 | 179164)          8.59
fc2.weight   :        18750 (1611 | 17139)           8.59
fcout.weight :          750 (235 | 515)             31.33
------------------------------------------------------------
[Prune_iter : (12/21), Remaining weight : 8.67 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29607) (accu: 0.1467)
[epoch : 1] (l_loss: 0.45766) (t_loss: 0.16189) (accu: 0.9545)
[epoch : 2] (l_loss: 0.13478) (t_loss: 0.11867) (accu: 0.9661)
[epoch : 3] (l_loss: 0.10182) (t_loss: 0.10266) (accu: 0.9703)
[epoch : 4] (l_loss: 0.08512) (t_loss: 0.09306) (accu: 0.9721)
[epoch : 5] (l_loss: 0.07546) (t_loss: 0.09099) (accu: 0.9722)
[epoch : 6] (l_loss: 0.06845) (t_loss: 0.08459) (accu: 0.9746)
[epoch : 7] (l_loss: 0.06335) (t_loss: 0.08027) (accu: 0.9754)
[epoch : 8] (l_loss: 0.05953) (t_loss: 0.08057) (accu: 0.9755)
[epoch : 9] (l_loss: 0.05779) (t_loss: 0.08244) (accu: 0.9752)
[epoch : 10] (l_loss: 0.05635) (t_loss: 0.07956) (accu: 0.9759)
[epoch : 11] (l_loss: 0.05540) (t_loss: 0.07490) (accu: 0.9783)
[epoch : 12] (l_loss: 0.05437) (t_loss: 0.07751) (accu: 0.9783)
[epoch : 13] (l_loss: 0.05394) (t_loss: 0.07889) (accu: 0.9763)
[epoch : 14] (l_loss: 0.05364) (t_loss: 0.07819) (accu: 0.9758)
[epoch : 15] (l_loss: 0.05352) (t_loss: 0.07939) (accu: 0.9771)
[epoch : 16] (l_loss: 0.05320) (t_loss: 0.07821) (accu: 0.9769)
[epoch : 17] (l_loss: 0.05269) (t_loss: 0.07803) (accu: 0.9754)
[epoch : 18] (l_loss: 0.05286) (t_loss: 0.07762) (accu: 0.9769)
[epoch : 19] (l_loss: 0.05204) (t_loss: 0.07846) (accu: 0.9774)
[epoch : 20] (l_loss: 0.05206) (t_loss: 0.07532) (accu: 0.9771)
[epoch : 21] (l_loss: 0.05226) (t_loss: 0.07916) (accu: 0.9745)
[epoch : 22] (l_loss: 0.05180) (t_loss: 0.07713) (accu: 0.9773)
[epoch : 23] (l_loss: 0.05183) (t_loss: 0.07722) (accu: 0.9766)
[epoch : 24] (l_loss: 0.05170) (t_loss: 0.07566) (accu: 0.9763)
[epoch : 25] (l_loss: 0.05175) (t_loss: 0.07685) (accu: 0.9758)
[epoch : 26] (l_loss: 0.05174) (t_loss: 0.07577) (accu: 0.9770)
[epoch : 27] (l_loss: 0.05155) (t_loss: 0.07515) (accu: 0.9775)
[epoch : 28] (l_loss: 0.05140) (t_loss: 0.07698) (accu: 0.9761)
[epoch : 29] (l_loss: 0.05160) (t_loss: 0.07984) (accu: 0.9767)
[epoch : 30] (l_loss: 0.05200) (t_loss: 0.07574) (accu: 0.9770)
[epoch : 31] (l_loss: 0.05166) (t_loss: 0.07643) (accu: 0.9754)
[epoch : 32] (l_loss: 0.05135) (t_loss: 0.07514) (accu: 0.9761)
[epoch : 33] (l_loss: 0.05146) (t_loss: 0.07502) (accu: 0.9770)
[epoch : 34] (l_loss: 0.05177) (t_loss: 0.07546) (accu: 0.9762)
[epoch : 35] (l_loss: 0.05185) (t_loss: 0.07743) (accu: 0.9767)
[epoch : 36] (l_loss: 0.05163) (t_loss: 0.07621) (accu: 0.9759)
[epoch : 37] (l_loss: 0.05149) (t_loss: 0.07644) (accu: 0.9764)
[epoch : 38] (l_loss: 0.05123) (t_loss: 0.07730) (accu: 0.9764)
[epoch : 39] (l_loss: 0.05157) (t_loss: 0.07557) (accu: 0.9769)
[epoch : 40] (l_loss: 0.05161) (t_loss: 0.07758) (accu: 0.9775)
[epoch : 41] (l_loss: 0.05163) (t_loss: 0.07632) (accu: 0.9764)
[epoch : 42] (l_loss: 0.05162) (t_loss: 0.07874) (accu: 0.9752)
[epoch : 43] (l_loss: 0.05185) (t_loss: 0.07793) (accu: 0.9765)
[epoch : 44] (l_loss: 0.05178) (t_loss: 0.07645) (accu: 0.9772)
[epoch : 45] (l_loss: 0.05123) (t_loss: 0.07887) (accu: 0.9749)
[epoch : 46] (l_loss: 0.05167) (t_loss: 0.07684) (accu: 0.9771)
[epoch : 47] (l_loss: 0.05122) (t_loss: 0.07464) (accu: 0.9765)
[epoch : 48] (l_loss: 0.05134) (t_loss: 0.07585) (accu: 0.9764)
[epoch : 49] (l_loss: 0.05228) (t_loss: 0.07755) (accu: 0.9763)
[epoch : 50] (l_loss: 0.05147) (t_loss: 0.07620) (accu: 0.9758)
Finish! (Best accu: 0.9783) (Time taken(sec) : 684.29) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (14969 | 200531)          6.95
fc1.weight   :      196000 (13469 | 182531)          6.87
fc2.weight   :        18750 (1288 | 17462)           6.87
fcout.weight :          750 (212 | 538)             28.27
------------------------------------------------------------
[Prune_iter : (13/21), Remaining weight : 6.95 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30203) (accu: 0.1356)
[epoch : 1] (l_loss: 0.45515) (t_loss: 0.16768) (accu: 0.9524)
[epoch : 2] (l_loss: 0.14000) (t_loss: 0.12032) (accu: 0.9641)
[epoch : 3] (l_loss: 0.10422) (t_loss: 0.10527) (accu: 0.9680)
[epoch : 4] (l_loss: 0.08788) (t_loss: 0.09340) (accu: 0.9721)
[epoch : 5] (l_loss: 0.07726) (t_loss: 0.08733) (accu: 0.9734)
[epoch : 6] (l_loss: 0.06989) (t_loss: 0.08650) (accu: 0.9743)
[epoch : 7] (l_loss: 0.06516) (t_loss: 0.08287) (accu: 0.9762)
[epoch : 8] (l_loss: 0.06134) (t_loss: 0.08171) (accu: 0.9758)
[epoch : 9] (l_loss: 0.05977) (t_loss: 0.08313) (accu: 0.9754)
[epoch : 10] (l_loss: 0.05784) (t_loss: 0.07903) (accu: 0.9761)
[epoch : 11] (l_loss: 0.05673) (t_loss: 0.07884) (accu: 0.9757)
[epoch : 12] (l_loss: 0.05596) (t_loss: 0.07811) (accu: 0.9776)
[epoch : 13] (l_loss: 0.05590) (t_loss: 0.07940) (accu: 0.9756)
[epoch : 14] (l_loss: 0.05491) (t_loss: 0.07692) (accu: 0.9765)
[epoch : 15] (l_loss: 0.05483) (t_loss: 0.07655) (accu: 0.9769)
[epoch : 16] (l_loss: 0.05439) (t_loss: 0.08193) (accu: 0.9759)
[epoch : 17] (l_loss: 0.05369) (t_loss: 0.07637) (accu: 0.9784)
[epoch : 18] (l_loss: 0.05371) (t_loss: 0.08108) (accu: 0.9767)
[epoch : 19] (l_loss: 0.05394) (t_loss: 0.07702) (accu: 0.9756)
[epoch : 20] (l_loss: 0.05398) (t_loss: 0.07467) (accu: 0.9779)
[epoch : 21] (l_loss: 0.05411) (t_loss: 0.07787) (accu: 0.9774)
[epoch : 22] (l_loss: 0.05340) (t_loss: 0.07738) (accu: 0.9765)
[epoch : 23] (l_loss: 0.05363) (t_loss: 0.07618) (accu: 0.9780)
[epoch : 24] (l_loss: 0.05332) (t_loss: 0.07817) (accu: 0.9766)
[epoch : 25] (l_loss: 0.05339) (t_loss: 0.08055) (accu: 0.9749)
[epoch : 26] (l_loss: 0.05379) (t_loss: 0.07823) (accu: 0.9768)
[epoch : 27] (l_loss: 0.05337) (t_loss: 0.07761) (accu: 0.9768)
[epoch : 28] (l_loss: 0.05317) (t_loss: 0.07876) (accu: 0.9759)
[epoch : 29] (l_loss: 0.05308) (t_loss: 0.07751) (accu: 0.9774)
[epoch : 30] (l_loss: 0.05358) (t_loss: 0.07650) (accu: 0.9770)
[epoch : 31] (l_loss: 0.05305) (t_loss: 0.07844) (accu: 0.9757)
[epoch : 32] (l_loss: 0.05292) (t_loss: 0.07570) (accu: 0.9766)
[epoch : 33] (l_loss: 0.05340) (t_loss: 0.07738) (accu: 0.9759)
[epoch : 34] (l_loss: 0.05294) (t_loss: 0.08269) (accu: 0.9760)
[epoch : 35] (l_loss: 0.05323) (t_loss: 0.07742) (accu: 0.9757)
[epoch : 36] (l_loss: 0.05325) (t_loss: 0.07940) (accu: 0.9762)
[epoch : 37] (l_loss: 0.05336) (t_loss: 0.08015) (accu: 0.9762)
[epoch : 38] (l_loss: 0.05315) (t_loss: 0.07834) (accu: 0.9759)
[epoch : 39] (l_loss: 0.05295) (t_loss: 0.07632) (accu: 0.9769)
[epoch : 40] (l_loss: 0.05317) (t_loss: 0.07807) (accu: 0.9758)
[epoch : 41] (l_loss: 0.05266) (t_loss: 0.07847) (accu: 0.9745)
[epoch : 42] (l_loss: 0.05363) (t_loss: 0.07739) (accu: 0.9765)
[epoch : 43] (l_loss: 0.05283) (t_loss: 0.08083) (accu: 0.9771)
[epoch : 44] (l_loss: 0.05306) (t_loss: 0.07616) (accu: 0.9768)
[epoch : 45] (l_loss: 0.05256) (t_loss: 0.07656) (accu: 0.9785)
[epoch : 46] (l_loss: 0.05302) (t_loss: 0.07807) (accu: 0.9759)
[epoch : 47] (l_loss: 0.05325) (t_loss: 0.07806) (accu: 0.9765)
[epoch : 48] (l_loss: 0.05288) (t_loss: 0.07994) (accu: 0.9749)
[epoch : 49] (l_loss: 0.05336) (t_loss: 0.07766) (accu: 0.9756)
[epoch : 50] (l_loss: 0.05332) (t_loss: 0.07783) (accu: 0.9754)
Finish! (Best accu: 0.9785) (Time taken(sec) : 697.71) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (11997 | 203503)          5.57
fc1.weight   :      196000 (10775 | 185225)          5.50
fc2.weight   :        18750 (1031 | 17719)           5.50
fcout.weight :          750 (191 | 559)             25.47
------------------------------------------------------------
[Prune_iter : (14/21), Remaining weight : 5.57 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30304) (accu: 0.1388)
[epoch : 1] (l_loss: 0.45749) (t_loss: 0.16241) (accu: 0.9534)
[epoch : 2] (l_loss: 0.13393) (t_loss: 0.11562) (accu: 0.9657)
[epoch : 3] (l_loss: 0.10052) (t_loss: 0.10214) (accu: 0.9703)
[epoch : 4] (l_loss: 0.08475) (t_loss: 0.09463) (accu: 0.9719)
[epoch : 5] (l_loss: 0.07542) (t_loss: 0.08791) (accu: 0.9729)
[epoch : 6] (l_loss: 0.06904) (t_loss: 0.08158) (accu: 0.9757)
[epoch : 7] (l_loss: 0.06442) (t_loss: 0.08257) (accu: 0.9753)
[epoch : 8] (l_loss: 0.06075) (t_loss: 0.07955) (accu: 0.9770)
[epoch : 9] (l_loss: 0.05814) (t_loss: 0.07836) (accu: 0.9766)
[epoch : 10] (l_loss: 0.05697) (t_loss: 0.07698) (accu: 0.9769)
[epoch : 11] (l_loss: 0.05529) (t_loss: 0.07792) (accu: 0.9772)
[epoch : 12] (l_loss: 0.05411) (t_loss: 0.07768) (accu: 0.9778)
[epoch : 13] (l_loss: 0.05379) (t_loss: 0.07873) (accu: 0.9767)
[epoch : 14] (l_loss: 0.05333) (t_loss: 0.07924) (accu: 0.9759)
[epoch : 15] (l_loss: 0.05292) (t_loss: 0.07947) (accu: 0.9759)
[epoch : 16] (l_loss: 0.05312) (t_loss: 0.07547) (accu: 0.9767)
[epoch : 17] (l_loss: 0.05236) (t_loss: 0.07504) (accu: 0.9776)
[epoch : 18] (l_loss: 0.05290) (t_loss: 0.07776) (accu: 0.9747)
[epoch : 19] (l_loss: 0.05222) (t_loss: 0.07640) (accu: 0.9766)
[epoch : 20] (l_loss: 0.05212) (t_loss: 0.07440) (accu: 0.9765)
[epoch : 21] (l_loss: 0.05237) (t_loss: 0.07701) (accu: 0.9769)
[epoch : 22] (l_loss: 0.05206) (t_loss: 0.07601) (accu: 0.9762)
[epoch : 23] (l_loss: 0.05198) (t_loss: 0.07586) (accu: 0.9762)
[epoch : 24] (l_loss: 0.05191) (t_loss: 0.07748) (accu: 0.9760)
[epoch : 25] (l_loss: 0.05201) (t_loss: 0.07528) (accu: 0.9765)
[epoch : 26] (l_loss: 0.05173) (t_loss: 0.07557) (accu: 0.9763)
[epoch : 27] (l_loss: 0.05176) (t_loss: 0.07833) (accu: 0.9752)
[epoch : 28] (l_loss: 0.05165) (t_loss: 0.07582) (accu: 0.9772)
[epoch : 29] (l_loss: 0.05144) (t_loss: 0.07946) (accu: 0.9746)
[epoch : 30] (l_loss: 0.05145) (t_loss: 0.07478) (accu: 0.9763)
[epoch : 31] (l_loss: 0.05171) (t_loss: 0.07813) (accu: 0.9766)
[epoch : 32] (l_loss: 0.05114) (t_loss: 0.07849) (accu: 0.9760)
[epoch : 33] (l_loss: 0.05192) (t_loss: 0.07687) (accu: 0.9767)
[epoch : 34] (l_loss: 0.05105) (t_loss: 0.07805) (accu: 0.9760)
[epoch : 35] (l_loss: 0.05174) (t_loss: 0.07711) (accu: 0.9765)
[epoch : 36] (l_loss: 0.05161) (t_loss: 0.07599) (accu: 0.9763)
[epoch : 37] (l_loss: 0.05144) (t_loss: 0.07636) (accu: 0.9773)
[epoch : 38] (l_loss: 0.05143) (t_loss: 0.07739) (accu: 0.9765)
[epoch : 39] (l_loss: 0.05179) (t_loss: 0.07753) (accu: 0.9780)
[epoch : 40] (l_loss: 0.05191) (t_loss: 0.07770) (accu: 0.9766)
[epoch : 41] (l_loss: 0.05130) (t_loss: 0.07783) (accu: 0.9764)
[epoch : 42] (l_loss: 0.05153) (t_loss: 0.07715) (accu: 0.9756)
[epoch : 43] (l_loss: 0.05150) (t_loss: 0.07715) (accu: 0.9754)
[epoch : 44] (l_loss: 0.05167) (t_loss: 0.07825) (accu: 0.9749)
[epoch : 45] (l_loss: 0.05152) (t_loss: 0.07909) (accu: 0.9765)
[epoch : 46] (l_loss: 0.05189) (t_loss: 0.07997) (accu: 0.9764)
[epoch : 47] (l_loss: 0.05134) (t_loss: 0.07558) (accu: 0.9752)
[epoch : 48] (l_loss: 0.05151) (t_loss: 0.07702) (accu: 0.9760)
[epoch : 49] (l_loss: 0.05159) (t_loss: 0.07455) (accu: 0.9775)
[epoch : 50] (l_loss: 0.05162) (t_loss: 0.07589) (accu: 0.9759)
Finish! (Best accu: 0.9780) (Time taken(sec) : 674.43) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (9617 | 205883)          4.46
fc1.weight   :       196000 (8620 | 187380)          4.40
fc2.weight   :        18750 (825 | 17925)            4.40
fcout.weight :          750 (172 | 578)             22.93
------------------------------------------------------------
[Prune_iter : (15/21), Remaining weight : 4.46 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30265) (accu: 0.1433)
[epoch : 1] (l_loss: 0.45192) (t_loss: 0.16496) (accu: 0.9526)
[epoch : 2] (l_loss: 0.13775) (t_loss: 0.12177) (accu: 0.9640)
[epoch : 3] (l_loss: 0.10580) (t_loss: 0.10552) (accu: 0.9676)
[epoch : 4] (l_loss: 0.09011) (t_loss: 0.09702) (accu: 0.9714)
[epoch : 5] (l_loss: 0.08006) (t_loss: 0.09069) (accu: 0.9724)
[epoch : 6] (l_loss: 0.07227) (t_loss: 0.08892) (accu: 0.9721)
[epoch : 7] (l_loss: 0.06678) (t_loss: 0.08466) (accu: 0.9741)
[epoch : 8] (l_loss: 0.06312) (t_loss: 0.08381) (accu: 0.9742)
[epoch : 9] (l_loss: 0.06042) (t_loss: 0.07938) (accu: 0.9754)
[epoch : 10] (l_loss: 0.05874) (t_loss: 0.07943) (accu: 0.9756)
[epoch : 11] (l_loss: 0.05696) (t_loss: 0.08174) (accu: 0.9747)
[epoch : 12] (l_loss: 0.05635) (t_loss: 0.07813) (accu: 0.9763)
[epoch : 13] (l_loss: 0.05534) (t_loss: 0.07761) (accu: 0.9763)
[epoch : 14] (l_loss: 0.05523) (t_loss: 0.07866) (accu: 0.9752)
[epoch : 15] (l_loss: 0.05482) (t_loss: 0.07615) (accu: 0.9768)
[epoch : 16] (l_loss: 0.05466) (t_loss: 0.08034) (accu: 0.9774)
[epoch : 17] (l_loss: 0.05429) (t_loss: 0.07800) (accu: 0.9769)
[epoch : 18] (l_loss: 0.05410) (t_loss: 0.07624) (accu: 0.9768)
[epoch : 19] (l_loss: 0.05372) (t_loss: 0.07704) (accu: 0.9754)
[epoch : 20] (l_loss: 0.05391) (t_loss: 0.07622) (accu: 0.9772)
[epoch : 21] (l_loss: 0.05380) (t_loss: 0.07731) (accu: 0.9767)
[epoch : 22] (l_loss: 0.05370) (t_loss: 0.07779) (accu: 0.9764)
[epoch : 23] (l_loss: 0.05349) (t_loss: 0.07657) (accu: 0.9779)
[epoch : 24] (l_loss: 0.05368) (t_loss: 0.07997) (accu: 0.9754)
[epoch : 25] (l_loss: 0.05370) (t_loss: 0.07733) (accu: 0.9773)
[epoch : 26] (l_loss: 0.05338) (t_loss: 0.07414) (accu: 0.9777)
[epoch : 27] (l_loss: 0.05380) (t_loss: 0.07800) (accu: 0.9770)
[epoch : 28] (l_loss: 0.05348) (t_loss: 0.07848) (accu: 0.9757)
[epoch : 29] (l_loss: 0.05360) (t_loss: 0.07461) (accu: 0.9764)
[epoch : 30] (l_loss: 0.05374) (t_loss: 0.07947) (accu: 0.9757)
[epoch : 31] (l_loss: 0.05315) (t_loss: 0.07608) (accu: 0.9772)
[epoch : 32] (l_loss: 0.05295) (t_loss: 0.07903) (accu: 0.9771)
[epoch : 33] (l_loss: 0.05354) (t_loss: 0.07508) (accu: 0.9777)
[epoch : 34] (l_loss: 0.05292) (t_loss: 0.07793) (accu: 0.9760)
[epoch : 35] (l_loss: 0.05307) (t_loss: 0.07876) (accu: 0.9761)
[epoch : 36] (l_loss: 0.05321) (t_loss: 0.07872) (accu: 0.9759)
[epoch : 37] (l_loss: 0.05314) (t_loss: 0.07855) (accu: 0.9767)
[epoch : 38] (l_loss: 0.05315) (t_loss: 0.07807) (accu: 0.9756)
[epoch : 39] (l_loss: 0.05287) (t_loss: 0.07717) (accu: 0.9770)
[epoch : 40] (l_loss: 0.05306) (t_loss: 0.07862) (accu: 0.9771)
[epoch : 41] (l_loss: 0.05317) (t_loss: 0.08038) (accu: 0.9755)
[epoch : 42] (l_loss: 0.05329) (t_loss: 0.07641) (accu: 0.9766)
[epoch : 43] (l_loss: 0.05276) (t_loss: 0.07614) (accu: 0.9770)
[epoch : 44] (l_loss: 0.05294) (t_loss: 0.08004) (accu: 0.9765)
[epoch : 45] (l_loss: 0.05308) (t_loss: 0.07730) (accu: 0.9771)
[epoch : 46] (l_loss: 0.05310) (t_loss: 0.07684) (accu: 0.9763)
[epoch : 47] (l_loss: 0.05333) (t_loss: 0.07792) (accu: 0.9764)
[epoch : 48] (l_loss: 0.05294) (t_loss: 0.07765) (accu: 0.9769)
[epoch : 49] (l_loss: 0.05311) (t_loss: 0.07483) (accu: 0.9769)
[epoch : 50] (l_loss: 0.05315) (t_loss: 0.07476) (accu: 0.9774)
Finish! (Best accu: 0.9779) (Time taken(sec) : 672.16) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (7710 | 207790)          3.58
fc1.weight   :       196000 (6896 | 189104)          3.52
fc2.weight   :        18750 (660 | 18090)            3.52
fcout.weight :          750 (154 | 596)             20.53
------------------------------------------------------------
[Prune_iter : (16/21), Remaining weight : 3.58 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30187) (accu: 0.1234)
[epoch : 1] (l_loss: 0.45029) (t_loss: 0.16673) (accu: 0.9496)
[epoch : 2] (l_loss: 0.13304) (t_loss: 0.11621) (accu: 0.9658)
[epoch : 3] (l_loss: 0.10193) (t_loss: 0.10539) (accu: 0.9687)
[epoch : 4] (l_loss: 0.08795) (t_loss: 0.09809) (accu: 0.9706)
[epoch : 5] (l_loss: 0.07834) (t_loss: 0.09413) (accu: 0.9724)
[epoch : 6] (l_loss: 0.07202) (t_loss: 0.09220) (accu: 0.9715)
[epoch : 7] (l_loss: 0.06788) (t_loss: 0.08894) (accu: 0.9729)
[epoch : 8] (l_loss: 0.06461) (t_loss: 0.08764) (accu: 0.9742)
[epoch : 9] (l_loss: 0.06259) (t_loss: 0.08306) (accu: 0.9742)
[epoch : 10] (l_loss: 0.06093) (t_loss: 0.08235) (accu: 0.9750)
[epoch : 11] (l_loss: 0.05896) (t_loss: 0.08196) (accu: 0.9754)
[epoch : 12] (l_loss: 0.05795) (t_loss: 0.07936) (accu: 0.9755)
[epoch : 13] (l_loss: 0.05680) (t_loss: 0.07994) (accu: 0.9760)
[epoch : 14] (l_loss: 0.05654) (t_loss: 0.08056) (accu: 0.9741)
[epoch : 15] (l_loss: 0.05584) (t_loss: 0.07988) (accu: 0.9751)
[epoch : 16] (l_loss: 0.05573) (t_loss: 0.08405) (accu: 0.9749)
[epoch : 17] (l_loss: 0.05532) (t_loss: 0.07881) (accu: 0.9754)
[epoch : 18] (l_loss: 0.05524) (t_loss: 0.07789) (accu: 0.9751)
[epoch : 19] (l_loss: 0.05439) (t_loss: 0.08130) (accu: 0.9748)
[epoch : 20] (l_loss: 0.05488) (t_loss: 0.08008) (accu: 0.9742)
[epoch : 21] (l_loss: 0.05481) (t_loss: 0.08081) (accu: 0.9756)
[epoch : 22] (l_loss: 0.05491) (t_loss: 0.07927) (accu: 0.9751)
[epoch : 23] (l_loss: 0.05452) (t_loss: 0.08012) (accu: 0.9760)
[epoch : 24] (l_loss: 0.05465) (t_loss: 0.07855) (accu: 0.9751)
[epoch : 25] (l_loss: 0.05433) (t_loss: 0.07889) (accu: 0.9752)
[epoch : 26] (l_loss: 0.05449) (t_loss: 0.07770) (accu: 0.9752)
[epoch : 27] (l_loss: 0.05442) (t_loss: 0.07850) (accu: 0.9765)
[epoch : 28] (l_loss: 0.05407) (t_loss: 0.07674) (accu: 0.9758)
[epoch : 29] (l_loss: 0.05425) (t_loss: 0.08025) (accu: 0.9743)
[epoch : 30] (l_loss: 0.05432) (t_loss: 0.08137) (accu: 0.9752)
[epoch : 31] (l_loss: 0.05452) (t_loss: 0.07803) (accu: 0.9765)
[epoch : 32] (l_loss: 0.05437) (t_loss: 0.07806) (accu: 0.9757)
[epoch : 33] (l_loss: 0.05421) (t_loss: 0.07742) (accu: 0.9749)
[epoch : 34] (l_loss: 0.05461) (t_loss: 0.07857) (accu: 0.9762)
[epoch : 35] (l_loss: 0.05481) (t_loss: 0.07978) (accu: 0.9744)
[epoch : 36] (l_loss: 0.05436) (t_loss: 0.07671) (accu: 0.9751)
[epoch : 37] (l_loss: 0.05435) (t_loss: 0.07908) (accu: 0.9756)
[epoch : 38] (l_loss: 0.05438) (t_loss: 0.08024) (accu: 0.9758)
[epoch : 39] (l_loss: 0.05433) (t_loss: 0.07968) (accu: 0.9759)
[epoch : 40] (l_loss: 0.05407) (t_loss: 0.07724) (accu: 0.9763)
[epoch : 41] (l_loss: 0.05397) (t_loss: 0.07842) (accu: 0.9746)
[epoch : 42] (l_loss: 0.05385) (t_loss: 0.08100) (accu: 0.9754)
[epoch : 43] (l_loss: 0.05427) (t_loss: 0.07827) (accu: 0.9758)
[epoch : 44] (l_loss: 0.05418) (t_loss: 0.08045) (accu: 0.9760)
[epoch : 45] (l_loss: 0.05424) (t_loss: 0.08192) (accu: 0.9751)
[epoch : 46] (l_loss: 0.05445) (t_loss: 0.07986) (accu: 0.9758)
[epoch : 47] (l_loss: 0.05413) (t_loss: 0.07989) (accu: 0.9754)
[epoch : 48] (l_loss: 0.05431) (t_loss: 0.07920) (accu: 0.9756)
[epoch : 49] (l_loss: 0.05417) (t_loss: 0.07745) (accu: 0.9756)
[epoch : 50] (l_loss: 0.05378) (t_loss: 0.08080) (accu: 0.9756)
Finish! (Best accu: 0.9765) (Time taken(sec) : 686.05) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (6184 | 209316)          2.87
fc1.weight   :       196000 (5517 | 190483)          2.81
fc2.weight   :        18750 (528 | 18222)            2.82
fcout.weight :          750 (139 | 611)             18.53
------------------------------------------------------------
[Prune_iter : (17/21), Remaining weight : 2.87 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29976) (accu: 0.1069)
[epoch : 1] (l_loss: 0.45166) (t_loss: 0.16632) (accu: 0.9516)
[epoch : 2] (l_loss: 0.13746) (t_loss: 0.12522) (accu: 0.9612)
[epoch : 3] (l_loss: 0.10462) (t_loss: 0.10371) (accu: 0.9688)
[epoch : 4] (l_loss: 0.08859) (t_loss: 0.09844) (accu: 0.9711)
[epoch : 5] (l_loss: 0.07927) (t_loss: 0.09527) (accu: 0.9694)
[epoch : 6] (l_loss: 0.07347) (t_loss: 0.09019) (accu: 0.9719)
[epoch : 7] (l_loss: 0.06952) (t_loss: 0.08623) (accu: 0.9737)
[epoch : 8] (l_loss: 0.06627) (t_loss: 0.08500) (accu: 0.9744)
[epoch : 9] (l_loss: 0.06368) (t_loss: 0.08501) (accu: 0.9740)
[epoch : 10] (l_loss: 0.06211) (t_loss: 0.08340) (accu: 0.9738)
[epoch : 11] (l_loss: 0.06071) (t_loss: 0.08278) (accu: 0.9739)
[epoch : 12] (l_loss: 0.05966) (t_loss: 0.08652) (accu: 0.9726)
[epoch : 13] (l_loss: 0.05916) (t_loss: 0.08255) (accu: 0.9749)
[epoch : 14] (l_loss: 0.05882) (t_loss: 0.08101) (accu: 0.9755)
[epoch : 15] (l_loss: 0.05761) (t_loss: 0.08212) (accu: 0.9748)
[epoch : 16] (l_loss: 0.05760) (t_loss: 0.07841) (accu: 0.9752)
[epoch : 17] (l_loss: 0.05681) (t_loss: 0.08037) (accu: 0.9736)
[epoch : 18] (l_loss: 0.05711) (t_loss: 0.07979) (accu: 0.9755)
[epoch : 19] (l_loss: 0.05666) (t_loss: 0.08249) (accu: 0.9741)
[epoch : 20] (l_loss: 0.05604) (t_loss: 0.08030) (accu: 0.9756)
[epoch : 21] (l_loss: 0.05657) (t_loss: 0.08066) (accu: 0.9751)
[epoch : 22] (l_loss: 0.05621) (t_loss: 0.08126) (accu: 0.9749)
[epoch : 23] (l_loss: 0.05616) (t_loss: 0.08067) (accu: 0.9754)
[epoch : 24] (l_loss: 0.05614) (t_loss: 0.08011) (accu: 0.9749)
[epoch : 25] (l_loss: 0.05629) (t_loss: 0.07901) (accu: 0.9750)
[epoch : 26] (l_loss: 0.05587) (t_loss: 0.08270) (accu: 0.9753)
[epoch : 27] (l_loss: 0.05567) (t_loss: 0.07804) (accu: 0.9750)
[epoch : 28] (l_loss: 0.05584) (t_loss: 0.07885) (accu: 0.9740)
[epoch : 29] (l_loss: 0.05584) (t_loss: 0.08011) (accu: 0.9749)
[epoch : 30] (l_loss: 0.05580) (t_loss: 0.08173) (accu: 0.9733)
[epoch : 31] (l_loss: 0.05564) (t_loss: 0.07987) (accu: 0.9753)
[epoch : 32] (l_loss: 0.05580) (t_loss: 0.08093) (accu: 0.9743)
[epoch : 33] (l_loss: 0.05615) (t_loss: 0.08014) (accu: 0.9749)
[epoch : 34] (l_loss: 0.05570) (t_loss: 0.07847) (accu: 0.9767)
[epoch : 35] (l_loss: 0.05552) (t_loss: 0.08215) (accu: 0.9744)
[epoch : 36] (l_loss: 0.05587) (t_loss: 0.07866) (accu: 0.9755)
[epoch : 37] (l_loss: 0.05577) (t_loss: 0.07949) (accu: 0.9748)
[epoch : 38] (l_loss: 0.05569) (t_loss: 0.08272) (accu: 0.9748)
[epoch : 39] (l_loss: 0.05600) (t_loss: 0.08167) (accu: 0.9748)
[epoch : 40] (l_loss: 0.05529) (t_loss: 0.08073) (accu: 0.9748)
[epoch : 41] (l_loss: 0.05547) (t_loss: 0.08303) (accu: 0.9732)
[epoch : 42] (l_loss: 0.05537) (t_loss: 0.07773) (accu: 0.9756)
[epoch : 43] (l_loss: 0.05573) (t_loss: 0.07811) (accu: 0.9762)
[epoch : 44] (l_loss: 0.05594) (t_loss: 0.07952) (accu: 0.9761)
[epoch : 45] (l_loss: 0.05552) (t_loss: 0.08118) (accu: 0.9743)
[epoch : 46] (l_loss: 0.05507) (t_loss: 0.08085) (accu: 0.9752)
[epoch : 47] (l_loss: 0.05562) (t_loss: 0.08063) (accu: 0.9753)
[epoch : 48] (l_loss: 0.05551) (t_loss: 0.07785) (accu: 0.9755)
[epoch : 49] (l_loss: 0.05543) (t_loss: 0.07861) (accu: 0.9753)
[epoch : 50] (l_loss: 0.05558) (t_loss: 0.07892) (accu: 0.9760)
Finish! (Best accu: 0.9767) (Time taken(sec) : 683.14) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (4961 | 210539)          2.30
fc1.weight   :       196000 (4414 | 191586)          2.25
fc2.weight   :        18750 (422 | 18328)            2.25
fcout.weight :          750 (125 | 625)             16.67
------------------------------------------------------------
[Prune_iter : (18/21), Remaining weight : 2.3 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29722) (accu: 0.0950)
[epoch : 1] (l_loss: 0.44649) (t_loss: 0.15907) (accu: 0.9557)
[epoch : 2] (l_loss: 0.13175) (t_loss: 0.11704) (accu: 0.9656)
[epoch : 3] (l_loss: 0.10143) (t_loss: 0.10456) (accu: 0.9689)
[epoch : 4] (l_loss: 0.08704) (t_loss: 0.09720) (accu: 0.9699)
[epoch : 5] (l_loss: 0.07807) (t_loss: 0.09257) (accu: 0.9720)
[epoch : 6] (l_loss: 0.07088) (t_loss: 0.08730) (accu: 0.9727)
[epoch : 7] (l_loss: 0.06643) (t_loss: 0.08425) (accu: 0.9734)
[epoch : 8] (l_loss: 0.06321) (t_loss: 0.08487) (accu: 0.9738)
[epoch : 9] (l_loss: 0.06088) (t_loss: 0.08235) (accu: 0.9752)
[epoch : 10] (l_loss: 0.05912) (t_loss: 0.08253) (accu: 0.9741)
[epoch : 11] (l_loss: 0.05843) (t_loss: 0.08178) (accu: 0.9765)
[epoch : 12] (l_loss: 0.05680) (t_loss: 0.08008) (accu: 0.9755)
[epoch : 13] (l_loss: 0.05689) (t_loss: 0.07769) (accu: 0.9762)
[epoch : 14] (l_loss: 0.05591) (t_loss: 0.07948) (accu: 0.9756)
[epoch : 15] (l_loss: 0.05587) (t_loss: 0.07908) (accu: 0.9754)
[epoch : 16] (l_loss: 0.05572) (t_loss: 0.08241) (accu: 0.9752)
[epoch : 17] (l_loss: 0.05521) (t_loss: 0.08045) (accu: 0.9747)
[epoch : 18] (l_loss: 0.05512) (t_loss: 0.07974) (accu: 0.9746)
[epoch : 19] (l_loss: 0.05454) (t_loss: 0.08137) (accu: 0.9749)
[epoch : 20] (l_loss: 0.05475) (t_loss: 0.08023) (accu: 0.9752)
[epoch : 21] (l_loss: 0.05495) (t_loss: 0.07928) (accu: 0.9747)
[epoch : 22] (l_loss: 0.05462) (t_loss: 0.07761) (accu: 0.9757)
[epoch : 23] (l_loss: 0.05432) (t_loss: 0.07826) (accu: 0.9755)
[epoch : 24] (l_loss: 0.05440) (t_loss: 0.07733) (accu: 0.9758)
[epoch : 25] (l_loss: 0.05445) (t_loss: 0.07836) (accu: 0.9754)
[epoch : 26] (l_loss: 0.05421) (t_loss: 0.07789) (accu: 0.9757)
[epoch : 27] (l_loss: 0.05402) (t_loss: 0.07981) (accu: 0.9750)
[epoch : 28] (l_loss: 0.05426) (t_loss: 0.08031) (accu: 0.9759)
[epoch : 29] (l_loss: 0.05390) (t_loss: 0.07860) (accu: 0.9759)
[epoch : 30] (l_loss: 0.05475) (t_loss: 0.07878) (accu: 0.9753)
[epoch : 31] (l_loss: 0.05409) (t_loss: 0.07696) (accu: 0.9760)
[epoch : 32] (l_loss: 0.05412) (t_loss: 0.08000) (accu: 0.9751)
[epoch : 33] (l_loss: 0.05424) (t_loss: 0.08111) (accu: 0.9757)
[epoch : 34] (l_loss: 0.05416) (t_loss: 0.07725) (accu: 0.9758)
[epoch : 35] (l_loss: 0.05443) (t_loss: 0.08119) (accu: 0.9750)
[epoch : 36] (l_loss: 0.05425) (t_loss: 0.07839) (accu: 0.9771)
[epoch : 37] (l_loss: 0.05371) (t_loss: 0.07903) (accu: 0.9746)
[epoch : 38] (l_loss: 0.05396) (t_loss: 0.07904) (accu: 0.9761)
[epoch : 39] (l_loss: 0.05402) (t_loss: 0.07703) (accu: 0.9757)
[epoch : 40] (l_loss: 0.05388) (t_loss: 0.07875) (accu: 0.9744)
[epoch : 41] (l_loss: 0.05408) (t_loss: 0.07790) (accu: 0.9754)
[epoch : 42] (l_loss: 0.05401) (t_loss: 0.07962) (accu: 0.9745)
[epoch : 43] (l_loss: 0.05377) (t_loss: 0.07789) (accu: 0.9755)
[epoch : 44] (l_loss: 0.05404) (t_loss: 0.07882) (accu: 0.9761)
[epoch : 45] (l_loss: 0.05396) (t_loss: 0.07603) (accu: 0.9766)
[epoch : 46] (l_loss: 0.05435) (t_loss: 0.07828) (accu: 0.9746)
[epoch : 47] (l_loss: 0.05349) (t_loss: 0.07819) (accu: 0.9747)
[epoch : 48] (l_loss: 0.05379) (t_loss: 0.07796) (accu: 0.9761)
[epoch : 49] (l_loss: 0.05405) (t_loss: 0.08092) (accu: 0.9752)
[epoch : 50] (l_loss: 0.05382) (t_loss: 0.08169) (accu: 0.9741)
Finish! (Best accu: 0.9771) (Time taken(sec) : 702.00) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (3982 | 211518)          1.85
fc1.weight   :       196000 (3531 | 192469)          1.80
fc2.weight   :        18750 (338 | 18412)            1.80
fcout.weight :          750 (113 | 637)             15.07
------------------------------------------------------------
[Prune_iter : (19/21), Remaining weight : 1.85 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29876) (accu: 0.0970)
[epoch : 1] (l_loss: 0.44892) (t_loss: 0.16326) (accu: 0.9546)
[epoch : 2] (l_loss: 0.13421) (t_loss: 0.11598) (accu: 0.9663)
[epoch : 3] (l_loss: 0.10089) (t_loss: 0.10461) (accu: 0.9685)
[epoch : 4] (l_loss: 0.08562) (t_loss: 0.09564) (accu: 0.9706)
[epoch : 5] (l_loss: 0.07635) (t_loss: 0.09213) (accu: 0.9715)
[epoch : 6] (l_loss: 0.07069) (t_loss: 0.08497) (accu: 0.9745)
[epoch : 7] (l_loss: 0.06684) (t_loss: 0.08536) (accu: 0.9745)
[epoch : 8] (l_loss: 0.06441) (t_loss: 0.08242) (accu: 0.9742)
[epoch : 9] (l_loss: 0.06251) (t_loss: 0.08302) (accu: 0.9740)
[epoch : 10] (l_loss: 0.06064) (t_loss: 0.08274) (accu: 0.9750)
[epoch : 11] (l_loss: 0.05896) (t_loss: 0.08324) (accu: 0.9738)
[epoch : 12] (l_loss: 0.05835) (t_loss: 0.07892) (accu: 0.9754)
[epoch : 13] (l_loss: 0.05724) (t_loss: 0.08031) (accu: 0.9756)
[epoch : 14] (l_loss: 0.05668) (t_loss: 0.07893) (accu: 0.9759)
[epoch : 15] (l_loss: 0.05595) (t_loss: 0.07830) (accu: 0.9753)
[epoch : 16] (l_loss: 0.05613) (t_loss: 0.07634) (accu: 0.9769)
[epoch : 17] (l_loss: 0.05549) (t_loss: 0.07970) (accu: 0.9750)
[epoch : 18] (l_loss: 0.05513) (t_loss: 0.07864) (accu: 0.9768)
[epoch : 19] (l_loss: 0.05488) (t_loss: 0.08143) (accu: 0.9743)
[epoch : 20] (l_loss: 0.05493) (t_loss: 0.08091) (accu: 0.9760)
[epoch : 21] (l_loss: 0.05499) (t_loss: 0.07929) (accu: 0.9756)
[epoch : 22] (l_loss: 0.05472) (t_loss: 0.08127) (accu: 0.9750)
[epoch : 23] (l_loss: 0.05436) (t_loss: 0.07871) (accu: 0.9754)
[epoch : 24] (l_loss: 0.05435) (t_loss: 0.07650) (accu: 0.9755)
[epoch : 25] (l_loss: 0.05488) (t_loss: 0.07912) (accu: 0.9750)
[epoch : 26] (l_loss: 0.05412) (t_loss: 0.07921) (accu: 0.9751)
[epoch : 27] (l_loss: 0.05408) (t_loss: 0.07903) (accu: 0.9748)
[epoch : 28] (l_loss: 0.05403) (t_loss: 0.08010) (accu: 0.9752)
[epoch : 29] (l_loss: 0.05433) (t_loss: 0.07780) (accu: 0.9757)
[epoch : 30] (l_loss: 0.05431) (t_loss: 0.07838) (accu: 0.9749)
[epoch : 31] (l_loss: 0.05411) (t_loss: 0.08425) (accu: 0.9746)
[epoch : 32] (l_loss: 0.05425) (t_loss: 0.07781) (accu: 0.9756)
[epoch : 33] (l_loss: 0.05436) (t_loss: 0.07982) (accu: 0.9757)
[epoch : 34] (l_loss: 0.05399) (t_loss: 0.07745) (accu: 0.9749)
[epoch : 35] (l_loss: 0.05413) (t_loss: 0.07727) (accu: 0.9761)
[epoch : 36] (l_loss: 0.05410) (t_loss: 0.08049) (accu: 0.9745)
[epoch : 37] (l_loss: 0.05430) (t_loss: 0.07691) (accu: 0.9760)
[epoch : 38] (l_loss: 0.05420) (t_loss: 0.07684) (accu: 0.9753)
[epoch : 39] (l_loss: 0.05398) (t_loss: 0.07759) (accu: 0.9750)
[epoch : 40] (l_loss: 0.05425) (t_loss: 0.07920) (accu: 0.9756)
[epoch : 41] (l_loss: 0.05422) (t_loss: 0.07752) (accu: 0.9757)
[epoch : 42] (l_loss: 0.05396) (t_loss: 0.07807) (accu: 0.9745)
[epoch : 43] (l_loss: 0.05380) (t_loss: 0.07720) (accu: 0.9761)
[epoch : 44] (l_loss: 0.05343) (t_loss: 0.07961) (accu: 0.9748)
[epoch : 45] (l_loss: 0.05405) (t_loss: 0.07878) (accu: 0.9750)
[epoch : 46] (l_loss: 0.05379) (t_loss: 0.07852) (accu: 0.9753)
[epoch : 47] (l_loss: 0.05388) (t_loss: 0.07917) (accu: 0.9751)
[epoch : 48] (l_loss: 0.05373) (t_loss: 0.07689) (accu: 0.9756)
[epoch : 49] (l_loss: 0.05401) (t_loss: 0.07797) (accu: 0.9755)
[epoch : 50] (l_loss: 0.05386) (t_loss: 0.07659) (accu: 0.9758)
Finish! (Best accu: 0.9769) (Time taken(sec) : 697.27) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (3196 | 212304)          1.48
fc1.weight   :       196000 (2825 | 193175)          1.44
fc2.weight   :        18750 (270 | 18480)            1.44
fcout.weight :          750 (101 | 649)             13.47
------------------------------------------------------------
[Prune_iter : (20/21), Remaining weight : 1.48 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30056) (accu: 0.0971)
[epoch : 1] (l_loss: 0.44929) (t_loss: 0.16292) (accu: 0.9532)
[epoch : 2] (l_loss: 0.13701) (t_loss: 0.12078) (accu: 0.9645)
[epoch : 3] (l_loss: 0.10437) (t_loss: 0.10446) (accu: 0.9684)
[epoch : 4] (l_loss: 0.08823) (t_loss: 0.09589) (accu: 0.9699)
[epoch : 5] (l_loss: 0.07888) (t_loss: 0.09121) (accu: 0.9720)
[epoch : 6] (l_loss: 0.07267) (t_loss: 0.08856) (accu: 0.9726)
[epoch : 7] (l_loss: 0.06905) (t_loss: 0.08413) (accu: 0.9745)
[epoch : 8] (l_loss: 0.06609) (t_loss: 0.08494) (accu: 0.9736)
[epoch : 9] (l_loss: 0.06409) (t_loss: 0.08548) (accu: 0.9742)
[epoch : 10] (l_loss: 0.06258) (t_loss: 0.08469) (accu: 0.9746)
[epoch : 11] (l_loss: 0.06147) (t_loss: 0.08350) (accu: 0.9755)
[epoch : 12] (l_loss: 0.06016) (t_loss: 0.08247) (accu: 0.9750)
[epoch : 13] (l_loss: 0.05950) (t_loss: 0.08243) (accu: 0.9742)
[epoch : 14] (l_loss: 0.05895) (t_loss: 0.08093) (accu: 0.9749)
[epoch : 15] (l_loss: 0.05792) (t_loss: 0.08340) (accu: 0.9745)
[epoch : 16] (l_loss: 0.05796) (t_loss: 0.08038) (accu: 0.9758)
[epoch : 17] (l_loss: 0.05796) (t_loss: 0.08153) (accu: 0.9753)
[epoch : 18] (l_loss: 0.05737) (t_loss: 0.08188) (accu: 0.9749)
[epoch : 19] (l_loss: 0.05772) (t_loss: 0.08356) (accu: 0.9744)
[epoch : 20] (l_loss: 0.05760) (t_loss: 0.07800) (accu: 0.9756)
[epoch : 21] (l_loss: 0.05733) (t_loss: 0.07621) (accu: 0.9760)
[epoch : 22] (l_loss: 0.05707) (t_loss: 0.08020) (accu: 0.9759)
[epoch : 23] (l_loss: 0.05726) (t_loss: 0.08278) (accu: 0.9737)
[epoch : 24] (l_loss: 0.05692) (t_loss: 0.08003) (accu: 0.9760)
[epoch : 25] (l_loss: 0.05706) (t_loss: 0.07980) (accu: 0.9755)
[epoch : 26] (l_loss: 0.05668) (t_loss: 0.08407) (accu: 0.9733)
[epoch : 27] (l_loss: 0.05682) (t_loss: 0.08435) (accu: 0.9736)
[epoch : 28] (l_loss: 0.05682) (t_loss: 0.07790) (accu: 0.9776)
[epoch : 29] (l_loss: 0.05664) (t_loss: 0.07974) (accu: 0.9743)
[epoch : 30] (l_loss: 0.05659) (t_loss: 0.07790) (accu: 0.9757)
[epoch : 31] (l_loss: 0.05691) (t_loss: 0.08265) (accu: 0.9751)
[epoch : 32] (l_loss: 0.05645) (t_loss: 0.07929) (accu: 0.9750)
[epoch : 33] (l_loss: 0.05658) (t_loss: 0.07823) (accu: 0.9757)
[epoch : 34] (l_loss: 0.05594) (t_loss: 0.07847) (accu: 0.9753)
[epoch : 35] (l_loss: 0.05517) (t_loss: 0.07947) (accu: 0.9763)
[epoch : 36] (l_loss: 0.05568) (t_loss: 0.08031) (accu: 0.9747)
[epoch : 37] (l_loss: 0.05551) (t_loss: 0.08146) (accu: 0.9749)
[epoch : 38] (l_loss: 0.05598) (t_loss: 0.07934) (accu: 0.9743)
[epoch : 39] (l_loss: 0.05554) (t_loss: 0.07841) (accu: 0.9760)
[epoch : 40] (l_loss: 0.05590) (t_loss: 0.08106) (accu: 0.9749)
[epoch : 41] (l_loss: 0.05556) (t_loss: 0.08146) (accu: 0.9744)
[epoch : 42] (l_loss: 0.05572) (t_loss: 0.07942) (accu: 0.9757)
[epoch : 43] (l_loss: 0.05536) (t_loss: 0.08064) (accu: 0.9750)
[epoch : 44] (l_loss: 0.05560) (t_loss: 0.07971) (accu: 0.9742)
[epoch : 45] (l_loss: 0.05555) (t_loss: 0.07916) (accu: 0.9756)
[epoch : 46] (l_loss: 0.05525) (t_loss: 0.07907) (accu: 0.9756)
[epoch : 47] (l_loss: 0.05540) (t_loss: 0.07951) (accu: 0.9749)
[epoch : 48] (l_loss: 0.05553) (t_loss: 0.08115) (accu: 0.9751)
[epoch : 49] (l_loss: 0.05551) (t_loss: 0.07825) (accu: 0.9758)
[epoch : 50] (l_loss: 0.05568) (t_loss: 0.07898) (accu: 0.9751)
Finish! (Best accu: 0.9776) (Time taken(sec) : 752.45) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (2567 | 212933)          1.19
fc1.weight   :       196000 (2260 | 193740)          1.15
fc2.weight   :        18750 (216 | 18534)            1.15
fcout.weight :           750 (91 | 659)             12.13
------------------------------------------------------------
[Prune_iter : (21/21), Remaining weight : 1.19 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30001) (accu: 0.0916)
[epoch : 1] (l_loss: 0.45201) (t_loss: 0.16242) (accu: 0.9549)
[epoch : 2] (l_loss: 0.13354) (t_loss: 0.11767) (accu: 0.9663)
[epoch : 3] (l_loss: 0.10113) (t_loss: 0.10185) (accu: 0.9692)
[epoch : 4] (l_loss: 0.08516) (t_loss: 0.09518) (accu: 0.9710)
[epoch : 5] (l_loss: 0.07653) (t_loss: 0.09086) (accu: 0.9720)
[epoch : 6] (l_loss: 0.07013) (t_loss: 0.08768) (accu: 0.9726)
[epoch : 7] (l_loss: 0.06624) (t_loss: 0.08720) (accu: 0.9729)
[epoch : 8] (l_loss: 0.06345) (t_loss: 0.08667) (accu: 0.9741)
[epoch : 9] (l_loss: 0.06123) (t_loss: 0.08343) (accu: 0.9738)
[epoch : 10] (l_loss: 0.05977) (t_loss: 0.08202) (accu: 0.9756)
[epoch : 11] (l_loss: 0.05918) (t_loss: 0.08282) (accu: 0.9738)
[epoch : 12] (l_loss: 0.05764) (t_loss: 0.08236) (accu: 0.9746)
[epoch : 13] (l_loss: 0.05760) (t_loss: 0.07857) (accu: 0.9756)
[epoch : 14] (l_loss: 0.05688) (t_loss: 0.08032) (accu: 0.9750)
[epoch : 15] (l_loss: 0.05683) (t_loss: 0.08146) (accu: 0.9741)
[epoch : 16] (l_loss: 0.05679) (t_loss: 0.07869) (accu: 0.9762)
[epoch : 17] (l_loss: 0.05610) (t_loss: 0.08008) (accu: 0.9749)
[epoch : 18] (l_loss: 0.05584) (t_loss: 0.08018) (accu: 0.9744)
[epoch : 19] (l_loss: 0.05543) (t_loss: 0.07936) (accu: 0.9744)
[epoch : 20] (l_loss: 0.05544) (t_loss: 0.08153) (accu: 0.9758)
[epoch : 21] (l_loss: 0.05566) (t_loss: 0.07885) (accu: 0.9762)
[epoch : 22] (l_loss: 0.05532) (t_loss: 0.07887) (accu: 0.9746)
[epoch : 23] (l_loss: 0.05538) (t_loss: 0.07898) (accu: 0.9754)
[epoch : 24] (l_loss: 0.05468) (t_loss: 0.07900) (accu: 0.9741)
[epoch : 25] (l_loss: 0.05539) (t_loss: 0.08046) (accu: 0.9744)
[epoch : 26] (l_loss: 0.05519) (t_loss: 0.07882) (accu: 0.9751)
[epoch : 27] (l_loss: 0.05504) (t_loss: 0.08026) (accu: 0.9746)
[epoch : 28] (l_loss: 0.05449) (t_loss: 0.08220) (accu: 0.9742)
[epoch : 29] (l_loss: 0.05534) (t_loss: 0.08035) (accu: 0.9753)
[epoch : 30] (l_loss: 0.05456) (t_loss: 0.07747) (accu: 0.9754)
[epoch : 31] (l_loss: 0.05500) (t_loss: 0.08258) (accu: 0.9752)
[epoch : 32] (l_loss: 0.05481) (t_loss: 0.07973) (accu: 0.9747)
[epoch : 33] (l_loss: 0.05505) (t_loss: 0.07927) (accu: 0.9746)
[epoch : 34] (l_loss: 0.05494) (t_loss: 0.07841) (accu: 0.9754)
[epoch : 35] (l_loss: 0.05488) (t_loss: 0.07824) (accu: 0.9756)
[epoch : 36] (l_loss: 0.05482) (t_loss: 0.08153) (accu: 0.9744)
[epoch : 37] (l_loss: 0.05477) (t_loss: 0.07803) (accu: 0.9758)
[epoch : 38] (l_loss: 0.05485) (t_loss: 0.07658) (accu: 0.9762)
[epoch : 39] (l_loss: 0.05454) (t_loss: 0.07795) (accu: 0.9755)
[epoch : 40] (l_loss: 0.05479) (t_loss: 0.07718) (accu: 0.9756)
[epoch : 41] (l_loss: 0.05443) (t_loss: 0.07640) (accu: 0.9757)
[epoch : 42] (l_loss: 0.05396) (t_loss: 0.08021) (accu: 0.9744)
[epoch : 43] (l_loss: 0.05440) (t_loss: 0.07801) (accu: 0.9759)
[epoch : 44] (l_loss: 0.05375) (t_loss: 0.07819) (accu: 0.9756)
[epoch : 45] (l_loss: 0.05369) (t_loss: 0.07752) (accu: 0.9761)
[epoch : 46] (l_loss: 0.05400) (t_loss: 0.07800) (accu: 0.9752)
[epoch : 47] (l_loss: 0.05448) (t_loss: 0.07652) (accu: 0.9762)
[epoch : 48] (l_loss: 0.05393) (t_loss: 0.08102) (accu: 0.9761)
[epoch : 49] (l_loss: 0.05404) (t_loss: 0.08005) (accu: 0.9761)
[epoch : 50] (l_loss: 0.05389) (t_loss: 0.08302) (accu: 0.9744)
Finish! (Best accu: 0.9762) (Time taken(sec) : 776.84) 


Maximum accuracy per weight remaining
Remaining weight 100.0 %  Epoch 41 Accu 0.9773
Remaining weight 80.03 %  Epoch 31 Accu 0.9760
Remaining weight 64.06 %  Epoch 40 Accu 0.9781
Remaining weight 51.28 %  Epoch 40 Accu 0.9783
Remaining weight 41.05 %  Epoch 45 Accu 0.9771
Remaining weight 32.86 %  Epoch 31 Accu 0.9781
Remaining weight 26.31 %  Epoch 16 Accu 0.9778
Remaining weight 21.06 %  Epoch 42 Accu 0.9780
Remaining weight 16.87 %  Epoch 48 Accu 0.9779
Remaining weight 13.51 %  Epoch 32 Accu 0.9778
Remaining weight 10.82 %  Epoch 49 Accu 0.9779
Remaining weight 8.67 %  Epoch 11 Accu 0.9783
Remaining weight 6.95 %  Epoch 44 Accu 0.9785
Remaining weight 5.57 %  Epoch 38 Accu 0.9780
Remaining weight 4.46 %  Epoch 22 Accu 0.9779
Remaining weight 3.58 %  Epoch 30 Accu 0.9765
Remaining weight 2.87 %  Epoch 33 Accu 0.9767
Remaining weight 2.3 %  Epoch 35 Accu 0.9771
Remaining weight 1.85 %  Epoch 15 Accu 0.9769
Remaining weight 1.48 %  Epoch 27 Accu 0.9776
Remaining weight 1.19 %  Epoch 46 Accu 0.9762
===================================================================== 

Test_Iter (3/5)
------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :        215500 (215500 | 0)          100.00
fc1.weight   :        196000 (196000 | 0)          100.00
fc2.weight   :         18750 (18750 | 0)           100.00
fcout.weight :           750 (750 | 0)             100.00
------------------------------------------------------------
[Prune_iter : (1/21), Remaining weight : 100.0 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.71259) (accu: 0.0803)
[epoch : 1] (l_loss: 0.50870) (t_loss: 0.16498) (accu: 0.9515)
[epoch : 2] (l_loss: 0.13593) (t_loss: 0.11656) (accu: 0.9655)
[epoch : 3] (l_loss: 0.10154) (t_loss: 0.09899) (accu: 0.9700)
[epoch : 4] (l_loss: 0.08533) (t_loss: 0.09246) (accu: 0.9715)
[epoch : 5] (l_loss: 0.07599) (t_loss: 0.08718) (accu: 0.9740)
[epoch : 6] (l_loss: 0.06996) (t_loss: 0.08412) (accu: 0.9735)
[epoch : 7] (l_loss: 0.06599) (t_loss: 0.08214) (accu: 0.9742)
[epoch : 8] (l_loss: 0.06294) (t_loss: 0.08156) (accu: 0.9743)
[epoch : 9] (l_loss: 0.06108) (t_loss: 0.08004) (accu: 0.9740)
[epoch : 10] (l_loss: 0.05947) (t_loss: 0.07904) (accu: 0.9761)
[epoch : 11] (l_loss: 0.05766) (t_loss: 0.08002) (accu: 0.9750)
[epoch : 12] (l_loss: 0.05783) (t_loss: 0.07925) (accu: 0.9756)
[epoch : 13] (l_loss: 0.05712) (t_loss: 0.07912) (accu: 0.9756)
[epoch : 14] (l_loss: 0.05709) (t_loss: 0.08000) (accu: 0.9762)
[epoch : 15] (l_loss: 0.05654) (t_loss: 0.07551) (accu: 0.9769)
[epoch : 16] (l_loss: 0.05582) (t_loss: 0.07788) (accu: 0.9756)
[epoch : 17] (l_loss: 0.05647) (t_loss: 0.07876) (accu: 0.9761)
[epoch : 18] (l_loss: 0.05575) (t_loss: 0.07960) (accu: 0.9755)
[epoch : 19] (l_loss: 0.05570) (t_loss: 0.07824) (accu: 0.9751)
[epoch : 20] (l_loss: 0.05544) (t_loss: 0.07733) (accu: 0.9746)
[epoch : 21] (l_loss: 0.05536) (t_loss: 0.07774) (accu: 0.9757)
[epoch : 22] (l_loss: 0.05558) (t_loss: 0.07707) (accu: 0.9768)
[epoch : 23] (l_loss: 0.05545) (t_loss: 0.08014) (accu: 0.9753)
[epoch : 24] (l_loss: 0.05554) (t_loss: 0.08088) (accu: 0.9751)
[epoch : 25] (l_loss: 0.05551) (t_loss: 0.07870) (accu: 0.9762)
[epoch : 26] (l_loss: 0.05511) (t_loss: 0.07815) (accu: 0.9751)
[epoch : 27] (l_loss: 0.05550) (t_loss: 0.07557) (accu: 0.9757)
[epoch : 28] (l_loss: 0.05501) (t_loss: 0.07974) (accu: 0.9756)
[epoch : 29] (l_loss: 0.05522) (t_loss: 0.07782) (accu: 0.9761)
[epoch : 30] (l_loss: 0.05538) (t_loss: 0.07797) (accu: 0.9753)
[epoch : 31] (l_loss: 0.05486) (t_loss: 0.07679) (accu: 0.9773)
[epoch : 32] (l_loss: 0.05522) (t_loss: 0.07812) (accu: 0.9757)
[epoch : 33] (l_loss: 0.05512) (t_loss: 0.07715) (accu: 0.9767)
[epoch : 34] (l_loss: 0.05488) (t_loss: 0.07874) (accu: 0.9746)
[epoch : 35] (l_loss: 0.05499) (t_loss: 0.07676) (accu: 0.9769)
[epoch : 36] (l_loss: 0.05514) (t_loss: 0.07979) (accu: 0.9753)
[epoch : 37] (l_loss: 0.05475) (t_loss: 0.07687) (accu: 0.9758)
[epoch : 38] (l_loss: 0.05467) (t_loss: 0.07730) (accu: 0.9757)
[epoch : 39] (l_loss: 0.05489) (t_loss: 0.07900) (accu: 0.9754)
[epoch : 40] (l_loss: 0.05487) (t_loss: 0.07864) (accu: 0.9759)
[epoch : 41] (l_loss: 0.05484) (t_loss: 0.07448) (accu: 0.9768)
[epoch : 42] (l_loss: 0.05469) (t_loss: 0.08261) (accu: 0.9743)
[epoch : 43] (l_loss: 0.05502) (t_loss: 0.07946) (accu: 0.9739)
[epoch : 44] (l_loss: 0.05511) (t_loss: 0.07906) (accu: 0.9750)
[epoch : 45] (l_loss: 0.05475) (t_loss: 0.08133) (accu: 0.9752)
[epoch : 46] (l_loss: 0.05469) (t_loss: 0.07682) (accu: 0.9769)
[epoch : 47] (l_loss: 0.05527) (t_loss: 0.07609) (accu: 0.9763)
[epoch : 48] (l_loss: 0.05498) (t_loss: 0.07936) (accu: 0.9765)
[epoch : 49] (l_loss: 0.05482) (t_loss: 0.07862) (accu: 0.9754)
[epoch : 50] (l_loss: 0.05476) (t_loss: 0.07932) (accu: 0.9748)
Finish! (Best accu: 0.9773) (Time taken(sec) : 761.57) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (172475 | 43025)         80.03
fc1.weight   :      196000 (156800 | 39200)         80.00
fc2.weight   :        18750 (15000 | 3750)          80.00
fcout.weight :           750 (675 | 75)             90.00
------------------------------------------------------------
[Prune_iter : (2/21), Remaining weight : 80.03 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.48247) (accu: 0.0990)
[epoch : 1] (l_loss: 0.50564) (t_loss: 0.16679) (accu: 0.9517)
[epoch : 2] (l_loss: 0.13717) (t_loss: 0.11967) (accu: 0.9634)
[epoch : 3] (l_loss: 0.10196) (t_loss: 0.10260) (accu: 0.9690)
[epoch : 4] (l_loss: 0.08592) (t_loss: 0.09384) (accu: 0.9712)
[epoch : 5] (l_loss: 0.07714) (t_loss: 0.09061) (accu: 0.9720)
[epoch : 6] (l_loss: 0.07081) (t_loss: 0.08446) (accu: 0.9748)
[epoch : 7] (l_loss: 0.06741) (t_loss: 0.08225) (accu: 0.9741)
[epoch : 8] (l_loss: 0.06489) (t_loss: 0.08447) (accu: 0.9730)
[epoch : 9] (l_loss: 0.06354) (t_loss: 0.08281) (accu: 0.9742)
[epoch : 10] (l_loss: 0.06206) (t_loss: 0.08205) (accu: 0.9742)
[epoch : 11] (l_loss: 0.06082) (t_loss: 0.08450) (accu: 0.9738)
[epoch : 12] (l_loss: 0.06028) (t_loss: 0.07991) (accu: 0.9751)
[epoch : 13] (l_loss: 0.05954) (t_loss: 0.08139) (accu: 0.9753)
[epoch : 14] (l_loss: 0.05976) (t_loss: 0.07734) (accu: 0.9761)
[epoch : 15] (l_loss: 0.05859) (t_loss: 0.07855) (accu: 0.9759)
[epoch : 16] (l_loss: 0.05857) (t_loss: 0.08052) (accu: 0.9743)
[epoch : 17] (l_loss: 0.05795) (t_loss: 0.08113) (accu: 0.9746)
[epoch : 18] (l_loss: 0.05862) (t_loss: 0.08238) (accu: 0.9743)
[epoch : 19] (l_loss: 0.05781) (t_loss: 0.07935) (accu: 0.9755)
[epoch : 20] (l_loss: 0.05803) (t_loss: 0.07850) (accu: 0.9745)
[epoch : 21] (l_loss: 0.05759) (t_loss: 0.08146) (accu: 0.9753)
[epoch : 22] (l_loss: 0.05729) (t_loss: 0.07916) (accu: 0.9764)
[epoch : 23] (l_loss: 0.05774) (t_loss: 0.07791) (accu: 0.9759)
[epoch : 24] (l_loss: 0.05746) (t_loss: 0.07999) (accu: 0.9757)
[epoch : 25] (l_loss: 0.05719) (t_loss: 0.07908) (accu: 0.9748)
[epoch : 26] (l_loss: 0.05718) (t_loss: 0.07845) (accu: 0.9764)
[epoch : 27] (l_loss: 0.05711) (t_loss: 0.07806) (accu: 0.9760)
[epoch : 28] (l_loss: 0.05755) (t_loss: 0.07971) (accu: 0.9749)
[epoch : 29] (l_loss: 0.05763) (t_loss: 0.08502) (accu: 0.9736)
[epoch : 30] (l_loss: 0.05701) (t_loss: 0.07916) (accu: 0.9751)
[epoch : 31] (l_loss: 0.05739) (t_loss: 0.08036) (accu: 0.9747)
[epoch : 32] (l_loss: 0.05701) (t_loss: 0.08482) (accu: 0.9731)
[epoch : 33] (l_loss: 0.05734) (t_loss: 0.08207) (accu: 0.9740)
[epoch : 34] (l_loss: 0.05704) (t_loss: 0.08021) (accu: 0.9745)
[epoch : 35] (l_loss: 0.05716) (t_loss: 0.08174) (accu: 0.9740)
[epoch : 36] (l_loss: 0.05706) (t_loss: 0.08272) (accu: 0.9738)
[epoch : 37] (l_loss: 0.05687) (t_loss: 0.08081) (accu: 0.9748)
[epoch : 38] (l_loss: 0.05734) (t_loss: 0.08074) (accu: 0.9754)
[epoch : 39] (l_loss: 0.05711) (t_loss: 0.08028) (accu: 0.9748)
[epoch : 40] (l_loss: 0.05676) (t_loss: 0.08640) (accu: 0.9741)
[epoch : 41] (l_loss: 0.05694) (t_loss: 0.07870) (accu: 0.9751)
[epoch : 42] (l_loss: 0.05675) (t_loss: 0.08426) (accu: 0.9737)
[epoch : 43] (l_loss: 0.05724) (t_loss: 0.08330) (accu: 0.9747)
[epoch : 44] (l_loss: 0.05698) (t_loss: 0.08055) (accu: 0.9745)
[epoch : 45] (l_loss: 0.05691) (t_loss: 0.08491) (accu: 0.9735)
[epoch : 46] (l_loss: 0.05685) (t_loss: 0.08278) (accu: 0.9738)
[epoch : 47] (l_loss: 0.05726) (t_loss: 0.07810) (accu: 0.9753)
[epoch : 48] (l_loss: 0.05685) (t_loss: 0.08090) (accu: 0.9748)
[epoch : 49] (l_loss: 0.05662) (t_loss: 0.08021) (accu: 0.9743)
[epoch : 50] (l_loss: 0.05670) (t_loss: 0.08025) (accu: 0.9758)
Finish! (Best accu: 0.9764) (Time taken(sec) : 744.53) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (138048 | 77452)         64.06
fc1.weight   :      196000 (125440 | 70560)         64.00
fc2.weight   :        18750 (12000 | 6750)          64.00
fcout.weight :          750 (608 | 142)             81.07
------------------------------------------------------------
[Prune_iter : (3/21), Remaining weight : 64.06 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.37768) (accu: 0.0987)
[epoch : 1] (l_loss: 0.48510) (t_loss: 0.15601) (accu: 0.9551)
[epoch : 2] (l_loss: 0.12803) (t_loss: 0.10928) (accu: 0.9683)
[epoch : 3] (l_loss: 0.09500) (t_loss: 0.09410) (accu: 0.9722)
[epoch : 4] (l_loss: 0.07902) (t_loss: 0.08768) (accu: 0.9733)
[epoch : 5] (l_loss: 0.07029) (t_loss: 0.08763) (accu: 0.9745)
[epoch : 6] (l_loss: 0.06454) (t_loss: 0.08419) (accu: 0.9742)
[epoch : 7] (l_loss: 0.06178) (t_loss: 0.08186) (accu: 0.9751)
[epoch : 8] (l_loss: 0.05943) (t_loss: 0.08124) (accu: 0.9753)
[epoch : 9] (l_loss: 0.05682) (t_loss: 0.08179) (accu: 0.9756)
[epoch : 10] (l_loss: 0.05594) (t_loss: 0.08062) (accu: 0.9764)
[epoch : 11] (l_loss: 0.05456) (t_loss: 0.07930) (accu: 0.9761)
[epoch : 12] (l_loss: 0.05437) (t_loss: 0.07840) (accu: 0.9770)
[epoch : 13] (l_loss: 0.05391) (t_loss: 0.07770) (accu: 0.9754)
[epoch : 14] (l_loss: 0.05316) (t_loss: 0.07744) (accu: 0.9767)
[epoch : 15] (l_loss: 0.05299) (t_loss: 0.07511) (accu: 0.9762)
[epoch : 16] (l_loss: 0.05274) (t_loss: 0.07905) (accu: 0.9761)
[epoch : 17] (l_loss: 0.05226) (t_loss: 0.07905) (accu: 0.9751)
[epoch : 18] (l_loss: 0.05246) (t_loss: 0.07628) (accu: 0.9776)
[epoch : 19] (l_loss: 0.05246) (t_loss: 0.07676) (accu: 0.9760)
[epoch : 20] (l_loss: 0.05257) (t_loss: 0.07637) (accu: 0.9759)
[epoch : 21] (l_loss: 0.05211) (t_loss: 0.07658) (accu: 0.9758)
[epoch : 22] (l_loss: 0.05239) (t_loss: 0.07733) (accu: 0.9763)
[epoch : 23] (l_loss: 0.05213) (t_loss: 0.07686) (accu: 0.9768)
[epoch : 24] (l_loss: 0.05206) (t_loss: 0.07674) (accu: 0.9768)
[epoch : 25] (l_loss: 0.05218) (t_loss: 0.07622) (accu: 0.9772)
[epoch : 26] (l_loss: 0.05182) (t_loss: 0.07515) (accu: 0.9775)
[epoch : 27] (l_loss: 0.05217) (t_loss: 0.07583) (accu: 0.9779)
[epoch : 28] (l_loss: 0.05236) (t_loss: 0.07542) (accu: 0.9770)
[epoch : 29] (l_loss: 0.05165) (t_loss: 0.07930) (accu: 0.9751)
[epoch : 30] (l_loss: 0.05166) (t_loss: 0.07902) (accu: 0.9761)
[epoch : 31] (l_loss: 0.05172) (t_loss: 0.07836) (accu: 0.9754)
[epoch : 32] (l_loss: 0.05164) (t_loss: 0.07692) (accu: 0.9773)
[epoch : 33] (l_loss: 0.05174) (t_loss: 0.07744) (accu: 0.9765)
[epoch : 34] (l_loss: 0.05187) (t_loss: 0.07801) (accu: 0.9771)
[epoch : 35] (l_loss: 0.05187) (t_loss: 0.07608) (accu: 0.9767)
[epoch : 36] (l_loss: 0.05173) (t_loss: 0.07776) (accu: 0.9775)
[epoch : 37] (l_loss: 0.05225) (t_loss: 0.07692) (accu: 0.9766)
[epoch : 38] (l_loss: 0.05178) (t_loss: 0.07780) (accu: 0.9764)
[epoch : 39] (l_loss: 0.05159) (t_loss: 0.07558) (accu: 0.9759)
[epoch : 40] (l_loss: 0.05158) (t_loss: 0.07767) (accu: 0.9748)
[epoch : 41] (l_loss: 0.05178) (t_loss: 0.07504) (accu: 0.9771)
[epoch : 42] (l_loss: 0.05179) (t_loss: 0.07523) (accu: 0.9776)
[epoch : 43] (l_loss: 0.05188) (t_loss: 0.07647) (accu: 0.9787)
[epoch : 44] (l_loss: 0.05177) (t_loss: 0.07671) (accu: 0.9755)
[epoch : 45] (l_loss: 0.05167) (t_loss: 0.07529) (accu: 0.9769)
[epoch : 46] (l_loss: 0.05173) (t_loss: 0.07603) (accu: 0.9766)
[epoch : 47] (l_loss: 0.05223) (t_loss: 0.07906) (accu: 0.9758)
[epoch : 48] (l_loss: 0.05154) (t_loss: 0.07758) (accu: 0.9751)
[epoch : 49] (l_loss: 0.05220) (t_loss: 0.07531) (accu: 0.9777)
[epoch : 50] (l_loss: 0.05192) (t_loss: 0.07669) (accu: 0.9774)
Finish! (Best accu: 0.9787) (Time taken(sec) : 716.89) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (110499 | 105001)        51.28
fc1.weight   :      196000 (100352 | 95648)         51.20
fc2.weight   :        18750 (9600 | 9150)           51.20
fcout.weight :          750 (547 | 203)             72.93
------------------------------------------------------------
[Prune_iter : (4/21), Remaining weight : 51.28 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.33904) (accu: 0.0974)
[epoch : 1] (l_loss: 0.48172) (t_loss: 0.15925) (accu: 0.9535)
[epoch : 2] (l_loss: 0.13140) (t_loss: 0.11592) (accu: 0.9656)
[epoch : 3] (l_loss: 0.09839) (t_loss: 0.09961) (accu: 0.9708)
[epoch : 4] (l_loss: 0.08345) (t_loss: 0.09357) (accu: 0.9718)
[epoch : 5] (l_loss: 0.07487) (t_loss: 0.08817) (accu: 0.9738)
[epoch : 6] (l_loss: 0.06881) (t_loss: 0.08637) (accu: 0.9756)
[epoch : 7] (l_loss: 0.06436) (t_loss: 0.08478) (accu: 0.9736)
[epoch : 8] (l_loss: 0.06089) (t_loss: 0.08366) (accu: 0.9751)
[epoch : 9] (l_loss: 0.05914) (t_loss: 0.08131) (accu: 0.9760)
[epoch : 10] (l_loss: 0.05753) (t_loss: 0.08039) (accu: 0.9752)
[epoch : 11] (l_loss: 0.05611) (t_loss: 0.07881) (accu: 0.9761)
[epoch : 12] (l_loss: 0.05539) (t_loss: 0.08138) (accu: 0.9765)
[epoch : 13] (l_loss: 0.05426) (t_loss: 0.07725) (accu: 0.9762)
[epoch : 14] (l_loss: 0.05417) (t_loss: 0.07995) (accu: 0.9764)
[epoch : 15] (l_loss: 0.05375) (t_loss: 0.08076) (accu: 0.9757)
[epoch : 16] (l_loss: 0.05393) (t_loss: 0.07779) (accu: 0.9763)
[epoch : 17] (l_loss: 0.05370) (t_loss: 0.07652) (accu: 0.9771)
[epoch : 18] (l_loss: 0.05320) (t_loss: 0.07617) (accu: 0.9770)
[epoch : 19] (l_loss: 0.05345) (t_loss: 0.07761) (accu: 0.9770)
[epoch : 20] (l_loss: 0.05273) (t_loss: 0.07849) (accu: 0.9772)
[epoch : 21] (l_loss: 0.05279) (t_loss: 0.07599) (accu: 0.9780)
[epoch : 22] (l_loss: 0.05274) (t_loss: 0.07802) (accu: 0.9763)
[epoch : 23] (l_loss: 0.05268) (t_loss: 0.07846) (accu: 0.9752)
[epoch : 24] (l_loss: 0.05293) (t_loss: 0.07917) (accu: 0.9753)
[epoch : 25] (l_loss: 0.05300) (t_loss: 0.07739) (accu: 0.9771)
[epoch : 26] (l_loss: 0.05252) (t_loss: 0.07916) (accu: 0.9770)
[epoch : 27] (l_loss: 0.05263) (t_loss: 0.07757) (accu: 0.9762)
[epoch : 28] (l_loss: 0.05211) (t_loss: 0.07770) (accu: 0.9762)
[epoch : 29] (l_loss: 0.05279) (t_loss: 0.07611) (accu: 0.9771)
[epoch : 30] (l_loss: 0.05269) (t_loss: 0.07598) (accu: 0.9772)
[epoch : 31] (l_loss: 0.05242) (t_loss: 0.07780) (accu: 0.9762)
[epoch : 32] (l_loss: 0.05169) (t_loss: 0.07591) (accu: 0.9766)
[epoch : 33] (l_loss: 0.05203) (t_loss: 0.07701) (accu: 0.9762)
[epoch : 34] (l_loss: 0.05225) (t_loss: 0.07786) (accu: 0.9764)
[epoch : 35] (l_loss: 0.05199) (t_loss: 0.07704) (accu: 0.9757)
[epoch : 36] (l_loss: 0.05210) (t_loss: 0.07788) (accu: 0.9764)
[epoch : 37] (l_loss: 0.05222) (t_loss: 0.07923) (accu: 0.9764)
[epoch : 38] (l_loss: 0.05169) (t_loss: 0.07416) (accu: 0.9769)
[epoch : 39] (l_loss: 0.05169) (t_loss: 0.07725) (accu: 0.9776)
[epoch : 40] (l_loss: 0.05149) (t_loss: 0.07698) (accu: 0.9778)
[epoch : 41] (l_loss: 0.05150) (t_loss: 0.07706) (accu: 0.9765)
[epoch : 42] (l_loss: 0.05195) (t_loss: 0.07520) (accu: 0.9770)
[epoch : 43] (l_loss: 0.05111) (t_loss: 0.07776) (accu: 0.9762)
[epoch : 44] (l_loss: 0.05204) (t_loss: 0.07836) (accu: 0.9756)
[epoch : 45] (l_loss: 0.05177) (t_loss: 0.07597) (accu: 0.9762)
[epoch : 46] (l_loss: 0.05154) (t_loss: 0.07650) (accu: 0.9771)
[epoch : 47] (l_loss: 0.05140) (t_loss: 0.07795) (accu: 0.9769)
[epoch : 48] (l_loss: 0.05195) (t_loss: 0.07621) (accu: 0.9751)
[epoch : 49] (l_loss: 0.05164) (t_loss: 0.07920) (accu: 0.9750)
[epoch : 50] (l_loss: 0.05129) (t_loss: 0.07746) (accu: 0.9761)
Finish! (Best accu: 0.9780) (Time taken(sec) : 714.40) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (88454 | 127046)         41.05
fc1.weight   :      196000 (80282 | 115718)         40.96
fc2.weight   :        18750 (7680 | 11070)          40.96
fcout.weight :          750 (492 | 258)             65.60
------------------------------------------------------------
[Prune_iter : (5/21), Remaining weight : 41.05 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.34488) (accu: 0.1115)
[epoch : 1] (l_loss: 0.48202) (t_loss: 0.16964) (accu: 0.9498)
[epoch : 2] (l_loss: 0.13879) (t_loss: 0.11726) (accu: 0.9647)
[epoch : 3] (l_loss: 0.10155) (t_loss: 0.09829) (accu: 0.9708)
[epoch : 4] (l_loss: 0.08387) (t_loss: 0.09028) (accu: 0.9733)
[epoch : 5] (l_loss: 0.07382) (t_loss: 0.08495) (accu: 0.9750)
[epoch : 6] (l_loss: 0.06771) (t_loss: 0.08558) (accu: 0.9730)
[epoch : 7] (l_loss: 0.06381) (t_loss: 0.08777) (accu: 0.9724)
[epoch : 8] (l_loss: 0.06091) (t_loss: 0.08102) (accu: 0.9762)
[epoch : 9] (l_loss: 0.05909) (t_loss: 0.08006) (accu: 0.9757)
[epoch : 10] (l_loss: 0.05756) (t_loss: 0.08334) (accu: 0.9745)
[epoch : 11] (l_loss: 0.05654) (t_loss: 0.08136) (accu: 0.9765)
[epoch : 12] (l_loss: 0.05573) (t_loss: 0.08143) (accu: 0.9743)
[epoch : 13] (l_loss: 0.05554) (t_loss: 0.07923) (accu: 0.9763)
[epoch : 14] (l_loss: 0.05513) (t_loss: 0.07977) (accu: 0.9756)
[epoch : 15] (l_loss: 0.05458) (t_loss: 0.07984) (accu: 0.9761)
[epoch : 16] (l_loss: 0.05493) (t_loss: 0.07971) (accu: 0.9762)
[epoch : 17] (l_loss: 0.05460) (t_loss: 0.08034) (accu: 0.9766)
[epoch : 18] (l_loss: 0.05416) (t_loss: 0.07977) (accu: 0.9764)
[epoch : 19] (l_loss: 0.05426) (t_loss: 0.07802) (accu: 0.9752)
[epoch : 20] (l_loss: 0.05369) (t_loss: 0.07861) (accu: 0.9773)
[epoch : 21] (l_loss: 0.05414) (t_loss: 0.08018) (accu: 0.9748)
[epoch : 22] (l_loss: 0.05390) (t_loss: 0.07985) (accu: 0.9759)
[epoch : 23] (l_loss: 0.05386) (t_loss: 0.07584) (accu: 0.9772)
[epoch : 24] (l_loss: 0.05395) (t_loss: 0.07851) (accu: 0.9763)
[epoch : 25] (l_loss: 0.05380) (t_loss: 0.07701) (accu: 0.9755)
[epoch : 26] (l_loss: 0.05388) (t_loss: 0.07646) (accu: 0.9765)
[epoch : 27] (l_loss: 0.05335) (t_loss: 0.07758) (accu: 0.9772)
[epoch : 28] (l_loss: 0.05317) (t_loss: 0.07906) (accu: 0.9763)
[epoch : 29] (l_loss: 0.05373) (t_loss: 0.08047) (accu: 0.9763)
[epoch : 30] (l_loss: 0.05376) (t_loss: 0.07625) (accu: 0.9767)
[epoch : 31] (l_loss: 0.05371) (t_loss: 0.07842) (accu: 0.9762)
[epoch : 32] (l_loss: 0.05332) (t_loss: 0.08541) (accu: 0.9736)
[epoch : 33] (l_loss: 0.05338) (t_loss: 0.08196) (accu: 0.9760)
[epoch : 34] (l_loss: 0.05373) (t_loss: 0.07942) (accu: 0.9767)
[epoch : 35] (l_loss: 0.05359) (t_loss: 0.07680) (accu: 0.9766)
[epoch : 36] (l_loss: 0.05348) (t_loss: 0.07822) (accu: 0.9756)
[epoch : 37] (l_loss: 0.05348) (t_loss: 0.07730) (accu: 0.9767)
[epoch : 38] (l_loss: 0.05326) (t_loss: 0.07994) (accu: 0.9767)
[epoch : 39] (l_loss: 0.05359) (t_loss: 0.08104) (accu: 0.9756)
[epoch : 40] (l_loss: 0.05309) (t_loss: 0.08028) (accu: 0.9751)
[epoch : 41] (l_loss: 0.05324) (t_loss: 0.07967) (accu: 0.9761)
[epoch : 42] (l_loss: 0.05316) (t_loss: 0.07946) (accu: 0.9749)
[epoch : 43] (l_loss: 0.05365) (t_loss: 0.07760) (accu: 0.9773)
[epoch : 44] (l_loss: 0.05318) (t_loss: 0.07712) (accu: 0.9766)
[epoch : 45] (l_loss: 0.05371) (t_loss: 0.07965) (accu: 0.9767)
[epoch : 46] (l_loss: 0.05354) (t_loss: 0.07744) (accu: 0.9768)
[epoch : 47] (l_loss: 0.05303) (t_loss: 0.07897) (accu: 0.9759)
[epoch : 48] (l_loss: 0.05380) (t_loss: 0.07971) (accu: 0.9762)
[epoch : 49] (l_loss: 0.05324) (t_loss: 0.08059) (accu: 0.9757)
[epoch : 50] (l_loss: 0.05347) (t_loss: 0.08031) (accu: 0.9756)
Finish! (Best accu: 0.9773) (Time taken(sec) : 706.75) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (70812 | 144688)         32.86
fc1.weight   :      196000 (64225 | 131775)         32.77
fc2.weight   :        18750 (6144 | 12606)          32.77
fcout.weight :          750 (443 | 307)             59.07
------------------------------------------------------------
[Prune_iter : (6/21), Remaining weight : 32.86 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.32570) (accu: 0.1216)
[epoch : 1] (l_loss: 0.48319) (t_loss: 0.16385) (accu: 0.9531)
[epoch : 2] (l_loss: 0.13781) (t_loss: 0.12175) (accu: 0.9630)
[epoch : 3] (l_loss: 0.10469) (t_loss: 0.10602) (accu: 0.9679)
[epoch : 4] (l_loss: 0.08941) (t_loss: 0.09524) (accu: 0.9702)
[epoch : 5] (l_loss: 0.07930) (t_loss: 0.09321) (accu: 0.9718)
[epoch : 6] (l_loss: 0.07334) (t_loss: 0.09095) (accu: 0.9731)
[epoch : 7] (l_loss: 0.06869) (t_loss: 0.08574) (accu: 0.9740)
[epoch : 8] (l_loss: 0.06530) (t_loss: 0.08501) (accu: 0.9740)
[epoch : 9] (l_loss: 0.06264) (t_loss: 0.08522) (accu: 0.9743)
[epoch : 10] (l_loss: 0.06090) (t_loss: 0.08376) (accu: 0.9746)
[epoch : 11] (l_loss: 0.05883) (t_loss: 0.07857) (accu: 0.9757)
[epoch : 12] (l_loss: 0.05708) (t_loss: 0.07911) (accu: 0.9757)
[epoch : 13] (l_loss: 0.05623) (t_loss: 0.07846) (accu: 0.9766)
[epoch : 14] (l_loss: 0.05625) (t_loss: 0.08002) (accu: 0.9759)
[epoch : 15] (l_loss: 0.05549) (t_loss: 0.08232) (accu: 0.9743)
[epoch : 16] (l_loss: 0.05545) (t_loss: 0.07884) (accu: 0.9768)
[epoch : 17] (l_loss: 0.05473) (t_loss: 0.08273) (accu: 0.9741)
[epoch : 18] (l_loss: 0.05431) (t_loss: 0.07869) (accu: 0.9768)
[epoch : 19] (l_loss: 0.05442) (t_loss: 0.08156) (accu: 0.9748)
[epoch : 20] (l_loss: 0.05448) (t_loss: 0.07796) (accu: 0.9766)
[epoch : 21] (l_loss: 0.05438) (t_loss: 0.08332) (accu: 0.9743)
[epoch : 22] (l_loss: 0.05449) (t_loss: 0.08125) (accu: 0.9749)
[epoch : 23] (l_loss: 0.05392) (t_loss: 0.07936) (accu: 0.9760)
[epoch : 24] (l_loss: 0.05405) (t_loss: 0.07866) (accu: 0.9768)
[epoch : 25] (l_loss: 0.05389) (t_loss: 0.07903) (accu: 0.9762)
[epoch : 26] (l_loss: 0.05393) (t_loss: 0.08230) (accu: 0.9759)
[epoch : 27] (l_loss: 0.05394) (t_loss: 0.08112) (accu: 0.9740)
[epoch : 28] (l_loss: 0.05386) (t_loss: 0.08002) (accu: 0.9758)
[epoch : 29] (l_loss: 0.05398) (t_loss: 0.08039) (accu: 0.9756)
[epoch : 30] (l_loss: 0.05399) (t_loss: 0.08050) (accu: 0.9740)
[epoch : 31] (l_loss: 0.05395) (t_loss: 0.07895) (accu: 0.9767)
[epoch : 32] (l_loss: 0.05390) (t_loss: 0.07944) (accu: 0.9754)
[epoch : 33] (l_loss: 0.05401) (t_loss: 0.07758) (accu: 0.9767)
[epoch : 34] (l_loss: 0.05325) (t_loss: 0.07833) (accu: 0.9750)
[epoch : 35] (l_loss: 0.05388) (t_loss: 0.07812) (accu: 0.9764)
[epoch : 36] (l_loss: 0.05392) (t_loss: 0.07879) (accu: 0.9753)
[epoch : 37] (l_loss: 0.05375) (t_loss: 0.08195) (accu: 0.9756)
[epoch : 38] (l_loss: 0.05391) (t_loss: 0.07926) (accu: 0.9759)
[epoch : 39] (l_loss: 0.05373) (t_loss: 0.07944) (accu: 0.9764)
[epoch : 40] (l_loss: 0.05362) (t_loss: 0.08101) (accu: 0.9754)
[epoch : 41] (l_loss: 0.05371) (t_loss: 0.08068) (accu: 0.9751)
[epoch : 42] (l_loss: 0.05385) (t_loss: 0.07963) (accu: 0.9759)
[epoch : 43] (l_loss: 0.05390) (t_loss: 0.07690) (accu: 0.9771)
[epoch : 44] (l_loss: 0.05381) (t_loss: 0.08198) (accu: 0.9753)
[epoch : 45] (l_loss: 0.05406) (t_loss: 0.07624) (accu: 0.9769)
[epoch : 46] (l_loss: 0.05413) (t_loss: 0.07687) (accu: 0.9769)
[epoch : 47] (l_loss: 0.05382) (t_loss: 0.08038) (accu: 0.9771)
[epoch : 48] (l_loss: 0.05339) (t_loss: 0.07922) (accu: 0.9751)
[epoch : 49] (l_loss: 0.05410) (t_loss: 0.07805) (accu: 0.9765)
[epoch : 50] (l_loss: 0.05400) (t_loss: 0.07860) (accu: 0.9766)
Finish! (Best accu: 0.9771) (Time taken(sec) : 726.77) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (56694 | 158806)         26.31
fc1.weight   :      196000 (51380 | 144620)         26.21
fc2.weight   :        18750 (4915 | 13835)          26.21
fcout.weight :          750 (399 | 351)             53.20
------------------------------------------------------------
[Prune_iter : (7/21), Remaining weight : 26.31 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29659) (accu: 0.1446)
[epoch : 1] (l_loss: 0.46892) (t_loss: 0.16066) (accu: 0.9547)
[epoch : 2] (l_loss: 0.13195) (t_loss: 0.11245) (accu: 0.9657)
[epoch : 3] (l_loss: 0.09852) (t_loss: 0.09862) (accu: 0.9708)
[epoch : 4] (l_loss: 0.08262) (t_loss: 0.08910) (accu: 0.9731)
[epoch : 5] (l_loss: 0.07212) (t_loss: 0.08480) (accu: 0.9745)
[epoch : 6] (l_loss: 0.06612) (t_loss: 0.08161) (accu: 0.9753)
[epoch : 7] (l_loss: 0.06233) (t_loss: 0.08085) (accu: 0.9767)
[epoch : 8] (l_loss: 0.05900) (t_loss: 0.08039) (accu: 0.9761)
[epoch : 9] (l_loss: 0.05770) (t_loss: 0.07916) (accu: 0.9752)
[epoch : 10] (l_loss: 0.05642) (t_loss: 0.08234) (accu: 0.9738)
[epoch : 11] (l_loss: 0.05551) (t_loss: 0.08189) (accu: 0.9754)
[epoch : 12] (l_loss: 0.05459) (t_loss: 0.07782) (accu: 0.9762)
[epoch : 13] (l_loss: 0.05451) (t_loss: 0.07927) (accu: 0.9764)
[epoch : 14] (l_loss: 0.05417) (t_loss: 0.07815) (accu: 0.9768)
[epoch : 15] (l_loss: 0.05357) (t_loss: 0.07475) (accu: 0.9773)
[epoch : 16] (l_loss: 0.05316) (t_loss: 0.07652) (accu: 0.9758)
[epoch : 17] (l_loss: 0.05334) (t_loss: 0.07840) (accu: 0.9757)
[epoch : 18] (l_loss: 0.05297) (t_loss: 0.07642) (accu: 0.9770)
[epoch : 19] (l_loss: 0.05253) (t_loss: 0.07598) (accu: 0.9767)
[epoch : 20] (l_loss: 0.05284) (t_loss: 0.07952) (accu: 0.9767)
[epoch : 21] (l_loss: 0.05302) (t_loss: 0.07667) (accu: 0.9771)
[epoch : 22] (l_loss: 0.05321) (t_loss: 0.07711) (accu: 0.9761)
[epoch : 23] (l_loss: 0.05275) (t_loss: 0.07508) (accu: 0.9776)
[epoch : 24] (l_loss: 0.05275) (t_loss: 0.07651) (accu: 0.9771)
[epoch : 25] (l_loss: 0.05285) (t_loss: 0.08054) (accu: 0.9746)
[epoch : 26] (l_loss: 0.05241) (t_loss: 0.07650) (accu: 0.9759)
[epoch : 27] (l_loss: 0.05281) (t_loss: 0.07962) (accu: 0.9761)
[epoch : 28] (l_loss: 0.05265) (t_loss: 0.07765) (accu: 0.9772)
[epoch : 29] (l_loss: 0.05275) (t_loss: 0.07658) (accu: 0.9762)
[epoch : 30] (l_loss: 0.05273) (t_loss: 0.07702) (accu: 0.9762)
[epoch : 31] (l_loss: 0.05262) (t_loss: 0.07676) (accu: 0.9763)
[epoch : 32] (l_loss: 0.05203) (t_loss: 0.07808) (accu: 0.9766)
[epoch : 33] (l_loss: 0.05265) (t_loss: 0.07517) (accu: 0.9765)
[epoch : 34] (l_loss: 0.05256) (t_loss: 0.07805) (accu: 0.9763)
[epoch : 35] (l_loss: 0.05234) (t_loss: 0.07668) (accu: 0.9763)
[epoch : 36] (l_loss: 0.05221) (t_loss: 0.07829) (accu: 0.9761)
[epoch : 37] (l_loss: 0.05209) (t_loss: 0.07727) (accu: 0.9749)
[epoch : 38] (l_loss: 0.05269) (t_loss: 0.07642) (accu: 0.9770)
[epoch : 39] (l_loss: 0.05249) (t_loss: 0.07555) (accu: 0.9759)
[epoch : 40] (l_loss: 0.05244) (t_loss: 0.07808) (accu: 0.9764)
[epoch : 41] (l_loss: 0.05251) (t_loss: 0.07706) (accu: 0.9765)
[epoch : 42] (l_loss: 0.05281) (t_loss: 0.07533) (accu: 0.9777)
[epoch : 43] (l_loss: 0.05227) (t_loss: 0.07623) (accu: 0.9758)
[epoch : 44] (l_loss: 0.05215) (t_loss: 0.07751) (accu: 0.9754)
[epoch : 45] (l_loss: 0.05217) (t_loss: 0.07808) (accu: 0.9762)
[epoch : 46] (l_loss: 0.05224) (t_loss: 0.07786) (accu: 0.9768)
[epoch : 47] (l_loss: 0.05232) (t_loss: 0.07787) (accu: 0.9759)
[epoch : 48] (l_loss: 0.05213) (t_loss: 0.07881) (accu: 0.9745)
[epoch : 49] (l_loss: 0.05277) (t_loss: 0.07733) (accu: 0.9743)
[epoch : 50] (l_loss: 0.05227) (t_loss: 0.07657) (accu: 0.9765)
Finish! (Best accu: 0.9777) (Time taken(sec) : 716.56) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (45395 | 170105)         21.06
fc1.weight   :      196000 (41104 | 154896)         20.97
fc2.weight   :        18750 (3932 | 14818)          20.97
fcout.weight :          750 (359 | 391)             47.87
------------------------------------------------------------
[Prune_iter : (8/21), Remaining weight : 21.06 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.31096) (accu: 0.1361)
[epoch : 1] (l_loss: 0.46728) (t_loss: 0.16392) (accu: 0.9527)
[epoch : 2] (l_loss: 0.13806) (t_loss: 0.12158) (accu: 0.9646)
[epoch : 3] (l_loss: 0.10325) (t_loss: 0.10488) (accu: 0.9681)
[epoch : 4] (l_loss: 0.08550) (t_loss: 0.09321) (accu: 0.9709)
[epoch : 5] (l_loss: 0.07426) (t_loss: 0.08609) (accu: 0.9730)
[epoch : 6] (l_loss: 0.06688) (t_loss: 0.08320) (accu: 0.9753)
[epoch : 7] (l_loss: 0.06201) (t_loss: 0.08156) (accu: 0.9755)
[epoch : 8] (l_loss: 0.05913) (t_loss: 0.08064) (accu: 0.9761)
[epoch : 9] (l_loss: 0.05697) (t_loss: 0.07539) (accu: 0.9776)
[epoch : 10] (l_loss: 0.05547) (t_loss: 0.07912) (accu: 0.9757)
[epoch : 11] (l_loss: 0.05465) (t_loss: 0.07660) (accu: 0.9758)
[epoch : 12] (l_loss: 0.05421) (t_loss: 0.07665) (accu: 0.9766)
[epoch : 13] (l_loss: 0.05332) (t_loss: 0.07751) (accu: 0.9761)
[epoch : 14] (l_loss: 0.05306) (t_loss: 0.07645) (accu: 0.9765)
[epoch : 15] (l_loss: 0.05287) (t_loss: 0.07866) (accu: 0.9762)
[epoch : 16] (l_loss: 0.05273) (t_loss: 0.07391) (accu: 0.9780)
[epoch : 17] (l_loss: 0.05265) (t_loss: 0.07596) (accu: 0.9775)
[epoch : 18] (l_loss: 0.05201) (t_loss: 0.07710) (accu: 0.9764)
[epoch : 19] (l_loss: 0.05217) (t_loss: 0.07572) (accu: 0.9765)
[epoch : 20] (l_loss: 0.05203) (t_loss: 0.07506) (accu: 0.9773)
[epoch : 21] (l_loss: 0.05206) (t_loss: 0.07437) (accu: 0.9773)
[epoch : 22] (l_loss: 0.05196) (t_loss: 0.07718) (accu: 0.9759)
[epoch : 23] (l_loss: 0.05182) (t_loss: 0.07513) (accu: 0.9760)
[epoch : 24] (l_loss: 0.05195) (t_loss: 0.07509) (accu: 0.9761)
[epoch : 25] (l_loss: 0.05168) (t_loss: 0.07514) (accu: 0.9767)
[epoch : 26] (l_loss: 0.05215) (t_loss: 0.07641) (accu: 0.9763)
[epoch : 27] (l_loss: 0.05210) (t_loss: 0.07699) (accu: 0.9761)
[epoch : 28] (l_loss: 0.05175) (t_loss: 0.07583) (accu: 0.9756)
[epoch : 29] (l_loss: 0.05174) (t_loss: 0.07712) (accu: 0.9768)
[epoch : 30] (l_loss: 0.05121) (t_loss: 0.07571) (accu: 0.9752)
[epoch : 31] (l_loss: 0.05195) (t_loss: 0.08129) (accu: 0.9738)
[epoch : 32] (l_loss: 0.05179) (t_loss: 0.07727) (accu: 0.9752)
[epoch : 33] (l_loss: 0.05194) (t_loss: 0.07789) (accu: 0.9748)
[epoch : 34] (l_loss: 0.05124) (t_loss: 0.07946) (accu: 0.9745)
[epoch : 35] (l_loss: 0.05178) (t_loss: 0.07832) (accu: 0.9759)
[epoch : 36] (l_loss: 0.05171) (t_loss: 0.07487) (accu: 0.9759)
[epoch : 37] (l_loss: 0.05138) (t_loss: 0.07520) (accu: 0.9752)
[epoch : 38] (l_loss: 0.05193) (t_loss: 0.07689) (accu: 0.9774)
[epoch : 39] (l_loss: 0.05158) (t_loss: 0.07667) (accu: 0.9765)
[epoch : 40] (l_loss: 0.05137) (t_loss: 0.07838) (accu: 0.9754)
[epoch : 41] (l_loss: 0.05147) (t_loss: 0.07415) (accu: 0.9771)
[epoch : 42] (l_loss: 0.05150) (t_loss: 0.07466) (accu: 0.9767)
[epoch : 43] (l_loss: 0.05176) (t_loss: 0.07544) (accu: 0.9759)
[epoch : 44] (l_loss: 0.05146) (t_loss: 0.07822) (accu: 0.9770)
[epoch : 45] (l_loss: 0.05193) (t_loss: 0.07487) (accu: 0.9768)
[epoch : 46] (l_loss: 0.05127) (t_loss: 0.07766) (accu: 0.9764)
[epoch : 47] (l_loss: 0.05167) (t_loss: 0.07610) (accu: 0.9766)
[epoch : 48] (l_loss: 0.05176) (t_loss: 0.07595) (accu: 0.9757)
[epoch : 49] (l_loss: 0.05120) (t_loss: 0.07600) (accu: 0.9749)
[epoch : 50] (l_loss: 0.05194) (t_loss: 0.07630) (accu: 0.9754)
Finish! (Best accu: 0.9780) (Time taken(sec) : 731.99) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (36352 | 179148)         16.87
fc1.weight   :      196000 (32883 | 163117)         16.78
fc2.weight   :        18750 (3146 | 15604)          16.78
fcout.weight :          750 (323 | 427)             43.07
------------------------------------------------------------
[Prune_iter : (9/21), Remaining weight : 16.87 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30951) (accu: 0.1638)
[epoch : 1] (l_loss: 0.46319) (t_loss: 0.16172) (accu: 0.9550)
[epoch : 2] (l_loss: 0.13573) (t_loss: 0.11564) (accu: 0.9659)
[epoch : 3] (l_loss: 0.10299) (t_loss: 0.10622) (accu: 0.9674)
[epoch : 4] (l_loss: 0.08775) (t_loss: 0.09388) (accu: 0.9723)
[epoch : 5] (l_loss: 0.07768) (t_loss: 0.08849) (accu: 0.9730)
[epoch : 6] (l_loss: 0.07152) (t_loss: 0.08399) (accu: 0.9745)
[epoch : 7] (l_loss: 0.06658) (t_loss: 0.08363) (accu: 0.9743)
[epoch : 8] (l_loss: 0.06348) (t_loss: 0.08333) (accu: 0.9753)
[epoch : 9] (l_loss: 0.06083) (t_loss: 0.07997) (accu: 0.9764)
[epoch : 10] (l_loss: 0.05926) (t_loss: 0.07991) (accu: 0.9757)
[epoch : 11] (l_loss: 0.05768) (t_loss: 0.07935) (accu: 0.9772)
[epoch : 12] (l_loss: 0.05707) (t_loss: 0.07844) (accu: 0.9769)
[epoch : 13] (l_loss: 0.05595) (t_loss: 0.07743) (accu: 0.9768)
[epoch : 14] (l_loss: 0.05550) (t_loss: 0.07876) (accu: 0.9756)
[epoch : 15] (l_loss: 0.05522) (t_loss: 0.07761) (accu: 0.9766)
[epoch : 16] (l_loss: 0.05409) (t_loss: 0.07623) (accu: 0.9766)
[epoch : 17] (l_loss: 0.05504) (t_loss: 0.07762) (accu: 0.9764)
[epoch : 18] (l_loss: 0.05395) (t_loss: 0.08250) (accu: 0.9755)
[epoch : 19] (l_loss: 0.05456) (t_loss: 0.07845) (accu: 0.9758)
[epoch : 20] (l_loss: 0.05455) (t_loss: 0.07963) (accu: 0.9751)
[epoch : 21] (l_loss: 0.05353) (t_loss: 0.07698) (accu: 0.9772)
[epoch : 22] (l_loss: 0.05400) (t_loss: 0.07901) (accu: 0.9762)
[epoch : 23] (l_loss: 0.05387) (t_loss: 0.07593) (accu: 0.9765)
[epoch : 24] (l_loss: 0.05401) (t_loss: 0.07894) (accu: 0.9758)
[epoch : 25] (l_loss: 0.05390) (t_loss: 0.07916) (accu: 0.9761)
[epoch : 26] (l_loss: 0.05381) (t_loss: 0.07568) (accu: 0.9777)
[epoch : 27] (l_loss: 0.05355) (t_loss: 0.07812) (accu: 0.9753)
[epoch : 28] (l_loss: 0.05374) (t_loss: 0.07739) (accu: 0.9765)
[epoch : 29] (l_loss: 0.05369) (t_loss: 0.08106) (accu: 0.9753)
[epoch : 30] (l_loss: 0.05354) (t_loss: 0.07772) (accu: 0.9762)
[epoch : 31] (l_loss: 0.05385) (t_loss: 0.08032) (accu: 0.9753)
[epoch : 32] (l_loss: 0.05356) (t_loss: 0.07690) (accu: 0.9771)
[epoch : 33] (l_loss: 0.05356) (t_loss: 0.07641) (accu: 0.9772)
[epoch : 34] (l_loss: 0.05353) (t_loss: 0.07933) (accu: 0.9753)
[epoch : 35] (l_loss: 0.05336) (t_loss: 0.07677) (accu: 0.9746)
[epoch : 36] (l_loss: 0.05314) (t_loss: 0.08011) (accu: 0.9758)
[epoch : 37] (l_loss: 0.05338) (t_loss: 0.07992) (accu: 0.9764)
[epoch : 38] (l_loss: 0.05403) (t_loss: 0.07796) (accu: 0.9772)
[epoch : 39] (l_loss: 0.05327) (t_loss: 0.07696) (accu: 0.9769)
[epoch : 40] (l_loss: 0.05328) (t_loss: 0.07781) (accu: 0.9771)
[epoch : 41] (l_loss: 0.05354) (t_loss: 0.07562) (accu: 0.9762)
[epoch : 42] (l_loss: 0.05330) (t_loss: 0.07859) (accu: 0.9766)
[epoch : 43] (l_loss: 0.05351) (t_loss: 0.07896) (accu: 0.9763)
[epoch : 44] (l_loss: 0.05307) (t_loss: 0.07971) (accu: 0.9743)
[epoch : 45] (l_loss: 0.05334) (t_loss: 0.07845) (accu: 0.9769)
[epoch : 46] (l_loss: 0.05371) (t_loss: 0.07696) (accu: 0.9768)
[epoch : 47] (l_loss: 0.05346) (t_loss: 0.07850) (accu: 0.9766)
[epoch : 48] (l_loss: 0.05351) (t_loss: 0.07637) (accu: 0.9777)
[epoch : 49] (l_loss: 0.05354) (t_loss: 0.07675) (accu: 0.9760)
[epoch : 50] (l_loss: 0.05318) (t_loss: 0.07715) (accu: 0.9772)
Finish! (Best accu: 0.9777) (Time taken(sec) : 724.36) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (29115 | 186385)         13.51
fc1.weight   :      196000 (26307 | 169693)         13.42
fc2.weight   :        18750 (2517 | 16233)          13.42
fcout.weight :          750 (291 | 459)             38.80
------------------------------------------------------------
[Prune_iter : (10/21), Remaining weight : 13.51 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30605) (accu: 0.1563)
[epoch : 1] (l_loss: 0.46486) (t_loss: 0.16439) (accu: 0.9519)
[epoch : 2] (l_loss: 0.13830) (t_loss: 0.12223) (accu: 0.9642)
[epoch : 3] (l_loss: 0.10537) (t_loss: 0.10725) (accu: 0.9675)
[epoch : 4] (l_loss: 0.08908) (t_loss: 0.09434) (accu: 0.9715)
[epoch : 5] (l_loss: 0.07961) (t_loss: 0.09161) (accu: 0.9728)
[epoch : 6] (l_loss: 0.07423) (t_loss: 0.08846) (accu: 0.9735)
[epoch : 7] (l_loss: 0.06933) (t_loss: 0.08608) (accu: 0.9739)
[epoch : 8] (l_loss: 0.06513) (t_loss: 0.08258) (accu: 0.9753)
[epoch : 9] (l_loss: 0.06205) (t_loss: 0.08189) (accu: 0.9760)
[epoch : 10] (l_loss: 0.05986) (t_loss: 0.07999) (accu: 0.9761)
[epoch : 11] (l_loss: 0.05802) (t_loss: 0.07922) (accu: 0.9768)
[epoch : 12] (l_loss: 0.05678) (t_loss: 0.08042) (accu: 0.9755)
[epoch : 13] (l_loss: 0.05611) (t_loss: 0.07779) (accu: 0.9765)
[epoch : 14] (l_loss: 0.05591) (t_loss: 0.08095) (accu: 0.9757)
[epoch : 15] (l_loss: 0.05489) (t_loss: 0.07789) (accu: 0.9757)
[epoch : 16] (l_loss: 0.05470) (t_loss: 0.07890) (accu: 0.9761)
[epoch : 17] (l_loss: 0.05480) (t_loss: 0.07724) (accu: 0.9763)
[epoch : 18] (l_loss: 0.05424) (t_loss: 0.07780) (accu: 0.9754)
[epoch : 19] (l_loss: 0.05417) (t_loss: 0.07609) (accu: 0.9767)
[epoch : 20] (l_loss: 0.05363) (t_loss: 0.07891) (accu: 0.9759)
[epoch : 21] (l_loss: 0.05380) (t_loss: 0.07825) (accu: 0.9770)
[epoch : 22] (l_loss: 0.05343) (t_loss: 0.07812) (accu: 0.9752)
[epoch : 23] (l_loss: 0.05358) (t_loss: 0.08004) (accu: 0.9756)
[epoch : 24] (l_loss: 0.05387) (t_loss: 0.07926) (accu: 0.9761)
[epoch : 25] (l_loss: 0.05360) (t_loss: 0.07979) (accu: 0.9767)
[epoch : 26] (l_loss: 0.05384) (t_loss: 0.07758) (accu: 0.9768)
[epoch : 27] (l_loss: 0.05379) (t_loss: 0.07790) (accu: 0.9763)
[epoch : 28] (l_loss: 0.05304) (t_loss: 0.07447) (accu: 0.9773)
[epoch : 29] (l_loss: 0.05373) (t_loss: 0.07719) (accu: 0.9757)
[epoch : 30] (l_loss: 0.05322) (t_loss: 0.07900) (accu: 0.9749)
[epoch : 31] (l_loss: 0.05345) (t_loss: 0.08022) (accu: 0.9755)
[epoch : 32] (l_loss: 0.05299) (t_loss: 0.07855) (accu: 0.9756)
[epoch : 33] (l_loss: 0.05309) (t_loss: 0.07574) (accu: 0.9773)
[epoch : 34] (l_loss: 0.05325) (t_loss: 0.07524) (accu: 0.9769)
[epoch : 35] (l_loss: 0.05310) (t_loss: 0.07550) (accu: 0.9774)
[epoch : 36] (l_loss: 0.05327) (t_loss: 0.07782) (accu: 0.9762)
[epoch : 37] (l_loss: 0.05330) (t_loss: 0.07838) (accu: 0.9765)
[epoch : 38] (l_loss: 0.05323) (t_loss: 0.07860) (accu: 0.9752)
[epoch : 39] (l_loss: 0.05291) (t_loss: 0.07714) (accu: 0.9763)
[epoch : 40] (l_loss: 0.05306) (t_loss: 0.08143) (accu: 0.9746)
[epoch : 41] (l_loss: 0.05301) (t_loss: 0.07584) (accu: 0.9773)
[epoch : 42] (l_loss: 0.05304) (t_loss: 0.07911) (accu: 0.9758)
[epoch : 43] (l_loss: 0.05358) (t_loss: 0.07697) (accu: 0.9765)
[epoch : 44] (l_loss: 0.05311) (t_loss: 0.07769) (accu: 0.9772)
[epoch : 45] (l_loss: 0.05304) (t_loss: 0.07916) (accu: 0.9748)
[epoch : 46] (l_loss: 0.05289) (t_loss: 0.07631) (accu: 0.9771)
[epoch : 47] (l_loss: 0.05307) (t_loss: 0.07588) (accu: 0.9784)
[epoch : 48] (l_loss: 0.05325) (t_loss: 0.07684) (accu: 0.9766)
[epoch : 49] (l_loss: 0.05281) (t_loss: 0.07561) (accu: 0.9771)
[epoch : 50] (l_loss: 0.05313) (t_loss: 0.07883) (accu: 0.9751)
Finish! (Best accu: 0.9784) (Time taken(sec) : 722.35) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (23320 | 192180)         10.82
fc1.weight   :      196000 (21045 | 174955)         10.74
fc2.weight   :        18750 (2013 | 16737)          10.74
fcout.weight :          750 (262 | 488)             34.93
------------------------------------------------------------
[Prune_iter : (11/21), Remaining weight : 10.82 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29615) (accu: 0.1292)
[epoch : 1] (l_loss: 0.45341) (t_loss: 0.15679) (accu: 0.9563)
[epoch : 2] (l_loss: 0.12986) (t_loss: 0.11421) (accu: 0.9665)
[epoch : 3] (l_loss: 0.09784) (t_loss: 0.10127) (accu: 0.9691)
[epoch : 4] (l_loss: 0.08310) (t_loss: 0.09271) (accu: 0.9720)
[epoch : 5] (l_loss: 0.07395) (t_loss: 0.08623) (accu: 0.9743)
[epoch : 6] (l_loss: 0.06809) (t_loss: 0.08351) (accu: 0.9748)
[epoch : 7] (l_loss: 0.06334) (t_loss: 0.08081) (accu: 0.9747)
[epoch : 8] (l_loss: 0.06021) (t_loss: 0.07885) (accu: 0.9759)
[epoch : 9] (l_loss: 0.05847) (t_loss: 0.07786) (accu: 0.9761)
[epoch : 10] (l_loss: 0.05660) (t_loss: 0.08096) (accu: 0.9757)
[epoch : 11] (l_loss: 0.05524) (t_loss: 0.07974) (accu: 0.9765)
[epoch : 12] (l_loss: 0.05393) (t_loss: 0.07814) (accu: 0.9760)
[epoch : 13] (l_loss: 0.05340) (t_loss: 0.07716) (accu: 0.9773)
[epoch : 14] (l_loss: 0.05338) (t_loss: 0.07670) (accu: 0.9767)
[epoch : 15] (l_loss: 0.05287) (t_loss: 0.07606) (accu: 0.9779)
[epoch : 16] (l_loss: 0.05286) (t_loss: 0.07877) (accu: 0.9760)
[epoch : 17] (l_loss: 0.05252) (t_loss: 0.07748) (accu: 0.9777)
[epoch : 18] (l_loss: 0.05242) (t_loss: 0.07735) (accu: 0.9763)
[epoch : 19] (l_loss: 0.05191) (t_loss: 0.07602) (accu: 0.9763)
[epoch : 20] (l_loss: 0.05229) (t_loss: 0.07842) (accu: 0.9752)
[epoch : 21] (l_loss: 0.05193) (t_loss: 0.07536) (accu: 0.9773)
[epoch : 22] (l_loss: 0.05192) (t_loss: 0.08077) (accu: 0.9755)
[epoch : 23] (l_loss: 0.05156) (t_loss: 0.07701) (accu: 0.9765)
[epoch : 24] (l_loss: 0.05200) (t_loss: 0.08018) (accu: 0.9747)
[epoch : 25] (l_loss: 0.05175) (t_loss: 0.07728) (accu: 0.9766)
[epoch : 26] (l_loss: 0.05227) (t_loss: 0.07831) (accu: 0.9760)
[epoch : 27] (l_loss: 0.05166) (t_loss: 0.07884) (accu: 0.9766)
[epoch : 28] (l_loss: 0.05187) (t_loss: 0.07873) (accu: 0.9758)
[epoch : 29] (l_loss: 0.05128) (t_loss: 0.07630) (accu: 0.9766)
[epoch : 30] (l_loss: 0.05171) (t_loss: 0.07623) (accu: 0.9767)
[epoch : 31] (l_loss: 0.05153) (t_loss: 0.08036) (accu: 0.9751)
[epoch : 32] (l_loss: 0.05216) (t_loss: 0.07511) (accu: 0.9780)
[epoch : 33] (l_loss: 0.05185) (t_loss: 0.07469) (accu: 0.9759)
[epoch : 34] (l_loss: 0.05158) (t_loss: 0.07615) (accu: 0.9761)
[epoch : 35] (l_loss: 0.05179) (t_loss: 0.07816) (accu: 0.9758)
[epoch : 36] (l_loss: 0.05170) (t_loss: 0.07772) (accu: 0.9770)
[epoch : 37] (l_loss: 0.05152) (t_loss: 0.07589) (accu: 0.9769)
[epoch : 38] (l_loss: 0.05163) (t_loss: 0.07764) (accu: 0.9762)
[epoch : 39] (l_loss: 0.05110) (t_loss: 0.07805) (accu: 0.9763)
[epoch : 40] (l_loss: 0.05152) (t_loss: 0.07764) (accu: 0.9753)
[epoch : 41] (l_loss: 0.05175) (t_loss: 0.07487) (accu: 0.9766)
[epoch : 42] (l_loss: 0.05201) (t_loss: 0.07683) (accu: 0.9766)
[epoch : 43] (l_loss: 0.05161) (t_loss: 0.07625) (accu: 0.9764)
[epoch : 44] (l_loss: 0.05133) (t_loss: 0.07600) (accu: 0.9765)
[epoch : 45] (l_loss: 0.05144) (t_loss: 0.07551) (accu: 0.9775)
[epoch : 46] (l_loss: 0.05181) (t_loss: 0.07690) (accu: 0.9764)
[epoch : 47] (l_loss: 0.05143) (t_loss: 0.07705) (accu: 0.9759)
[epoch : 48] (l_loss: 0.05140) (t_loss: 0.07455) (accu: 0.9772)
[epoch : 49] (l_loss: 0.05172) (t_loss: 0.07673) (accu: 0.9759)
[epoch : 50] (l_loss: 0.05131) (t_loss: 0.07894) (accu: 0.9759)
Finish! (Best accu: 0.9780) (Time taken(sec) : 729.35) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (18682 | 196818)          8.67
fc1.weight   :      196000 (16836 | 179164)          8.59
fc2.weight   :        18750 (1611 | 17139)           8.59
fcout.weight :          750 (235 | 515)             31.33
------------------------------------------------------------
[Prune_iter : (12/21), Remaining weight : 8.67 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29658) (accu: 0.1443)
[epoch : 1] (l_loss: 0.45938) (t_loss: 0.16440) (accu: 0.9533)
[epoch : 2] (l_loss: 0.14016) (t_loss: 0.12368) (accu: 0.9622)
[epoch : 3] (l_loss: 0.10650) (t_loss: 0.10632) (accu: 0.9674)
[epoch : 4] (l_loss: 0.08945) (t_loss: 0.09659) (accu: 0.9710)
[epoch : 5] (l_loss: 0.07825) (t_loss: 0.08869) (accu: 0.9725)
[epoch : 6] (l_loss: 0.07087) (t_loss: 0.08698) (accu: 0.9738)
[epoch : 7] (l_loss: 0.06582) (t_loss: 0.08550) (accu: 0.9751)
[epoch : 8] (l_loss: 0.06311) (t_loss: 0.08133) (accu: 0.9753)
[epoch : 9] (l_loss: 0.06012) (t_loss: 0.08123) (accu: 0.9761)
[epoch : 10] (l_loss: 0.05902) (t_loss: 0.08346) (accu: 0.9749)
[epoch : 11] (l_loss: 0.05759) (t_loss: 0.08068) (accu: 0.9770)
[epoch : 12] (l_loss: 0.05636) (t_loss: 0.08089) (accu: 0.9758)
[epoch : 13] (l_loss: 0.05583) (t_loss: 0.07928) (accu: 0.9759)
[epoch : 14] (l_loss: 0.05573) (t_loss: 0.07713) (accu: 0.9770)
[epoch : 15] (l_loss: 0.05509) (t_loss: 0.07826) (accu: 0.9758)
[epoch : 16] (l_loss: 0.05502) (t_loss: 0.07775) (accu: 0.9763)
[epoch : 17] (l_loss: 0.05423) (t_loss: 0.07776) (accu: 0.9769)
[epoch : 18] (l_loss: 0.05453) (t_loss: 0.07868) (accu: 0.9761)
[epoch : 19] (l_loss: 0.05396) (t_loss: 0.07719) (accu: 0.9760)
[epoch : 20] (l_loss: 0.05435) (t_loss: 0.07828) (accu: 0.9763)
[epoch : 21] (l_loss: 0.05400) (t_loss: 0.07657) (accu: 0.9762)
[epoch : 22] (l_loss: 0.05409) (t_loss: 0.07822) (accu: 0.9764)
[epoch : 23] (l_loss: 0.05373) (t_loss: 0.07746) (accu: 0.9763)
[epoch : 24] (l_loss: 0.05342) (t_loss: 0.07793) (accu: 0.9766)
[epoch : 25] (l_loss: 0.05359) (t_loss: 0.07740) (accu: 0.9756)
[epoch : 26] (l_loss: 0.05347) (t_loss: 0.07577) (accu: 0.9762)
[epoch : 27] (l_loss: 0.05306) (t_loss: 0.07712) (accu: 0.9769)
[epoch : 28] (l_loss: 0.05371) (t_loss: 0.07817) (accu: 0.9772)
[epoch : 29] (l_loss: 0.05295) (t_loss: 0.07778) (accu: 0.9768)
[epoch : 30] (l_loss: 0.05315) (t_loss: 0.07788) (accu: 0.9757)
[epoch : 31] (l_loss: 0.05360) (t_loss: 0.07717) (accu: 0.9758)
[epoch : 32] (l_loss: 0.05311) (t_loss: 0.07768) (accu: 0.9766)
[epoch : 33] (l_loss: 0.05310) (t_loss: 0.07785) (accu: 0.9774)
[epoch : 34] (l_loss: 0.05311) (t_loss: 0.07568) (accu: 0.9769)
[epoch : 35] (l_loss: 0.05353) (t_loss: 0.07778) (accu: 0.9760)
[epoch : 36] (l_loss: 0.05299) (t_loss: 0.07887) (accu: 0.9762)
[epoch : 37] (l_loss: 0.05293) (t_loss: 0.07737) (accu: 0.9760)
[epoch : 38] (l_loss: 0.05300) (t_loss: 0.07961) (accu: 0.9753)
[epoch : 39] (l_loss: 0.05335) (t_loss: 0.07755) (accu: 0.9759)
[epoch : 40] (l_loss: 0.05309) (t_loss: 0.07628) (accu: 0.9754)
[epoch : 41] (l_loss: 0.05325) (t_loss: 0.07752) (accu: 0.9767)
[epoch : 42] (l_loss: 0.05303) (t_loss: 0.07761) (accu: 0.9757)
[epoch : 43] (l_loss: 0.05320) (t_loss: 0.08036) (accu: 0.9747)
[epoch : 44] (l_loss: 0.05310) (t_loss: 0.08093) (accu: 0.9753)
[epoch : 45] (l_loss: 0.05340) (t_loss: 0.07726) (accu: 0.9759)
[epoch : 46] (l_loss: 0.05322) (t_loss: 0.07621) (accu: 0.9764)
[epoch : 47] (l_loss: 0.05281) (t_loss: 0.07768) (accu: 0.9756)
[epoch : 48] (l_loss: 0.05278) (t_loss: 0.07892) (accu: 0.9762)
[epoch : 49] (l_loss: 0.05291) (t_loss: 0.07805) (accu: 0.9773)
[epoch : 50] (l_loss: 0.05359) (t_loss: 0.07773) (accu: 0.9773)
Finish! (Best accu: 0.9774) (Time taken(sec) : 733.49) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (14969 | 200531)          6.95
fc1.weight   :      196000 (13469 | 182531)          6.87
fc2.weight   :        18750 (1288 | 17462)           6.87
fcout.weight :          750 (212 | 538)             28.27
------------------------------------------------------------
[Prune_iter : (13/21), Remaining weight : 6.95 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30087) (accu: 0.1525)
[epoch : 1] (l_loss: 0.46060) (t_loss: 0.16659) (accu: 0.9528)
[epoch : 2] (l_loss: 0.13779) (t_loss: 0.12025) (accu: 0.9631)
[epoch : 3] (l_loss: 0.10449) (t_loss: 0.10441) (accu: 0.9681)
[epoch : 4] (l_loss: 0.08812) (t_loss: 0.09284) (accu: 0.9720)
[epoch : 5] (l_loss: 0.07773) (t_loss: 0.08674) (accu: 0.9747)
[epoch : 6] (l_loss: 0.07072) (t_loss: 0.08574) (accu: 0.9746)
[epoch : 7] (l_loss: 0.06655) (t_loss: 0.08285) (accu: 0.9756)
[epoch : 8] (l_loss: 0.06316) (t_loss: 0.08241) (accu: 0.9758)
[epoch : 9] (l_loss: 0.06019) (t_loss: 0.08081) (accu: 0.9754)
[epoch : 10] (l_loss: 0.05878) (t_loss: 0.08025) (accu: 0.9762)
[epoch : 11] (l_loss: 0.05775) (t_loss: 0.07900) (accu: 0.9769)
[epoch : 12] (l_loss: 0.05675) (t_loss: 0.07870) (accu: 0.9762)
[epoch : 13] (l_loss: 0.05579) (t_loss: 0.07774) (accu: 0.9778)
[epoch : 14] (l_loss: 0.05528) (t_loss: 0.08182) (accu: 0.9753)
[epoch : 15] (l_loss: 0.05489) (t_loss: 0.07804) (accu: 0.9763)
[epoch : 16] (l_loss: 0.05491) (t_loss: 0.07836) (accu: 0.9764)
[epoch : 17] (l_loss: 0.05448) (t_loss: 0.08206) (accu: 0.9743)
[epoch : 18] (l_loss: 0.05427) (t_loss: 0.07851) (accu: 0.9756)
[epoch : 19] (l_loss: 0.05426) (t_loss: 0.07778) (accu: 0.9749)
[epoch : 20] (l_loss: 0.05360) (t_loss: 0.07900) (accu: 0.9759)
[epoch : 21] (l_loss: 0.05362) (t_loss: 0.07727) (accu: 0.9770)
[epoch : 22] (l_loss: 0.05355) (t_loss: 0.07945) (accu: 0.9760)
[epoch : 23] (l_loss: 0.05349) (t_loss: 0.07886) (accu: 0.9749)
[epoch : 24] (l_loss: 0.05330) (t_loss: 0.08125) (accu: 0.9759)
[epoch : 25] (l_loss: 0.05371) (t_loss: 0.07617) (accu: 0.9768)
[epoch : 26] (l_loss: 0.05362) (t_loss: 0.07856) (accu: 0.9755)
[epoch : 27] (l_loss: 0.05306) (t_loss: 0.07586) (accu: 0.9776)
[epoch : 28] (l_loss: 0.05327) (t_loss: 0.07745) (accu: 0.9757)
[epoch : 29] (l_loss: 0.05343) (t_loss: 0.07760) (accu: 0.9760)
[epoch : 30] (l_loss: 0.05330) (t_loss: 0.07717) (accu: 0.9758)
[epoch : 31] (l_loss: 0.05345) (t_loss: 0.07662) (accu: 0.9754)
[epoch : 32] (l_loss: 0.05339) (t_loss: 0.07568) (accu: 0.9779)
[epoch : 33] (l_loss: 0.05332) (t_loss: 0.07711) (accu: 0.9767)
[epoch : 34] (l_loss: 0.05309) (t_loss: 0.08080) (accu: 0.9753)
[epoch : 35] (l_loss: 0.05311) (t_loss: 0.07934) (accu: 0.9767)
[epoch : 36] (l_loss: 0.05356) (t_loss: 0.08169) (accu: 0.9749)
[epoch : 37] (l_loss: 0.05294) (t_loss: 0.07557) (accu: 0.9762)
[epoch : 38] (l_loss: 0.05282) (t_loss: 0.07815) (accu: 0.9766)
[epoch : 39] (l_loss: 0.05324) (t_loss: 0.07663) (accu: 0.9771)
[epoch : 40] (l_loss: 0.05332) (t_loss: 0.07685) (accu: 0.9775)
[epoch : 41] (l_loss: 0.05275) (t_loss: 0.07804) (accu: 0.9757)
[epoch : 42] (l_loss: 0.05304) (t_loss: 0.08026) (accu: 0.9756)
[epoch : 43] (l_loss: 0.05305) (t_loss: 0.07869) (accu: 0.9764)
[epoch : 44] (l_loss: 0.05335) (t_loss: 0.07712) (accu: 0.9763)
[epoch : 45] (l_loss: 0.05314) (t_loss: 0.07999) (accu: 0.9759)
[epoch : 46] (l_loss: 0.05321) (t_loss: 0.07549) (accu: 0.9779)
[epoch : 47] (l_loss: 0.05301) (t_loss: 0.07959) (accu: 0.9766)
[epoch : 48] (l_loss: 0.05340) (t_loss: 0.07810) (accu: 0.9758)
[epoch : 49] (l_loss: 0.05332) (t_loss: 0.07705) (accu: 0.9775)
[epoch : 50] (l_loss: 0.05282) (t_loss: 0.07809) (accu: 0.9770)
Finish! (Best accu: 0.9779) (Time taken(sec) : 726.14) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (11997 | 203503)          5.57
fc1.weight   :      196000 (10775 | 185225)          5.50
fc2.weight   :        18750 (1031 | 17719)           5.50
fcout.weight :          750 (191 | 559)             25.47
------------------------------------------------------------
[Prune_iter : (14/21), Remaining weight : 5.57 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30319) (accu: 0.1347)
[epoch : 1] (l_loss: 0.45891) (t_loss: 0.16834) (accu: 0.9524)
[epoch : 2] (l_loss: 0.13578) (t_loss: 0.12051) (accu: 0.9649)
[epoch : 3] (l_loss: 0.10028) (t_loss: 0.10108) (accu: 0.9690)
[epoch : 4] (l_loss: 0.08499) (t_loss: 0.09519) (accu: 0.9697)
[epoch : 5] (l_loss: 0.07519) (t_loss: 0.08902) (accu: 0.9737)
[epoch : 6] (l_loss: 0.06912) (t_loss: 0.08395) (accu: 0.9739)
[epoch : 7] (l_loss: 0.06495) (t_loss: 0.08140) (accu: 0.9750)
[epoch : 8] (l_loss: 0.06107) (t_loss: 0.08182) (accu: 0.9746)
[epoch : 9] (l_loss: 0.05891) (t_loss: 0.07894) (accu: 0.9763)
[epoch : 10] (l_loss: 0.05618) (t_loss: 0.07819) (accu: 0.9762)
[epoch : 11] (l_loss: 0.05564) (t_loss: 0.07975) (accu: 0.9767)
[epoch : 12] (l_loss: 0.05436) (t_loss: 0.08004) (accu: 0.9748)
[epoch : 13] (l_loss: 0.05424) (t_loss: 0.07632) (accu: 0.9757)
[epoch : 14] (l_loss: 0.05384) (t_loss: 0.07744) (accu: 0.9763)
[epoch : 15] (l_loss: 0.05295) (t_loss: 0.07655) (accu: 0.9766)
[epoch : 16] (l_loss: 0.05273) (t_loss: 0.07961) (accu: 0.9758)
[epoch : 17] (l_loss: 0.05247) (t_loss: 0.07866) (accu: 0.9771)
[epoch : 18] (l_loss: 0.05230) (t_loss: 0.07655) (accu: 0.9767)
[epoch : 19] (l_loss: 0.05261) (t_loss: 0.07657) (accu: 0.9758)
[epoch : 20] (l_loss: 0.05191) (t_loss: 0.07915) (accu: 0.9755)
[epoch : 21] (l_loss: 0.05237) (t_loss: 0.08053) (accu: 0.9751)
[epoch : 22] (l_loss: 0.05184) (t_loss: 0.07802) (accu: 0.9759)
[epoch : 23] (l_loss: 0.05204) (t_loss: 0.07935) (accu: 0.9749)
[epoch : 24] (l_loss: 0.05211) (t_loss: 0.07866) (accu: 0.9758)
[epoch : 25] (l_loss: 0.05133) (t_loss: 0.07714) (accu: 0.9764)
[epoch : 26] (l_loss: 0.05183) (t_loss: 0.07636) (accu: 0.9776)
[epoch : 27] (l_loss: 0.05195) (t_loss: 0.07833) (accu: 0.9750)
[epoch : 28] (l_loss: 0.05169) (t_loss: 0.07493) (accu: 0.9765)
[epoch : 29] (l_loss: 0.05191) (t_loss: 0.07713) (accu: 0.9749)
[epoch : 30] (l_loss: 0.05166) (t_loss: 0.07395) (accu: 0.9769)
[epoch : 31] (l_loss: 0.05172) (t_loss: 0.07643) (accu: 0.9776)
[epoch : 32] (l_loss: 0.05209) (t_loss: 0.07562) (accu: 0.9767)
[epoch : 33] (l_loss: 0.05186) (t_loss: 0.07611) (accu: 0.9767)
[epoch : 34] (l_loss: 0.05145) (t_loss: 0.07774) (accu: 0.9763)
[epoch : 35] (l_loss: 0.05148) (t_loss: 0.07636) (accu: 0.9765)
[epoch : 36] (l_loss: 0.05122) (t_loss: 0.07553) (accu: 0.9769)
[epoch : 37] (l_loss: 0.05160) (t_loss: 0.07718) (accu: 0.9772)
[epoch : 38] (l_loss: 0.05172) (t_loss: 0.07670) (accu: 0.9765)
[epoch : 39] (l_loss: 0.05152) (t_loss: 0.07694) (accu: 0.9773)
[epoch : 40] (l_loss: 0.05144) (t_loss: 0.07869) (accu: 0.9765)
[epoch : 41] (l_loss: 0.05157) (t_loss: 0.07443) (accu: 0.9779)
[epoch : 42] (l_loss: 0.05186) (t_loss: 0.07502) (accu: 0.9783)
[epoch : 43] (l_loss: 0.05167) (t_loss: 0.07656) (accu: 0.9766)
[epoch : 44] (l_loss: 0.05171) (t_loss: 0.07660) (accu: 0.9770)
[epoch : 45] (l_loss: 0.05180) (t_loss: 0.07579) (accu: 0.9777)
[epoch : 46] (l_loss: 0.05146) (t_loss: 0.07865) (accu: 0.9758)
[epoch : 47] (l_loss: 0.05202) (t_loss: 0.07592) (accu: 0.9766)
[epoch : 48] (l_loss: 0.05184) (t_loss: 0.07584) (accu: 0.9764)
[epoch : 49] (l_loss: 0.05173) (t_loss: 0.07803) (accu: 0.9760)
[epoch : 50] (l_loss: 0.05134) (t_loss: 0.07682) (accu: 0.9761)
Finish! (Best accu: 0.9783) (Time taken(sec) : 729.01) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (9617 | 205883)          4.46
fc1.weight   :       196000 (8620 | 187380)          4.40
fc2.weight   :        18750 (825 | 17925)            4.40
fcout.weight :          750 (172 | 578)             22.93
------------------------------------------------------------
[Prune_iter : (15/21), Remaining weight : 4.46 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30234) (accu: 0.1447)
[epoch : 1] (l_loss: 0.45128) (t_loss: 0.16203) (accu: 0.9530)
[epoch : 2] (l_loss: 0.13726) (t_loss: 0.12104) (accu: 0.9640)
[epoch : 3] (l_loss: 0.10441) (t_loss: 0.10499) (accu: 0.9675)
[epoch : 4] (l_loss: 0.08809) (t_loss: 0.09542) (accu: 0.9709)
[epoch : 5] (l_loss: 0.07761) (t_loss: 0.08908) (accu: 0.9729)
[epoch : 6] (l_loss: 0.07050) (t_loss: 0.08725) (accu: 0.9736)
[epoch : 7] (l_loss: 0.06574) (t_loss: 0.08340) (accu: 0.9755)
[epoch : 8] (l_loss: 0.06256) (t_loss: 0.08175) (accu: 0.9761)
[epoch : 9] (l_loss: 0.05999) (t_loss: 0.07997) (accu: 0.9772)
[epoch : 10] (l_loss: 0.05827) (t_loss: 0.08125) (accu: 0.9760)
[epoch : 11] (l_loss: 0.05679) (t_loss: 0.07737) (accu: 0.9762)
[epoch : 12] (l_loss: 0.05579) (t_loss: 0.07832) (accu: 0.9768)
[epoch : 13] (l_loss: 0.05568) (t_loss: 0.07693) (accu: 0.9767)
[epoch : 14] (l_loss: 0.05527) (t_loss: 0.07983) (accu: 0.9773)
[epoch : 15] (l_loss: 0.05485) (t_loss: 0.07760) (accu: 0.9761)
[epoch : 16] (l_loss: 0.05474) (t_loss: 0.07572) (accu: 0.9767)
[epoch : 17] (l_loss: 0.05447) (t_loss: 0.07648) (accu: 0.9765)
[epoch : 18] (l_loss: 0.05412) (t_loss: 0.07924) (accu: 0.9762)
[epoch : 19] (l_loss: 0.05405) (t_loss: 0.07787) (accu: 0.9763)
[epoch : 20] (l_loss: 0.05392) (t_loss: 0.07854) (accu: 0.9745)
[epoch : 21] (l_loss: 0.05373) (t_loss: 0.07837) (accu: 0.9760)
[epoch : 22] (l_loss: 0.05385) (t_loss: 0.08015) (accu: 0.9766)
[epoch : 23] (l_loss: 0.05383) (t_loss: 0.07600) (accu: 0.9780)
[epoch : 24] (l_loss: 0.05340) (t_loss: 0.07837) (accu: 0.9756)
[epoch : 25] (l_loss: 0.05360) (t_loss: 0.07856) (accu: 0.9761)
[epoch : 26] (l_loss: 0.05342) (t_loss: 0.07529) (accu: 0.9765)
[epoch : 27] (l_loss: 0.05357) (t_loss: 0.07959) (accu: 0.9752)
[epoch : 28] (l_loss: 0.05334) (t_loss: 0.07920) (accu: 0.9756)
[epoch : 29] (l_loss: 0.05361) (t_loss: 0.07877) (accu: 0.9750)
[epoch : 30] (l_loss: 0.05318) (t_loss: 0.07533) (accu: 0.9765)
[epoch : 31] (l_loss: 0.05351) (t_loss: 0.07664) (accu: 0.9777)
[epoch : 32] (l_loss: 0.05312) (t_loss: 0.07773) (accu: 0.9751)
[epoch : 33] (l_loss: 0.05311) (t_loss: 0.08100) (accu: 0.9744)
[epoch : 34] (l_loss: 0.05344) (t_loss: 0.07614) (accu: 0.9772)
[epoch : 35] (l_loss: 0.05306) (t_loss: 0.08004) (accu: 0.9760)
[epoch : 36] (l_loss: 0.05306) (t_loss: 0.07714) (accu: 0.9766)
[epoch : 37] (l_loss: 0.05332) (t_loss: 0.07938) (accu: 0.9758)
[epoch : 38] (l_loss: 0.05301) (t_loss: 0.07407) (accu: 0.9772)
[epoch : 39] (l_loss: 0.05345) (t_loss: 0.07694) (accu: 0.9766)
[epoch : 40] (l_loss: 0.05314) (t_loss: 0.07889) (accu: 0.9765)
[epoch : 41] (l_loss: 0.05331) (t_loss: 0.07730) (accu: 0.9762)
[epoch : 42] (l_loss: 0.05329) (t_loss: 0.07934) (accu: 0.9761)
[epoch : 43] (l_loss: 0.05377) (t_loss: 0.07524) (accu: 0.9772)
[epoch : 44] (l_loss: 0.05314) (t_loss: 0.07682) (accu: 0.9776)
[epoch : 45] (l_loss: 0.05322) (t_loss: 0.07822) (accu: 0.9752)
[epoch : 46] (l_loss: 0.05332) (t_loss: 0.07731) (accu: 0.9754)
[epoch : 47] (l_loss: 0.05301) (t_loss: 0.07791) (accu: 0.9770)
[epoch : 48] (l_loss: 0.05336) (t_loss: 0.07666) (accu: 0.9770)
[epoch : 49] (l_loss: 0.05364) (t_loss: 0.07688) (accu: 0.9777)
[epoch : 50] (l_loss: 0.05293) (t_loss: 0.07976) (accu: 0.9765)
Finish! (Best accu: 0.9780) (Time taken(sec) : 746.23) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (7710 | 207790)          3.58
fc1.weight   :       196000 (6896 | 189104)          3.52
fc2.weight   :        18750 (660 | 18090)            3.52
fcout.weight :          750 (154 | 596)             20.53
------------------------------------------------------------
[Prune_iter : (16/21), Remaining weight : 3.58 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30235) (accu: 0.1226)
[epoch : 1] (l_loss: 0.45051) (t_loss: 0.16072) (accu: 0.9540)
[epoch : 2] (l_loss: 0.13187) (t_loss: 0.11552) (accu: 0.9664)
[epoch : 3] (l_loss: 0.09874) (t_loss: 0.09977) (accu: 0.9701)
[epoch : 4] (l_loss: 0.08325) (t_loss: 0.09193) (accu: 0.9723)
[epoch : 5] (l_loss: 0.07434) (t_loss: 0.08616) (accu: 0.9737)
[epoch : 6] (l_loss: 0.06768) (t_loss: 0.08565) (accu: 0.9746)
[epoch : 7] (l_loss: 0.06270) (t_loss: 0.08052) (accu: 0.9754)
[epoch : 8] (l_loss: 0.05982) (t_loss: 0.07998) (accu: 0.9750)
[epoch : 9] (l_loss: 0.05749) (t_loss: 0.07928) (accu: 0.9757)
[epoch : 10] (l_loss: 0.05549) (t_loss: 0.08045) (accu: 0.9750)
[epoch : 11] (l_loss: 0.05468) (t_loss: 0.07970) (accu: 0.9759)
[epoch : 12] (l_loss: 0.05424) (t_loss: 0.07847) (accu: 0.9770)
[epoch : 13] (l_loss: 0.05340) (t_loss: 0.07738) (accu: 0.9761)
[epoch : 14] (l_loss: 0.05338) (t_loss: 0.07964) (accu: 0.9764)
[epoch : 15] (l_loss: 0.05234) (t_loss: 0.07791) (accu: 0.9763)
[epoch : 16] (l_loss: 0.05252) (t_loss: 0.07770) (accu: 0.9766)
[epoch : 17] (l_loss: 0.05242) (t_loss: 0.07599) (accu: 0.9757)
[epoch : 18] (l_loss: 0.05224) (t_loss: 0.07741) (accu: 0.9775)
[epoch : 19] (l_loss: 0.05226) (t_loss: 0.07617) (accu: 0.9769)
[epoch : 20] (l_loss: 0.05230) (t_loss: 0.07741) (accu: 0.9765)
[epoch : 21] (l_loss: 0.05238) (t_loss: 0.07813) (accu: 0.9753)
[epoch : 22] (l_loss: 0.05189) (t_loss: 0.07794) (accu: 0.9759)
[epoch : 23] (l_loss: 0.05160) (t_loss: 0.07670) (accu: 0.9774)
[epoch : 24] (l_loss: 0.05208) (t_loss: 0.07860) (accu: 0.9750)
[epoch : 25] (l_loss: 0.05181) (t_loss: 0.07708) (accu: 0.9764)
[epoch : 26] (l_loss: 0.05205) (t_loss: 0.07972) (accu: 0.9762)
[epoch : 27] (l_loss: 0.05184) (t_loss: 0.07668) (accu: 0.9771)
[epoch : 28] (l_loss: 0.05167) (t_loss: 0.07659) (accu: 0.9765)
[epoch : 29] (l_loss: 0.05173) (t_loss: 0.07806) (accu: 0.9763)
[epoch : 30] (l_loss: 0.05137) (t_loss: 0.07707) (accu: 0.9759)
[epoch : 31] (l_loss: 0.05167) (t_loss: 0.07356) (accu: 0.9768)
[epoch : 32] (l_loss: 0.05162) (t_loss: 0.07887) (accu: 0.9757)
[epoch : 33] (l_loss: 0.05169) (t_loss: 0.07857) (accu: 0.9764)
[epoch : 34] (l_loss: 0.05088) (t_loss: 0.07679) (accu: 0.9767)
[epoch : 35] (l_loss: 0.05181) (t_loss: 0.07774) (accu: 0.9759)
[epoch : 36] (l_loss: 0.05148) (t_loss: 0.07749) (accu: 0.9762)
[epoch : 37] (l_loss: 0.05165) (t_loss: 0.07659) (accu: 0.9772)
[epoch : 38] (l_loss: 0.05142) (t_loss: 0.07580) (accu: 0.9789)
[epoch : 39] (l_loss: 0.05144) (t_loss: 0.07599) (accu: 0.9764)
[epoch : 40] (l_loss: 0.05187) (t_loss: 0.07676) (accu: 0.9758)
[epoch : 41] (l_loss: 0.05168) (t_loss: 0.07493) (accu: 0.9773)
[epoch : 42] (l_loss: 0.05156) (t_loss: 0.07636) (accu: 0.9774)
[epoch : 43] (l_loss: 0.05145) (t_loss: 0.07492) (accu: 0.9773)
[epoch : 44] (l_loss: 0.05165) (t_loss: 0.07631) (accu: 0.9778)
[epoch : 45] (l_loss: 0.05143) (t_loss: 0.07701) (accu: 0.9759)
[epoch : 46] (l_loss: 0.05129) (t_loss: 0.07585) (accu: 0.9765)
[epoch : 47] (l_loss: 0.05170) (t_loss: 0.07980) (accu: 0.9751)
[epoch : 48] (l_loss: 0.05199) (t_loss: 0.07750) (accu: 0.9763)
[epoch : 49] (l_loss: 0.05138) (t_loss: 0.07356) (accu: 0.9770)
[epoch : 50] (l_loss: 0.05131) (t_loss: 0.07712) (accu: 0.9758)
Finish! (Best accu: 0.9789) (Time taken(sec) : 737.00) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (6184 | 209316)          2.87
fc1.weight   :       196000 (5517 | 190483)          2.81
fc2.weight   :        18750 (528 | 18222)            2.82
fcout.weight :          750 (139 | 611)             18.53
------------------------------------------------------------
[Prune_iter : (17/21), Remaining weight : 2.87 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30039) (accu: 0.1155)
[epoch : 1] (l_loss: 0.44707) (t_loss: 0.15997) (accu: 0.9547)
[epoch : 2] (l_loss: 0.13164) (t_loss: 0.11924) (accu: 0.9638)
[epoch : 3] (l_loss: 0.09892) (t_loss: 0.09970) (accu: 0.9683)
[epoch : 4] (l_loss: 0.08328) (t_loss: 0.09147) (accu: 0.9716)
[epoch : 5] (l_loss: 0.07413) (t_loss: 0.08809) (accu: 0.9730)
[epoch : 6] (l_loss: 0.06869) (t_loss: 0.08372) (accu: 0.9746)
[epoch : 7] (l_loss: 0.06472) (t_loss: 0.08234) (accu: 0.9754)
[epoch : 8] (l_loss: 0.06150) (t_loss: 0.08171) (accu: 0.9758)
[epoch : 9] (l_loss: 0.05940) (t_loss: 0.07986) (accu: 0.9768)
[epoch : 10] (l_loss: 0.05806) (t_loss: 0.07965) (accu: 0.9758)
[epoch : 11] (l_loss: 0.05633) (t_loss: 0.07853) (accu: 0.9773)
[epoch : 12] (l_loss: 0.05516) (t_loss: 0.07860) (accu: 0.9762)
[epoch : 13] (l_loss: 0.05432) (t_loss: 0.08165) (accu: 0.9751)
[epoch : 14] (l_loss: 0.05348) (t_loss: 0.07743) (accu: 0.9764)
[epoch : 15] (l_loss: 0.05321) (t_loss: 0.07585) (accu: 0.9767)
[epoch : 16] (l_loss: 0.05330) (t_loss: 0.07668) (accu: 0.9765)
[epoch : 17] (l_loss: 0.05266) (t_loss: 0.07779) (accu: 0.9768)
[epoch : 18] (l_loss: 0.05284) (t_loss: 0.07749) (accu: 0.9771)
[epoch : 19] (l_loss: 0.05241) (t_loss: 0.07529) (accu: 0.9771)
[epoch : 20] (l_loss: 0.05217) (t_loss: 0.07694) (accu: 0.9770)
[epoch : 21] (l_loss: 0.05201) (t_loss: 0.07964) (accu: 0.9755)
[epoch : 22] (l_loss: 0.05219) (t_loss: 0.07502) (accu: 0.9766)
[epoch : 23] (l_loss: 0.05199) (t_loss: 0.07698) (accu: 0.9774)
[epoch : 24] (l_loss: 0.05196) (t_loss: 0.07574) (accu: 0.9777)
[epoch : 25] (l_loss: 0.05156) (t_loss: 0.07785) (accu: 0.9755)
[epoch : 26] (l_loss: 0.05205) (t_loss: 0.07724) (accu: 0.9764)
[epoch : 27] (l_loss: 0.05114) (t_loss: 0.07740) (accu: 0.9766)
[epoch : 28] (l_loss: 0.05203) (t_loss: 0.07654) (accu: 0.9780)
[epoch : 29] (l_loss: 0.05167) (t_loss: 0.07840) (accu: 0.9753)
[epoch : 30] (l_loss: 0.05100) (t_loss: 0.07742) (accu: 0.9749)
[epoch : 31] (l_loss: 0.05173) (t_loss: 0.07650) (accu: 0.9761)
[epoch : 32] (l_loss: 0.05150) (t_loss: 0.07832) (accu: 0.9767)
[epoch : 33] (l_loss: 0.05177) (t_loss: 0.07569) (accu: 0.9763)
[epoch : 34] (l_loss: 0.05140) (t_loss: 0.08096) (accu: 0.9733)
[epoch : 35] (l_loss: 0.05144) (t_loss: 0.07654) (accu: 0.9765)
[epoch : 36] (l_loss: 0.05173) (t_loss: 0.07690) (accu: 0.9764)
[epoch : 37] (l_loss: 0.05163) (t_loss: 0.07721) (accu: 0.9773)
[epoch : 38] (l_loss: 0.05139) (t_loss: 0.07788) (accu: 0.9757)
[epoch : 39] (l_loss: 0.05192) (t_loss: 0.07735) (accu: 0.9759)
[epoch : 40] (l_loss: 0.05184) (t_loss: 0.07531) (accu: 0.9770)
[epoch : 41] (l_loss: 0.05154) (t_loss: 0.07629) (accu: 0.9769)
[epoch : 42] (l_loss: 0.05178) (t_loss: 0.07698) (accu: 0.9758)
[epoch : 43] (l_loss: 0.05162) (t_loss: 0.07814) (accu: 0.9763)
[epoch : 44] (l_loss: 0.05182) (t_loss: 0.07592) (accu: 0.9775)
[epoch : 45] (l_loss: 0.05144) (t_loss: 0.07647) (accu: 0.9757)
[epoch : 46] (l_loss: 0.05158) (t_loss: 0.07510) (accu: 0.9762)
[epoch : 47] (l_loss: 0.05143) (t_loss: 0.07758) (accu: 0.9757)
[epoch : 48] (l_loss: 0.05178) (t_loss: 0.07665) (accu: 0.9769)
[epoch : 49] (l_loss: 0.05130) (t_loss: 0.07678) (accu: 0.9769)
[epoch : 50] (l_loss: 0.05162) (t_loss: 0.07946) (accu: 0.9756)
Finish! (Best accu: 0.9780) (Time taken(sec) : 734.80) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (4961 | 210539)          2.30
fc1.weight   :       196000 (4414 | 191586)          2.25
fc2.weight   :        18750 (422 | 18328)            2.25
fcout.weight :          750 (125 | 625)             16.67
------------------------------------------------------------
[Prune_iter : (18/21), Remaining weight : 2.3 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29902) (accu: 0.1004)
[epoch : 1] (l_loss: 0.44963) (t_loss: 0.16197) (accu: 0.9544)
[epoch : 2] (l_loss: 0.13851) (t_loss: 0.11857) (accu: 0.9646)
[epoch : 3] (l_loss: 0.10527) (t_loss: 0.10266) (accu: 0.9684)
[epoch : 4] (l_loss: 0.08969) (t_loss: 0.09590) (accu: 0.9709)
[epoch : 5] (l_loss: 0.08084) (t_loss: 0.09309) (accu: 0.9712)
[epoch : 6] (l_loss: 0.07420) (t_loss: 0.09151) (accu: 0.9725)
[epoch : 7] (l_loss: 0.06993) (t_loss: 0.09262) (accu: 0.9714)
[epoch : 8] (l_loss: 0.06691) (t_loss: 0.08688) (accu: 0.9731)
[epoch : 9] (l_loss: 0.06477) (t_loss: 0.09139) (accu: 0.9710)
[epoch : 10] (l_loss: 0.06281) (t_loss: 0.08271) (accu: 0.9756)
[epoch : 11] (l_loss: 0.06141) (t_loss: 0.08489) (accu: 0.9737)
[epoch : 12] (l_loss: 0.06042) (t_loss: 0.08293) (accu: 0.9744)
[epoch : 13] (l_loss: 0.06015) (t_loss: 0.07986) (accu: 0.9756)
[epoch : 14] (l_loss: 0.05863) (t_loss: 0.08213) (accu: 0.9753)
[epoch : 15] (l_loss: 0.05823) (t_loss: 0.08213) (accu: 0.9742)
[epoch : 16] (l_loss: 0.05785) (t_loss: 0.08264) (accu: 0.9736)
[epoch : 17] (l_loss: 0.05770) (t_loss: 0.07778) (accu: 0.9767)
[epoch : 18] (l_loss: 0.05772) (t_loss: 0.08129) (accu: 0.9750)
[epoch : 19] (l_loss: 0.05708) (t_loss: 0.07846) (accu: 0.9761)
[epoch : 20] (l_loss: 0.05692) (t_loss: 0.07855) (accu: 0.9749)
[epoch : 21] (l_loss: 0.05714) (t_loss: 0.07854) (accu: 0.9759)
[epoch : 22] (l_loss: 0.05635) (t_loss: 0.08015) (accu: 0.9755)
[epoch : 23] (l_loss: 0.05642) (t_loss: 0.08648) (accu: 0.9725)
[epoch : 24] (l_loss: 0.05627) (t_loss: 0.08237) (accu: 0.9742)
[epoch : 25] (l_loss: 0.05591) (t_loss: 0.07971) (accu: 0.9751)
[epoch : 26] (l_loss: 0.05595) (t_loss: 0.07879) (accu: 0.9755)
[epoch : 27] (l_loss: 0.05598) (t_loss: 0.08088) (accu: 0.9741)
[epoch : 28] (l_loss: 0.05607) (t_loss: 0.07930) (accu: 0.9760)
[epoch : 29] (l_loss: 0.05569) (t_loss: 0.07926) (accu: 0.9753)
[epoch : 30] (l_loss: 0.05563) (t_loss: 0.08527) (accu: 0.9736)
[epoch : 31] (l_loss: 0.05611) (t_loss: 0.07947) (accu: 0.9747)
[epoch : 32] (l_loss: 0.05523) (t_loss: 0.08117) (accu: 0.9740)
[epoch : 33] (l_loss: 0.05585) (t_loss: 0.07920) (accu: 0.9751)
[epoch : 34] (l_loss: 0.05596) (t_loss: 0.07753) (accu: 0.9761)
[epoch : 35] (l_loss: 0.05575) (t_loss: 0.07991) (accu: 0.9744)
[epoch : 36] (l_loss: 0.05562) (t_loss: 0.07962) (accu: 0.9746)
[epoch : 37] (l_loss: 0.05559) (t_loss: 0.07957) (accu: 0.9750)
[epoch : 38] (l_loss: 0.05553) (t_loss: 0.07697) (accu: 0.9750)
[epoch : 39] (l_loss: 0.05604) (t_loss: 0.08120) (accu: 0.9749)
[epoch : 40] (l_loss: 0.05568) (t_loss: 0.08470) (accu: 0.9738)
[epoch : 41] (l_loss: 0.05539) (t_loss: 0.07815) (accu: 0.9753)
[epoch : 42] (l_loss: 0.05547) (t_loss: 0.08105) (accu: 0.9757)
[epoch : 43] (l_loss: 0.05537) (t_loss: 0.08066) (accu: 0.9752)
[epoch : 44] (l_loss: 0.05563) (t_loss: 0.08003) (accu: 0.9746)
[epoch : 45] (l_loss: 0.05536) (t_loss: 0.08036) (accu: 0.9754)
[epoch : 46] (l_loss: 0.05569) (t_loss: 0.08022) (accu: 0.9753)
[epoch : 47] (l_loss: 0.05550) (t_loss: 0.07642) (accu: 0.9757)
[epoch : 48] (l_loss: 0.05567) (t_loss: 0.07910) (accu: 0.9759)
[epoch : 49] (l_loss: 0.05554) (t_loss: 0.08236) (accu: 0.9744)
[epoch : 50] (l_loss: 0.05539) (t_loss: 0.07942) (accu: 0.9755)
Finish! (Best accu: 0.9767) (Time taken(sec) : 746.20) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (3982 | 211518)          1.85
fc1.weight   :       196000 (3531 | 192469)          1.80
fc2.weight   :        18750 (338 | 18412)            1.80
fcout.weight :          750 (113 | 637)             15.07
------------------------------------------------------------
[Prune_iter : (19/21), Remaining weight : 1.85 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29816) (accu: 0.0997)
[epoch : 1] (l_loss: 0.45558) (t_loss: 0.16246) (accu: 0.9560)
[epoch : 2] (l_loss: 0.13621) (t_loss: 0.12091) (accu: 0.9653)
[epoch : 3] (l_loss: 0.10369) (t_loss: 0.10355) (accu: 0.9693)
[epoch : 4] (l_loss: 0.08817) (t_loss: 0.09953) (accu: 0.9700)
[epoch : 5] (l_loss: 0.07886) (t_loss: 0.09424) (accu: 0.9704)
[epoch : 6] (l_loss: 0.07177) (t_loss: 0.08841) (accu: 0.9721)
[epoch : 7] (l_loss: 0.06726) (t_loss: 0.08558) (accu: 0.9734)
[epoch : 8] (l_loss: 0.06402) (t_loss: 0.08576) (accu: 0.9738)
[epoch : 9] (l_loss: 0.06243) (t_loss: 0.08509) (accu: 0.9741)
[epoch : 10] (l_loss: 0.06057) (t_loss: 0.08643) (accu: 0.9740)
[epoch : 11] (l_loss: 0.05892) (t_loss: 0.08266) (accu: 0.9745)
[epoch : 12] (l_loss: 0.05874) (t_loss: 0.08304) (accu: 0.9742)
[epoch : 13] (l_loss: 0.05803) (t_loss: 0.08103) (accu: 0.9756)
[epoch : 14] (l_loss: 0.05742) (t_loss: 0.08072) (accu: 0.9749)
[epoch : 15] (l_loss: 0.05700) (t_loss: 0.07983) (accu: 0.9752)
[epoch : 16] (l_loss: 0.05659) (t_loss: 0.08108) (accu: 0.9761)
[epoch : 17] (l_loss: 0.05658) (t_loss: 0.07998) (accu: 0.9749)
[epoch : 18] (l_loss: 0.05606) (t_loss: 0.08333) (accu: 0.9740)
[epoch : 19] (l_loss: 0.05617) (t_loss: 0.08038) (accu: 0.9764)
[epoch : 20] (l_loss: 0.05567) (t_loss: 0.08188) (accu: 0.9743)
[epoch : 21] (l_loss: 0.05540) (t_loss: 0.08269) (accu: 0.9743)
[epoch : 22] (l_loss: 0.05538) (t_loss: 0.08152) (accu: 0.9732)
[epoch : 23] (l_loss: 0.05522) (t_loss: 0.07995) (accu: 0.9752)
[epoch : 24] (l_loss: 0.05525) (t_loss: 0.08143) (accu: 0.9736)
[epoch : 25] (l_loss: 0.05523) (t_loss: 0.07932) (accu: 0.9755)
[epoch : 26] (l_loss: 0.05496) (t_loss: 0.07982) (accu: 0.9756)
[epoch : 27] (l_loss: 0.05505) (t_loss: 0.07845) (accu: 0.9763)
[epoch : 28] (l_loss: 0.05548) (t_loss: 0.07916) (accu: 0.9759)
[epoch : 29] (l_loss: 0.05502) (t_loss: 0.08042) (accu: 0.9756)
[epoch : 30] (l_loss: 0.05492) (t_loss: 0.07878) (accu: 0.9748)
[epoch : 31] (l_loss: 0.05486) (t_loss: 0.07904) (accu: 0.9756)
[epoch : 32] (l_loss: 0.05480) (t_loss: 0.07766) (accu: 0.9758)
[epoch : 33] (l_loss: 0.05481) (t_loss: 0.07862) (accu: 0.9748)
[epoch : 34] (l_loss: 0.05471) (t_loss: 0.07857) (accu: 0.9745)
[epoch : 35] (l_loss: 0.05501) (t_loss: 0.07876) (accu: 0.9752)
[epoch : 36] (l_loss: 0.05489) (t_loss: 0.07915) (accu: 0.9757)
[epoch : 37] (l_loss: 0.05488) (t_loss: 0.07970) (accu: 0.9752)
[epoch : 38] (l_loss: 0.05428) (t_loss: 0.07851) (accu: 0.9766)
[epoch : 39] (l_loss: 0.05484) (t_loss: 0.07804) (accu: 0.9744)
[epoch : 40] (l_loss: 0.05462) (t_loss: 0.07884) (accu: 0.9744)
[epoch : 41] (l_loss: 0.05448) (t_loss: 0.07873) (accu: 0.9749)
[epoch : 42] (l_loss: 0.05432) (t_loss: 0.07728) (accu: 0.9760)
[epoch : 43] (l_loss: 0.05460) (t_loss: 0.07762) (accu: 0.9747)
[epoch : 44] (l_loss: 0.05431) (t_loss: 0.07845) (accu: 0.9746)
[epoch : 45] (l_loss: 0.05421) (t_loss: 0.08038) (accu: 0.9745)
[epoch : 46] (l_loss: 0.05459) (t_loss: 0.07941) (accu: 0.9757)
[epoch : 47] (l_loss: 0.05481) (t_loss: 0.07840) (accu: 0.9755)
[epoch : 48] (l_loss: 0.05441) (t_loss: 0.07914) (accu: 0.9752)
[epoch : 49] (l_loss: 0.05442) (t_loss: 0.07669) (accu: 0.9758)
[epoch : 50] (l_loss: 0.05480) (t_loss: 0.07688) (accu: 0.9756)
Finish! (Best accu: 0.9766) (Time taken(sec) : 746.17) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (3196 | 212304)          1.48
fc1.weight   :       196000 (2825 | 193175)          1.44
fc2.weight   :        18750 (270 | 18480)            1.44
fcout.weight :          750 (101 | 649)             13.47
------------------------------------------------------------
[Prune_iter : (20/21), Remaining weight : 1.48 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30089) (accu: 0.0975)
[epoch : 1] (l_loss: 0.44552) (t_loss: 0.15581) (accu: 0.9552)
[epoch : 2] (l_loss: 0.13108) (t_loss: 0.11694) (accu: 0.9648)
[epoch : 3] (l_loss: 0.10016) (t_loss: 0.10506) (accu: 0.9688)
[epoch : 4] (l_loss: 0.08459) (t_loss: 0.09175) (accu: 0.9721)
[epoch : 5] (l_loss: 0.07627) (t_loss: 0.09029) (accu: 0.9722)
[epoch : 6] (l_loss: 0.07041) (t_loss: 0.08798) (accu: 0.9727)
[epoch : 7] (l_loss: 0.06636) (t_loss: 0.08606) (accu: 0.9725)
[epoch : 8] (l_loss: 0.06333) (t_loss: 0.08382) (accu: 0.9736)
[epoch : 9] (l_loss: 0.06176) (t_loss: 0.08133) (accu: 0.9749)
[epoch : 10] (l_loss: 0.05994) (t_loss: 0.08750) (accu: 0.9725)
[epoch : 11] (l_loss: 0.05881) (t_loss: 0.08134) (accu: 0.9737)
[epoch : 12] (l_loss: 0.05851) (t_loss: 0.08057) (accu: 0.9757)
[epoch : 13] (l_loss: 0.05737) (t_loss: 0.08006) (accu: 0.9765)
[epoch : 14] (l_loss: 0.05731) (t_loss: 0.08127) (accu: 0.9757)
[epoch : 15] (l_loss: 0.05688) (t_loss: 0.07917) (accu: 0.9760)
[epoch : 16] (l_loss: 0.05635) (t_loss: 0.07956) (accu: 0.9755)
[epoch : 17] (l_loss: 0.05587) (t_loss: 0.07879) (accu: 0.9757)
[epoch : 18] (l_loss: 0.05574) (t_loss: 0.08348) (accu: 0.9747)
[epoch : 19] (l_loss: 0.05573) (t_loss: 0.07921) (accu: 0.9754)
[epoch : 20] (l_loss: 0.05543) (t_loss: 0.07901) (accu: 0.9747)
[epoch : 21] (l_loss: 0.05556) (t_loss: 0.07943) (accu: 0.9749)
[epoch : 22] (l_loss: 0.05522) (t_loss: 0.07784) (accu: 0.9760)
[epoch : 23] (l_loss: 0.05565) (t_loss: 0.08036) (accu: 0.9752)
[epoch : 24] (l_loss: 0.05527) (t_loss: 0.07872) (accu: 0.9760)
[epoch : 25] (l_loss: 0.05471) (t_loss: 0.07934) (accu: 0.9764)
[epoch : 26] (l_loss: 0.05476) (t_loss: 0.08086) (accu: 0.9746)
[epoch : 27] (l_loss: 0.05486) (t_loss: 0.07944) (accu: 0.9753)
[epoch : 28] (l_loss: 0.05483) (t_loss: 0.07897) (accu: 0.9749)
[epoch : 29] (l_loss: 0.05504) (t_loss: 0.08157) (accu: 0.9748)
[epoch : 30] (l_loss: 0.05481) (t_loss: 0.07671) (accu: 0.9754)
[epoch : 31] (l_loss: 0.05470) (t_loss: 0.07947) (accu: 0.9747)
[epoch : 32] (l_loss: 0.05478) (t_loss: 0.08027) (accu: 0.9746)
[epoch : 33] (l_loss: 0.05464) (t_loss: 0.07932) (accu: 0.9762)
[epoch : 34] (l_loss: 0.05483) (t_loss: 0.07948) (accu: 0.9753)
[epoch : 35] (l_loss: 0.05480) (t_loss: 0.08052) (accu: 0.9749)
[epoch : 36] (l_loss: 0.05497) (t_loss: 0.08002) (accu: 0.9750)
[epoch : 37] (l_loss: 0.05470) (t_loss: 0.08208) (accu: 0.9744)
[epoch : 38] (l_loss: 0.05488) (t_loss: 0.08035) (accu: 0.9742)
[epoch : 39] (l_loss: 0.05474) (t_loss: 0.08013) (accu: 0.9757)
[epoch : 40] (l_loss: 0.05468) (t_loss: 0.08128) (accu: 0.9759)
[epoch : 41] (l_loss: 0.05445) (t_loss: 0.07977) (accu: 0.9752)
[epoch : 42] (l_loss: 0.05474) (t_loss: 0.07754) (accu: 0.9758)
[epoch : 43] (l_loss: 0.05443) (t_loss: 0.08046) (accu: 0.9749)
[epoch : 44] (l_loss: 0.05457) (t_loss: 0.07895) (accu: 0.9760)
[epoch : 45] (l_loss: 0.05460) (t_loss: 0.07821) (accu: 0.9751)
[epoch : 46] (l_loss: 0.05406) (t_loss: 0.07840) (accu: 0.9752)
[epoch : 47] (l_loss: 0.05505) (t_loss: 0.07751) (accu: 0.9749)
[epoch : 48] (l_loss: 0.05458) (t_loss: 0.07740) (accu: 0.9760)
[epoch : 49] (l_loss: 0.05431) (t_loss: 0.07939) (accu: 0.9746)
[epoch : 50] (l_loss: 0.05469) (t_loss: 0.07961) (accu: 0.9760)
Finish! (Best accu: 0.9765) (Time taken(sec) : 743.90) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (2567 | 212933)          1.19
fc1.weight   :       196000 (2260 | 193740)          1.15
fc2.weight   :        18750 (216 | 18534)            1.15
fcout.weight :           750 (91 | 659)             12.13
------------------------------------------------------------
[Prune_iter : (21/21), Remaining weight : 1.19 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30019) (accu: 0.0990)
[epoch : 1] (l_loss: 0.44904) (t_loss: 0.15945) (accu: 0.9553)
[epoch : 2] (l_loss: 0.13294) (t_loss: 0.11908) (accu: 0.9656)
[epoch : 3] (l_loss: 0.10149) (t_loss: 0.10134) (accu: 0.9701)
[epoch : 4] (l_loss: 0.08534) (t_loss: 0.09368) (accu: 0.9719)
[epoch : 5] (l_loss: 0.07651) (t_loss: 0.09039) (accu: 0.9710)
[epoch : 6] (l_loss: 0.07066) (t_loss: 0.08630) (accu: 0.9741)
[epoch : 7] (l_loss: 0.06654) (t_loss: 0.08305) (accu: 0.9742)
[epoch : 8] (l_loss: 0.06363) (t_loss: 0.08390) (accu: 0.9736)
[epoch : 9] (l_loss: 0.06153) (t_loss: 0.08316) (accu: 0.9751)
[epoch : 10] (l_loss: 0.05998) (t_loss: 0.08744) (accu: 0.9736)
[epoch : 11] (l_loss: 0.05919) (t_loss: 0.08314) (accu: 0.9746)
[epoch : 12] (l_loss: 0.05807) (t_loss: 0.08042) (accu: 0.9754)
[epoch : 13] (l_loss: 0.05738) (t_loss: 0.08273) (accu: 0.9748)
[epoch : 14] (l_loss: 0.05692) (t_loss: 0.07964) (accu: 0.9749)
[epoch : 15] (l_loss: 0.05638) (t_loss: 0.08005) (accu: 0.9749)
[epoch : 16] (l_loss: 0.05612) (t_loss: 0.07951) (accu: 0.9759)
[epoch : 17] (l_loss: 0.05602) (t_loss: 0.07933) (accu: 0.9743)
[epoch : 18] (l_loss: 0.05591) (t_loss: 0.07732) (accu: 0.9759)
[epoch : 19] (l_loss: 0.05539) (t_loss: 0.07660) (accu: 0.9768)
[epoch : 20] (l_loss: 0.05535) (t_loss: 0.08055) (accu: 0.9747)
[epoch : 21] (l_loss: 0.05510) (t_loss: 0.07730) (accu: 0.9750)
[epoch : 22] (l_loss: 0.05547) (t_loss: 0.07892) (accu: 0.9760)
[epoch : 23] (l_loss: 0.05503) (t_loss: 0.08194) (accu: 0.9762)
[epoch : 24] (l_loss: 0.05501) (t_loss: 0.07799) (accu: 0.9765)
[epoch : 25] (l_loss: 0.05519) (t_loss: 0.07906) (accu: 0.9753)
[epoch : 26] (l_loss: 0.05492) (t_loss: 0.07864) (accu: 0.9761)
[epoch : 27] (l_loss: 0.05496) (t_loss: 0.08152) (accu: 0.9749)
[epoch : 28] (l_loss: 0.05458) (t_loss: 0.07822) (accu: 0.9768)
[epoch : 29] (l_loss: 0.05513) (t_loss: 0.08143) (accu: 0.9746)
[epoch : 30] (l_loss: 0.05490) (t_loss: 0.07921) (accu: 0.9755)
[epoch : 31] (l_loss: 0.05462) (t_loss: 0.07819) (accu: 0.9755)
[epoch : 32] (l_loss: 0.05511) (t_loss: 0.07861) (accu: 0.9756)
[epoch : 33] (l_loss: 0.05493) (t_loss: 0.08205) (accu: 0.9739)
[epoch : 34] (l_loss: 0.05483) (t_loss: 0.08417) (accu: 0.9732)
[epoch : 35] (l_loss: 0.05502) (t_loss: 0.08020) (accu: 0.9755)
[epoch : 36] (l_loss: 0.05457) (t_loss: 0.07904) (accu: 0.9750)
[epoch : 37] (l_loss: 0.05424) (t_loss: 0.07716) (accu: 0.9754)
[epoch : 38] (l_loss: 0.05506) (t_loss: 0.07852) (accu: 0.9753)
[epoch : 39] (l_loss: 0.05436) (t_loss: 0.07715) (accu: 0.9764)
[epoch : 40] (l_loss: 0.05459) (t_loss: 0.08040) (accu: 0.9741)
[epoch : 41] (l_loss: 0.05469) (t_loss: 0.08026) (accu: 0.9744)
[epoch : 42] (l_loss: 0.05439) (t_loss: 0.07909) (accu: 0.9749)
[epoch : 43] (l_loss: 0.05412) (t_loss: 0.07768) (accu: 0.9763)
[epoch : 44] (l_loss: 0.05439) (t_loss: 0.07887) (accu: 0.9762)
[epoch : 45] (l_loss: 0.05466) (t_loss: 0.07962) (accu: 0.9751)
[epoch : 46] (l_loss: 0.05459) (t_loss: 0.08469) (accu: 0.9727)
[epoch : 47] (l_loss: 0.05501) (t_loss: 0.07871) (accu: 0.9758)
[epoch : 48] (l_loss: 0.05467) (t_loss: 0.08003) (accu: 0.9759)
[epoch : 49] (l_loss: 0.05494) (t_loss: 0.07997) (accu: 0.9762)
[epoch : 50] (l_loss: 0.05446) (t_loss: 0.08008) (accu: 0.9749)
Finish! (Best accu: 0.9768) (Time taken(sec) : 752.99) 


Maximum accuracy per weight remaining
Remaining weight 100.0 %  Epoch 30 Accu 0.9773
Remaining weight 80.03 %  Epoch 25 Accu 0.9764
Remaining weight 64.06 %  Epoch 42 Accu 0.9787
Remaining weight 51.28 %  Epoch 20 Accu 0.9780
Remaining weight 41.05 %  Epoch 42 Accu 0.9773
Remaining weight 32.86 %  Epoch 46 Accu 0.9771
Remaining weight 26.31 %  Epoch 41 Accu 0.9777
Remaining weight 21.06 %  Epoch 15 Accu 0.9780
Remaining weight 16.87 %  Epoch 47 Accu 0.9777
Remaining weight 13.51 %  Epoch 46 Accu 0.9784
Remaining weight 10.82 %  Epoch 31 Accu 0.9780
Remaining weight 8.67 %  Epoch 32 Accu 0.9774
Remaining weight 6.95 %  Epoch 45 Accu 0.9779
Remaining weight 5.57 %  Epoch 41 Accu 0.9783
Remaining weight 4.46 %  Epoch 22 Accu 0.9780
Remaining weight 3.58 %  Epoch 37 Accu 0.9789
Remaining weight 2.87 %  Epoch 27 Accu 0.9780
Remaining weight 2.3 %  Epoch 16 Accu 0.9767
Remaining weight 1.85 %  Epoch 37 Accu 0.9766
Remaining weight 1.48 %  Epoch 12 Accu 0.9765
Remaining weight 1.19 %  Epoch 27 Accu 0.9768
===================================================================== 

Test_Iter (4/5)
------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :        215500 (215500 | 0)          100.00
fc1.weight   :        196000 (196000 | 0)          100.00
fc2.weight   :         18750 (18750 | 0)           100.00
fcout.weight :           750 (750 | 0)             100.00
------------------------------------------------------------
[Prune_iter : (1/21), Remaining weight : 100.0 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.71028) (accu: 0.0805)
[epoch : 1] (l_loss: 0.50160) (t_loss: 0.15809) (accu: 0.9535)
[epoch : 2] (l_loss: 0.12723) (t_loss: 0.10919) (accu: 0.9680)
[epoch : 3] (l_loss: 0.09323) (t_loss: 0.09529) (accu: 0.9714)
[epoch : 4] (l_loss: 0.07798) (t_loss: 0.08931) (accu: 0.9730)
[epoch : 5] (l_loss: 0.06954) (t_loss: 0.08490) (accu: 0.9749)
[epoch : 6] (l_loss: 0.06426) (t_loss: 0.08476) (accu: 0.9744)
[epoch : 7] (l_loss: 0.06111) (t_loss: 0.08021) (accu: 0.9755)
[epoch : 8] (l_loss: 0.05893) (t_loss: 0.08037) (accu: 0.9743)
[epoch : 9] (l_loss: 0.05757) (t_loss: 0.07847) (accu: 0.9757)
[epoch : 10] (l_loss: 0.05690) (t_loss: 0.07952) (accu: 0.9760)
[epoch : 11] (l_loss: 0.05621) (t_loss: 0.08141) (accu: 0.9758)
[epoch : 12] (l_loss: 0.05550) (t_loss: 0.07801) (accu: 0.9756)
[epoch : 13] (l_loss: 0.05518) (t_loss: 0.08030) (accu: 0.9755)
[epoch : 14] (l_loss: 0.05473) (t_loss: 0.07881) (accu: 0.9742)
[epoch : 15] (l_loss: 0.05455) (t_loss: 0.08141) (accu: 0.9756)
[epoch : 16] (l_loss: 0.05461) (t_loss: 0.08224) (accu: 0.9734)
[epoch : 17] (l_loss: 0.05467) (t_loss: 0.07730) (accu: 0.9761)
[epoch : 18] (l_loss: 0.05427) (t_loss: 0.07843) (accu: 0.9765)
[epoch : 19] (l_loss: 0.05421) (t_loss: 0.07815) (accu: 0.9747)
[epoch : 20] (l_loss: 0.05406) (t_loss: 0.07765) (accu: 0.9764)
[epoch : 21] (l_loss: 0.05454) (t_loss: 0.07765) (accu: 0.9760)
[epoch : 22] (l_loss: 0.05379) (t_loss: 0.07901) (accu: 0.9763)
[epoch : 23] (l_loss: 0.05371) (t_loss: 0.07791) (accu: 0.9761)
[epoch : 24] (l_loss: 0.05418) (t_loss: 0.07809) (accu: 0.9757)
[epoch : 25] (l_loss: 0.05387) (t_loss: 0.08111) (accu: 0.9744)
[epoch : 26] (l_loss: 0.05369) (t_loss: 0.08001) (accu: 0.9751)
[epoch : 27] (l_loss: 0.05374) (t_loss: 0.07804) (accu: 0.9755)
[epoch : 28] (l_loss: 0.05360) (t_loss: 0.07586) (accu: 0.9766)
[epoch : 29] (l_loss: 0.05380) (t_loss: 0.07934) (accu: 0.9751)
[epoch : 30] (l_loss: 0.05335) (t_loss: 0.07777) (accu: 0.9754)
[epoch : 31] (l_loss: 0.05369) (t_loss: 0.07700) (accu: 0.9756)
[epoch : 32] (l_loss: 0.05316) (t_loss: 0.07920) (accu: 0.9749)
[epoch : 33] (l_loss: 0.05354) (t_loss: 0.07710) (accu: 0.9747)
[epoch : 34] (l_loss: 0.05332) (t_loss: 0.07694) (accu: 0.9759)
[epoch : 35] (l_loss: 0.05358) (t_loss: 0.08379) (accu: 0.9749)
[epoch : 36] (l_loss: 0.05373) (t_loss: 0.07924) (accu: 0.9751)
[epoch : 37] (l_loss: 0.05351) (t_loss: 0.07794) (accu: 0.9760)
[epoch : 38] (l_loss: 0.05313) (t_loss: 0.07746) (accu: 0.9757)
[epoch : 39] (l_loss: 0.05319) (t_loss: 0.07870) (accu: 0.9750)
[epoch : 40] (l_loss: 0.05352) (t_loss: 0.07857) (accu: 0.9756)
[epoch : 41] (l_loss: 0.05349) (t_loss: 0.07970) (accu: 0.9754)
[epoch : 42] (l_loss: 0.05289) (t_loss: 0.07891) (accu: 0.9756)
[epoch : 43] (l_loss: 0.05319) (t_loss: 0.07893) (accu: 0.9755)
[epoch : 44] (l_loss: 0.05305) (t_loss: 0.07709) (accu: 0.9760)
[epoch : 45] (l_loss: 0.05366) (t_loss: 0.07915) (accu: 0.9757)
[epoch : 46] (l_loss: 0.05307) (t_loss: 0.08082) (accu: 0.9749)
[epoch : 47] (l_loss: 0.05365) (t_loss: 0.07836) (accu: 0.9751)
[epoch : 48] (l_loss: 0.05324) (t_loss: 0.07785) (accu: 0.9758)
[epoch : 49] (l_loss: 0.05379) (t_loss: 0.07832) (accu: 0.9749)
[epoch : 50] (l_loss: 0.05286) (t_loss: 0.07795) (accu: 0.9758)
Finish! (Best accu: 0.9766) (Time taken(sec) : 751.54) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (172475 | 43025)         80.03
fc1.weight   :      196000 (156800 | 39200)         80.00
fc2.weight   :        18750 (15000 | 3750)          80.00
fcout.weight :           750 (675 | 75)             90.00
------------------------------------------------------------
[Prune_iter : (2/21), Remaining weight : 80.03 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.48013) (accu: 0.0990)
[epoch : 1] (l_loss: 0.49804) (t_loss: 0.16119) (accu: 0.9532)
[epoch : 2] (l_loss: 0.13531) (t_loss: 0.11975) (accu: 0.9647)
[epoch : 3] (l_loss: 0.10010) (t_loss: 0.09879) (accu: 0.9692)
[epoch : 4] (l_loss: 0.08406) (t_loss: 0.09287) (accu: 0.9713)
[epoch : 5] (l_loss: 0.07511) (t_loss: 0.08746) (accu: 0.9734)
[epoch : 6] (l_loss: 0.07068) (t_loss: 0.08462) (accu: 0.9729)
[epoch : 7] (l_loss: 0.06653) (t_loss: 0.08216) (accu: 0.9753)
[epoch : 8] (l_loss: 0.06480) (t_loss: 0.08468) (accu: 0.9736)
[epoch : 9] (l_loss: 0.06293) (t_loss: 0.08192) (accu: 0.9748)
[epoch : 10] (l_loss: 0.06225) (t_loss: 0.08068) (accu: 0.9759)
[epoch : 11] (l_loss: 0.06088) (t_loss: 0.08065) (accu: 0.9752)
[epoch : 12] (l_loss: 0.06044) (t_loss: 0.08059) (accu: 0.9760)
[epoch : 13] (l_loss: 0.05954) (t_loss: 0.08020) (accu: 0.9741)
[epoch : 14] (l_loss: 0.05903) (t_loss: 0.08028) (accu: 0.9754)
[epoch : 15] (l_loss: 0.05906) (t_loss: 0.07939) (accu: 0.9765)
[epoch : 16] (l_loss: 0.05828) (t_loss: 0.07816) (accu: 0.9750)
[epoch : 17] (l_loss: 0.05840) (t_loss: 0.08223) (accu: 0.9748)
[epoch : 18] (l_loss: 0.05791) (t_loss: 0.07916) (accu: 0.9757)
[epoch : 19] (l_loss: 0.05760) (t_loss: 0.08075) (accu: 0.9752)
[epoch : 20] (l_loss: 0.05752) (t_loss: 0.08190) (accu: 0.9745)
[epoch : 21] (l_loss: 0.05770) (t_loss: 0.07988) (accu: 0.9753)
[epoch : 22] (l_loss: 0.05730) (t_loss: 0.08038) (accu: 0.9746)
[epoch : 23] (l_loss: 0.05715) (t_loss: 0.08040) (accu: 0.9741)
[epoch : 24] (l_loss: 0.05766) (t_loss: 0.08251) (accu: 0.9744)
[epoch : 25] (l_loss: 0.05697) (t_loss: 0.08328) (accu: 0.9737)
[epoch : 26] (l_loss: 0.05801) (t_loss: 0.08058) (accu: 0.9741)
[epoch : 27] (l_loss: 0.05733) (t_loss: 0.07972) (accu: 0.9747)
[epoch : 28] (l_loss: 0.05727) (t_loss: 0.08117) (accu: 0.9753)
[epoch : 29] (l_loss: 0.05701) (t_loss: 0.08108) (accu: 0.9754)
[epoch : 30] (l_loss: 0.05722) (t_loss: 0.07963) (accu: 0.9757)
[epoch : 31] (l_loss: 0.05659) (t_loss: 0.08353) (accu: 0.9741)
[epoch : 32] (l_loss: 0.05712) (t_loss: 0.08119) (accu: 0.9738)
[epoch : 33] (l_loss: 0.05726) (t_loss: 0.08129) (accu: 0.9740)
[epoch : 34] (l_loss: 0.05671) (t_loss: 0.08321) (accu: 0.9744)
[epoch : 35] (l_loss: 0.05672) (t_loss: 0.08073) (accu: 0.9745)
[epoch : 36] (l_loss: 0.05668) (t_loss: 0.08107) (accu: 0.9745)
[epoch : 37] (l_loss: 0.05675) (t_loss: 0.07704) (accu: 0.9757)
[epoch : 38] (l_loss: 0.05730) (t_loss: 0.08351) (accu: 0.9740)
[epoch : 39] (l_loss: 0.05677) (t_loss: 0.07986) (accu: 0.9748)
[epoch : 40] (l_loss: 0.05717) (t_loss: 0.07881) (accu: 0.9758)
[epoch : 41] (l_loss: 0.05705) (t_loss: 0.08093) (accu: 0.9749)
[epoch : 42] (l_loss: 0.05670) (t_loss: 0.08111) (accu: 0.9745)
[epoch : 43] (l_loss: 0.05690) (t_loss: 0.08029) (accu: 0.9748)
[epoch : 44] (l_loss: 0.05669) (t_loss: 0.08050) (accu: 0.9759)
[epoch : 45] (l_loss: 0.05654) (t_loss: 0.07962) (accu: 0.9742)
[epoch : 46] (l_loss: 0.05698) (t_loss: 0.08155) (accu: 0.9743)
[epoch : 47] (l_loss: 0.05660) (t_loss: 0.08077) (accu: 0.9748)
[epoch : 48] (l_loss: 0.05672) (t_loss: 0.07978) (accu: 0.9737)
[epoch : 49] (l_loss: 0.05705) (t_loss: 0.08432) (accu: 0.9743)
[epoch : 50] (l_loss: 0.05697) (t_loss: 0.08009) (accu: 0.9753)
Finish! (Best accu: 0.9765) (Time taken(sec) : 745.39) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (138048 | 77452)         64.06
fc1.weight   :      196000 (125440 | 70560)         64.00
fc2.weight   :        18750 (12000 | 6750)          64.00
fcout.weight :          750 (608 | 142)             81.07
------------------------------------------------------------
[Prune_iter : (3/21), Remaining weight : 64.06 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.37593) (accu: 0.0973)
[epoch : 1] (l_loss: 0.48663) (t_loss: 0.15963) (accu: 0.9559)
[epoch : 2] (l_loss: 0.13190) (t_loss: 0.11267) (accu: 0.9658)
[epoch : 3] (l_loss: 0.09616) (t_loss: 0.09815) (accu: 0.9705)
[epoch : 4] (l_loss: 0.07954) (t_loss: 0.09220) (accu: 0.9727)
[epoch : 5] (l_loss: 0.07022) (t_loss: 0.08430) (accu: 0.9754)
[epoch : 6] (l_loss: 0.06376) (t_loss: 0.08314) (accu: 0.9750)
[epoch : 7] (l_loss: 0.06034) (t_loss: 0.08099) (accu: 0.9763)
[epoch : 8] (l_loss: 0.05829) (t_loss: 0.07979) (accu: 0.9764)
[epoch : 9] (l_loss: 0.05634) (t_loss: 0.07928) (accu: 0.9767)
[epoch : 10] (l_loss: 0.05509) (t_loss: 0.07837) (accu: 0.9764)
[epoch : 11] (l_loss: 0.05474) (t_loss: 0.07871) (accu: 0.9762)
[epoch : 12] (l_loss: 0.05375) (t_loss: 0.07882) (accu: 0.9752)
[epoch : 13] (l_loss: 0.05350) (t_loss: 0.07563) (accu: 0.9766)
[epoch : 14] (l_loss: 0.05334) (t_loss: 0.07838) (accu: 0.9769)
[epoch : 15] (l_loss: 0.05272) (t_loss: 0.07891) (accu: 0.9749)
[epoch : 16] (l_loss: 0.05293) (t_loss: 0.07803) (accu: 0.9750)
[epoch : 17] (l_loss: 0.05249) (t_loss: 0.07790) (accu: 0.9768)
[epoch : 18] (l_loss: 0.05229) (t_loss: 0.07817) (accu: 0.9757)
[epoch : 19] (l_loss: 0.05236) (t_loss: 0.07815) (accu: 0.9748)
[epoch : 20] (l_loss: 0.05239) (t_loss: 0.07640) (accu: 0.9766)
[epoch : 21] (l_loss: 0.05253) (t_loss: 0.07551) (accu: 0.9768)
[epoch : 22] (l_loss: 0.05207) (t_loss: 0.07799) (accu: 0.9754)
[epoch : 23] (l_loss: 0.05214) (t_loss: 0.07627) (accu: 0.9764)
[epoch : 24] (l_loss: 0.05206) (t_loss: 0.07625) (accu: 0.9757)
[epoch : 25] (l_loss: 0.05164) (t_loss: 0.07690) (accu: 0.9776)
[epoch : 26] (l_loss: 0.05239) (t_loss: 0.07664) (accu: 0.9767)
[epoch : 27] (l_loss: 0.05172) (t_loss: 0.07580) (accu: 0.9777)
[epoch : 28] (l_loss: 0.05212) (t_loss: 0.07762) (accu: 0.9753)
[epoch : 29] (l_loss: 0.05218) (t_loss: 0.07664) (accu: 0.9772)
[epoch : 30] (l_loss: 0.05161) (t_loss: 0.07791) (accu: 0.9776)
[epoch : 31] (l_loss: 0.05185) (t_loss: 0.07752) (accu: 0.9768)
[epoch : 32] (l_loss: 0.05197) (t_loss: 0.07684) (accu: 0.9769)
[epoch : 33] (l_loss: 0.05164) (t_loss: 0.07684) (accu: 0.9763)
[epoch : 34] (l_loss: 0.05185) (t_loss: 0.07585) (accu: 0.9771)
[epoch : 35] (l_loss: 0.05230) (t_loss: 0.07870) (accu: 0.9756)
[epoch : 36] (l_loss: 0.05180) (t_loss: 0.07841) (accu: 0.9765)
[epoch : 37] (l_loss: 0.05154) (t_loss: 0.07668) (accu: 0.9769)
[epoch : 38] (l_loss: 0.05191) (t_loss: 0.07489) (accu: 0.9776)
[epoch : 39] (l_loss: 0.05199) (t_loss: 0.07885) (accu: 0.9761)
[epoch : 40] (l_loss: 0.05213) (t_loss: 0.07663) (accu: 0.9769)
[epoch : 41] (l_loss: 0.05187) (t_loss: 0.07812) (accu: 0.9756)
[epoch : 42] (l_loss: 0.05187) (t_loss: 0.07722) (accu: 0.9772)
[epoch : 43] (l_loss: 0.05185) (t_loss: 0.07740) (accu: 0.9766)
[epoch : 44] (l_loss: 0.05156) (t_loss: 0.07796) (accu: 0.9752)
[epoch : 45] (l_loss: 0.05188) (t_loss: 0.07525) (accu: 0.9763)
[epoch : 46] (l_loss: 0.05211) (t_loss: 0.07720) (accu: 0.9765)
[epoch : 47] (l_loss: 0.05193) (t_loss: 0.07889) (accu: 0.9757)
[epoch : 48] (l_loss: 0.05162) (t_loss: 0.07530) (accu: 0.9768)
[epoch : 49] (l_loss: 0.05207) (t_loss: 0.07732) (accu: 0.9765)
[epoch : 50] (l_loss: 0.05127) (t_loss: 0.07495) (accu: 0.9770)
Finish! (Best accu: 0.9777) (Time taken(sec) : 745.97) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (110499 | 105001)        51.28
fc1.weight   :      196000 (100352 | 95648)         51.20
fc2.weight   :        18750 (9600 | 9150)           51.20
fcout.weight :          750 (547 | 203)             72.93
------------------------------------------------------------
[Prune_iter : (4/21), Remaining weight : 51.28 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.33913) (accu: 0.0977)
[epoch : 1] (l_loss: 0.48814) (t_loss: 0.16327) (accu: 0.9527)
[epoch : 2] (l_loss: 0.13201) (t_loss: 0.11433) (accu: 0.9677)
[epoch : 3] (l_loss: 0.09765) (t_loss: 0.09987) (accu: 0.9698)
[epoch : 4] (l_loss: 0.08176) (t_loss: 0.09283) (accu: 0.9720)
[epoch : 5] (l_loss: 0.07260) (t_loss: 0.08416) (accu: 0.9749)
[epoch : 6] (l_loss: 0.06690) (t_loss: 0.08535) (accu: 0.9735)
[epoch : 7] (l_loss: 0.06221) (t_loss: 0.08126) (accu: 0.9751)
[epoch : 8] (l_loss: 0.05946) (t_loss: 0.08116) (accu: 0.9749)
[epoch : 9] (l_loss: 0.05804) (t_loss: 0.08147) (accu: 0.9762)
[epoch : 10] (l_loss: 0.05630) (t_loss: 0.08055) (accu: 0.9766)
[epoch : 11] (l_loss: 0.05531) (t_loss: 0.07718) (accu: 0.9772)
[epoch : 12] (l_loss: 0.05492) (t_loss: 0.07715) (accu: 0.9772)
[epoch : 13] (l_loss: 0.05454) (t_loss: 0.07699) (accu: 0.9764)
[epoch : 14] (l_loss: 0.05361) (t_loss: 0.07444) (accu: 0.9773)
[epoch : 15] (l_loss: 0.05409) (t_loss: 0.07906) (accu: 0.9755)
[epoch : 16] (l_loss: 0.05368) (t_loss: 0.07791) (accu: 0.9768)
[epoch : 17] (l_loss: 0.05349) (t_loss: 0.07673) (accu: 0.9776)
[epoch : 18] (l_loss: 0.05378) (t_loss: 0.07881) (accu: 0.9763)
[epoch : 19] (l_loss: 0.05337) (t_loss: 0.07883) (accu: 0.9771)
[epoch : 20] (l_loss: 0.05352) (t_loss: 0.07725) (accu: 0.9765)
[epoch : 21] (l_loss: 0.05334) (t_loss: 0.07789) (accu: 0.9767)
[epoch : 22] (l_loss: 0.05308) (t_loss: 0.07949) (accu: 0.9760)
[epoch : 23] (l_loss: 0.05274) (t_loss: 0.07576) (accu: 0.9745)
[epoch : 24] (l_loss: 0.05315) (t_loss: 0.07652) (accu: 0.9783)
[epoch : 25] (l_loss: 0.05303) (t_loss: 0.07673) (accu: 0.9767)
[epoch : 26] (l_loss: 0.05287) (t_loss: 0.07737) (accu: 0.9760)
[epoch : 27] (l_loss: 0.05241) (t_loss: 0.07608) (accu: 0.9751)
[epoch : 28] (l_loss: 0.05281) (t_loss: 0.07544) (accu: 0.9757)
[epoch : 29] (l_loss: 0.05208) (t_loss: 0.07530) (accu: 0.9774)
[epoch : 30] (l_loss: 0.05207) (t_loss: 0.07448) (accu: 0.9779)
[epoch : 31] (l_loss: 0.05211) (t_loss: 0.07682) (accu: 0.9766)
[epoch : 32] (l_loss: 0.05169) (t_loss: 0.07854) (accu: 0.9752)
[epoch : 33] (l_loss: 0.05174) (t_loss: 0.07733) (accu: 0.9753)
[epoch : 34] (l_loss: 0.05151) (t_loss: 0.07802) (accu: 0.9753)
[epoch : 35] (l_loss: 0.05212) (t_loss: 0.07608) (accu: 0.9761)
[epoch : 36] (l_loss: 0.05184) (t_loss: 0.08123) (accu: 0.9741)
[epoch : 37] (l_loss: 0.05150) (t_loss: 0.07712) (accu: 0.9757)
[epoch : 38] (l_loss: 0.05171) (t_loss: 0.07582) (accu: 0.9771)
[epoch : 39] (l_loss: 0.05185) (t_loss: 0.07586) (accu: 0.9774)
[epoch : 40] (l_loss: 0.05141) (t_loss: 0.07732) (accu: 0.9766)
[epoch : 41] (l_loss: 0.05128) (t_loss: 0.07795) (accu: 0.9761)
[epoch : 42] (l_loss: 0.05159) (t_loss: 0.07549) (accu: 0.9763)
[epoch : 43] (l_loss: 0.05143) (t_loss: 0.07761) (accu: 0.9751)
[epoch : 44] (l_loss: 0.05205) (t_loss: 0.07671) (accu: 0.9765)
[epoch : 45] (l_loss: 0.05162) (t_loss: 0.07554) (accu: 0.9761)
[epoch : 46] (l_loss: 0.05145) (t_loss: 0.07764) (accu: 0.9762)
[epoch : 47] (l_loss: 0.05148) (t_loss: 0.07719) (accu: 0.9757)
[epoch : 48] (l_loss: 0.05151) (t_loss: 0.07874) (accu: 0.9766)
[epoch : 49] (l_loss: 0.05173) (t_loss: 0.07533) (accu: 0.9766)
[epoch : 50] (l_loss: 0.05175) (t_loss: 0.07581) (accu: 0.9764)
Finish! (Best accu: 0.9783) (Time taken(sec) : 753.99) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (88454 | 127046)         41.05
fc1.weight   :      196000 (80282 | 115718)         40.96
fc2.weight   :        18750 (7680 | 11070)          40.96
fcout.weight :          750 (492 | 258)             65.60
------------------------------------------------------------
[Prune_iter : (5/21), Remaining weight : 41.05 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.34409) (accu: 0.1117)
[epoch : 1] (l_loss: 0.48918) (t_loss: 0.16779) (accu: 0.9499)
[epoch : 2] (l_loss: 0.14155) (t_loss: 0.11956) (accu: 0.9635)
[epoch : 3] (l_loss: 0.10510) (t_loss: 0.10542) (accu: 0.9693)
[epoch : 4] (l_loss: 0.08890) (t_loss: 0.09763) (accu: 0.9698)
[epoch : 5] (l_loss: 0.07936) (t_loss: 0.09235) (accu: 0.9732)
[epoch : 6] (l_loss: 0.07289) (t_loss: 0.09014) (accu: 0.9733)
[epoch : 7] (l_loss: 0.06867) (t_loss: 0.08635) (accu: 0.9738)
[epoch : 8] (l_loss: 0.06551) (t_loss: 0.08694) (accu: 0.9732)
[epoch : 9] (l_loss: 0.06368) (t_loss: 0.08615) (accu: 0.9744)
[epoch : 10] (l_loss: 0.06200) (t_loss: 0.08338) (accu: 0.9750)
[epoch : 11] (l_loss: 0.06092) (t_loss: 0.08350) (accu: 0.9750)
[epoch : 12] (l_loss: 0.06009) (t_loss: 0.08232) (accu: 0.9758)
[epoch : 13] (l_loss: 0.05984) (t_loss: 0.08297) (accu: 0.9741)
[epoch : 14] (l_loss: 0.05942) (t_loss: 0.08211) (accu: 0.9746)
[epoch : 15] (l_loss: 0.05905) (t_loss: 0.08543) (accu: 0.9750)
[epoch : 16] (l_loss: 0.05845) (t_loss: 0.08275) (accu: 0.9742)
[epoch : 17] (l_loss: 0.05820) (t_loss: 0.08253) (accu: 0.9749)
[epoch : 18] (l_loss: 0.05768) (t_loss: 0.08261) (accu: 0.9747)
[epoch : 19] (l_loss: 0.05807) (t_loss: 0.08181) (accu: 0.9751)
[epoch : 20] (l_loss: 0.05783) (t_loss: 0.08502) (accu: 0.9741)
[epoch : 21] (l_loss: 0.05754) (t_loss: 0.08196) (accu: 0.9748)
[epoch : 22] (l_loss: 0.05791) (t_loss: 0.08211) (accu: 0.9747)
[epoch : 23] (l_loss: 0.05751) (t_loss: 0.07954) (accu: 0.9754)
[epoch : 24] (l_loss: 0.05768) (t_loss: 0.08279) (accu: 0.9736)
[epoch : 25] (l_loss: 0.05756) (t_loss: 0.08131) (accu: 0.9748)
[epoch : 26] (l_loss: 0.05758) (t_loss: 0.08215) (accu: 0.9738)
[epoch : 27] (l_loss: 0.05707) (t_loss: 0.08392) (accu: 0.9741)
[epoch : 28] (l_loss: 0.05698) (t_loss: 0.08666) (accu: 0.9740)
[epoch : 29] (l_loss: 0.05744) (t_loss: 0.08317) (accu: 0.9740)
[epoch : 30] (l_loss: 0.05776) (t_loss: 0.08022) (accu: 0.9754)
[epoch : 31] (l_loss: 0.05693) (t_loss: 0.08393) (accu: 0.9724)
[epoch : 32] (l_loss: 0.05741) (t_loss: 0.08055) (accu: 0.9754)
[epoch : 33] (l_loss: 0.05656) (t_loss: 0.08105) (accu: 0.9764)
[epoch : 34] (l_loss: 0.05602) (t_loss: 0.08084) (accu: 0.9747)
[epoch : 35] (l_loss: 0.05555) (t_loss: 0.07874) (accu: 0.9761)
[epoch : 36] (l_loss: 0.05510) (t_loss: 0.07891) (accu: 0.9760)
[epoch : 37] (l_loss: 0.05473) (t_loss: 0.08058) (accu: 0.9745)
[epoch : 38] (l_loss: 0.05459) (t_loss: 0.08128) (accu: 0.9736)
[epoch : 39] (l_loss: 0.05425) (t_loss: 0.08073) (accu: 0.9750)
[epoch : 40] (l_loss: 0.05429) (t_loss: 0.07815) (accu: 0.9770)
[epoch : 41] (l_loss: 0.05396) (t_loss: 0.07788) (accu: 0.9757)
[epoch : 42] (l_loss: 0.05353) (t_loss: 0.07885) (accu: 0.9761)
[epoch : 43] (l_loss: 0.05378) (t_loss: 0.07923) (accu: 0.9759)
[epoch : 44] (l_loss: 0.05384) (t_loss: 0.07930) (accu: 0.9753)
[epoch : 45] (l_loss: 0.05390) (t_loss: 0.07692) (accu: 0.9750)
[epoch : 46] (l_loss: 0.05351) (t_loss: 0.07787) (accu: 0.9759)
[epoch : 47] (l_loss: 0.05342) (t_loss: 0.08155) (accu: 0.9749)
[epoch : 48] (l_loss: 0.05347) (t_loss: 0.07664) (accu: 0.9755)
[epoch : 49] (l_loss: 0.05362) (t_loss: 0.07884) (accu: 0.9762)
[epoch : 50] (l_loss: 0.05352) (t_loss: 0.07708) (accu: 0.9763)
Finish! (Best accu: 0.9770) (Time taken(sec) : 747.94) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (70812 | 144688)         32.86
fc1.weight   :      196000 (64225 | 131775)         32.77
fc2.weight   :        18750 (6144 | 12606)          32.77
fcout.weight :          750 (443 | 307)             59.07
------------------------------------------------------------
[Prune_iter : (6/21), Remaining weight : 32.86 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.32479) (accu: 0.1228)
[epoch : 1] (l_loss: 0.47515) (t_loss: 0.15951) (accu: 0.9551)
[epoch : 2] (l_loss: 0.13250) (t_loss: 0.11645) (accu: 0.9674)
[epoch : 3] (l_loss: 0.10093) (t_loss: 0.10425) (accu: 0.9692)
[epoch : 4] (l_loss: 0.08572) (t_loss: 0.09613) (accu: 0.9715)
[epoch : 5] (l_loss: 0.07658) (t_loss: 0.09091) (accu: 0.9725)
[epoch : 6] (l_loss: 0.07072) (t_loss: 0.08640) (accu: 0.9748)
[epoch : 7] (l_loss: 0.06599) (t_loss: 0.08509) (accu: 0.9744)
[epoch : 8] (l_loss: 0.06198) (t_loss: 0.08151) (accu: 0.9753)
[epoch : 9] (l_loss: 0.05937) (t_loss: 0.08332) (accu: 0.9761)
[epoch : 10] (l_loss: 0.05726) (t_loss: 0.08148) (accu: 0.9753)
[epoch : 11] (l_loss: 0.05656) (t_loss: 0.07911) (accu: 0.9763)
[epoch : 12] (l_loss: 0.05536) (t_loss: 0.07826) (accu: 0.9761)
[epoch : 13] (l_loss: 0.05484) (t_loss: 0.07821) (accu: 0.9769)
[epoch : 14] (l_loss: 0.05444) (t_loss: 0.08227) (accu: 0.9742)
[epoch : 15] (l_loss: 0.05427) (t_loss: 0.08279) (accu: 0.9745)
[epoch : 16] (l_loss: 0.05372) (t_loss: 0.07641) (accu: 0.9772)
[epoch : 17] (l_loss: 0.05355) (t_loss: 0.07676) (accu: 0.9761)
[epoch : 18] (l_loss: 0.05369) (t_loss: 0.07694) (accu: 0.9760)
[epoch : 19] (l_loss: 0.05301) (t_loss: 0.07906) (accu: 0.9756)
[epoch : 20] (l_loss: 0.05294) (t_loss: 0.07705) (accu: 0.9763)
[epoch : 21] (l_loss: 0.05349) (t_loss: 0.07717) (accu: 0.9762)
[epoch : 22] (l_loss: 0.05289) (t_loss: 0.08066) (accu: 0.9753)
[epoch : 23] (l_loss: 0.05302) (t_loss: 0.07832) (accu: 0.9769)
[epoch : 24] (l_loss: 0.05276) (t_loss: 0.07819) (accu: 0.9763)
[epoch : 25] (l_loss: 0.05297) (t_loss: 0.07761) (accu: 0.9768)
[epoch : 26] (l_loss: 0.05291) (t_loss: 0.07884) (accu: 0.9777)
[epoch : 27] (l_loss: 0.05253) (t_loss: 0.07766) (accu: 0.9759)
[epoch : 28] (l_loss: 0.05259) (t_loss: 0.07944) (accu: 0.9763)
[epoch : 29] (l_loss: 0.05242) (t_loss: 0.07733) (accu: 0.9746)
[epoch : 30] (l_loss: 0.05308) (t_loss: 0.07880) (accu: 0.9773)
[epoch : 31] (l_loss: 0.05260) (t_loss: 0.08082) (accu: 0.9747)
[epoch : 32] (l_loss: 0.05239) (t_loss: 0.07693) (accu: 0.9763)
[epoch : 33] (l_loss: 0.05264) (t_loss: 0.07584) (accu: 0.9765)
[epoch : 34] (l_loss: 0.05244) (t_loss: 0.07927) (accu: 0.9771)
[epoch : 35] (l_loss: 0.05259) (t_loss: 0.07680) (accu: 0.9771)
[epoch : 36] (l_loss: 0.05265) (t_loss: 0.07746) (accu: 0.9765)
[epoch : 37] (l_loss: 0.05281) (t_loss: 0.07777) (accu: 0.9755)
[epoch : 38] (l_loss: 0.05251) (t_loss: 0.07676) (accu: 0.9773)
[epoch : 39] (l_loss: 0.05249) (t_loss: 0.07479) (accu: 0.9781)
[epoch : 40] (l_loss: 0.05277) (t_loss: 0.07785) (accu: 0.9755)
[epoch : 41] (l_loss: 0.05270) (t_loss: 0.07998) (accu: 0.9741)
[epoch : 42] (l_loss: 0.05263) (t_loss: 0.07822) (accu: 0.9757)
[epoch : 43] (l_loss: 0.05275) (t_loss: 0.07701) (accu: 0.9774)
[epoch : 44] (l_loss: 0.05222) (t_loss: 0.07693) (accu: 0.9766)
[epoch : 45] (l_loss: 0.05263) (t_loss: 0.07622) (accu: 0.9770)
[epoch : 46] (l_loss: 0.05209) (t_loss: 0.07528) (accu: 0.9774)
[epoch : 47] (l_loss: 0.05241) (t_loss: 0.07799) (accu: 0.9760)
[epoch : 48] (l_loss: 0.05259) (t_loss: 0.08058) (accu: 0.9767)
[epoch : 49] (l_loss: 0.05266) (t_loss: 0.07773) (accu: 0.9771)
[epoch : 50] (l_loss: 0.05213) (t_loss: 0.07704) (accu: 0.9753)
Finish! (Best accu: 0.9781) (Time taken(sec) : 762.66) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (56694 | 158806)         26.31
fc1.weight   :      196000 (51380 | 144620)         26.21
fc2.weight   :        18750 (4915 | 13835)          26.21
fcout.weight :          750 (399 | 351)             53.20
------------------------------------------------------------
[Prune_iter : (7/21), Remaining weight : 26.31 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29642) (accu: 0.1459)
[epoch : 1] (l_loss: 0.46708) (t_loss: 0.15689) (accu: 0.9548)
[epoch : 2] (l_loss: 0.13268) (t_loss: 0.11899) (accu: 0.9642)
[epoch : 3] (l_loss: 0.10064) (t_loss: 0.10580) (accu: 0.9686)
[epoch : 4] (l_loss: 0.08466) (t_loss: 0.09253) (accu: 0.9714)
[epoch : 5] (l_loss: 0.07552) (t_loss: 0.09064) (accu: 0.9731)
[epoch : 6] (l_loss: 0.06826) (t_loss: 0.08457) (accu: 0.9743)
[epoch : 7] (l_loss: 0.06322) (t_loss: 0.08261) (accu: 0.9742)
[epoch : 8] (l_loss: 0.05989) (t_loss: 0.07945) (accu: 0.9753)
[epoch : 9] (l_loss: 0.05774) (t_loss: 0.07792) (accu: 0.9758)
[epoch : 10] (l_loss: 0.05610) (t_loss: 0.08028) (accu: 0.9750)
[epoch : 11] (l_loss: 0.05585) (t_loss: 0.07815) (accu: 0.9759)
[epoch : 12] (l_loss: 0.05473) (t_loss: 0.07948) (accu: 0.9768)
[epoch : 13] (l_loss: 0.05432) (t_loss: 0.07783) (accu: 0.9762)
[epoch : 14] (l_loss: 0.05434) (t_loss: 0.08027) (accu: 0.9767)
[epoch : 15] (l_loss: 0.05394) (t_loss: 0.07765) (accu: 0.9760)
[epoch : 16] (l_loss: 0.05353) (t_loss: 0.07741) (accu: 0.9760)
[epoch : 17] (l_loss: 0.05299) (t_loss: 0.07783) (accu: 0.9772)
[epoch : 18] (l_loss: 0.05293) (t_loss: 0.07669) (accu: 0.9767)
[epoch : 19] (l_loss: 0.05278) (t_loss: 0.07581) (accu: 0.9756)
[epoch : 20] (l_loss: 0.05312) (t_loss: 0.07703) (accu: 0.9767)
[epoch : 21] (l_loss: 0.05256) (t_loss: 0.07891) (accu: 0.9767)
[epoch : 22] (l_loss: 0.05296) (t_loss: 0.07964) (accu: 0.9760)
[epoch : 23] (l_loss: 0.05285) (t_loss: 0.07778) (accu: 0.9762)
[epoch : 24] (l_loss: 0.05250) (t_loss: 0.07556) (accu: 0.9770)
[epoch : 25] (l_loss: 0.05279) (t_loss: 0.07673) (accu: 0.9760)
[epoch : 26] (l_loss: 0.05265) (t_loss: 0.07928) (accu: 0.9758)
[epoch : 27] (l_loss: 0.05267) (t_loss: 0.07692) (accu: 0.9763)
[epoch : 28] (l_loss: 0.05225) (t_loss: 0.07805) (accu: 0.9752)
[epoch : 29] (l_loss: 0.05231) (t_loss: 0.07673) (accu: 0.9762)
[epoch : 30] (l_loss: 0.05246) (t_loss: 0.07817) (accu: 0.9757)
[epoch : 31] (l_loss: 0.05214) (t_loss: 0.07693) (accu: 0.9753)
[epoch : 32] (l_loss: 0.05227) (t_loss: 0.07509) (accu: 0.9770)
[epoch : 33] (l_loss: 0.05212) (t_loss: 0.07754) (accu: 0.9762)
[epoch : 34] (l_loss: 0.05224) (t_loss: 0.07748) (accu: 0.9772)
[epoch : 35] (l_loss: 0.05222) (t_loss: 0.07606) (accu: 0.9764)
[epoch : 36] (l_loss: 0.05247) (t_loss: 0.07697) (accu: 0.9757)
[epoch : 37] (l_loss: 0.05239) (t_loss: 0.07754) (accu: 0.9752)
[epoch : 38] (l_loss: 0.05214) (t_loss: 0.07796) (accu: 0.9754)
[epoch : 39] (l_loss: 0.05260) (t_loss: 0.07793) (accu: 0.9756)
[epoch : 40] (l_loss: 0.05251) (t_loss: 0.07706) (accu: 0.9768)
[epoch : 41] (l_loss: 0.05248) (t_loss: 0.07762) (accu: 0.9748)
[epoch : 42] (l_loss: 0.05237) (t_loss: 0.07579) (accu: 0.9763)
[epoch : 43] (l_loss: 0.05235) (t_loss: 0.07522) (accu: 0.9765)
[epoch : 44] (l_loss: 0.05247) (t_loss: 0.07986) (accu: 0.9760)
[epoch : 45] (l_loss: 0.05248) (t_loss: 0.07649) (accu: 0.9752)
[epoch : 46] (l_loss: 0.05227) (t_loss: 0.07773) (accu: 0.9766)
[epoch : 47] (l_loss: 0.05221) (t_loss: 0.07938) (accu: 0.9753)
[epoch : 48] (l_loss: 0.05219) (t_loss: 0.07661) (accu: 0.9757)
[epoch : 49] (l_loss: 0.05233) (t_loss: 0.07946) (accu: 0.9775)
[epoch : 50] (l_loss: 0.05237) (t_loss: 0.07689) (accu: 0.9767)
Finish! (Best accu: 0.9775) (Time taken(sec) : 754.84) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (45395 | 170105)         21.06
fc1.weight   :      196000 (41104 | 154896)         20.97
fc2.weight   :        18750 (3932 | 14818)          20.97
fcout.weight :          750 (359 | 391)             47.87
------------------------------------------------------------
[Prune_iter : (8/21), Remaining weight : 21.06 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.31123) (accu: 0.1349)
[epoch : 1] (l_loss: 0.46265) (t_loss: 0.16561) (accu: 0.9523)
[epoch : 2] (l_loss: 0.13862) (t_loss: 0.12033) (accu: 0.9653)
[epoch : 3] (l_loss: 0.10338) (t_loss: 0.10485) (accu: 0.9691)
[epoch : 4] (l_loss: 0.08564) (t_loss: 0.09245) (accu: 0.9723)
[epoch : 5] (l_loss: 0.07566) (t_loss: 0.08816) (accu: 0.9734)
[epoch : 6] (l_loss: 0.06865) (t_loss: 0.08783) (accu: 0.9739)
[epoch : 7] (l_loss: 0.06429) (t_loss: 0.08166) (accu: 0.9748)
[epoch : 8] (l_loss: 0.06099) (t_loss: 0.07998) (accu: 0.9751)
[epoch : 9] (l_loss: 0.05868) (t_loss: 0.08003) (accu: 0.9758)
[epoch : 10] (l_loss: 0.05674) (t_loss: 0.07967) (accu: 0.9754)
[epoch : 11] (l_loss: 0.05525) (t_loss: 0.07555) (accu: 0.9776)
[epoch : 12] (l_loss: 0.05508) (t_loss: 0.07611) (accu: 0.9773)
[epoch : 13] (l_loss: 0.05383) (t_loss: 0.07690) (accu: 0.9761)
[epoch : 14] (l_loss: 0.05365) (t_loss: 0.07310) (accu: 0.9779)
[epoch : 15] (l_loss: 0.05290) (t_loss: 0.07456) (accu: 0.9775)
[epoch : 16] (l_loss: 0.05295) (t_loss: 0.07348) (accu: 0.9777)
[epoch : 17] (l_loss: 0.05284) (t_loss: 0.07487) (accu: 0.9772)
[epoch : 18] (l_loss: 0.05291) (t_loss: 0.07605) (accu: 0.9774)
[epoch : 19] (l_loss: 0.05240) (t_loss: 0.07540) (accu: 0.9759)
[epoch : 20] (l_loss: 0.05211) (t_loss: 0.07833) (accu: 0.9758)
[epoch : 21] (l_loss: 0.05256) (t_loss: 0.07649) (accu: 0.9762)
[epoch : 22] (l_loss: 0.05214) (t_loss: 0.07659) (accu: 0.9779)
[epoch : 23] (l_loss: 0.05195) (t_loss: 0.07299) (accu: 0.9766)
[epoch : 24] (l_loss: 0.05244) (t_loss: 0.07545) (accu: 0.9758)
[epoch : 25] (l_loss: 0.05216) (t_loss: 0.07702) (accu: 0.9761)
[epoch : 26] (l_loss: 0.05248) (t_loss: 0.07774) (accu: 0.9772)
[epoch : 27] (l_loss: 0.05207) (t_loss: 0.07646) (accu: 0.9769)
[epoch : 28] (l_loss: 0.05200) (t_loss: 0.07654) (accu: 0.9760)
[epoch : 29] (l_loss: 0.05186) (t_loss: 0.07663) (accu: 0.9751)
[epoch : 30] (l_loss: 0.05203) (t_loss: 0.07693) (accu: 0.9750)
[epoch : 31] (l_loss: 0.05163) (t_loss: 0.07527) (accu: 0.9774)
[epoch : 32] (l_loss: 0.05218) (t_loss: 0.07518) (accu: 0.9770)
[epoch : 33] (l_loss: 0.05204) (t_loss: 0.07548) (accu: 0.9768)
[epoch : 34] (l_loss: 0.05186) (t_loss: 0.07728) (accu: 0.9754)
[epoch : 35] (l_loss: 0.05158) (t_loss: 0.07620) (accu: 0.9770)
[epoch : 36] (l_loss: 0.05181) (t_loss: 0.07688) (accu: 0.9771)
[epoch : 37] (l_loss: 0.05198) (t_loss: 0.07738) (accu: 0.9767)
[epoch : 38] (l_loss: 0.05197) (t_loss: 0.07660) (accu: 0.9766)
[epoch : 39] (l_loss: 0.05184) (t_loss: 0.07623) (accu: 0.9768)
[epoch : 40] (l_loss: 0.05179) (t_loss: 0.07573) (accu: 0.9750)
[epoch : 41] (l_loss: 0.05207) (t_loss: 0.07683) (accu: 0.9773)
[epoch : 42] (l_loss: 0.05162) (t_loss: 0.07656) (accu: 0.9779)
[epoch : 43] (l_loss: 0.05182) (t_loss: 0.07870) (accu: 0.9759)
[epoch : 44] (l_loss: 0.05170) (t_loss: 0.07707) (accu: 0.9770)
[epoch : 45] (l_loss: 0.05180) (t_loss: 0.07472) (accu: 0.9773)
[epoch : 46] (l_loss: 0.05171) (t_loss: 0.07419) (accu: 0.9769)
[epoch : 47] (l_loss: 0.05184) (t_loss: 0.07564) (accu: 0.9767)
[epoch : 48] (l_loss: 0.05124) (t_loss: 0.07645) (accu: 0.9760)
[epoch : 49] (l_loss: 0.05147) (t_loss: 0.07698) (accu: 0.9766)
[epoch : 50] (l_loss: 0.05217) (t_loss: 0.07914) (accu: 0.9765)
Finish! (Best accu: 0.9779) (Time taken(sec) : 768.68) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (36352 | 179148)         16.87
fc1.weight   :      196000 (32883 | 163117)         16.78
fc2.weight   :        18750 (3146 | 15604)          16.78
fcout.weight :          750 (323 | 427)             43.07
------------------------------------------------------------
[Prune_iter : (9/21), Remaining weight : 16.87 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.31012) (accu: 0.1636)
[epoch : 1] (l_loss: 0.45942) (t_loss: 0.16135) (accu: 0.9550)
[epoch : 2] (l_loss: 0.13768) (t_loss: 0.12210) (accu: 0.9640)
[epoch : 3] (l_loss: 0.10652) (t_loss: 0.10763) (accu: 0.9672)
[epoch : 4] (l_loss: 0.09016) (t_loss: 0.09686) (accu: 0.9701)
[epoch : 5] (l_loss: 0.07922) (t_loss: 0.09450) (accu: 0.9711)
[epoch : 6] (l_loss: 0.07238) (t_loss: 0.08704) (accu: 0.9736)
[epoch : 7] (l_loss: 0.06681) (t_loss: 0.08442) (accu: 0.9739)
[epoch : 8] (l_loss: 0.06303) (t_loss: 0.08394) (accu: 0.9747)
[epoch : 9] (l_loss: 0.06080) (t_loss: 0.08069) (accu: 0.9759)
[epoch : 10] (l_loss: 0.05914) (t_loss: 0.08077) (accu: 0.9763)
[epoch : 11] (l_loss: 0.05803) (t_loss: 0.07994) (accu: 0.9764)
[epoch : 12] (l_loss: 0.05665) (t_loss: 0.08269) (accu: 0.9755)
[epoch : 13] (l_loss: 0.05593) (t_loss: 0.08074) (accu: 0.9756)
[epoch : 14] (l_loss: 0.05618) (t_loss: 0.07926) (accu: 0.9754)
[epoch : 15] (l_loss: 0.05526) (t_loss: 0.07960) (accu: 0.9751)
[epoch : 16] (l_loss: 0.05488) (t_loss: 0.07853) (accu: 0.9771)
[epoch : 17] (l_loss: 0.05501) (t_loss: 0.07889) (accu: 0.9764)
[epoch : 18] (l_loss: 0.05433) (t_loss: 0.08323) (accu: 0.9741)
[epoch : 19] (l_loss: 0.05428) (t_loss: 0.07908) (accu: 0.9771)
[epoch : 20] (l_loss: 0.05393) (t_loss: 0.07875) (accu: 0.9768)
[epoch : 21] (l_loss: 0.05402) (t_loss: 0.07839) (accu: 0.9751)
[epoch : 22] (l_loss: 0.05386) (t_loss: 0.07787) (accu: 0.9750)
[epoch : 23] (l_loss: 0.05355) (t_loss: 0.07901) (accu: 0.9763)
[epoch : 24] (l_loss: 0.05388) (t_loss: 0.07703) (accu: 0.9766)
[epoch : 25] (l_loss: 0.05373) (t_loss: 0.07923) (accu: 0.9752)
[epoch : 26] (l_loss: 0.05377) (t_loss: 0.07979) (accu: 0.9764)
[epoch : 27] (l_loss: 0.05347) (t_loss: 0.08249) (accu: 0.9744)
[epoch : 28] (l_loss: 0.05378) (t_loss: 0.08007) (accu: 0.9754)
[epoch : 29] (l_loss: 0.05367) (t_loss: 0.08121) (accu: 0.9751)
[epoch : 30] (l_loss: 0.05353) (t_loss: 0.08240) (accu: 0.9756)
[epoch : 31] (l_loss: 0.05349) (t_loss: 0.07673) (accu: 0.9763)
[epoch : 32] (l_loss: 0.05394) (t_loss: 0.07796) (accu: 0.9756)
[epoch : 33] (l_loss: 0.05343) (t_loss: 0.07720) (accu: 0.9766)
[epoch : 34] (l_loss: 0.05347) (t_loss: 0.07900) (accu: 0.9754)
[epoch : 35] (l_loss: 0.05353) (t_loss: 0.07724) (accu: 0.9768)
[epoch : 36] (l_loss: 0.05399) (t_loss: 0.07501) (accu: 0.9778)
[epoch : 37] (l_loss: 0.05339) (t_loss: 0.07907) (accu: 0.9755)
[epoch : 38] (l_loss: 0.05371) (t_loss: 0.07716) (accu: 0.9762)
[epoch : 39] (l_loss: 0.05377) (t_loss: 0.07885) (accu: 0.9757)
[epoch : 40] (l_loss: 0.05381) (t_loss: 0.07715) (accu: 0.9756)
[epoch : 41] (l_loss: 0.05390) (t_loss: 0.08006) (accu: 0.9748)
[epoch : 42] (l_loss: 0.05353) (t_loss: 0.07612) (accu: 0.9762)
[epoch : 43] (l_loss: 0.05368) (t_loss: 0.07707) (accu: 0.9762)
[epoch : 44] (l_loss: 0.05362) (t_loss: 0.07831) (accu: 0.9744)
[epoch : 45] (l_loss: 0.05343) (t_loss: 0.07707) (accu: 0.9758)
[epoch : 46] (l_loss: 0.05364) (t_loss: 0.07779) (accu: 0.9762)
[epoch : 47] (l_loss: 0.05351) (t_loss: 0.07710) (accu: 0.9761)
[epoch : 48] (l_loss: 0.05336) (t_loss: 0.08187) (accu: 0.9753)
[epoch : 49] (l_loss: 0.05360) (t_loss: 0.07721) (accu: 0.9773)
[epoch : 50] (l_loss: 0.05338) (t_loss: 0.07900) (accu: 0.9753)
Finish! (Best accu: 0.9778) (Time taken(sec) : 763.76) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (29115 | 186385)         13.51
fc1.weight   :      196000 (26307 | 169693)         13.42
fc2.weight   :        18750 (2517 | 16233)          13.42
fcout.weight :          750 (291 | 459)             38.80
------------------------------------------------------------
[Prune_iter : (10/21), Remaining weight : 13.51 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30419) (accu: 0.1571)
[epoch : 1] (l_loss: 0.46208) (t_loss: 0.16459) (accu: 0.9527)
[epoch : 2] (l_loss: 0.13905) (t_loss: 0.12249) (accu: 0.9642)
[epoch : 3] (l_loss: 0.10649) (t_loss: 0.10576) (accu: 0.9687)
[epoch : 4] (l_loss: 0.08951) (t_loss: 0.09867) (accu: 0.9705)
[epoch : 5] (l_loss: 0.07825) (t_loss: 0.09161) (accu: 0.9728)
[epoch : 6] (l_loss: 0.07183) (t_loss: 0.08603) (accu: 0.9739)
[epoch : 7] (l_loss: 0.06653) (t_loss: 0.08807) (accu: 0.9733)
[epoch : 8] (l_loss: 0.06311) (t_loss: 0.08176) (accu: 0.9753)
[epoch : 9] (l_loss: 0.06042) (t_loss: 0.08110) (accu: 0.9768)
[epoch : 10] (l_loss: 0.05846) (t_loss: 0.08035) (accu: 0.9761)
[epoch : 11] (l_loss: 0.05724) (t_loss: 0.08003) (accu: 0.9763)
[epoch : 12] (l_loss: 0.05598) (t_loss: 0.07884) (accu: 0.9755)
[epoch : 13] (l_loss: 0.05546) (t_loss: 0.07764) (accu: 0.9768)
[epoch : 14] (l_loss: 0.05528) (t_loss: 0.07805) (accu: 0.9766)
[epoch : 15] (l_loss: 0.05463) (t_loss: 0.07680) (accu: 0.9770)
[epoch : 16] (l_loss: 0.05435) (t_loss: 0.07803) (accu: 0.9761)
[epoch : 17] (l_loss: 0.05408) (t_loss: 0.07839) (accu: 0.9746)
[epoch : 18] (l_loss: 0.05411) (t_loss: 0.07664) (accu: 0.9756)
[epoch : 19] (l_loss: 0.05389) (t_loss: 0.07711) (accu: 0.9769)
[epoch : 20] (l_loss: 0.05392) (t_loss: 0.07867) (accu: 0.9767)
[epoch : 21] (l_loss: 0.05361) (t_loss: 0.07783) (accu: 0.9761)
[epoch : 22] (l_loss: 0.05386) (t_loss: 0.07922) (accu: 0.9751)
[epoch : 23] (l_loss: 0.05359) (t_loss: 0.07632) (accu: 0.9768)
[epoch : 24] (l_loss: 0.05351) (t_loss: 0.07818) (accu: 0.9768)
[epoch : 25] (l_loss: 0.05365) (t_loss: 0.07644) (accu: 0.9763)
[epoch : 26] (l_loss: 0.05326) (t_loss: 0.07662) (accu: 0.9770)
[epoch : 27] (l_loss: 0.05365) (t_loss: 0.07943) (accu: 0.9763)
[epoch : 28] (l_loss: 0.05362) (t_loss: 0.07680) (accu: 0.9762)
[epoch : 29] (l_loss: 0.05340) (t_loss: 0.07984) (accu: 0.9765)
[epoch : 30] (l_loss: 0.05334) (t_loss: 0.07652) (accu: 0.9763)
[epoch : 31] (l_loss: 0.05309) (t_loss: 0.08029) (accu: 0.9762)
[epoch : 32] (l_loss: 0.05315) (t_loss: 0.07693) (accu: 0.9772)
[epoch : 33] (l_loss: 0.05345) (t_loss: 0.07609) (accu: 0.9759)
[epoch : 34] (l_loss: 0.05345) (t_loss: 0.07658) (accu: 0.9768)
[epoch : 35] (l_loss: 0.05301) (t_loss: 0.07671) (accu: 0.9768)
[epoch : 36] (l_loss: 0.05315) (t_loss: 0.07478) (accu: 0.9779)
[epoch : 37] (l_loss: 0.05340) (t_loss: 0.07671) (accu: 0.9767)
[epoch : 38] (l_loss: 0.05321) (t_loss: 0.07701) (accu: 0.9771)
[epoch : 39] (l_loss: 0.05336) (t_loss: 0.07693) (accu: 0.9763)
[epoch : 40] (l_loss: 0.05332) (t_loss: 0.07841) (accu: 0.9751)
[epoch : 41] (l_loss: 0.05303) (t_loss: 0.07607) (accu: 0.9760)
[epoch : 42] (l_loss: 0.05332) (t_loss: 0.07634) (accu: 0.9766)
[epoch : 43] (l_loss: 0.05289) (t_loss: 0.07806) (accu: 0.9767)
[epoch : 44] (l_loss: 0.05314) (t_loss: 0.07823) (accu: 0.9762)
[epoch : 45] (l_loss: 0.05304) (t_loss: 0.07827) (accu: 0.9756)
[epoch : 46] (l_loss: 0.05332) (t_loss: 0.07808) (accu: 0.9769)
[epoch : 47] (l_loss: 0.05309) (t_loss: 0.07824) (accu: 0.9768)
[epoch : 48] (l_loss: 0.05295) (t_loss: 0.07557) (accu: 0.9763)
[epoch : 49] (l_loss: 0.05310) (t_loss: 0.08089) (accu: 0.9752)
[epoch : 50] (l_loss: 0.05326) (t_loss: 0.07736) (accu: 0.9770)
Finish! (Best accu: 0.9779) (Time taken(sec) : 761.85) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (23320 | 192180)         10.82
fc1.weight   :      196000 (21045 | 174955)         10.74
fc2.weight   :        18750 (2013 | 16737)          10.74
fcout.weight :          750 (262 | 488)             34.93
------------------------------------------------------------
[Prune_iter : (11/21), Remaining weight : 10.82 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29603) (accu: 0.1297)
[epoch : 1] (l_loss: 0.45145) (t_loss: 0.16303) (accu: 0.9531)
[epoch : 2] (l_loss: 0.13208) (t_loss: 0.11698) (accu: 0.9657)
[epoch : 3] (l_loss: 0.09925) (t_loss: 0.09956) (accu: 0.9704)
[epoch : 4] (l_loss: 0.08456) (t_loss: 0.09262) (accu: 0.9720)
[epoch : 5] (l_loss: 0.07480) (t_loss: 0.08878) (accu: 0.9729)
[epoch : 6] (l_loss: 0.06874) (t_loss: 0.08488) (accu: 0.9750)
[epoch : 7] (l_loss: 0.06434) (t_loss: 0.08309) (accu: 0.9754)
[epoch : 8] (l_loss: 0.06076) (t_loss: 0.07825) (accu: 0.9754)
[epoch : 9] (l_loss: 0.05857) (t_loss: 0.07868) (accu: 0.9771)
[epoch : 10] (l_loss: 0.05668) (t_loss: 0.07826) (accu: 0.9757)
[epoch : 11] (l_loss: 0.05531) (t_loss: 0.08227) (accu: 0.9736)
[epoch : 12] (l_loss: 0.05440) (t_loss: 0.07797) (accu: 0.9777)
[epoch : 13] (l_loss: 0.05389) (t_loss: 0.07811) (accu: 0.9771)
[epoch : 14] (l_loss: 0.05350) (t_loss: 0.08044) (accu: 0.9747)
[epoch : 15] (l_loss: 0.05289) (t_loss: 0.07582) (accu: 0.9753)
[epoch : 16] (l_loss: 0.05292) (t_loss: 0.07711) (accu: 0.9759)
[epoch : 17] (l_loss: 0.05265) (t_loss: 0.07910) (accu: 0.9742)
[epoch : 18] (l_loss: 0.05222) (t_loss: 0.07648) (accu: 0.9766)
[epoch : 19] (l_loss: 0.05260) (t_loss: 0.07877) (accu: 0.9752)
[epoch : 20] (l_loss: 0.05216) (t_loss: 0.07576) (accu: 0.9777)
[epoch : 21] (l_loss: 0.05200) (t_loss: 0.07628) (accu: 0.9759)
[epoch : 22] (l_loss: 0.05199) (t_loss: 0.07605) (accu: 0.9770)
[epoch : 23] (l_loss: 0.05156) (t_loss: 0.07799) (accu: 0.9760)
[epoch : 24] (l_loss: 0.05190) (t_loss: 0.07637) (accu: 0.9763)
[epoch : 25] (l_loss: 0.05185) (t_loss: 0.07648) (accu: 0.9767)
[epoch : 26] (l_loss: 0.05150) (t_loss: 0.07987) (accu: 0.9743)
[epoch : 27] (l_loss: 0.05179) (t_loss: 0.07587) (accu: 0.9770)
[epoch : 28] (l_loss: 0.05152) (t_loss: 0.07747) (accu: 0.9759)
[epoch : 29] (l_loss: 0.05211) (t_loss: 0.07881) (accu: 0.9750)
[epoch : 30] (l_loss: 0.05191) (t_loss: 0.07700) (accu: 0.9770)
[epoch : 31] (l_loss: 0.05123) (t_loss: 0.07765) (accu: 0.9755)
[epoch : 32] (l_loss: 0.05205) (t_loss: 0.07535) (accu: 0.9759)
[epoch : 33] (l_loss: 0.05189) (t_loss: 0.07590) (accu: 0.9757)
[epoch : 34] (l_loss: 0.05136) (t_loss: 0.07652) (accu: 0.9769)
[epoch : 35] (l_loss: 0.05169) (t_loss: 0.07790) (accu: 0.9773)
[epoch : 36] (l_loss: 0.05173) (t_loss: 0.07676) (accu: 0.9774)
[epoch : 37] (l_loss: 0.05165) (t_loss: 0.07818) (accu: 0.9752)
[epoch : 38] (l_loss: 0.05146) (t_loss: 0.07552) (accu: 0.9770)
[epoch : 39] (l_loss: 0.05144) (t_loss: 0.07511) (accu: 0.9763)
[epoch : 40] (l_loss: 0.05149) (t_loss: 0.07713) (accu: 0.9753)
[epoch : 41] (l_loss: 0.05146) (t_loss: 0.07594) (accu: 0.9774)
[epoch : 42] (l_loss: 0.05145) (t_loss: 0.07546) (accu: 0.9761)
[epoch : 43] (l_loss: 0.05182) (t_loss: 0.07703) (accu: 0.9765)
[epoch : 44] (l_loss: 0.05151) (t_loss: 0.07516) (accu: 0.9758)
[epoch : 45] (l_loss: 0.05133) (t_loss: 0.07953) (accu: 0.9761)
[epoch : 46] (l_loss: 0.05178) (t_loss: 0.07856) (accu: 0.9770)
[epoch : 47] (l_loss: 0.05162) (t_loss: 0.07893) (accu: 0.9760)
[epoch : 48] (l_loss: 0.05204) (t_loss: 0.07604) (accu: 0.9772)
[epoch : 49] (l_loss: 0.05115) (t_loss: 0.07599) (accu: 0.9762)
[epoch : 50] (l_loss: 0.05173) (t_loss: 0.07414) (accu: 0.9758)
Finish! (Best accu: 0.9777) (Time taken(sec) : 771.45) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (18682 | 196818)          8.67
fc1.weight   :      196000 (16836 | 179164)          8.59
fc2.weight   :        18750 (1611 | 17139)           8.59
fcout.weight :          750 (235 | 515)             31.33
------------------------------------------------------------
[Prune_iter : (12/21), Remaining weight : 8.67 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29631) (accu: 0.1450)
[epoch : 1] (l_loss: 0.45674) (t_loss: 0.16511) (accu: 0.9545)
[epoch : 2] (l_loss: 0.13790) (t_loss: 0.11983) (accu: 0.9651)
[epoch : 3] (l_loss: 0.10332) (t_loss: 0.10156) (accu: 0.9691)
[epoch : 4] (l_loss: 0.08670) (t_loss: 0.09459) (accu: 0.9720)
[epoch : 5] (l_loss: 0.07618) (t_loss: 0.08927) (accu: 0.9742)
[epoch : 6] (l_loss: 0.07001) (t_loss: 0.08596) (accu: 0.9751)
[epoch : 7] (l_loss: 0.06555) (t_loss: 0.08175) (accu: 0.9742)
[epoch : 8] (l_loss: 0.06209) (t_loss: 0.08378) (accu: 0.9747)
[epoch : 9] (l_loss: 0.05947) (t_loss: 0.07793) (accu: 0.9765)
[epoch : 10] (l_loss: 0.05798) (t_loss: 0.07874) (accu: 0.9767)
[epoch : 11] (l_loss: 0.05674) (t_loss: 0.07910) (accu: 0.9767)
[epoch : 12] (l_loss: 0.05610) (t_loss: 0.07757) (accu: 0.9765)
[epoch : 13] (l_loss: 0.05525) (t_loss: 0.08371) (accu: 0.9760)
[epoch : 14] (l_loss: 0.05487) (t_loss: 0.07943) (accu: 0.9773)
[epoch : 15] (l_loss: 0.05486) (t_loss: 0.07780) (accu: 0.9751)
[epoch : 16] (l_loss: 0.05469) (t_loss: 0.07728) (accu: 0.9768)
[epoch : 17] (l_loss: 0.05409) (t_loss: 0.07672) (accu: 0.9774)
[epoch : 18] (l_loss: 0.05396) (t_loss: 0.07917) (accu: 0.9762)
[epoch : 19] (l_loss: 0.05396) (t_loss: 0.07956) (accu: 0.9752)
[epoch : 20] (l_loss: 0.05400) (t_loss: 0.07751) (accu: 0.9764)
[epoch : 21] (l_loss: 0.05344) (t_loss: 0.07747) (accu: 0.9754)
[epoch : 22] (l_loss: 0.05379) (t_loss: 0.07708) (accu: 0.9763)
[epoch : 23] (l_loss: 0.05372) (t_loss: 0.07742) (accu: 0.9772)
[epoch : 24] (l_loss: 0.05312) (t_loss: 0.07632) (accu: 0.9766)
[epoch : 25] (l_loss: 0.05363) (t_loss: 0.07827) (accu: 0.9767)
[epoch : 26] (l_loss: 0.05385) (t_loss: 0.08036) (accu: 0.9757)
[epoch : 27] (l_loss: 0.05352) (t_loss: 0.07658) (accu: 0.9763)
[epoch : 28] (l_loss: 0.05356) (t_loss: 0.07738) (accu: 0.9764)
[epoch : 29] (l_loss: 0.05283) (t_loss: 0.07818) (accu: 0.9762)
[epoch : 30] (l_loss: 0.05359) (t_loss: 0.07710) (accu: 0.9771)
[epoch : 31] (l_loss: 0.05327) (t_loss: 0.07803) (accu: 0.9750)
[epoch : 32] (l_loss: 0.05319) (t_loss: 0.07699) (accu: 0.9758)
[epoch : 33] (l_loss: 0.05293) (t_loss: 0.07727) (accu: 0.9763)
[epoch : 34] (l_loss: 0.05293) (t_loss: 0.07901) (accu: 0.9764)
[epoch : 35] (l_loss: 0.05299) (t_loss: 0.07861) (accu: 0.9758)
[epoch : 36] (l_loss: 0.05318) (t_loss: 0.07863) (accu: 0.9771)
[epoch : 37] (l_loss: 0.05366) (t_loss: 0.07881) (accu: 0.9767)
[epoch : 38] (l_loss: 0.05296) (t_loss: 0.07674) (accu: 0.9784)
[epoch : 39] (l_loss: 0.05309) (t_loss: 0.08067) (accu: 0.9760)
[epoch : 40] (l_loss: 0.05329) (t_loss: 0.07454) (accu: 0.9764)
[epoch : 41] (l_loss: 0.05295) (t_loss: 0.07793) (accu: 0.9775)
[epoch : 42] (l_loss: 0.05327) (t_loss: 0.08006) (accu: 0.9759)
[epoch : 43] (l_loss: 0.05282) (t_loss: 0.07942) (accu: 0.9756)
[epoch : 44] (l_loss: 0.05321) (t_loss: 0.07883) (accu: 0.9771)
[epoch : 45] (l_loss: 0.05324) (t_loss: 0.07510) (accu: 0.9784)
[epoch : 46] (l_loss: 0.05295) (t_loss: 0.07768) (accu: 0.9756)
[epoch : 47] (l_loss: 0.05316) (t_loss: 0.07835) (accu: 0.9757)
[epoch : 48] (l_loss: 0.05355) (t_loss: 0.07632) (accu: 0.9767)
[epoch : 49] (l_loss: 0.05251) (t_loss: 0.07737) (accu: 0.9766)
[epoch : 50] (l_loss: 0.05327) (t_loss: 0.07766) (accu: 0.9767)
Finish! (Best accu: 0.9784) (Time taken(sec) : 761.24) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (14969 | 200531)          6.95
fc1.weight   :      196000 (13469 | 182531)          6.87
fc2.weight   :        18750 (1288 | 17462)           6.87
fcout.weight :          750 (212 | 538)             28.27
------------------------------------------------------------
[Prune_iter : (13/21), Remaining weight : 6.95 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30091) (accu: 0.1456)
[epoch : 1] (l_loss: 0.45998) (t_loss: 0.16063) (accu: 0.9541)
[epoch : 2] (l_loss: 0.13593) (t_loss: 0.12008) (accu: 0.9650)
[epoch : 3] (l_loss: 0.10253) (t_loss: 0.10567) (accu: 0.9695)
[epoch : 4] (l_loss: 0.08646) (t_loss: 0.09209) (accu: 0.9719)
[epoch : 5] (l_loss: 0.07680) (t_loss: 0.08862) (accu: 0.9729)
[epoch : 6] (l_loss: 0.06979) (t_loss: 0.08560) (accu: 0.9740)
[epoch : 7] (l_loss: 0.06501) (t_loss: 0.07971) (accu: 0.9758)
[epoch : 8] (l_loss: 0.06167) (t_loss: 0.08330) (accu: 0.9753)
[epoch : 9] (l_loss: 0.05991) (t_loss: 0.08165) (accu: 0.9757)
[epoch : 10] (l_loss: 0.05783) (t_loss: 0.08361) (accu: 0.9762)
[epoch : 11] (l_loss: 0.05707) (t_loss: 0.07998) (accu: 0.9768)
[epoch : 12] (l_loss: 0.05607) (t_loss: 0.07984) (accu: 0.9762)
[epoch : 13] (l_loss: 0.05560) (t_loss: 0.07613) (accu: 0.9770)
[epoch : 14] (l_loss: 0.05534) (t_loss: 0.07845) (accu: 0.9765)
[epoch : 15] (l_loss: 0.05482) (t_loss: 0.07738) (accu: 0.9761)
[epoch : 16] (l_loss: 0.05460) (t_loss: 0.07685) (accu: 0.9781)
[epoch : 17] (l_loss: 0.05409) (t_loss: 0.07817) (accu: 0.9758)
[epoch : 18] (l_loss: 0.05438) (t_loss: 0.07675) (accu: 0.9766)
[epoch : 19] (l_loss: 0.05375) (t_loss: 0.07681) (accu: 0.9766)
[epoch : 20] (l_loss: 0.05382) (t_loss: 0.07973) (accu: 0.9777)
[epoch : 21] (l_loss: 0.05382) (t_loss: 0.07999) (accu: 0.9748)
[epoch : 22] (l_loss: 0.05356) (t_loss: 0.08092) (accu: 0.9756)
[epoch : 23] (l_loss: 0.05352) (t_loss: 0.07795) (accu: 0.9759)
[epoch : 24] (l_loss: 0.05333) (t_loss: 0.07783) (accu: 0.9759)
[epoch : 25] (l_loss: 0.05347) (t_loss: 0.07686) (accu: 0.9759)
[epoch : 26] (l_loss: 0.05370) (t_loss: 0.07817) (accu: 0.9765)
[epoch : 27] (l_loss: 0.05326) (t_loss: 0.07699) (accu: 0.9778)
[epoch : 28] (l_loss: 0.05314) (t_loss: 0.07958) (accu: 0.9757)
[epoch : 29] (l_loss: 0.05377) (t_loss: 0.07981) (accu: 0.9737)
[epoch : 30] (l_loss: 0.05310) (t_loss: 0.07746) (accu: 0.9768)
[epoch : 31] (l_loss: 0.05355) (t_loss: 0.07955) (accu: 0.9752)
[epoch : 32] (l_loss: 0.05361) (t_loss: 0.07691) (accu: 0.9767)
[epoch : 33] (l_loss: 0.05323) (t_loss: 0.07808) (accu: 0.9753)
[epoch : 34] (l_loss: 0.05324) (t_loss: 0.07586) (accu: 0.9762)
[epoch : 35] (l_loss: 0.05306) (t_loss: 0.07737) (accu: 0.9768)
[epoch : 36] (l_loss: 0.05370) (t_loss: 0.07837) (accu: 0.9765)
[epoch : 37] (l_loss: 0.05303) (t_loss: 0.07810) (accu: 0.9774)
[epoch : 38] (l_loss: 0.05315) (t_loss: 0.07583) (accu: 0.9768)
[epoch : 39] (l_loss: 0.05345) (t_loss: 0.07836) (accu: 0.9762)
[epoch : 40] (l_loss: 0.05294) (t_loss: 0.07568) (accu: 0.9774)
[epoch : 41] (l_loss: 0.05304) (t_loss: 0.07547) (accu: 0.9778)
[epoch : 42] (l_loss: 0.05318) (t_loss: 0.07778) (accu: 0.9772)
[epoch : 43] (l_loss: 0.05335) (t_loss: 0.07643) (accu: 0.9772)
[epoch : 44] (l_loss: 0.05316) (t_loss: 0.07581) (accu: 0.9758)
[epoch : 45] (l_loss: 0.05303) (t_loss: 0.07954) (accu: 0.9761)
[epoch : 46] (l_loss: 0.05382) (t_loss: 0.07853) (accu: 0.9754)
[epoch : 47] (l_loss: 0.05293) (t_loss: 0.07591) (accu: 0.9763)
[epoch : 48] (l_loss: 0.05336) (t_loss: 0.07668) (accu: 0.9763)
[epoch : 49] (l_loss: 0.05313) (t_loss: 0.07751) (accu: 0.9763)
[epoch : 50] (l_loss: 0.05324) (t_loss: 0.08019) (accu: 0.9764)
Finish! (Best accu: 0.9781) (Time taken(sec) : 770.01) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (11997 | 203503)          5.57
fc1.weight   :      196000 (10775 | 185225)          5.50
fc2.weight   :        18750 (1031 | 17719)           5.50
fcout.weight :          750 (191 | 559)             25.47
------------------------------------------------------------
[Prune_iter : (14/21), Remaining weight : 5.57 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30289) (accu: 0.1227)
[epoch : 1] (l_loss: 0.45941) (t_loss: 0.17074) (accu: 0.9514)
[epoch : 2] (l_loss: 0.13647) (t_loss: 0.11900) (accu: 0.9651)
[epoch : 3] (l_loss: 0.10287) (t_loss: 0.10089) (accu: 0.9696)
[epoch : 4] (l_loss: 0.08582) (t_loss: 0.09367) (accu: 0.9717)
[epoch : 5] (l_loss: 0.07581) (t_loss: 0.08852) (accu: 0.9728)
[epoch : 6] (l_loss: 0.06890) (t_loss: 0.08400) (accu: 0.9746)
[epoch : 7] (l_loss: 0.06362) (t_loss: 0.08254) (accu: 0.9746)
[epoch : 8] (l_loss: 0.06029) (t_loss: 0.08326) (accu: 0.9747)
[epoch : 9] (l_loss: 0.05817) (t_loss: 0.07991) (accu: 0.9770)
[epoch : 10] (l_loss: 0.05644) (t_loss: 0.07689) (accu: 0.9772)
[epoch : 11] (l_loss: 0.05480) (t_loss: 0.07870) (accu: 0.9754)
[epoch : 12] (l_loss: 0.05470) (t_loss: 0.08091) (accu: 0.9753)
[epoch : 13] (l_loss: 0.05354) (t_loss: 0.07761) (accu: 0.9775)
[epoch : 14] (l_loss: 0.05344) (t_loss: 0.07665) (accu: 0.9770)
[epoch : 15] (l_loss: 0.05303) (t_loss: 0.07684) (accu: 0.9770)
[epoch : 16] (l_loss: 0.05292) (t_loss: 0.07837) (accu: 0.9762)
[epoch : 17] (l_loss: 0.05249) (t_loss: 0.07844) (accu: 0.9770)
[epoch : 18] (l_loss: 0.05218) (t_loss: 0.07711) (accu: 0.9770)
[epoch : 19] (l_loss: 0.05214) (t_loss: 0.07579) (accu: 0.9767)
[epoch : 20] (l_loss: 0.05230) (t_loss: 0.07574) (accu: 0.9771)
[epoch : 21] (l_loss: 0.05184) (t_loss: 0.07991) (accu: 0.9759)
[epoch : 22] (l_loss: 0.05206) (t_loss: 0.07684) (accu: 0.9759)
[epoch : 23] (l_loss: 0.05205) (t_loss: 0.07769) (accu: 0.9770)
[epoch : 24] (l_loss: 0.05211) (t_loss: 0.07541) (accu: 0.9762)
[epoch : 25] (l_loss: 0.05154) (t_loss: 0.07714) (accu: 0.9776)
[epoch : 26] (l_loss: 0.05166) (t_loss: 0.07695) (accu: 0.9770)
[epoch : 27] (l_loss: 0.05186) (t_loss: 0.07478) (accu: 0.9772)
[epoch : 28] (l_loss: 0.05147) (t_loss: 0.07658) (accu: 0.9760)
[epoch : 29] (l_loss: 0.05177) (t_loss: 0.07742) (accu: 0.9774)
[epoch : 30] (l_loss: 0.05136) (t_loss: 0.07588) (accu: 0.9776)
[epoch : 31] (l_loss: 0.05194) (t_loss: 0.07585) (accu: 0.9772)
[epoch : 32] (l_loss: 0.05180) (t_loss: 0.07727) (accu: 0.9763)
[epoch : 33] (l_loss: 0.05164) (t_loss: 0.07551) (accu: 0.9762)
[epoch : 34] (l_loss: 0.05111) (t_loss: 0.07806) (accu: 0.9769)
[epoch : 35] (l_loss: 0.05160) (t_loss: 0.07740) (accu: 0.9753)
[epoch : 36] (l_loss: 0.05173) (t_loss: 0.07911) (accu: 0.9764)
[epoch : 37] (l_loss: 0.05144) (t_loss: 0.07771) (accu: 0.9757)
[epoch : 38] (l_loss: 0.05113) (t_loss: 0.08065) (accu: 0.9759)
[epoch : 39] (l_loss: 0.05163) (t_loss: 0.07709) (accu: 0.9758)
[epoch : 40] (l_loss: 0.05154) (t_loss: 0.07608) (accu: 0.9770)
[epoch : 41] (l_loss: 0.05143) (t_loss: 0.07868) (accu: 0.9755)
[epoch : 42] (l_loss: 0.05169) (t_loss: 0.07949) (accu: 0.9760)
[epoch : 43] (l_loss: 0.05169) (t_loss: 0.07596) (accu: 0.9763)
[epoch : 44] (l_loss: 0.05145) (t_loss: 0.07635) (accu: 0.9769)
[epoch : 45] (l_loss: 0.05186) (t_loss: 0.07956) (accu: 0.9763)
[epoch : 46] (l_loss: 0.05161) (t_loss: 0.07786) (accu: 0.9769)
[epoch : 47] (l_loss: 0.05181) (t_loss: 0.07824) (accu: 0.9758)
[epoch : 48] (l_loss: 0.05111) (t_loss: 0.07676) (accu: 0.9762)
[epoch : 49] (l_loss: 0.05138) (t_loss: 0.07662) (accu: 0.9763)
[epoch : 50] (l_loss: 0.05180) (t_loss: 0.07786) (accu: 0.9760)
Finish! (Best accu: 0.9776) (Time taken(sec) : 783.95) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (9617 | 205883)          4.46
fc1.weight   :       196000 (8620 | 187380)          4.40
fc2.weight   :        18750 (825 | 17925)            4.40
fcout.weight :          750 (172 | 578)             22.93
------------------------------------------------------------
[Prune_iter : (15/21), Remaining weight : 4.46 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30252) (accu: 0.1447)
[epoch : 1] (l_loss: 0.45306) (t_loss: 0.16296) (accu: 0.9498)
[epoch : 2] (l_loss: 0.13775) (t_loss: 0.12323) (accu: 0.9632)
[epoch : 3] (l_loss: 0.10528) (t_loss: 0.10557) (accu: 0.9679)
[epoch : 4] (l_loss: 0.09006) (t_loss: 0.09705) (accu: 0.9706)
[epoch : 5] (l_loss: 0.08031) (t_loss: 0.08967) (accu: 0.9719)
[epoch : 6] (l_loss: 0.07315) (t_loss: 0.08947) (accu: 0.9725)
[epoch : 7] (l_loss: 0.06918) (t_loss: 0.08734) (accu: 0.9740)
[epoch : 8] (l_loss: 0.06592) (t_loss: 0.08605) (accu: 0.9736)
[epoch : 9] (l_loss: 0.06351) (t_loss: 0.08291) (accu: 0.9744)
[epoch : 10] (l_loss: 0.06199) (t_loss: 0.08171) (accu: 0.9742)
[epoch : 11] (l_loss: 0.06089) (t_loss: 0.08390) (accu: 0.9738)
[epoch : 12] (l_loss: 0.05967) (t_loss: 0.08063) (accu: 0.9744)
[epoch : 13] (l_loss: 0.05935) (t_loss: 0.08155) (accu: 0.9754)
[epoch : 14] (l_loss: 0.05837) (t_loss: 0.08123) (accu: 0.9748)
[epoch : 15] (l_loss: 0.05792) (t_loss: 0.07996) (accu: 0.9753)
[epoch : 16] (l_loss: 0.05745) (t_loss: 0.08168) (accu: 0.9754)
[epoch : 17] (l_loss: 0.05736) (t_loss: 0.08223) (accu: 0.9747)
[epoch : 18] (l_loss: 0.05724) (t_loss: 0.08367) (accu: 0.9739)
[epoch : 19] (l_loss: 0.05665) (t_loss: 0.08166) (accu: 0.9741)
[epoch : 20] (l_loss: 0.05631) (t_loss: 0.08358) (accu: 0.9754)
[epoch : 21] (l_loss: 0.05681) (t_loss: 0.07947) (accu: 0.9765)
[epoch : 22] (l_loss: 0.05610) (t_loss: 0.07880) (accu: 0.9750)
[epoch : 23] (l_loss: 0.05605) (t_loss: 0.07899) (accu: 0.9763)
[epoch : 24] (l_loss: 0.05608) (t_loss: 0.08028) (accu: 0.9761)
[epoch : 25] (l_loss: 0.05624) (t_loss: 0.07857) (accu: 0.9761)
[epoch : 26] (l_loss: 0.05583) (t_loss: 0.08074) (accu: 0.9740)
[epoch : 27] (l_loss: 0.05566) (t_loss: 0.07923) (accu: 0.9758)
[epoch : 28] (l_loss: 0.05572) (t_loss: 0.08137) (accu: 0.9734)
[epoch : 29] (l_loss: 0.05593) (t_loss: 0.07995) (accu: 0.9754)
[epoch : 30] (l_loss: 0.05585) (t_loss: 0.08073) (accu: 0.9747)
[epoch : 31] (l_loss: 0.05592) (t_loss: 0.07862) (accu: 0.9746)
[epoch : 32] (l_loss: 0.05616) (t_loss: 0.07993) (accu: 0.9752)
[epoch : 33] (l_loss: 0.05617) (t_loss: 0.07978) (accu: 0.9745)
[epoch : 34] (l_loss: 0.05558) (t_loss: 0.07915) (accu: 0.9749)
[epoch : 35] (l_loss: 0.05583) (t_loss: 0.07855) (accu: 0.9763)
[epoch : 36] (l_loss: 0.05574) (t_loss: 0.07790) (accu: 0.9761)
[epoch : 37] (l_loss: 0.05579) (t_loss: 0.07955) (accu: 0.9749)
[epoch : 38] (l_loss: 0.05574) (t_loss: 0.07853) (accu: 0.9768)
[epoch : 39] (l_loss: 0.05588) (t_loss: 0.07819) (accu: 0.9753)
[epoch : 40] (l_loss: 0.05537) (t_loss: 0.07722) (accu: 0.9758)
[epoch : 41] (l_loss: 0.05568) (t_loss: 0.08197) (accu: 0.9736)
[epoch : 42] (l_loss: 0.05581) (t_loss: 0.07894) (accu: 0.9757)
[epoch : 43] (l_loss: 0.05520) (t_loss: 0.07992) (accu: 0.9744)
[epoch : 44] (l_loss: 0.05558) (t_loss: 0.07806) (accu: 0.9753)
[epoch : 45] (l_loss: 0.05540) (t_loss: 0.08095) (accu: 0.9751)
[epoch : 46] (l_loss: 0.05586) (t_loss: 0.08202) (accu: 0.9745)
[epoch : 47] (l_loss: 0.05561) (t_loss: 0.08034) (accu: 0.9750)
[epoch : 48] (l_loss: 0.05548) (t_loss: 0.07856) (accu: 0.9762)
[epoch : 49] (l_loss: 0.05578) (t_loss: 0.07606) (accu: 0.9759)
[epoch : 50] (l_loss: 0.05523) (t_loss: 0.08018) (accu: 0.9752)
Finish! (Best accu: 0.9768) (Time taken(sec) : 771.44) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (7710 | 207790)          3.58
fc1.weight   :       196000 (6896 | 189104)          3.52
fc2.weight   :        18750 (660 | 18090)            3.52
fcout.weight :          750 (154 | 596)             20.53
------------------------------------------------------------
[Prune_iter : (16/21), Remaining weight : 3.58 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30143) (accu: 0.0880)
[epoch : 1] (l_loss: 0.45061) (t_loss: 0.16076) (accu: 0.9550)
[epoch : 2] (l_loss: 0.13298) (t_loss: 0.11771) (accu: 0.9648)
[epoch : 3] (l_loss: 0.10267) (t_loss: 0.10850) (accu: 0.9674)
[epoch : 4] (l_loss: 0.08822) (t_loss: 0.09531) (accu: 0.9712)
[epoch : 5] (l_loss: 0.07855) (t_loss: 0.09277) (accu: 0.9721)
[epoch : 6] (l_loss: 0.07155) (t_loss: 0.09020) (accu: 0.9715)
[epoch : 7] (l_loss: 0.06709) (t_loss: 0.08535) (accu: 0.9741)
[epoch : 8] (l_loss: 0.06250) (t_loss: 0.08162) (accu: 0.9765)
[epoch : 9] (l_loss: 0.05939) (t_loss: 0.08146) (accu: 0.9756)
[epoch : 10] (l_loss: 0.05719) (t_loss: 0.08009) (accu: 0.9752)
[epoch : 11] (l_loss: 0.05609) (t_loss: 0.07744) (accu: 0.9763)
[epoch : 12] (l_loss: 0.05442) (t_loss: 0.07884) (accu: 0.9759)
[epoch : 13] (l_loss: 0.05424) (t_loss: 0.08136) (accu: 0.9740)
[epoch : 14] (l_loss: 0.05356) (t_loss: 0.07948) (accu: 0.9763)
[epoch : 15] (l_loss: 0.05338) (t_loss: 0.07725) (accu: 0.9771)
[epoch : 16] (l_loss: 0.05283) (t_loss: 0.07853) (accu: 0.9759)
[epoch : 17] (l_loss: 0.05260) (t_loss: 0.07711) (accu: 0.9763)
[epoch : 18] (l_loss: 0.05268) (t_loss: 0.07698) (accu: 0.9765)
[epoch : 19] (l_loss: 0.05241) (t_loss: 0.07700) (accu: 0.9788)
[epoch : 20] (l_loss: 0.05245) (t_loss: 0.07533) (accu: 0.9776)
[epoch : 21] (l_loss: 0.05187) (t_loss: 0.07778) (accu: 0.9764)
[epoch : 22] (l_loss: 0.05147) (t_loss: 0.07935) (accu: 0.9751)
[epoch : 23] (l_loss: 0.05181) (t_loss: 0.07597) (accu: 0.9780)
[epoch : 24] (l_loss: 0.05194) (t_loss: 0.07627) (accu: 0.9767)
[epoch : 25] (l_loss: 0.05173) (t_loss: 0.07499) (accu: 0.9787)
[epoch : 26] (l_loss: 0.05159) (t_loss: 0.07766) (accu: 0.9769)
[epoch : 27] (l_loss: 0.05165) (t_loss: 0.07940) (accu: 0.9758)
[epoch : 28] (l_loss: 0.05182) (t_loss: 0.07961) (accu: 0.9753)
[epoch : 29] (l_loss: 0.05174) (t_loss: 0.07685) (accu: 0.9767)
[epoch : 30] (l_loss: 0.05138) (t_loss: 0.07856) (accu: 0.9750)
[epoch : 31] (l_loss: 0.05197) (t_loss: 0.07712) (accu: 0.9769)
[epoch : 32] (l_loss: 0.05168) (t_loss: 0.07617) (accu: 0.9764)
[epoch : 33] (l_loss: 0.05172) (t_loss: 0.07712) (accu: 0.9737)
[epoch : 34] (l_loss: 0.05210) (t_loss: 0.07753) (accu: 0.9764)
[epoch : 35] (l_loss: 0.05171) (t_loss: 0.07503) (accu: 0.9758)
[epoch : 36] (l_loss: 0.05141) (t_loss: 0.07748) (accu: 0.9768)
[epoch : 37] (l_loss: 0.05125) (t_loss: 0.07764) (accu: 0.9766)
[epoch : 38] (l_loss: 0.05130) (t_loss: 0.07745) (accu: 0.9765)
[epoch : 39] (l_loss: 0.05165) (t_loss: 0.07738) (accu: 0.9765)
[epoch : 40] (l_loss: 0.05160) (t_loss: 0.07735) (accu: 0.9756)
[epoch : 41] (l_loss: 0.05187) (t_loss: 0.07879) (accu: 0.9769)
[epoch : 42] (l_loss: 0.05163) (t_loss: 0.07842) (accu: 0.9750)
[epoch : 43] (l_loss: 0.05136) (t_loss: 0.07705) (accu: 0.9775)
[epoch : 44] (l_loss: 0.05182) (t_loss: 0.07908) (accu: 0.9769)
[epoch : 45] (l_loss: 0.05123) (t_loss: 0.07814) (accu: 0.9767)
[epoch : 46] (l_loss: 0.05177) (t_loss: 0.08024) (accu: 0.9752)
[epoch : 47] (l_loss: 0.05172) (t_loss: 0.07721) (accu: 0.9761)
[epoch : 48] (l_loss: 0.05183) (t_loss: 0.07420) (accu: 0.9762)
[epoch : 49] (l_loss: 0.05145) (t_loss: 0.07654) (accu: 0.9764)
[epoch : 50] (l_loss: 0.05162) (t_loss: 0.07829) (accu: 0.9775)
Finish! (Best accu: 0.9788) (Time taken(sec) : 755.16) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (6184 | 209316)          2.87
fc1.weight   :       196000 (5517 | 190483)          2.81
fc2.weight   :        18750 (528 | 18222)            2.82
fcout.weight :          750 (139 | 611)             18.53
------------------------------------------------------------
[Prune_iter : (17/21), Remaining weight : 2.87 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29973) (accu: 0.1197)
[epoch : 1] (l_loss: 0.45458) (t_loss: 0.16156) (accu: 0.9532)
[epoch : 2] (l_loss: 0.13878) (t_loss: 0.12097) (accu: 0.9639)
[epoch : 3] (l_loss: 0.10658) (t_loss: 0.10474) (accu: 0.9694)
[epoch : 4] (l_loss: 0.08957) (t_loss: 0.09565) (accu: 0.9709)
[epoch : 5] (l_loss: 0.08019) (t_loss: 0.09464) (accu: 0.9712)
[epoch : 6] (l_loss: 0.07375) (t_loss: 0.08938) (accu: 0.9741)
[epoch : 7] (l_loss: 0.06920) (t_loss: 0.08524) (accu: 0.9735)
[epoch : 8] (l_loss: 0.06583) (t_loss: 0.08318) (accu: 0.9749)
[epoch : 9] (l_loss: 0.06350) (t_loss: 0.08210) (accu: 0.9750)
[epoch : 10] (l_loss: 0.06192) (t_loss: 0.08592) (accu: 0.9732)
[epoch : 11] (l_loss: 0.06073) (t_loss: 0.08240) (accu: 0.9753)
[epoch : 12] (l_loss: 0.05931) (t_loss: 0.08292) (accu: 0.9741)
[epoch : 13] (l_loss: 0.05903) (t_loss: 0.07917) (accu: 0.9749)
[epoch : 14] (l_loss: 0.05797) (t_loss: 0.08096) (accu: 0.9745)
[epoch : 15] (l_loss: 0.05788) (t_loss: 0.08005) (accu: 0.9755)
[epoch : 16] (l_loss: 0.05719) (t_loss: 0.08162) (accu: 0.9754)
[epoch : 17] (l_loss: 0.05726) (t_loss: 0.07978) (accu: 0.9749)
[epoch : 18] (l_loss: 0.05654) (t_loss: 0.07823) (accu: 0.9761)
[epoch : 19] (l_loss: 0.05676) (t_loss: 0.08089) (accu: 0.9744)
[epoch : 20] (l_loss: 0.05684) (t_loss: 0.07799) (accu: 0.9758)
[epoch : 21] (l_loss: 0.05656) (t_loss: 0.07971) (accu: 0.9750)
[epoch : 22] (l_loss: 0.05657) (t_loss: 0.07977) (accu: 0.9755)
[epoch : 23] (l_loss: 0.05604) (t_loss: 0.08089) (accu: 0.9752)
[epoch : 24] (l_loss: 0.05624) (t_loss: 0.07982) (accu: 0.9768)
[epoch : 25] (l_loss: 0.05598) (t_loss: 0.08038) (accu: 0.9744)
[epoch : 26] (l_loss: 0.05600) (t_loss: 0.07713) (accu: 0.9759)
[epoch : 27] (l_loss: 0.05595) (t_loss: 0.08044) (accu: 0.9754)
[epoch : 28] (l_loss: 0.05590) (t_loss: 0.07923) (accu: 0.9756)
[epoch : 29] (l_loss: 0.05581) (t_loss: 0.07849) (accu: 0.9765)
[epoch : 30] (l_loss: 0.05554) (t_loss: 0.07736) (accu: 0.9751)
[epoch : 31] (l_loss: 0.05580) (t_loss: 0.07865) (accu: 0.9754)
[epoch : 32] (l_loss: 0.05575) (t_loss: 0.07989) (accu: 0.9755)
[epoch : 33] (l_loss: 0.05600) (t_loss: 0.08074) (accu: 0.9759)
[epoch : 34] (l_loss: 0.05558) (t_loss: 0.08182) (accu: 0.9748)
[epoch : 35] (l_loss: 0.05530) (t_loss: 0.07893) (accu: 0.9742)
[epoch : 36] (l_loss: 0.05596) (t_loss: 0.07890) (accu: 0.9751)
[epoch : 37] (l_loss: 0.05548) (t_loss: 0.07840) (accu: 0.9761)
[epoch : 38] (l_loss: 0.05574) (t_loss: 0.07681) (accu: 0.9760)
[epoch : 39] (l_loss: 0.05577) (t_loss: 0.07690) (accu: 0.9767)
[epoch : 40] (l_loss: 0.05579) (t_loss: 0.08002) (accu: 0.9744)
[epoch : 41] (l_loss: 0.05525) (t_loss: 0.07994) (accu: 0.9756)
[epoch : 42] (l_loss: 0.05580) (t_loss: 0.08053) (accu: 0.9756)
[epoch : 43] (l_loss: 0.05569) (t_loss: 0.07811) (accu: 0.9763)
[epoch : 44] (l_loss: 0.05561) (t_loss: 0.07971) (accu: 0.9748)
[epoch : 45] (l_loss: 0.05556) (t_loss: 0.08104) (accu: 0.9751)
[epoch : 46] (l_loss: 0.05567) (t_loss: 0.08406) (accu: 0.9734)
[epoch : 47] (l_loss: 0.05543) (t_loss: 0.07849) (accu: 0.9764)
[epoch : 48] (l_loss: 0.05598) (t_loss: 0.08192) (accu: 0.9733)
[epoch : 49] (l_loss: 0.05562) (t_loss: 0.07995) (accu: 0.9758)
[epoch : 50] (l_loss: 0.05566) (t_loss: 0.08161) (accu: 0.9744)
Finish! (Best accu: 0.9768) (Time taken(sec) : 773.34) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (4961 | 210539)          2.30
fc1.weight   :       196000 (4414 | 191586)          2.25
fc2.weight   :        18750 (422 | 18328)            2.25
fcout.weight :          750 (125 | 625)             16.67
------------------------------------------------------------
[Prune_iter : (18/21), Remaining weight : 2.3 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29749) (accu: 0.0941)
[epoch : 1] (l_loss: 0.44843) (t_loss: 0.15966) (accu: 0.9544)
[epoch : 2] (l_loss: 0.13303) (t_loss: 0.11969) (accu: 0.9644)
[epoch : 3] (l_loss: 0.10084) (t_loss: 0.10189) (accu: 0.9697)
[epoch : 4] (l_loss: 0.08578) (t_loss: 0.09533) (accu: 0.9718)
[epoch : 5] (l_loss: 0.07623) (t_loss: 0.09197) (accu: 0.9714)
[epoch : 6] (l_loss: 0.07040) (t_loss: 0.08620) (accu: 0.9732)
[epoch : 7] (l_loss: 0.06658) (t_loss: 0.08632) (accu: 0.9732)
[epoch : 8] (l_loss: 0.06374) (t_loss: 0.08407) (accu: 0.9741)
[epoch : 9] (l_loss: 0.06123) (t_loss: 0.08210) (accu: 0.9751)
[epoch : 10] (l_loss: 0.05975) (t_loss: 0.08059) (accu: 0.9742)
[epoch : 11] (l_loss: 0.05864) (t_loss: 0.07994) (accu: 0.9763)
[epoch : 12] (l_loss: 0.05821) (t_loss: 0.08118) (accu: 0.9749)
[epoch : 13] (l_loss: 0.05734) (t_loss: 0.08158) (accu: 0.9744)
[epoch : 14] (l_loss: 0.05687) (t_loss: 0.08400) (accu: 0.9740)
[epoch : 15] (l_loss: 0.05586) (t_loss: 0.08187) (accu: 0.9734)
[epoch : 16] (l_loss: 0.05613) (t_loss: 0.08168) (accu: 0.9741)
[epoch : 17] (l_loss: 0.05583) (t_loss: 0.08038) (accu: 0.9753)
[epoch : 18] (l_loss: 0.05595) (t_loss: 0.07916) (accu: 0.9751)
[epoch : 19] (l_loss: 0.05563) (t_loss: 0.07977) (accu: 0.9753)
[epoch : 20] (l_loss: 0.05547) (t_loss: 0.07993) (accu: 0.9751)
[epoch : 21] (l_loss: 0.05538) (t_loss: 0.07935) (accu: 0.9773)
[epoch : 22] (l_loss: 0.05513) (t_loss: 0.08016) (accu: 0.9755)
[epoch : 23] (l_loss: 0.05498) (t_loss: 0.08060) (accu: 0.9739)
[epoch : 24] (l_loss: 0.05529) (t_loss: 0.07920) (accu: 0.9752)
[epoch : 25] (l_loss: 0.05539) (t_loss: 0.07971) (accu: 0.9757)
[epoch : 26] (l_loss: 0.05517) (t_loss: 0.08010) (accu: 0.9756)
[epoch : 27] (l_loss: 0.05488) (t_loss: 0.07901) (accu: 0.9744)
[epoch : 28] (l_loss: 0.05503) (t_loss: 0.07911) (accu: 0.9746)
[epoch : 29] (l_loss: 0.05485) (t_loss: 0.07821) (accu: 0.9752)
[epoch : 30] (l_loss: 0.05475) (t_loss: 0.07938) (accu: 0.9753)
[epoch : 31] (l_loss: 0.05466) (t_loss: 0.07854) (accu: 0.9754)
[epoch : 32] (l_loss: 0.05498) (t_loss: 0.07862) (accu: 0.9757)
[epoch : 33] (l_loss: 0.05472) (t_loss: 0.07749) (accu: 0.9759)
[epoch : 34] (l_loss: 0.05462) (t_loss: 0.08043) (accu: 0.9760)
[epoch : 35] (l_loss: 0.05439) (t_loss: 0.07733) (accu: 0.9752)
[epoch : 36] (l_loss: 0.05454) (t_loss: 0.07714) (accu: 0.9751)
[epoch : 37] (l_loss: 0.05481) (t_loss: 0.07895) (accu: 0.9745)
[epoch : 38] (l_loss: 0.05472) (t_loss: 0.07617) (accu: 0.9762)
[epoch : 39] (l_loss: 0.05471) (t_loss: 0.07947) (accu: 0.9753)
[epoch : 40] (l_loss: 0.05475) (t_loss: 0.07940) (accu: 0.9754)
[epoch : 41] (l_loss: 0.05465) (t_loss: 0.07673) (accu: 0.9754)
[epoch : 42] (l_loss: 0.05445) (t_loss: 0.07731) (accu: 0.9759)
[epoch : 43] (l_loss: 0.05438) (t_loss: 0.07938) (accu: 0.9756)
[epoch : 44] (l_loss: 0.05450) (t_loss: 0.07830) (accu: 0.9757)
[epoch : 45] (l_loss: 0.05473) (t_loss: 0.07888) (accu: 0.9753)
[epoch : 46] (l_loss: 0.05420) (t_loss: 0.08232) (accu: 0.9737)
[epoch : 47] (l_loss: 0.05446) (t_loss: 0.07927) (accu: 0.9756)
[epoch : 48] (l_loss: 0.05478) (t_loss: 0.07625) (accu: 0.9771)
[epoch : 49] (l_loss: 0.05425) (t_loss: 0.07947) (accu: 0.9752)
[epoch : 50] (l_loss: 0.05440) (t_loss: 0.08350) (accu: 0.9744)
Finish! (Best accu: 0.9773) (Time taken(sec) : 765.97) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (3982 | 211518)          1.85
fc1.weight   :       196000 (3531 | 192469)          1.80
fc2.weight   :        18750 (338 | 18412)            1.80
fcout.weight :          750 (113 | 637)             15.07
------------------------------------------------------------
[Prune_iter : (19/21), Remaining weight : 1.85 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29864) (accu: 0.0987)
[epoch : 1] (l_loss: 0.44499) (t_loss: 0.15951) (accu: 0.9566)
[epoch : 2] (l_loss: 0.13215) (t_loss: 0.11789) (accu: 0.9660)
[epoch : 3] (l_loss: 0.10066) (t_loss: 0.10333) (accu: 0.9697)
[epoch : 4] (l_loss: 0.08563) (t_loss: 0.09348) (accu: 0.9722)
[epoch : 5] (l_loss: 0.07654) (t_loss: 0.09257) (accu: 0.9716)
[epoch : 6] (l_loss: 0.07124) (t_loss: 0.08868) (accu: 0.9733)
[epoch : 7] (l_loss: 0.06720) (t_loss: 0.08766) (accu: 0.9719)
[epoch : 8] (l_loss: 0.06439) (t_loss: 0.08504) (accu: 0.9746)
[epoch : 9] (l_loss: 0.06236) (t_loss: 0.08747) (accu: 0.9728)
[epoch : 10] (l_loss: 0.06084) (t_loss: 0.08274) (accu: 0.9739)
[epoch : 11] (l_loss: 0.05956) (t_loss: 0.08302) (accu: 0.9747)
[epoch : 12] (l_loss: 0.05901) (t_loss: 0.08222) (accu: 0.9757)
[epoch : 13] (l_loss: 0.05832) (t_loss: 0.08220) (accu: 0.9741)
[epoch : 14] (l_loss: 0.05791) (t_loss: 0.08109) (accu: 0.9754)
[epoch : 15] (l_loss: 0.05768) (t_loss: 0.08346) (accu: 0.9743)
[epoch : 16] (l_loss: 0.05715) (t_loss: 0.08340) (accu: 0.9737)
[epoch : 17] (l_loss: 0.05651) (t_loss: 0.07967) (accu: 0.9759)
[epoch : 18] (l_loss: 0.05661) (t_loss: 0.08417) (accu: 0.9743)
[epoch : 19] (l_loss: 0.05637) (t_loss: 0.08130) (accu: 0.9734)
[epoch : 20] (l_loss: 0.05606) (t_loss: 0.07767) (accu: 0.9761)
[epoch : 21] (l_loss: 0.05533) (t_loss: 0.08024) (accu: 0.9744)
[epoch : 22] (l_loss: 0.05534) (t_loss: 0.07989) (accu: 0.9755)
[epoch : 23] (l_loss: 0.05548) (t_loss: 0.07982) (accu: 0.9747)
[epoch : 24] (l_loss: 0.05530) (t_loss: 0.07837) (accu: 0.9754)
[epoch : 25] (l_loss: 0.05491) (t_loss: 0.07682) (accu: 0.9759)
[epoch : 26] (l_loss: 0.05514) (t_loss: 0.08227) (accu: 0.9735)
[epoch : 27] (l_loss: 0.05536) (t_loss: 0.07925) (accu: 0.9746)
[epoch : 28] (l_loss: 0.05479) (t_loss: 0.07771) (accu: 0.9763)
[epoch : 29] (l_loss: 0.05503) (t_loss: 0.07833) (accu: 0.9749)
[epoch : 30] (l_loss: 0.05498) (t_loss: 0.07996) (accu: 0.9748)
[epoch : 31] (l_loss: 0.05497) (t_loss: 0.07889) (accu: 0.9758)
[epoch : 32] (l_loss: 0.05512) (t_loss: 0.07952) (accu: 0.9752)
[epoch : 33] (l_loss: 0.05479) (t_loss: 0.07643) (accu: 0.9770)
[epoch : 34] (l_loss: 0.05480) (t_loss: 0.07944) (accu: 0.9761)
[epoch : 35] (l_loss: 0.05541) (t_loss: 0.07923) (accu: 0.9746)
[epoch : 36] (l_loss: 0.05500) (t_loss: 0.07754) (accu: 0.9756)
[epoch : 37] (l_loss: 0.05452) (t_loss: 0.08420) (accu: 0.9741)
[epoch : 38] (l_loss: 0.05449) (t_loss: 0.07692) (accu: 0.9761)
[epoch : 39] (l_loss: 0.05407) (t_loss: 0.07834) (accu: 0.9759)
[epoch : 40] (l_loss: 0.05474) (t_loss: 0.07778) (accu: 0.9765)
[epoch : 41] (l_loss: 0.05471) (t_loss: 0.07557) (accu: 0.9760)
[epoch : 42] (l_loss: 0.05468) (t_loss: 0.07858) (accu: 0.9753)
[epoch : 43] (l_loss: 0.05457) (t_loss: 0.07740) (accu: 0.9755)
[epoch : 44] (l_loss: 0.05482) (t_loss: 0.07923) (accu: 0.9752)
[epoch : 45] (l_loss: 0.05436) (t_loss: 0.07794) (accu: 0.9757)
[epoch : 46] (l_loss: 0.05431) (t_loss: 0.07593) (accu: 0.9757)
[epoch : 47] (l_loss: 0.05446) (t_loss: 0.07619) (accu: 0.9756)
[epoch : 48] (l_loss: 0.05505) (t_loss: 0.08137) (accu: 0.9731)
[epoch : 49] (l_loss: 0.05416) (t_loss: 0.07918) (accu: 0.9758)
[epoch : 50] (l_loss: 0.05447) (t_loss: 0.07968) (accu: 0.9752)
Finish! (Best accu: 0.9770) (Time taken(sec) : 762.40) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (3196 | 212304)          1.48
fc1.weight   :       196000 (2825 | 193175)          1.44
fc2.weight   :        18750 (270 | 18480)            1.44
fcout.weight :          750 (101 | 649)             13.47
------------------------------------------------------------
[Prune_iter : (20/21), Remaining weight : 1.48 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30016) (accu: 0.0963)
[epoch : 1] (l_loss: 0.45249) (t_loss: 0.16225) (accu: 0.9549)
[epoch : 2] (l_loss: 0.13591) (t_loss: 0.11683) (accu: 0.9654)
[epoch : 3] (l_loss: 0.10314) (t_loss: 0.10234) (accu: 0.9685)
[epoch : 4] (l_loss: 0.08762) (t_loss: 0.09522) (accu: 0.9719)
[epoch : 5] (l_loss: 0.07765) (t_loss: 0.08915) (accu: 0.9734)
[epoch : 6] (l_loss: 0.07169) (t_loss: 0.09017) (accu: 0.9727)
[epoch : 7] (l_loss: 0.06732) (t_loss: 0.08557) (accu: 0.9740)
[epoch : 8] (l_loss: 0.06470) (t_loss: 0.08087) (accu: 0.9749)
[epoch : 9] (l_loss: 0.06308) (t_loss: 0.08277) (accu: 0.9742)
[epoch : 10] (l_loss: 0.06177) (t_loss: 0.08080) (accu: 0.9747)
[epoch : 11] (l_loss: 0.06057) (t_loss: 0.08045) (accu: 0.9750)
[epoch : 12] (l_loss: 0.05963) (t_loss: 0.08298) (accu: 0.9734)
[epoch : 13] (l_loss: 0.05932) (t_loss: 0.08025) (accu: 0.9762)
[epoch : 14] (l_loss: 0.05789) (t_loss: 0.07839) (accu: 0.9756)
[epoch : 15] (l_loss: 0.05812) (t_loss: 0.07894) (accu: 0.9750)
[epoch : 16] (l_loss: 0.05787) (t_loss: 0.08151) (accu: 0.9748)
[epoch : 17] (l_loss: 0.05719) (t_loss: 0.07953) (accu: 0.9766)
[epoch : 18] (l_loss: 0.05737) (t_loss: 0.07917) (accu: 0.9756)
[epoch : 19] (l_loss: 0.05731) (t_loss: 0.08416) (accu: 0.9739)
[epoch : 20] (l_loss: 0.05701) (t_loss: 0.07825) (accu: 0.9768)
[epoch : 21] (l_loss: 0.05683) (t_loss: 0.07948) (accu: 0.9755)
[epoch : 22] (l_loss: 0.05672) (t_loss: 0.08410) (accu: 0.9734)
[epoch : 23] (l_loss: 0.05678) (t_loss: 0.07799) (accu: 0.9765)
[epoch : 24] (l_loss: 0.05684) (t_loss: 0.08487) (accu: 0.9723)
[epoch : 25] (l_loss: 0.05646) (t_loss: 0.07868) (accu: 0.9751)
[epoch : 26] (l_loss: 0.05665) (t_loss: 0.07980) (accu: 0.9744)
[epoch : 27] (l_loss: 0.05659) (t_loss: 0.08025) (accu: 0.9757)
[epoch : 28] (l_loss: 0.05671) (t_loss: 0.07909) (accu: 0.9743)
[epoch : 29] (l_loss: 0.05663) (t_loss: 0.07942) (accu: 0.9743)
[epoch : 30] (l_loss: 0.05635) (t_loss: 0.07993) (accu: 0.9754)
[epoch : 31] (l_loss: 0.05657) (t_loss: 0.08107) (accu: 0.9741)
[epoch : 32] (l_loss: 0.05631) (t_loss: 0.08139) (accu: 0.9755)
[epoch : 33] (l_loss: 0.05657) (t_loss: 0.08079) (accu: 0.9745)
[epoch : 34] (l_loss: 0.05640) (t_loss: 0.07932) (accu: 0.9750)
[epoch : 35] (l_loss: 0.05633) (t_loss: 0.08104) (accu: 0.9751)
[epoch : 36] (l_loss: 0.05604) (t_loss: 0.07902) (accu: 0.9754)
[epoch : 37] (l_loss: 0.05646) (t_loss: 0.07796) (accu: 0.9767)
[epoch : 38] (l_loss: 0.05614) (t_loss: 0.08136) (accu: 0.9739)
[epoch : 39] (l_loss: 0.05662) (t_loss: 0.07688) (accu: 0.9757)
[epoch : 40] (l_loss: 0.05614) (t_loss: 0.08003) (accu: 0.9746)
[epoch : 41] (l_loss: 0.05624) (t_loss: 0.08087) (accu: 0.9739)
[epoch : 42] (l_loss: 0.05639) (t_loss: 0.07954) (accu: 0.9740)
[epoch : 43] (l_loss: 0.05645) (t_loss: 0.07776) (accu: 0.9754)
[epoch : 44] (l_loss: 0.05640) (t_loss: 0.08029) (accu: 0.9744)
[epoch : 45] (l_loss: 0.05610) (t_loss: 0.07880) (accu: 0.9759)
[epoch : 46] (l_loss: 0.05608) (t_loss: 0.07753) (accu: 0.9750)
[epoch : 47] (l_loss: 0.05639) (t_loss: 0.07865) (accu: 0.9758)
[epoch : 48] (l_loss: 0.05641) (t_loss: 0.08026) (accu: 0.9747)
[epoch : 49] (l_loss: 0.05668) (t_loss: 0.07889) (accu: 0.9753)
[epoch : 50] (l_loss: 0.05609) (t_loss: 0.07980) (accu: 0.9759)
Finish! (Best accu: 0.9768) (Time taken(sec) : 768.55) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (2567 | 212933)          1.19
fc1.weight   :       196000 (2260 | 193740)          1.15
fc2.weight   :        18750 (216 | 18534)            1.15
fcout.weight :           750 (91 | 659)             12.13
------------------------------------------------------------
[Prune_iter : (21/21), Remaining weight : 1.19 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29985) (accu: 0.0953)
[epoch : 1] (l_loss: 0.44591) (t_loss: 0.16246) (accu: 0.9556)
[epoch : 2] (l_loss: 0.13390) (t_loss: 0.11716) (accu: 0.9655)
[epoch : 3] (l_loss: 0.10089) (t_loss: 0.10232) (accu: 0.9700)
[epoch : 4] (l_loss: 0.08570) (t_loss: 0.09481) (accu: 0.9718)
[epoch : 5] (l_loss: 0.07576) (t_loss: 0.08990) (accu: 0.9729)
[epoch : 6] (l_loss: 0.07041) (t_loss: 0.08755) (accu: 0.9717)
[epoch : 7] (l_loss: 0.06682) (t_loss: 0.08656) (accu: 0.9741)
[epoch : 8] (l_loss: 0.06404) (t_loss: 0.08466) (accu: 0.9743)
[epoch : 9] (l_loss: 0.06175) (t_loss: 0.08294) (accu: 0.9742)
[epoch : 10] (l_loss: 0.06017) (t_loss: 0.08132) (accu: 0.9755)
[epoch : 11] (l_loss: 0.05854) (t_loss: 0.08127) (accu: 0.9754)
[epoch : 12] (l_loss: 0.05760) (t_loss: 0.08056) (accu: 0.9742)
[epoch : 13] (l_loss: 0.05684) (t_loss: 0.08098) (accu: 0.9750)
[epoch : 14] (l_loss: 0.05634) (t_loss: 0.07754) (accu: 0.9751)
[epoch : 15] (l_loss: 0.05583) (t_loss: 0.07976) (accu: 0.9753)
[epoch : 16] (l_loss: 0.05569) (t_loss: 0.08277) (accu: 0.9747)
[epoch : 17] (l_loss: 0.05544) (t_loss: 0.07781) (accu: 0.9756)
[epoch : 18] (l_loss: 0.05463) (t_loss: 0.08190) (accu: 0.9744)
[epoch : 19] (l_loss: 0.05536) (t_loss: 0.07877) (accu: 0.9757)
[epoch : 20] (l_loss: 0.05487) (t_loss: 0.07805) (accu: 0.9762)
[epoch : 21] (l_loss: 0.05501) (t_loss: 0.08118) (accu: 0.9753)
[epoch : 22] (l_loss: 0.05452) (t_loss: 0.07897) (accu: 0.9768)
[epoch : 23] (l_loss: 0.05435) (t_loss: 0.07977) (accu: 0.9759)
[epoch : 24] (l_loss: 0.05438) (t_loss: 0.07743) (accu: 0.9757)
[epoch : 25] (l_loss: 0.05436) (t_loss: 0.07890) (accu: 0.9753)
[epoch : 26] (l_loss: 0.05450) (t_loss: 0.07840) (accu: 0.9760)
[epoch : 27] (l_loss: 0.05416) (t_loss: 0.07997) (accu: 0.9739)
[epoch : 28] (l_loss: 0.05384) (t_loss: 0.07917) (accu: 0.9752)
[epoch : 29] (l_loss: 0.05431) (t_loss: 0.08229) (accu: 0.9745)
[epoch : 30] (l_loss: 0.05434) (t_loss: 0.07990) (accu: 0.9757)
[epoch : 31] (l_loss: 0.05430) (t_loss: 0.07739) (accu: 0.9759)
[epoch : 32] (l_loss: 0.05405) (t_loss: 0.07928) (accu: 0.9744)
[epoch : 33] (l_loss: 0.05411) (t_loss: 0.07833) (accu: 0.9753)
[epoch : 34] (l_loss: 0.05426) (t_loss: 0.07945) (accu: 0.9762)
[epoch : 35] (l_loss: 0.05427) (t_loss: 0.07935) (accu: 0.9764)
[epoch : 36] (l_loss: 0.05437) (t_loss: 0.07696) (accu: 0.9766)
[epoch : 37] (l_loss: 0.05395) (t_loss: 0.08307) (accu: 0.9742)
[epoch : 38] (l_loss: 0.05381) (t_loss: 0.07877) (accu: 0.9765)
[epoch : 39] (l_loss: 0.05411) (t_loss: 0.07867) (accu: 0.9761)
[epoch : 40] (l_loss: 0.05367) (t_loss: 0.07936) (accu: 0.9760)
[epoch : 41] (l_loss: 0.05402) (t_loss: 0.07775) (accu: 0.9766)
[epoch : 42] (l_loss: 0.05400) (t_loss: 0.08235) (accu: 0.9751)
[epoch : 43] (l_loss: 0.05434) (t_loss: 0.07734) (accu: 0.9764)
[epoch : 44] (l_loss: 0.05410) (t_loss: 0.07895) (accu: 0.9754)
[epoch : 45] (l_loss: 0.05416) (t_loss: 0.07653) (accu: 0.9764)
[epoch : 46] (l_loss: 0.05396) (t_loss: 0.07924) (accu: 0.9754)
[epoch : 47] (l_loss: 0.05409) (t_loss: 0.07775) (accu: 0.9746)
[epoch : 48] (l_loss: 0.05413) (t_loss: 0.07490) (accu: 0.9759)
[epoch : 49] (l_loss: 0.05378) (t_loss: 0.07871) (accu: 0.9745)
[epoch : 50] (l_loss: 0.05409) (t_loss: 0.07542) (accu: 0.9763)
Finish! (Best accu: 0.9768) (Time taken(sec) : 771.02) 


Maximum accuracy per weight remaining
Remaining weight 100.0 %  Epoch 27 Accu 0.9766
Remaining weight 80.03 %  Epoch 14 Accu 0.9765
Remaining weight 64.06 %  Epoch 26 Accu 0.9777
Remaining weight 51.28 %  Epoch 23 Accu 0.9783
Remaining weight 41.05 %  Epoch 39 Accu 0.9770
Remaining weight 32.86 %  Epoch 38 Accu 0.9781
Remaining weight 26.31 %  Epoch 48 Accu 0.9775
Remaining weight 21.06 %  Epoch 41 Accu 0.9779
Remaining weight 16.87 %  Epoch 35 Accu 0.9778
Remaining weight 13.51 %  Epoch 35 Accu 0.9779
Remaining weight 10.82 %  Epoch 19 Accu 0.9777
Remaining weight 8.67 %  Epoch 44 Accu 0.9784
Remaining weight 6.95 %  Epoch 15 Accu 0.9781
Remaining weight 5.57 %  Epoch 29 Accu 0.9776
Remaining weight 4.46 %  Epoch 37 Accu 0.9768
Remaining weight 3.58 %  Epoch 18 Accu 0.9788
Remaining weight 2.87 %  Epoch 23 Accu 0.9768
Remaining weight 2.3 %  Epoch 20 Accu 0.9773
Remaining weight 1.85 %  Epoch 32 Accu 0.9770
Remaining weight 1.48 %  Epoch 19 Accu 0.9768
Remaining weight 1.19 %  Epoch 21 Accu 0.9768
===================================================================== 

Test_Iter (5/5)
------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :        215500 (215500 | 0)          100.00
fc1.weight   :        196000 (196000 | 0)          100.00
fc2.weight   :         18750 (18750 | 0)           100.00
fcout.weight :           750 (750 | 0)             100.00
------------------------------------------------------------
[Prune_iter : (1/21), Remaining weight : 100.0 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.71474) (accu: 0.0809)
[epoch : 1] (l_loss: 0.50281) (t_loss: 0.16236) (accu: 0.9539)
[epoch : 2] (l_loss: 0.13335) (t_loss: 0.11384) (accu: 0.9662)
[epoch : 3] (l_loss: 0.09852) (t_loss: 0.10027) (accu: 0.9693)
[epoch : 4] (l_loss: 0.08272) (t_loss: 0.09128) (accu: 0.9724)
[epoch : 5] (l_loss: 0.07454) (t_loss: 0.08915) (accu: 0.9732)
[epoch : 6] (l_loss: 0.06886) (t_loss: 0.08466) (accu: 0.9737)
[epoch : 7] (l_loss: 0.06554) (t_loss: 0.08175) (accu: 0.9755)
[epoch : 8] (l_loss: 0.06312) (t_loss: 0.08329) (accu: 0.9742)
[epoch : 9] (l_loss: 0.06167) (t_loss: 0.08143) (accu: 0.9741)
[epoch : 10] (l_loss: 0.06020) (t_loss: 0.07942) (accu: 0.9750)
[epoch : 11] (l_loss: 0.05936) (t_loss: 0.08102) (accu: 0.9759)
[epoch : 12] (l_loss: 0.05939) (t_loss: 0.07932) (accu: 0.9753)
[epoch : 13] (l_loss: 0.05869) (t_loss: 0.08266) (accu: 0.9744)
[epoch : 14] (l_loss: 0.05793) (t_loss: 0.08021) (accu: 0.9746)
[epoch : 15] (l_loss: 0.05797) (t_loss: 0.08278) (accu: 0.9746)
[epoch : 16] (l_loss: 0.05790) (t_loss: 0.07807) (accu: 0.9758)
[epoch : 17] (l_loss: 0.05721) (t_loss: 0.08035) (accu: 0.9752)
[epoch : 18] (l_loss: 0.05684) (t_loss: 0.08187) (accu: 0.9758)
[epoch : 19] (l_loss: 0.05725) (t_loss: 0.07939) (accu: 0.9751)
[epoch : 20] (l_loss: 0.05724) (t_loss: 0.07894) (accu: 0.9754)
[epoch : 21] (l_loss: 0.05756) (t_loss: 0.08014) (accu: 0.9751)
[epoch : 22] (l_loss: 0.05714) (t_loss: 0.08031) (accu: 0.9755)
[epoch : 23] (l_loss: 0.05721) (t_loss: 0.07862) (accu: 0.9763)
[epoch : 24] (l_loss: 0.05689) (t_loss: 0.07950) (accu: 0.9745)
[epoch : 25] (l_loss: 0.05669) (t_loss: 0.08481) (accu: 0.9731)
[epoch : 26] (l_loss: 0.05714) (t_loss: 0.08007) (accu: 0.9746)
[epoch : 27] (l_loss: 0.05696) (t_loss: 0.08121) (accu: 0.9743)
[epoch : 28] (l_loss: 0.05707) (t_loss: 0.07904) (accu: 0.9752)
[epoch : 29] (l_loss: 0.05661) (t_loss: 0.08079) (accu: 0.9749)
[epoch : 30] (l_loss: 0.05657) (t_loss: 0.07953) (accu: 0.9755)
[epoch : 31] (l_loss: 0.05679) (t_loss: 0.07879) (accu: 0.9759)
[epoch : 32] (l_loss: 0.05663) (t_loss: 0.08336) (accu: 0.9733)
[epoch : 33] (l_loss: 0.05591) (t_loss: 0.08068) (accu: 0.9749)
[epoch : 34] (l_loss: 0.05635) (t_loss: 0.08023) (accu: 0.9759)
[epoch : 35] (l_loss: 0.05651) (t_loss: 0.08199) (accu: 0.9750)
[epoch : 36] (l_loss: 0.05624) (t_loss: 0.07862) (accu: 0.9755)
[epoch : 37] (l_loss: 0.05627) (t_loss: 0.07845) (accu: 0.9759)
[epoch : 38] (l_loss: 0.05638) (t_loss: 0.07932) (accu: 0.9753)
[epoch : 39] (l_loss: 0.05607) (t_loss: 0.08262) (accu: 0.9755)
[epoch : 40] (l_loss: 0.05620) (t_loss: 0.07974) (accu: 0.9749)
[epoch : 41] (l_loss: 0.05599) (t_loss: 0.08061) (accu: 0.9750)
[epoch : 42] (l_loss: 0.05632) (t_loss: 0.08053) (accu: 0.9750)
[epoch : 43] (l_loss: 0.05533) (t_loss: 0.07941) (accu: 0.9757)
[epoch : 44] (l_loss: 0.05637) (t_loss: 0.08018) (accu: 0.9751)
[epoch : 45] (l_loss: 0.05590) (t_loss: 0.08112) (accu: 0.9743)
[epoch : 46] (l_loss: 0.05661) (t_loss: 0.08239) (accu: 0.9744)
[epoch : 47] (l_loss: 0.05631) (t_loss: 0.07800) (accu: 0.9751)
[epoch : 48] (l_loss: 0.05628) (t_loss: 0.08050) (accu: 0.9752)
[epoch : 49] (l_loss: 0.05661) (t_loss: 0.08099) (accu: 0.9747)
[epoch : 50] (l_loss: 0.05582) (t_loss: 0.07822) (accu: 0.9754)
Finish! (Best accu: 0.9763) (Time taken(sec) : 773.05) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (172475 | 43025)         80.03
fc1.weight   :      196000 (156800 | 39200)         80.00
fc2.weight   :        18750 (15000 | 3750)          80.00
fcout.weight :           750 (675 | 75)             90.00
------------------------------------------------------------
[Prune_iter : (2/21), Remaining weight : 80.03 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.48230) (accu: 0.0993)
[epoch : 1] (l_loss: 0.50344) (t_loss: 0.17408) (accu: 0.9483)
[epoch : 2] (l_loss: 0.14080) (t_loss: 0.12186) (accu: 0.9643)
[epoch : 3] (l_loss: 0.10410) (t_loss: 0.10241) (accu: 0.9690)
[epoch : 4] (l_loss: 0.08739) (t_loss: 0.09411) (accu: 0.9720)
[epoch : 5] (l_loss: 0.07726) (t_loss: 0.09226) (accu: 0.9705)
[epoch : 6] (l_loss: 0.07155) (t_loss: 0.08846) (accu: 0.9742)
[epoch : 7] (l_loss: 0.06815) (t_loss: 0.08687) (accu: 0.9734)
[epoch : 8] (l_loss: 0.06535) (t_loss: 0.08790) (accu: 0.9721)
[epoch : 9] (l_loss: 0.06338) (t_loss: 0.08123) (accu: 0.9748)
[epoch : 10] (l_loss: 0.06248) (t_loss: 0.08280) (accu: 0.9746)
[epoch : 11] (l_loss: 0.06118) (t_loss: 0.08085) (accu: 0.9751)
[epoch : 12] (l_loss: 0.06081) (t_loss: 0.08067) (accu: 0.9752)
[epoch : 13] (l_loss: 0.06026) (t_loss: 0.08243) (accu: 0.9743)
[epoch : 14] (l_loss: 0.05943) (t_loss: 0.08237) (accu: 0.9748)
[epoch : 15] (l_loss: 0.05902) (t_loss: 0.08142) (accu: 0.9752)
[epoch : 16] (l_loss: 0.05866) (t_loss: 0.08148) (accu: 0.9744)
[epoch : 17] (l_loss: 0.05880) (t_loss: 0.07884) (accu: 0.9762)
[epoch : 18] (l_loss: 0.05886) (t_loss: 0.08041) (accu: 0.9744)
[epoch : 19] (l_loss: 0.05859) (t_loss: 0.07982) (accu: 0.9752)
[epoch : 20] (l_loss: 0.05786) (t_loss: 0.08019) (accu: 0.9749)
[epoch : 21] (l_loss: 0.05799) (t_loss: 0.08329) (accu: 0.9743)
[epoch : 22] (l_loss: 0.05774) (t_loss: 0.07917) (accu: 0.9749)
[epoch : 23] (l_loss: 0.05710) (t_loss: 0.08367) (accu: 0.9740)
[epoch : 24] (l_loss: 0.05729) (t_loss: 0.07805) (accu: 0.9759)
[epoch : 25] (l_loss: 0.05758) (t_loss: 0.07999) (accu: 0.9744)
[epoch : 26] (l_loss: 0.05725) (t_loss: 0.08127) (accu: 0.9746)
[epoch : 27] (l_loss: 0.05715) (t_loss: 0.07982) (accu: 0.9757)
[epoch : 28] (l_loss: 0.05708) (t_loss: 0.08053) (accu: 0.9751)
[epoch : 29] (l_loss: 0.05683) (t_loss: 0.07943) (accu: 0.9752)
[epoch : 30] (l_loss: 0.05714) (t_loss: 0.07996) (accu: 0.9739)
[epoch : 31] (l_loss: 0.05715) (t_loss: 0.07998) (accu: 0.9744)
[epoch : 32] (l_loss: 0.05729) (t_loss: 0.08062) (accu: 0.9762)
[epoch : 33] (l_loss: 0.05705) (t_loss: 0.08068) (accu: 0.9755)
[epoch : 34] (l_loss: 0.05717) (t_loss: 0.08330) (accu: 0.9741)
[epoch : 35] (l_loss: 0.05666) (t_loss: 0.08016) (accu: 0.9757)
[epoch : 36] (l_loss: 0.05736) (t_loss: 0.07815) (accu: 0.9766)
[epoch : 37] (l_loss: 0.05661) (t_loss: 0.07923) (accu: 0.9754)
[epoch : 38] (l_loss: 0.05705) (t_loss: 0.08142) (accu: 0.9748)
[epoch : 39] (l_loss: 0.05699) (t_loss: 0.08330) (accu: 0.9736)
[epoch : 40] (l_loss: 0.05655) (t_loss: 0.08321) (accu: 0.9731)
[epoch : 41] (l_loss: 0.05641) (t_loss: 0.08001) (accu: 0.9746)
[epoch : 42] (l_loss: 0.05676) (t_loss: 0.08082) (accu: 0.9733)
[epoch : 43] (l_loss: 0.05676) (t_loss: 0.08202) (accu: 0.9741)
[epoch : 44] (l_loss: 0.05699) (t_loss: 0.08235) (accu: 0.9740)
[epoch : 45] (l_loss: 0.05693) (t_loss: 0.07857) (accu: 0.9755)
[epoch : 46] (l_loss: 0.05731) (t_loss: 0.08048) (accu: 0.9746)
[epoch : 47] (l_loss: 0.05703) (t_loss: 0.08030) (accu: 0.9739)
[epoch : 48] (l_loss: 0.05667) (t_loss: 0.08182) (accu: 0.9743)
[epoch : 49] (l_loss: 0.05705) (t_loss: 0.08300) (accu: 0.9746)
[epoch : 50] (l_loss: 0.05727) (t_loss: 0.08218) (accu: 0.9748)
Finish! (Best accu: 0.9766) (Time taken(sec) : 757.27) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (138048 | 77452)         64.06
fc1.weight   :      196000 (125440 | 70560)         64.00
fc2.weight   :        18750 (12000 | 6750)          64.00
fcout.weight :          750 (608 | 142)             81.07
------------------------------------------------------------
[Prune_iter : (3/21), Remaining weight : 64.06 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.37818) (accu: 0.0976)
[epoch : 1] (l_loss: 0.48239) (t_loss: 0.15763) (accu: 0.9537)
[epoch : 2] (l_loss: 0.12833) (t_loss: 0.11069) (accu: 0.9682)
[epoch : 3] (l_loss: 0.09358) (t_loss: 0.09635) (accu: 0.9717)
[epoch : 4] (l_loss: 0.07749) (t_loss: 0.08811) (accu: 0.9734)
[epoch : 5] (l_loss: 0.06894) (t_loss: 0.08398) (accu: 0.9751)
[epoch : 6] (l_loss: 0.06363) (t_loss: 0.08204) (accu: 0.9759)
[epoch : 7] (l_loss: 0.06047) (t_loss: 0.08123) (accu: 0.9762)
[epoch : 8] (l_loss: 0.05855) (t_loss: 0.08013) (accu: 0.9754)
[epoch : 9] (l_loss: 0.05692) (t_loss: 0.07774) (accu: 0.9763)
[epoch : 10] (l_loss: 0.05634) (t_loss: 0.07777) (accu: 0.9774)
[epoch : 11] (l_loss: 0.05564) (t_loss: 0.08073) (accu: 0.9748)
[epoch : 12] (l_loss: 0.05498) (t_loss: 0.08044) (accu: 0.9759)
[epoch : 13] (l_loss: 0.05458) (t_loss: 0.07840) (accu: 0.9782)
[epoch : 14] (l_loss: 0.05428) (t_loss: 0.07617) (accu: 0.9778)
[epoch : 15] (l_loss: 0.05426) (t_loss: 0.08062) (accu: 0.9756)
[epoch : 16] (l_loss: 0.05354) (t_loss: 0.07813) (accu: 0.9757)
[epoch : 17] (l_loss: 0.05366) (t_loss: 0.07608) (accu: 0.9774)
[epoch : 18] (l_loss: 0.05352) (t_loss: 0.07665) (accu: 0.9764)
[epoch : 19] (l_loss: 0.05349) (t_loss: 0.07790) (accu: 0.9769)
[epoch : 20] (l_loss: 0.05348) (t_loss: 0.07800) (accu: 0.9772)
[epoch : 21] (l_loss: 0.05346) (t_loss: 0.07630) (accu: 0.9764)
[epoch : 22] (l_loss: 0.05355) (t_loss: 0.07903) (accu: 0.9751)
[epoch : 23] (l_loss: 0.05322) (t_loss: 0.07777) (accu: 0.9765)
[epoch : 24] (l_loss: 0.05305) (t_loss: 0.07716) (accu: 0.9766)
[epoch : 25] (l_loss: 0.05369) (t_loss: 0.07849) (accu: 0.9769)
[epoch : 26] (l_loss: 0.05339) (t_loss: 0.07681) (accu: 0.9775)
[epoch : 27] (l_loss: 0.05299) (t_loss: 0.07700) (accu: 0.9771)
[epoch : 28] (l_loss: 0.05309) (t_loss: 0.07637) (accu: 0.9769)
[epoch : 29] (l_loss: 0.05328) (t_loss: 0.07705) (accu: 0.9770)
[epoch : 30] (l_loss: 0.05298) (t_loss: 0.07722) (accu: 0.9776)
[epoch : 31] (l_loss: 0.05261) (t_loss: 0.07766) (accu: 0.9763)
[epoch : 32] (l_loss: 0.05263) (t_loss: 0.07806) (accu: 0.9762)
[epoch : 33] (l_loss: 0.05253) (t_loss: 0.07986) (accu: 0.9764)
[epoch : 34] (l_loss: 0.05303) (t_loss: 0.07493) (accu: 0.9765)
[epoch : 35] (l_loss: 0.05289) (t_loss: 0.07739) (accu: 0.9759)
[epoch : 36] (l_loss: 0.05284) (t_loss: 0.07810) (accu: 0.9761)
[epoch : 37] (l_loss: 0.05324) (t_loss: 0.07685) (accu: 0.9765)
[epoch : 38] (l_loss: 0.05300) (t_loss: 0.07690) (accu: 0.9752)
[epoch : 39] (l_loss: 0.05290) (t_loss: 0.07801) (accu: 0.9773)
[epoch : 40] (l_loss: 0.05303) (t_loss: 0.07860) (accu: 0.9759)
[epoch : 41] (l_loss: 0.05311) (t_loss: 0.07969) (accu: 0.9758)
[epoch : 42] (l_loss: 0.05308) (t_loss: 0.07504) (accu: 0.9768)
[epoch : 43] (l_loss: 0.05284) (t_loss: 0.07690) (accu: 0.9764)
[epoch : 44] (l_loss: 0.05319) (t_loss: 0.07789) (accu: 0.9768)
[epoch : 45] (l_loss: 0.05235) (t_loss: 0.07531) (accu: 0.9775)
[epoch : 46] (l_loss: 0.05268) (t_loss: 0.07871) (accu: 0.9763)
[epoch : 47] (l_loss: 0.05257) (t_loss: 0.07892) (accu: 0.9772)
[epoch : 48] (l_loss: 0.05327) (t_loss: 0.07636) (accu: 0.9773)
[epoch : 49] (l_loss: 0.05225) (t_loss: 0.07766) (accu: 0.9757)
[epoch : 50] (l_loss: 0.05288) (t_loss: 0.07808) (accu: 0.9753)
Finish! (Best accu: 0.9782) (Time taken(sec) : 783.32) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (110499 | 105001)        51.28
fc1.weight   :      196000 (100352 | 95648)         51.20
fc2.weight   :        18750 (9600 | 9150)           51.20
fcout.weight :          750 (547 | 203)             72.93
------------------------------------------------------------
[Prune_iter : (4/21), Remaining weight : 51.28 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.33786) (accu: 0.0961)
[epoch : 1] (l_loss: 0.48612) (t_loss: 0.16560) (accu: 0.9518)
[epoch : 2] (l_loss: 0.13369) (t_loss: 0.11574) (accu: 0.9647)
[epoch : 3] (l_loss: 0.09861) (t_loss: 0.10112) (accu: 0.9704)
[epoch : 4] (l_loss: 0.08288) (t_loss: 0.09184) (accu: 0.9717)
[epoch : 5] (l_loss: 0.07349) (t_loss: 0.08756) (accu: 0.9739)
[epoch : 6] (l_loss: 0.06726) (t_loss: 0.08490) (accu: 0.9741)
[epoch : 7] (l_loss: 0.06315) (t_loss: 0.08096) (accu: 0.9750)
[epoch : 8] (l_loss: 0.06009) (t_loss: 0.07974) (accu: 0.9774)
[epoch : 9] (l_loss: 0.05824) (t_loss: 0.08140) (accu: 0.9768)
[epoch : 10] (l_loss: 0.05682) (t_loss: 0.07768) (accu: 0.9769)
[epoch : 11] (l_loss: 0.05557) (t_loss: 0.08044) (accu: 0.9756)
[epoch : 12] (l_loss: 0.05500) (t_loss: 0.07487) (accu: 0.9771)
[epoch : 13] (l_loss: 0.05468) (t_loss: 0.07755) (accu: 0.9774)
[epoch : 14] (l_loss: 0.05440) (t_loss: 0.07707) (accu: 0.9770)
[epoch : 15] (l_loss: 0.05362) (t_loss: 0.07801) (accu: 0.9765)
[epoch : 16] (l_loss: 0.05368) (t_loss: 0.07637) (accu: 0.9770)
[epoch : 17] (l_loss: 0.05356) (t_loss: 0.07875) (accu: 0.9753)
[epoch : 18] (l_loss: 0.05381) (t_loss: 0.07837) (accu: 0.9768)
[epoch : 19] (l_loss: 0.05372) (t_loss: 0.07674) (accu: 0.9774)
[epoch : 20] (l_loss: 0.05372) (t_loss: 0.07802) (accu: 0.9758)
[epoch : 21] (l_loss: 0.05310) (t_loss: 0.07911) (accu: 0.9769)
[epoch : 22] (l_loss: 0.05306) (t_loss: 0.07822) (accu: 0.9749)
[epoch : 23] (l_loss: 0.05299) (t_loss: 0.07817) (accu: 0.9768)
[epoch : 24] (l_loss: 0.05260) (t_loss: 0.07592) (accu: 0.9769)
[epoch : 25] (l_loss: 0.05298) (t_loss: 0.07447) (accu: 0.9769)
[epoch : 26] (l_loss: 0.05265) (t_loss: 0.07755) (accu: 0.9768)
[epoch : 27] (l_loss: 0.05306) (t_loss: 0.07643) (accu: 0.9771)
[epoch : 28] (l_loss: 0.05296) (t_loss: 0.07900) (accu: 0.9752)
[epoch : 29] (l_loss: 0.05316) (t_loss: 0.07678) (accu: 0.9759)
[epoch : 30] (l_loss: 0.05234) (t_loss: 0.07820) (accu: 0.9763)
[epoch : 31] (l_loss: 0.05278) (t_loss: 0.07660) (accu: 0.9762)
[epoch : 32] (l_loss: 0.05253) (t_loss: 0.07816) (accu: 0.9781)
[epoch : 33] (l_loss: 0.05298) (t_loss: 0.07596) (accu: 0.9778)
[epoch : 34] (l_loss: 0.05194) (t_loss: 0.07835) (accu: 0.9763)
[epoch : 35] (l_loss: 0.05193) (t_loss: 0.07522) (accu: 0.9779)
[epoch : 36] (l_loss: 0.05187) (t_loss: 0.07834) (accu: 0.9766)
[epoch : 37] (l_loss: 0.05162) (t_loss: 0.07637) (accu: 0.9760)
[epoch : 38] (l_loss: 0.05153) (t_loss: 0.07306) (accu: 0.9769)
[epoch : 39] (l_loss: 0.05130) (t_loss: 0.07896) (accu: 0.9740)
[epoch : 40] (l_loss: 0.05155) (t_loss: 0.07717) (accu: 0.9762)
[epoch : 41] (l_loss: 0.05171) (t_loss: 0.07757) (accu: 0.9763)
[epoch : 42] (l_loss: 0.05157) (t_loss: 0.07349) (accu: 0.9777)
[epoch : 43] (l_loss: 0.05134) (t_loss: 0.07558) (accu: 0.9769)
[epoch : 44] (l_loss: 0.05156) (t_loss: 0.07801) (accu: 0.9768)
[epoch : 45] (l_loss: 0.05146) (t_loss: 0.07263) (accu: 0.9778)
[epoch : 46] (l_loss: 0.05106) (t_loss: 0.07486) (accu: 0.9776)
[epoch : 47] (l_loss: 0.05165) (t_loss: 0.07595) (accu: 0.9769)
[epoch : 48] (l_loss: 0.05141) (t_loss: 0.07578) (accu: 0.9759)
[epoch : 49] (l_loss: 0.05151) (t_loss: 0.07689) (accu: 0.9766)
[epoch : 50] (l_loss: 0.05169) (t_loss: 0.07549) (accu: 0.9763)
Finish! (Best accu: 0.9781) (Time taken(sec) : 795.62) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (88454 | 127046)         41.05
fc1.weight   :      196000 (80282 | 115718)         40.96
fc2.weight   :        18750 (7680 | 11070)          40.96
fcout.weight :          750 (492 | 258)             65.60
------------------------------------------------------------
[Prune_iter : (5/21), Remaining weight : 41.05 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.34392) (accu: 0.1097)
[epoch : 1] (l_loss: 0.48055) (t_loss: 0.16681) (accu: 0.9497)
[epoch : 2] (l_loss: 0.14064) (t_loss: 0.12193) (accu: 0.9641)
[epoch : 3] (l_loss: 0.10420) (t_loss: 0.10167) (accu: 0.9697)
[epoch : 4] (l_loss: 0.08759) (t_loss: 0.09741) (accu: 0.9706)
[epoch : 5] (l_loss: 0.07837) (t_loss: 0.09358) (accu: 0.9717)
[epoch : 6] (l_loss: 0.07239) (t_loss: 0.09155) (accu: 0.9714)
[epoch : 7] (l_loss: 0.06923) (t_loss: 0.08786) (accu: 0.9737)
[epoch : 8] (l_loss: 0.06552) (t_loss: 0.08736) (accu: 0.9740)
[epoch : 9] (l_loss: 0.06319) (t_loss: 0.08427) (accu: 0.9736)
[epoch : 10] (l_loss: 0.06146) (t_loss: 0.08286) (accu: 0.9734)
[epoch : 11] (l_loss: 0.05938) (t_loss: 0.08144) (accu: 0.9748)
[epoch : 12] (l_loss: 0.05806) (t_loss: 0.07925) (accu: 0.9754)
[epoch : 13] (l_loss: 0.05726) (t_loss: 0.08074) (accu: 0.9744)
[epoch : 14] (l_loss: 0.05678) (t_loss: 0.07946) (accu: 0.9766)
[epoch : 15] (l_loss: 0.05652) (t_loss: 0.07914) (accu: 0.9757)
[epoch : 16] (l_loss: 0.05646) (t_loss: 0.08142) (accu: 0.9751)
[epoch : 17] (l_loss: 0.05627) (t_loss: 0.07796) (accu: 0.9766)
[epoch : 18] (l_loss: 0.05567) (t_loss: 0.08015) (accu: 0.9753)
[epoch : 19] (l_loss: 0.05565) (t_loss: 0.08426) (accu: 0.9755)
[epoch : 20] (l_loss: 0.05605) (t_loss: 0.08161) (accu: 0.9757)
[epoch : 21] (l_loss: 0.05502) (t_loss: 0.08229) (accu: 0.9750)
[epoch : 22] (l_loss: 0.05501) (t_loss: 0.08187) (accu: 0.9750)
[epoch : 23] (l_loss: 0.05553) (t_loss: 0.07905) (accu: 0.9756)
[epoch : 24] (l_loss: 0.05515) (t_loss: 0.07910) (accu: 0.9758)
[epoch : 25] (l_loss: 0.05513) (t_loss: 0.08123) (accu: 0.9753)
[epoch : 26] (l_loss: 0.05495) (t_loss: 0.08129) (accu: 0.9754)
[epoch : 27] (l_loss: 0.05575) (t_loss: 0.07885) (accu: 0.9763)
[epoch : 28] (l_loss: 0.05487) (t_loss: 0.07976) (accu: 0.9753)
[epoch : 29] (l_loss: 0.05488) (t_loss: 0.08197) (accu: 0.9753)
[epoch : 30] (l_loss: 0.05522) (t_loss: 0.07896) (accu: 0.9763)
[epoch : 31] (l_loss: 0.05471) (t_loss: 0.08066) (accu: 0.9748)
[epoch : 32] (l_loss: 0.05516) (t_loss: 0.08070) (accu: 0.9747)
[epoch : 33] (l_loss: 0.05471) (t_loss: 0.07883) (accu: 0.9762)
[epoch : 34] (l_loss: 0.05505) (t_loss: 0.08129) (accu: 0.9745)
[epoch : 35] (l_loss: 0.05523) (t_loss: 0.08111) (accu: 0.9746)
[epoch : 36] (l_loss: 0.05481) (t_loss: 0.08150) (accu: 0.9760)
[epoch : 37] (l_loss: 0.05533) (t_loss: 0.08026) (accu: 0.9754)
[epoch : 38] (l_loss: 0.05433) (t_loss: 0.08295) (accu: 0.9752)
[epoch : 39] (l_loss: 0.05477) (t_loss: 0.08248) (accu: 0.9755)
[epoch : 40] (l_loss: 0.05488) (t_loss: 0.08015) (accu: 0.9767)
[epoch : 41] (l_loss: 0.05463) (t_loss: 0.08044) (accu: 0.9754)
[epoch : 42] (l_loss: 0.05494) (t_loss: 0.08014) (accu: 0.9749)
[epoch : 43] (l_loss: 0.05485) (t_loss: 0.08003) (accu: 0.9760)
[epoch : 44] (l_loss: 0.05501) (t_loss: 0.08055) (accu: 0.9757)
[epoch : 45] (l_loss: 0.05485) (t_loss: 0.07807) (accu: 0.9755)
[epoch : 46] (l_loss: 0.05476) (t_loss: 0.07853) (accu: 0.9764)
[epoch : 47] (l_loss: 0.05469) (t_loss: 0.08026) (accu: 0.9761)
[epoch : 48] (l_loss: 0.05474) (t_loss: 0.08007) (accu: 0.9763)
[epoch : 49] (l_loss: 0.05493) (t_loss: 0.08096) (accu: 0.9746)
[epoch : 50] (l_loss: 0.05475) (t_loss: 0.07975) (accu: 0.9758)
Finish! (Best accu: 0.9767) (Time taken(sec) : 788.52) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (70812 | 144688)         32.86
fc1.weight   :      196000 (64225 | 131775)         32.77
fc2.weight   :        18750 (6144 | 12606)          32.77
fcout.weight :          750 (443 | 307)             59.07
------------------------------------------------------------
[Prune_iter : (6/21), Remaining weight : 32.86 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.32571) (accu: 0.1213)
[epoch : 1] (l_loss: 0.48218) (t_loss: 0.16928) (accu: 0.9502)
[epoch : 2] (l_loss: 0.14168) (t_loss: 0.12159) (accu: 0.9635)
[epoch : 3] (l_loss: 0.10501) (t_loss: 0.10341) (accu: 0.9683)
[epoch : 4] (l_loss: 0.08788) (t_loss: 0.09642) (accu: 0.9711)
[epoch : 5] (l_loss: 0.07794) (t_loss: 0.08805) (accu: 0.9732)
[epoch : 6] (l_loss: 0.07175) (t_loss: 0.08757) (accu: 0.9739)
[epoch : 7] (l_loss: 0.06689) (t_loss: 0.08412) (accu: 0.9752)
[epoch : 8] (l_loss: 0.06360) (t_loss: 0.08484) (accu: 0.9747)
[epoch : 9] (l_loss: 0.06164) (t_loss: 0.08238) (accu: 0.9755)
[epoch : 10] (l_loss: 0.06055) (t_loss: 0.08211) (accu: 0.9749)
[epoch : 11] (l_loss: 0.05906) (t_loss: 0.08196) (accu: 0.9761)
[epoch : 12] (l_loss: 0.05867) (t_loss: 0.08047) (accu: 0.9763)
[epoch : 13] (l_loss: 0.05764) (t_loss: 0.08019) (accu: 0.9765)
[epoch : 14] (l_loss: 0.05703) (t_loss: 0.08311) (accu: 0.9758)
[epoch : 15] (l_loss: 0.05670) (t_loss: 0.08113) (accu: 0.9758)
[epoch : 16] (l_loss: 0.05633) (t_loss: 0.07989) (accu: 0.9753)
[epoch : 17] (l_loss: 0.05608) (t_loss: 0.07982) (accu: 0.9772)
[epoch : 18] (l_loss: 0.05622) (t_loss: 0.08175) (accu: 0.9762)
[epoch : 19] (l_loss: 0.05564) (t_loss: 0.08106) (accu: 0.9757)
[epoch : 20] (l_loss: 0.05577) (t_loss: 0.08075) (accu: 0.9759)
[epoch : 21] (l_loss: 0.05568) (t_loss: 0.07994) (accu: 0.9748)
[epoch : 22] (l_loss: 0.05469) (t_loss: 0.07994) (accu: 0.9759)
[epoch : 23] (l_loss: 0.05422) (t_loss: 0.07722) (accu: 0.9758)
[epoch : 24] (l_loss: 0.05330) (t_loss: 0.07770) (accu: 0.9759)
[epoch : 25] (l_loss: 0.05273) (t_loss: 0.07932) (accu: 0.9761)
[epoch : 26] (l_loss: 0.05292) (t_loss: 0.07582) (accu: 0.9772)
[epoch : 27] (l_loss: 0.05315) (t_loss: 0.07771) (accu: 0.9764)
[epoch : 28] (l_loss: 0.05297) (t_loss: 0.07621) (accu: 0.9773)
[epoch : 29] (l_loss: 0.05312) (t_loss: 0.07879) (accu: 0.9766)
[epoch : 30] (l_loss: 0.05248) (t_loss: 0.07780) (accu: 0.9768)
[epoch : 31] (l_loss: 0.05238) (t_loss: 0.07760) (accu: 0.9774)
[epoch : 32] (l_loss: 0.05261) (t_loss: 0.07493) (accu: 0.9783)
[epoch : 33] (l_loss: 0.05250) (t_loss: 0.08076) (accu: 0.9757)
[epoch : 34] (l_loss: 0.05236) (t_loss: 0.07748) (accu: 0.9757)
[epoch : 35] (l_loss: 0.05272) (t_loss: 0.07568) (accu: 0.9767)
[epoch : 36] (l_loss: 0.05289) (t_loss: 0.07698) (accu: 0.9770)
[epoch : 37] (l_loss: 0.05285) (t_loss: 0.07516) (accu: 0.9776)
[epoch : 38] (l_loss: 0.05240) (t_loss: 0.07792) (accu: 0.9772)
[epoch : 39] (l_loss: 0.05287) (t_loss: 0.07651) (accu: 0.9775)
[epoch : 40] (l_loss: 0.05252) (t_loss: 0.08239) (accu: 0.9740)
[epoch : 41] (l_loss: 0.05265) (t_loss: 0.07577) (accu: 0.9764)
[epoch : 42] (l_loss: 0.05292) (t_loss: 0.08166) (accu: 0.9744)
[epoch : 43] (l_loss: 0.05272) (t_loss: 0.07697) (accu: 0.9768)
[epoch : 44] (l_loss: 0.05254) (t_loss: 0.07682) (accu: 0.9765)
[epoch : 45] (l_loss: 0.05268) (t_loss: 0.07570) (accu: 0.9772)
[epoch : 46] (l_loss: 0.05209) (t_loss: 0.07537) (accu: 0.9763)
[epoch : 47] (l_loss: 0.05256) (t_loss: 0.07713) (accu: 0.9759)
[epoch : 48] (l_loss: 0.05244) (t_loss: 0.07734) (accu: 0.9760)
[epoch : 49] (l_loss: 0.05224) (t_loss: 0.07803) (accu: 0.9767)
[epoch : 50] (l_loss: 0.05274) (t_loss: 0.07852) (accu: 0.9763)
Finish! (Best accu: 0.9783) (Time taken(sec) : 798.85) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (56694 | 158806)         26.31
fc1.weight   :      196000 (51380 | 144620)         26.21
fc2.weight   :        18750 (4915 | 13835)          26.21
fcout.weight :          750 (399 | 351)             53.20
------------------------------------------------------------
[Prune_iter : (7/21), Remaining weight : 26.31 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29641) (accu: 0.1425)
[epoch : 1] (l_loss: 0.46429) (t_loss: 0.15511) (accu: 0.9562)
[epoch : 2] (l_loss: 0.12855) (t_loss: 0.11139) (accu: 0.9671)
[epoch : 3] (l_loss: 0.09761) (t_loss: 0.09959) (accu: 0.9693)
[epoch : 4] (l_loss: 0.08263) (t_loss: 0.08990) (accu: 0.9728)
[epoch : 5] (l_loss: 0.07366) (t_loss: 0.08389) (accu: 0.9751)
[epoch : 6] (l_loss: 0.06857) (t_loss: 0.08435) (accu: 0.9745)
[epoch : 7] (l_loss: 0.06456) (t_loss: 0.08268) (accu: 0.9735)
[epoch : 8] (l_loss: 0.06236) (t_loss: 0.08095) (accu: 0.9751)
[epoch : 9] (l_loss: 0.05941) (t_loss: 0.08246) (accu: 0.9745)
[epoch : 10] (l_loss: 0.05763) (t_loss: 0.07794) (accu: 0.9760)
[epoch : 11] (l_loss: 0.05641) (t_loss: 0.07889) (accu: 0.9766)
[epoch : 12] (l_loss: 0.05524) (t_loss: 0.07867) (accu: 0.9764)
[epoch : 13] (l_loss: 0.05487) (t_loss: 0.07621) (accu: 0.9762)
[epoch : 14] (l_loss: 0.05413) (t_loss: 0.07823) (accu: 0.9768)
[epoch : 15] (l_loss: 0.05421) (t_loss: 0.07881) (accu: 0.9748)
[epoch : 16] (l_loss: 0.05368) (t_loss: 0.07797) (accu: 0.9772)
[epoch : 17] (l_loss: 0.05320) (t_loss: 0.07695) (accu: 0.9763)
[epoch : 18] (l_loss: 0.05309) (t_loss: 0.07959) (accu: 0.9752)
[epoch : 19] (l_loss: 0.05297) (t_loss: 0.07672) (accu: 0.9761)
[epoch : 20] (l_loss: 0.05299) (t_loss: 0.07870) (accu: 0.9757)
[epoch : 21] (l_loss: 0.05291) (t_loss: 0.07721) (accu: 0.9762)
[epoch : 22] (l_loss: 0.05252) (t_loss: 0.08013) (accu: 0.9749)
[epoch : 23] (l_loss: 0.05311) (t_loss: 0.07765) (accu: 0.9765)
[epoch : 24] (l_loss: 0.05315) (t_loss: 0.07387) (accu: 0.9764)
[epoch : 25] (l_loss: 0.05269) (t_loss: 0.07527) (accu: 0.9773)
[epoch : 26] (l_loss: 0.05281) (t_loss: 0.07999) (accu: 0.9745)
[epoch : 27] (l_loss: 0.05239) (t_loss: 0.07978) (accu: 0.9750)
[epoch : 28] (l_loss: 0.05268) (t_loss: 0.07816) (accu: 0.9763)
[epoch : 29] (l_loss: 0.05204) (t_loss: 0.07718) (accu: 0.9765)
[epoch : 30] (l_loss: 0.05285) (t_loss: 0.07655) (accu: 0.9770)
[epoch : 31] (l_loss: 0.05240) (t_loss: 0.07634) (accu: 0.9759)
[epoch : 32] (l_loss: 0.05249) (t_loss: 0.08060) (accu: 0.9749)
[epoch : 33] (l_loss: 0.05300) (t_loss: 0.07692) (accu: 0.9758)
[epoch : 34] (l_loss: 0.05237) (t_loss: 0.07647) (accu: 0.9772)
[epoch : 35] (l_loss: 0.05268) (t_loss: 0.07715) (accu: 0.9771)
[epoch : 36] (l_loss: 0.05235) (t_loss: 0.07833) (accu: 0.9766)
[epoch : 37] (l_loss: 0.05246) (t_loss: 0.07718) (accu: 0.9763)
[epoch : 38] (l_loss: 0.05245) (t_loss: 0.07435) (accu: 0.9770)
[epoch : 39] (l_loss: 0.05252) (t_loss: 0.07802) (accu: 0.9764)
[epoch : 40] (l_loss: 0.05292) (t_loss: 0.07988) (accu: 0.9745)
[epoch : 41] (l_loss: 0.05215) (t_loss: 0.07428) (accu: 0.9773)
[epoch : 42] (l_loss: 0.05245) (t_loss: 0.07518) (accu: 0.9770)
[epoch : 43] (l_loss: 0.05233) (t_loss: 0.07806) (accu: 0.9769)
[epoch : 44] (l_loss: 0.05234) (t_loss: 0.07657) (accu: 0.9769)
[epoch : 45] (l_loss: 0.05273) (t_loss: 0.07631) (accu: 0.9772)
[epoch : 46] (l_loss: 0.05270) (t_loss: 0.08156) (accu: 0.9749)
[epoch : 47] (l_loss: 0.05234) (t_loss: 0.07700) (accu: 0.9767)
[epoch : 48] (l_loss: 0.05223) (t_loss: 0.07604) (accu: 0.9773)
[epoch : 49] (l_loss: 0.05227) (t_loss: 0.07822) (accu: 0.9749)
[epoch : 50] (l_loss: 0.05220) (t_loss: 0.07734) (accu: 0.9753)
Finish! (Best accu: 0.9773) (Time taken(sec) : 794.21) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (45395 | 170105)         21.06
fc1.weight   :      196000 (41104 | 154896)         20.97
fc2.weight   :        18750 (3932 | 14818)          20.97
fcout.weight :          750 (359 | 391)             47.87
------------------------------------------------------------
[Prune_iter : (8/21), Remaining weight : 21.06 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.31133) (accu: 0.1316)
[epoch : 1] (l_loss: 0.46063) (t_loss: 0.16503) (accu: 0.9521)
[epoch : 2] (l_loss: 0.13603) (t_loss: 0.11939) (accu: 0.9645)
[epoch : 3] (l_loss: 0.10254) (t_loss: 0.10061) (accu: 0.9684)
[epoch : 4] (l_loss: 0.08670) (t_loss: 0.09067) (accu: 0.9735)
[epoch : 5] (l_loss: 0.07612) (t_loss: 0.08904) (accu: 0.9728)
[epoch : 6] (l_loss: 0.06840) (t_loss: 0.08413) (accu: 0.9733)
[epoch : 7] (l_loss: 0.06315) (t_loss: 0.08125) (accu: 0.9765)
[epoch : 8] (l_loss: 0.05957) (t_loss: 0.08104) (accu: 0.9757)
[epoch : 9] (l_loss: 0.05754) (t_loss: 0.07916) (accu: 0.9768)
[epoch : 10] (l_loss: 0.05612) (t_loss: 0.07684) (accu: 0.9765)
[epoch : 11] (l_loss: 0.05498) (t_loss: 0.07886) (accu: 0.9775)
[epoch : 12] (l_loss: 0.05437) (t_loss: 0.07754) (accu: 0.9764)
[epoch : 13] (l_loss: 0.05413) (t_loss: 0.07702) (accu: 0.9767)
[epoch : 14] (l_loss: 0.05354) (t_loss: 0.07893) (accu: 0.9767)
[epoch : 15] (l_loss: 0.05315) (t_loss: 0.07672) (accu: 0.9768)
[epoch : 16] (l_loss: 0.05267) (t_loss: 0.07721) (accu: 0.9773)
[epoch : 17] (l_loss: 0.05285) (t_loss: 0.07723) (accu: 0.9773)
[epoch : 18] (l_loss: 0.05243) (t_loss: 0.07782) (accu: 0.9759)
[epoch : 19] (l_loss: 0.05278) (t_loss: 0.07891) (accu: 0.9764)
[epoch : 20] (l_loss: 0.05181) (t_loss: 0.07806) (accu: 0.9756)
[epoch : 21] (l_loss: 0.05172) (t_loss: 0.07988) (accu: 0.9751)
[epoch : 22] (l_loss: 0.05171) (t_loss: 0.07524) (accu: 0.9778)
[epoch : 23] (l_loss: 0.05150) (t_loss: 0.07790) (accu: 0.9767)
[epoch : 24] (l_loss: 0.05181) (t_loss: 0.07509) (accu: 0.9766)
[epoch : 25] (l_loss: 0.05160) (t_loss: 0.07673) (accu: 0.9769)
[epoch : 26] (l_loss: 0.05215) (t_loss: 0.07737) (accu: 0.9759)
[epoch : 27] (l_loss: 0.05175) (t_loss: 0.07780) (accu: 0.9776)
[epoch : 28] (l_loss: 0.05178) (t_loss: 0.07826) (accu: 0.9761)
[epoch : 29] (l_loss: 0.05157) (t_loss: 0.07936) (accu: 0.9755)
[epoch : 30] (l_loss: 0.05157) (t_loss: 0.07612) (accu: 0.9762)
[epoch : 31] (l_loss: 0.05180) (t_loss: 0.07771) (accu: 0.9761)
[epoch : 32] (l_loss: 0.05178) (t_loss: 0.07814) (accu: 0.9757)
[epoch : 33] (l_loss: 0.05196) (t_loss: 0.07809) (accu: 0.9753)
[epoch : 34] (l_loss: 0.05117) (t_loss: 0.07500) (accu: 0.9776)
[epoch : 35] (l_loss: 0.05179) (t_loss: 0.07729) (accu: 0.9767)
[epoch : 36] (l_loss: 0.05148) (t_loss: 0.07384) (accu: 0.9764)
[epoch : 37] (l_loss: 0.05190) (t_loss: 0.07868) (accu: 0.9748)
[epoch : 38] (l_loss: 0.05167) (t_loss: 0.07785) (accu: 0.9770)
[epoch : 39] (l_loss: 0.05194) (t_loss: 0.07660) (accu: 0.9771)
[epoch : 40] (l_loss: 0.05127) (t_loss: 0.07562) (accu: 0.9760)
[epoch : 41] (l_loss: 0.05132) (t_loss: 0.07678) (accu: 0.9763)
[epoch : 42] (l_loss: 0.05196) (t_loss: 0.07708) (accu: 0.9764)
[epoch : 43] (l_loss: 0.05187) (t_loss: 0.07569) (accu: 0.9774)
[epoch : 44] (l_loss: 0.05133) (t_loss: 0.07602) (accu: 0.9775)
[epoch : 45] (l_loss: 0.05138) (t_loss: 0.07578) (accu: 0.9775)
[epoch : 46] (l_loss: 0.05170) (t_loss: 0.07670) (accu: 0.9762)
[epoch : 47] (l_loss: 0.05168) (t_loss: 0.07688) (accu: 0.9757)
[epoch : 48] (l_loss: 0.05167) (t_loss: 0.07474) (accu: 0.9759)
[epoch : 49] (l_loss: 0.05144) (t_loss: 0.07647) (accu: 0.9768)
[epoch : 50] (l_loss: 0.05159) (t_loss: 0.08012) (accu: 0.9751)
Finish! (Best accu: 0.9778) (Time taken(sec) : 789.96) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (36352 | 179148)         16.87
fc1.weight   :      196000 (32883 | 163117)         16.78
fc2.weight   :        18750 (3146 | 15604)          16.78
fcout.weight :          750 (323 | 427)             43.07
------------------------------------------------------------
[Prune_iter : (9/21), Remaining weight : 16.87 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.31069) (accu: 0.1613)
[epoch : 1] (l_loss: 0.45585) (t_loss: 0.15486) (accu: 0.9578)
[epoch : 2] (l_loss: 0.13127) (t_loss: 0.11580) (accu: 0.9674)
[epoch : 3] (l_loss: 0.09983) (t_loss: 0.10165) (accu: 0.9704)
[epoch : 4] (l_loss: 0.08454) (t_loss: 0.09351) (accu: 0.9722)
[epoch : 5] (l_loss: 0.07465) (t_loss: 0.08836) (accu: 0.9736)
[epoch : 6] (l_loss: 0.06731) (t_loss: 0.08473) (accu: 0.9747)
[epoch : 7] (l_loss: 0.06345) (t_loss: 0.08151) (accu: 0.9762)
[epoch : 8] (l_loss: 0.06013) (t_loss: 0.08156) (accu: 0.9762)
[epoch : 9] (l_loss: 0.05802) (t_loss: 0.07669) (accu: 0.9767)
[epoch : 10] (l_loss: 0.05607) (t_loss: 0.07823) (accu: 0.9779)
[epoch : 11] (l_loss: 0.05530) (t_loss: 0.07654) (accu: 0.9771)
[epoch : 12] (l_loss: 0.05446) (t_loss: 0.08022) (accu: 0.9758)
[epoch : 13] (l_loss: 0.05393) (t_loss: 0.07709) (accu: 0.9763)
[epoch : 14] (l_loss: 0.05386) (t_loss: 0.07555) (accu: 0.9768)
[epoch : 15] (l_loss: 0.05372) (t_loss: 0.07706) (accu: 0.9760)
[epoch : 16] (l_loss: 0.05280) (t_loss: 0.07575) (accu: 0.9783)
[epoch : 17] (l_loss: 0.05261) (t_loss: 0.07705) (accu: 0.9765)
[epoch : 18] (l_loss: 0.05269) (t_loss: 0.07684) (accu: 0.9775)
[epoch : 19] (l_loss: 0.05244) (t_loss: 0.08007) (accu: 0.9760)
[epoch : 20] (l_loss: 0.05258) (t_loss: 0.07602) (accu: 0.9773)
[epoch : 21] (l_loss: 0.05284) (t_loss: 0.07468) (accu: 0.9785)
[epoch : 22] (l_loss: 0.05257) (t_loss: 0.07778) (accu: 0.9765)
[epoch : 23] (l_loss: 0.05230) (t_loss: 0.07613) (accu: 0.9772)
[epoch : 24] (l_loss: 0.05166) (t_loss: 0.07673) (accu: 0.9762)
[epoch : 25] (l_loss: 0.05205) (t_loss: 0.07841) (accu: 0.9767)
[epoch : 26] (l_loss: 0.05224) (t_loss: 0.07634) (accu: 0.9769)
[epoch : 27] (l_loss: 0.05214) (t_loss: 0.07451) (accu: 0.9776)
[epoch : 28] (l_loss: 0.05225) (t_loss: 0.07722) (accu: 0.9768)
[epoch : 29] (l_loss: 0.05174) (t_loss: 0.07566) (accu: 0.9763)
[epoch : 30] (l_loss: 0.05196) (t_loss: 0.07478) (accu: 0.9764)
[epoch : 31] (l_loss: 0.05213) (t_loss: 0.07658) (accu: 0.9772)
[epoch : 32] (l_loss: 0.05137) (t_loss: 0.07782) (accu: 0.9769)
[epoch : 33] (l_loss: 0.05196) (t_loss: 0.07554) (accu: 0.9773)
[epoch : 34] (l_loss: 0.05177) (t_loss: 0.07499) (accu: 0.9784)
[epoch : 35] (l_loss: 0.05180) (t_loss: 0.07584) (accu: 0.9760)
[epoch : 36] (l_loss: 0.05179) (t_loss: 0.07789) (accu: 0.9760)
[epoch : 37] (l_loss: 0.05184) (t_loss: 0.07443) (accu: 0.9777)
[epoch : 38] (l_loss: 0.05153) (t_loss: 0.07818) (accu: 0.9754)
[epoch : 39] (l_loss: 0.05178) (t_loss: 0.07421) (accu: 0.9784)
[epoch : 40] (l_loss: 0.05196) (t_loss: 0.07804) (accu: 0.9770)
[epoch : 41] (l_loss: 0.05201) (t_loss: 0.07772) (accu: 0.9761)
[epoch : 42] (l_loss: 0.05194) (t_loss: 0.07714) (accu: 0.9762)
[epoch : 43] (l_loss: 0.05203) (t_loss: 0.07845) (accu: 0.9761)
[epoch : 44] (l_loss: 0.05164) (t_loss: 0.07651) (accu: 0.9765)
[epoch : 45] (l_loss: 0.05207) (t_loss: 0.07953) (accu: 0.9744)
[epoch : 46] (l_loss: 0.05182) (t_loss: 0.07722) (accu: 0.9754)
[epoch : 47] (l_loss: 0.05196) (t_loss: 0.07640) (accu: 0.9760)
[epoch : 48] (l_loss: 0.05163) (t_loss: 0.07834) (accu: 0.9752)
[epoch : 49] (l_loss: 0.05176) (t_loss: 0.07504) (accu: 0.9759)
[epoch : 50] (l_loss: 0.05154) (t_loss: 0.07731) (accu: 0.9740)
Finish! (Best accu: 0.9785) (Time taken(sec) : 777.06) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (29115 | 186385)         13.51
fc1.weight   :      196000 (26307 | 169693)         13.42
fc2.weight   :        18750 (2517 | 16233)          13.42
fcout.weight :          750 (291 | 459)             38.80
------------------------------------------------------------
[Prune_iter : (10/21), Remaining weight : 13.51 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30378) (accu: 0.1515)
[epoch : 1] (l_loss: 0.45875) (t_loss: 0.16571) (accu: 0.9527)
[epoch : 2] (l_loss: 0.14062) (t_loss: 0.12215) (accu: 0.9657)
[epoch : 3] (l_loss: 0.10640) (t_loss: 0.10634) (accu: 0.9675)
[epoch : 4] (l_loss: 0.08856) (t_loss: 0.09634) (accu: 0.9709)
[epoch : 5] (l_loss: 0.07782) (t_loss: 0.09038) (accu: 0.9730)
[epoch : 6] (l_loss: 0.07033) (t_loss: 0.08994) (accu: 0.9738)
[epoch : 7] (l_loss: 0.06618) (t_loss: 0.08625) (accu: 0.9742)
[epoch : 8] (l_loss: 0.06269) (t_loss: 0.08114) (accu: 0.9746)
[epoch : 9] (l_loss: 0.06004) (t_loss: 0.07997) (accu: 0.9754)
[epoch : 10] (l_loss: 0.05858) (t_loss: 0.08128) (accu: 0.9751)
[epoch : 11] (l_loss: 0.05775) (t_loss: 0.07846) (accu: 0.9764)
[epoch : 12] (l_loss: 0.05638) (t_loss: 0.08102) (accu: 0.9762)
[epoch : 13] (l_loss: 0.05562) (t_loss: 0.08406) (accu: 0.9744)
[epoch : 14] (l_loss: 0.05526) (t_loss: 0.07858) (accu: 0.9772)
[epoch : 15] (l_loss: 0.05497) (t_loss: 0.08129) (accu: 0.9747)
[epoch : 16] (l_loss: 0.05482) (t_loss: 0.07662) (accu: 0.9772)
[epoch : 17] (l_loss: 0.05468) (t_loss: 0.07932) (accu: 0.9752)
[epoch : 18] (l_loss: 0.05394) (t_loss: 0.08126) (accu: 0.9752)
[epoch : 19] (l_loss: 0.05394) (t_loss: 0.08295) (accu: 0.9758)
[epoch : 20] (l_loss: 0.05408) (t_loss: 0.07672) (accu: 0.9762)
[epoch : 21] (l_loss: 0.05380) (t_loss: 0.08019) (accu: 0.9749)
[epoch : 22] (l_loss: 0.05385) (t_loss: 0.07693) (accu: 0.9768)
[epoch : 23] (l_loss: 0.05320) (t_loss: 0.07906) (accu: 0.9760)
[epoch : 24] (l_loss: 0.05403) (t_loss: 0.07886) (accu: 0.9769)
[epoch : 25] (l_loss: 0.05362) (t_loss: 0.07880) (accu: 0.9750)
[epoch : 26] (l_loss: 0.05337) (t_loss: 0.07606) (accu: 0.9760)
[epoch : 27] (l_loss: 0.05330) (t_loss: 0.07764) (accu: 0.9765)
[epoch : 28] (l_loss: 0.05305) (t_loss: 0.08017) (accu: 0.9759)
[epoch : 29] (l_loss: 0.05339) (t_loss: 0.07905) (accu: 0.9751)
[epoch : 30] (l_loss: 0.05329) (t_loss: 0.07615) (accu: 0.9764)
[epoch : 31] (l_loss: 0.05330) (t_loss: 0.07827) (accu: 0.9753)
[epoch : 32] (l_loss: 0.05266) (t_loss: 0.07673) (accu: 0.9764)
[epoch : 33] (l_loss: 0.05338) (t_loss: 0.07975) (accu: 0.9761)
[epoch : 34] (l_loss: 0.05285) (t_loss: 0.07625) (accu: 0.9780)
[epoch : 35] (l_loss: 0.05338) (t_loss: 0.07985) (accu: 0.9761)
[epoch : 36] (l_loss: 0.05319) (t_loss: 0.07899) (accu: 0.9758)
[epoch : 37] (l_loss: 0.05338) (t_loss: 0.07959) (accu: 0.9757)
[epoch : 38] (l_loss: 0.05362) (t_loss: 0.07659) (accu: 0.9771)
[epoch : 39] (l_loss: 0.05324) (t_loss: 0.08243) (accu: 0.9731)
[epoch : 40] (l_loss: 0.05305) (t_loss: 0.07693) (accu: 0.9762)
[epoch : 41] (l_loss: 0.05340) (t_loss: 0.07702) (accu: 0.9760)
[epoch : 42] (l_loss: 0.05307) (t_loss: 0.07610) (accu: 0.9774)
[epoch : 43] (l_loss: 0.05333) (t_loss: 0.07557) (accu: 0.9770)
[epoch : 44] (l_loss: 0.05289) (t_loss: 0.07632) (accu: 0.9766)
[epoch : 45] (l_loss: 0.05349) (t_loss: 0.07654) (accu: 0.9772)
[epoch : 46] (l_loss: 0.05315) (t_loss: 0.07764) (accu: 0.9767)
[epoch : 47] (l_loss: 0.05304) (t_loss: 0.07899) (accu: 0.9772)
[epoch : 48] (l_loss: 0.05308) (t_loss: 0.08031) (accu: 0.9752)
[epoch : 49] (l_loss: 0.05361) (t_loss: 0.07760) (accu: 0.9769)
[epoch : 50] (l_loss: 0.05361) (t_loss: 0.07844) (accu: 0.9755)
Finish! (Best accu: 0.9780) (Time taken(sec) : 786.54) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (23320 | 192180)         10.82
fc1.weight   :      196000 (21045 | 174955)         10.74
fc2.weight   :        18750 (2013 | 16737)          10.74
fcout.weight :          750 (262 | 488)             34.93
------------------------------------------------------------
[Prune_iter : (11/21), Remaining weight : 10.82 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29593) (accu: 0.1276)
[epoch : 1] (l_loss: 0.45667) (t_loss: 0.15919) (accu: 0.9543)
[epoch : 2] (l_loss: 0.13172) (t_loss: 0.11669) (accu: 0.9657)
[epoch : 3] (l_loss: 0.09829) (t_loss: 0.10102) (accu: 0.9706)
[epoch : 4] (l_loss: 0.08172) (t_loss: 0.09137) (accu: 0.9726)
[epoch : 5] (l_loss: 0.07191) (t_loss: 0.08607) (accu: 0.9733)
[epoch : 6] (l_loss: 0.06570) (t_loss: 0.08246) (accu: 0.9747)
[epoch : 7] (l_loss: 0.06167) (t_loss: 0.07960) (accu: 0.9761)
[epoch : 8] (l_loss: 0.05862) (t_loss: 0.07885) (accu: 0.9768)
[epoch : 9] (l_loss: 0.05690) (t_loss: 0.08045) (accu: 0.9753)
[epoch : 10] (l_loss: 0.05521) (t_loss: 0.07883) (accu: 0.9773)
[epoch : 11] (l_loss: 0.05438) (t_loss: 0.07730) (accu: 0.9764)
[epoch : 12] (l_loss: 0.05395) (t_loss: 0.08132) (accu: 0.9753)
[epoch : 13] (l_loss: 0.05329) (t_loss: 0.07565) (accu: 0.9762)
[epoch : 14] (l_loss: 0.05298) (t_loss: 0.07866) (accu: 0.9768)
[epoch : 15] (l_loss: 0.05295) (t_loss: 0.07878) (accu: 0.9746)
[epoch : 16] (l_loss: 0.05259) (t_loss: 0.07687) (accu: 0.9759)
[epoch : 17] (l_loss: 0.05208) (t_loss: 0.07851) (accu: 0.9777)
[epoch : 18] (l_loss: 0.05233) (t_loss: 0.07777) (accu: 0.9764)
[epoch : 19] (l_loss: 0.05226) (t_loss: 0.07593) (accu: 0.9768)
[epoch : 20] (l_loss: 0.05190) (t_loss: 0.08089) (accu: 0.9752)
[epoch : 21] (l_loss: 0.05183) (t_loss: 0.07695) (accu: 0.9766)
[epoch : 22] (l_loss: 0.05220) (t_loss: 0.07735) (accu: 0.9755)
[epoch : 23] (l_loss: 0.05188) (t_loss: 0.08040) (accu: 0.9746)
[epoch : 24] (l_loss: 0.05176) (t_loss: 0.07914) (accu: 0.9756)
[epoch : 25] (l_loss: 0.05191) (t_loss: 0.07522) (accu: 0.9773)
[epoch : 26] (l_loss: 0.05203) (t_loss: 0.07498) (accu: 0.9774)
[epoch : 27] (l_loss: 0.05179) (t_loss: 0.07844) (accu: 0.9756)
[epoch : 28] (l_loss: 0.05188) (t_loss: 0.07633) (accu: 0.9762)
[epoch : 29] (l_loss: 0.05170) (t_loss: 0.07641) (accu: 0.9767)
[epoch : 30] (l_loss: 0.05152) (t_loss: 0.07734) (accu: 0.9767)
[epoch : 31] (l_loss: 0.05194) (t_loss: 0.07637) (accu: 0.9769)
[epoch : 32] (l_loss: 0.05153) (t_loss: 0.08119) (accu: 0.9743)
[epoch : 33] (l_loss: 0.05188) (t_loss: 0.07884) (accu: 0.9760)
[epoch : 34] (l_loss: 0.05142) (t_loss: 0.07619) (accu: 0.9764)
[epoch : 35] (l_loss: 0.05202) (t_loss: 0.07789) (accu: 0.9761)
[epoch : 36] (l_loss: 0.05191) (t_loss: 0.07704) (accu: 0.9771)
[epoch : 37] (l_loss: 0.05168) (t_loss: 0.07483) (accu: 0.9767)
[epoch : 38] (l_loss: 0.05131) (t_loss: 0.07657) (accu: 0.9766)
[epoch : 39] (l_loss: 0.05164) (t_loss: 0.07695) (accu: 0.9769)
[epoch : 40] (l_loss: 0.05180) (t_loss: 0.07532) (accu: 0.9768)
[epoch : 41] (l_loss: 0.05167) (t_loss: 0.07544) (accu: 0.9766)
[epoch : 42] (l_loss: 0.05107) (t_loss: 0.07639) (accu: 0.9778)
[epoch : 43] (l_loss: 0.05178) (t_loss: 0.07716) (accu: 0.9762)
[epoch : 44] (l_loss: 0.05163) (t_loss: 0.07727) (accu: 0.9759)
[epoch : 45] (l_loss: 0.05164) (t_loss: 0.07739) (accu: 0.9758)
[epoch : 46] (l_loss: 0.05192) (t_loss: 0.07591) (accu: 0.9765)
[epoch : 47] (l_loss: 0.05182) (t_loss: 0.07608) (accu: 0.9764)
[epoch : 48] (l_loss: 0.05149) (t_loss: 0.07959) (accu: 0.9753)
[epoch : 49] (l_loss: 0.05146) (t_loss: 0.07731) (accu: 0.9774)
[epoch : 50] (l_loss: 0.05183) (t_loss: 0.07821) (accu: 0.9761)
Finish! (Best accu: 0.9778) (Time taken(sec) : 783.34) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (18682 | 196818)          8.67
fc1.weight   :      196000 (16836 | 179164)          8.59
fc2.weight   :        18750 (1611 | 17139)           8.59
fcout.weight :          750 (235 | 515)             31.33
------------------------------------------------------------
[Prune_iter : (12/21), Remaining weight : 8.67 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29682) (accu: 0.1420)
[epoch : 1] (l_loss: 0.45860) (t_loss: 0.16796) (accu: 0.9531)
[epoch : 2] (l_loss: 0.13981) (t_loss: 0.12184) (accu: 0.9630)
[epoch : 3] (l_loss: 0.10436) (t_loss: 0.10440) (accu: 0.9683)
[epoch : 4] (l_loss: 0.08759) (t_loss: 0.09347) (accu: 0.9726)
[epoch : 5] (l_loss: 0.07728) (t_loss: 0.08839) (accu: 0.9730)
[epoch : 6] (l_loss: 0.07009) (t_loss: 0.08686) (accu: 0.9741)
[epoch : 7] (l_loss: 0.06558) (t_loss: 0.08338) (accu: 0.9756)
[epoch : 8] (l_loss: 0.06215) (t_loss: 0.08404) (accu: 0.9756)
[epoch : 9] (l_loss: 0.06051) (t_loss: 0.08282) (accu: 0.9748)
[epoch : 10] (l_loss: 0.05824) (t_loss: 0.07962) (accu: 0.9773)
[epoch : 11] (l_loss: 0.05712) (t_loss: 0.08151) (accu: 0.9760)
[epoch : 12] (l_loss: 0.05646) (t_loss: 0.07895) (accu: 0.9771)
[epoch : 13] (l_loss: 0.05568) (t_loss: 0.07608) (accu: 0.9772)
[epoch : 14] (l_loss: 0.05486) (t_loss: 0.07765) (accu: 0.9757)
[epoch : 15] (l_loss: 0.05523) (t_loss: 0.07734) (accu: 0.9772)
[epoch : 16] (l_loss: 0.05470) (t_loss: 0.07881) (accu: 0.9769)
[epoch : 17] (l_loss: 0.05468) (t_loss: 0.08081) (accu: 0.9753)
[epoch : 18] (l_loss: 0.05421) (t_loss: 0.07691) (accu: 0.9766)
[epoch : 19] (l_loss: 0.05400) (t_loss: 0.07753) (accu: 0.9764)
[epoch : 20] (l_loss: 0.05387) (t_loss: 0.07841) (accu: 0.9771)
[epoch : 21] (l_loss: 0.05364) (t_loss: 0.07769) (accu: 0.9760)
[epoch : 22] (l_loss: 0.05346) (t_loss: 0.07774) (accu: 0.9771)
[epoch : 23] (l_loss: 0.05351) (t_loss: 0.07883) (accu: 0.9771)
[epoch : 24] (l_loss: 0.05362) (t_loss: 0.07997) (accu: 0.9750)
[epoch : 25] (l_loss: 0.05339) (t_loss: 0.07906) (accu: 0.9752)
[epoch : 26] (l_loss: 0.05390) (t_loss: 0.07754) (accu: 0.9759)
[epoch : 27] (l_loss: 0.05351) (t_loss: 0.07676) (accu: 0.9770)
[epoch : 28] (l_loss: 0.05338) (t_loss: 0.07860) (accu: 0.9766)
[epoch : 29] (l_loss: 0.05331) (t_loss: 0.07822) (accu: 0.9751)
[epoch : 30] (l_loss: 0.05377) (t_loss: 0.07707) (accu: 0.9763)
[epoch : 31] (l_loss: 0.05318) (t_loss: 0.07755) (accu: 0.9757)
[epoch : 32] (l_loss: 0.05279) (t_loss: 0.07793) (accu: 0.9758)
[epoch : 33] (l_loss: 0.05320) (t_loss: 0.07716) (accu: 0.9769)
[epoch : 34] (l_loss: 0.05329) (t_loss: 0.07544) (accu: 0.9767)
[epoch : 35] (l_loss: 0.05334) (t_loss: 0.07737) (accu: 0.9759)
[epoch : 36] (l_loss: 0.05314) (t_loss: 0.07648) (accu: 0.9758)
[epoch : 37] (l_loss: 0.05303) (t_loss: 0.07789) (accu: 0.9745)
[epoch : 38] (l_loss: 0.05281) (t_loss: 0.07625) (accu: 0.9779)
[epoch : 39] (l_loss: 0.05301) (t_loss: 0.08182) (accu: 0.9762)
[epoch : 40] (l_loss: 0.05279) (t_loss: 0.07638) (accu: 0.9765)
[epoch : 41] (l_loss: 0.05272) (t_loss: 0.07738) (accu: 0.9755)
[epoch : 42] (l_loss: 0.05274) (t_loss: 0.07847) (accu: 0.9777)
[epoch : 43] (l_loss: 0.05340) (t_loss: 0.07548) (accu: 0.9769)
[epoch : 44] (l_loss: 0.05339) (t_loss: 0.07594) (accu: 0.9767)
[epoch : 45] (l_loss: 0.05283) (t_loss: 0.07875) (accu: 0.9766)
[epoch : 46] (l_loss: 0.05293) (t_loss: 0.07515) (accu: 0.9769)
[epoch : 47] (l_loss: 0.05303) (t_loss: 0.08118) (accu: 0.9752)
[epoch : 48] (l_loss: 0.05338) (t_loss: 0.08010) (accu: 0.9740)
[epoch : 49] (l_loss: 0.05328) (t_loss: 0.07672) (accu: 0.9760)
[epoch : 50] (l_loss: 0.05327) (t_loss: 0.08101) (accu: 0.9768)
Finish! (Best accu: 0.9779) (Time taken(sec) : 784.92) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (14969 | 200531)          6.95
fc1.weight   :      196000 (13469 | 182531)          6.87
fc2.weight   :        18750 (1288 | 17462)           6.87
fcout.weight :          750 (212 | 538)             28.27
------------------------------------------------------------
[Prune_iter : (13/21), Remaining weight : 6.95 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30053) (accu: 0.1537)
[epoch : 1] (l_loss: 0.45937) (t_loss: 0.16615) (accu: 0.9534)
[epoch : 2] (l_loss: 0.13818) (t_loss: 0.11853) (accu: 0.9655)
[epoch : 3] (l_loss: 0.10395) (t_loss: 0.10094) (accu: 0.9695)
[epoch : 4] (l_loss: 0.08636) (t_loss: 0.09380) (accu: 0.9719)
[epoch : 5] (l_loss: 0.07643) (t_loss: 0.08481) (accu: 0.9744)
[epoch : 6] (l_loss: 0.06933) (t_loss: 0.08360) (accu: 0.9750)
[epoch : 7] (l_loss: 0.06496) (t_loss: 0.08200) (accu: 0.9748)
[epoch : 8] (l_loss: 0.06209) (t_loss: 0.08033) (accu: 0.9754)
[epoch : 9] (l_loss: 0.06023) (t_loss: 0.08000) (accu: 0.9763)
[epoch : 10] (l_loss: 0.05821) (t_loss: 0.07893) (accu: 0.9764)
[epoch : 11] (l_loss: 0.05673) (t_loss: 0.08404) (accu: 0.9738)
[epoch : 12] (l_loss: 0.05628) (t_loss: 0.08067) (accu: 0.9753)
[epoch : 13] (l_loss: 0.05545) (t_loss: 0.08148) (accu: 0.9769)
[epoch : 14] (l_loss: 0.05509) (t_loss: 0.07780) (accu: 0.9770)
[epoch : 15] (l_loss: 0.05460) (t_loss: 0.07750) (accu: 0.9774)
[epoch : 16] (l_loss: 0.05467) (t_loss: 0.07662) (accu: 0.9765)
[epoch : 17] (l_loss: 0.05468) (t_loss: 0.08015) (accu: 0.9771)
[epoch : 18] (l_loss: 0.05388) (t_loss: 0.07891) (accu: 0.9752)
[epoch : 19] (l_loss: 0.05342) (t_loss: 0.08158) (accu: 0.9756)
[epoch : 20] (l_loss: 0.05358) (t_loss: 0.07887) (accu: 0.9754)
[epoch : 21] (l_loss: 0.05382) (t_loss: 0.07841) (accu: 0.9768)
[epoch : 22] (l_loss: 0.05356) (t_loss: 0.08021) (accu: 0.9742)
[epoch : 23] (l_loss: 0.05383) (t_loss: 0.07897) (accu: 0.9756)
[epoch : 24] (l_loss: 0.05377) (t_loss: 0.08045) (accu: 0.9743)
[epoch : 25] (l_loss: 0.05354) (t_loss: 0.07748) (accu: 0.9768)
[epoch : 26] (l_loss: 0.05384) (t_loss: 0.07831) (accu: 0.9767)
[epoch : 27] (l_loss: 0.05336) (t_loss: 0.07806) (accu: 0.9756)
[epoch : 28] (l_loss: 0.05323) (t_loss: 0.07840) (accu: 0.9755)
[epoch : 29] (l_loss: 0.05287) (t_loss: 0.07761) (accu: 0.9768)
[epoch : 30] (l_loss: 0.05301) (t_loss: 0.08017) (accu: 0.9770)
[epoch : 31] (l_loss: 0.05296) (t_loss: 0.07914) (accu: 0.9765)
[epoch : 32] (l_loss: 0.05321) (t_loss: 0.07648) (accu: 0.9763)
[epoch : 33] (l_loss: 0.05296) (t_loss: 0.07509) (accu: 0.9778)
[epoch : 34] (l_loss: 0.05320) (t_loss: 0.07823) (accu: 0.9769)
[epoch : 35] (l_loss: 0.05310) (t_loss: 0.07723) (accu: 0.9768)
[epoch : 36] (l_loss: 0.05324) (t_loss: 0.07850) (accu: 0.9762)
[epoch : 37] (l_loss: 0.05342) (t_loss: 0.07880) (accu: 0.9766)
[epoch : 38] (l_loss: 0.05327) (t_loss: 0.07811) (accu: 0.9756)
[epoch : 39] (l_loss: 0.05347) (t_loss: 0.07764) (accu: 0.9776)
[epoch : 40] (l_loss: 0.05294) (t_loss: 0.07821) (accu: 0.9747)
[epoch : 41] (l_loss: 0.05346) (t_loss: 0.08035) (accu: 0.9755)
[epoch : 42] (l_loss: 0.05304) (t_loss: 0.07728) (accu: 0.9777)
[epoch : 43] (l_loss: 0.05325) (t_loss: 0.07938) (accu: 0.9756)
[epoch : 44] (l_loss: 0.05341) (t_loss: 0.07884) (accu: 0.9759)
[epoch : 45] (l_loss: 0.05278) (t_loss: 0.07613) (accu: 0.9766)
[epoch : 46] (l_loss: 0.05291) (t_loss: 0.07903) (accu: 0.9762)
[epoch : 47] (l_loss: 0.05320) (t_loss: 0.07663) (accu: 0.9768)
[epoch : 48] (l_loss: 0.05317) (t_loss: 0.07671) (accu: 0.9752)
[epoch : 49] (l_loss: 0.05326) (t_loss: 0.07665) (accu: 0.9767)
[epoch : 50] (l_loss: 0.05312) (t_loss: 0.07734) (accu: 0.9761)
Finish! (Best accu: 0.9778) (Time taken(sec) : 791.75) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      215500 (11997 | 203503)          5.57
fc1.weight   :      196000 (10775 | 185225)          5.50
fc2.weight   :        18750 (1031 | 17719)           5.50
fcout.weight :          750 (191 | 559)             25.47
------------------------------------------------------------
[Prune_iter : (14/21), Remaining weight : 5.57 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30295) (accu: 0.1199)
[epoch : 1] (l_loss: 0.45381) (t_loss: 0.16088) (accu: 0.9543)
[epoch : 2] (l_loss: 0.13210) (t_loss: 0.11830) (accu: 0.9643)
[epoch : 3] (l_loss: 0.09993) (t_loss: 0.10159) (accu: 0.9686)
[epoch : 4] (l_loss: 0.08508) (t_loss: 0.09281) (accu: 0.9716)
[epoch : 5] (l_loss: 0.07583) (t_loss: 0.08869) (accu: 0.9734)
[epoch : 6] (l_loss: 0.07026) (t_loss: 0.08947) (accu: 0.9743)
[epoch : 7] (l_loss: 0.06565) (t_loss: 0.08570) (accu: 0.9743)
[epoch : 8] (l_loss: 0.06237) (t_loss: 0.08141) (accu: 0.9760)
[epoch : 9] (l_loss: 0.05953) (t_loss: 0.08108) (accu: 0.9760)
[epoch : 10] (l_loss: 0.05835) (t_loss: 0.08134) (accu: 0.9754)
[epoch : 11] (l_loss: 0.05612) (t_loss: 0.07702) (accu: 0.9767)
[epoch : 12] (l_loss: 0.05519) (t_loss: 0.08067) (accu: 0.9757)
[epoch : 13] (l_loss: 0.05384) (t_loss: 0.07982) (accu: 0.9754)
[epoch : 14] (l_loss: 0.05350) (t_loss: 0.07886) (accu: 0.9755)
[epoch : 15] (l_loss: 0.05295) (t_loss: 0.07842) (accu: 0.9764)
[epoch : 16] (l_loss: 0.05299) (t_loss: 0.07684) (accu: 0.9764)
[epoch : 17] (l_loss: 0.05257) (t_loss: 0.07662) (accu: 0.9767)
[epoch : 18] (l_loss: 0.05252) (t_loss: 0.07765) (accu: 0.9771)
[epoch : 19] (l_loss: 0.05196) (t_loss: 0.07805) (accu: 0.9753)
[epoch : 20] (l_loss: 0.05220) (t_loss: 0.07706) (accu: 0.9770)
[epoch : 21] (l_loss: 0.05202) (t_loss: 0.07587) (accu: 0.9773)
[epoch : 22] (l_loss: 0.05165) (t_loss: 0.07693) (accu: 0.9756)
[epoch : 23] (l_loss: 0.05189) (t_loss: 0.07559) (accu: 0.9782)
[epoch : 24] (l_loss: 0.05124) (t_loss: 0.07597) (accu: 0.9774)
[epoch : 25] (l_loss: 0.05169) (t_loss: 0.07757) (accu: 0.9768)
[epoch : 26] (l_loss: 0.05197) (t_loss: 0.07862) (accu: 0.9769)
[epoch : 27] (l_loss: 0.05179) (t_loss: 0.07488) (accu: 0.9766)
[epoch : 28] (l_loss: 0.05190) (t_loss: 0.07699) (accu: 0.9767)
[epoch : 29] (l_loss: 0.05183) (t_loss: 0.07775) (accu: 0.9767)
[epoch : 30] (l_loss: 0.05196) (t_loss: 0.07580) (accu: 0.9751)
[epoch : 31] (l_loss: 0.05155) (t_loss: 0.07721) (accu: 0.9773)
[epoch : 32] (l_loss: 0.05161) (t_loss: 0.07734) (accu: 0.9766)
[epoch : 33] (l_loss: 0.05182) (t_loss: 0.07845) (accu: 0.9759)
[epoch : 34] (l_loss: 0.05181) (t_loss: 0.07794) (accu: 0.9752)
[epoch : 35] (l_loss: 0.05159) (t_loss: 0.07616) (accu: 0.9768)
[epoch : 36] (l_loss: 0.05162) (t_loss: 0.07883) (accu: 0.9758)
[epoch : 37] (l_loss: 0.05168) (t_loss: 0.07540) (accu: 0.9771)
[epoch : 38] (l_loss: 0.05188) (t_loss: 0.07370) (accu: 0.9784)
[epoch : 39] (l_loss: 0.05183) (t_loss: 0.07619) (accu: 0.9761)
[epoch : 40] (l_loss: 0.05171) (t_loss: 0.07597) (accu: 0.9778)
[epoch : 41] (l_loss: 0.05165) (t_loss: 0.07609) (accu: 0.9760)
[epoch : 42] (l_loss: 0.05109) (t_loss: 0.07866) (accu: 0.9760)
[epoch : 43] (l_loss: 0.05162) (t_loss: 0.07674) (accu: 0.9766)
[epoch : 44] (l_loss: 0.05117) (t_loss: 0.07852) (accu: 0.9766)
[epoch : 45] (l_loss: 0.05158) (t_loss: 0.07937) (accu: 0.9756)
[epoch : 46] (l_loss: 0.05149) (t_loss: 0.07730) (accu: 0.9751)
[epoch : 47] (l_loss: 0.05161) (t_loss: 0.07759) (accu: 0.9774)
[epoch : 48] (l_loss: 0.05150) (t_loss: 0.07905) (accu: 0.9766)
[epoch : 49] (l_loss: 0.05145) (t_loss: 0.08013) (accu: 0.9759)
[epoch : 50] (l_loss: 0.05172) (t_loss: 0.07701) (accu: 0.9756)
Finish! (Best accu: 0.9784) (Time taken(sec) : 779.03) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (9617 | 205883)          4.46
fc1.weight   :       196000 (8620 | 187380)          4.40
fc2.weight   :        18750 (825 | 17925)            4.40
fcout.weight :          750 (172 | 578)             22.93
------------------------------------------------------------
[Prune_iter : (15/21), Remaining weight : 4.46 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30242) (accu: 0.1405)
[epoch : 1] (l_loss: 0.45348) (t_loss: 0.16364) (accu: 0.9526)
[epoch : 2] (l_loss: 0.13777) (t_loss: 0.12087) (accu: 0.9645)
[epoch : 3] (l_loss: 0.10497) (t_loss: 0.10574) (accu: 0.9675)
[epoch : 4] (l_loss: 0.08856) (t_loss: 0.09554) (accu: 0.9719)
[epoch : 5] (l_loss: 0.07821) (t_loss: 0.08825) (accu: 0.9721)
[epoch : 6] (l_loss: 0.07071) (t_loss: 0.09096) (accu: 0.9717)
[epoch : 7] (l_loss: 0.06644) (t_loss: 0.08195) (accu: 0.9747)
[epoch : 8] (l_loss: 0.06261) (t_loss: 0.08351) (accu: 0.9748)
[epoch : 9] (l_loss: 0.05991) (t_loss: 0.08002) (accu: 0.9752)
[epoch : 10] (l_loss: 0.05865) (t_loss: 0.07943) (accu: 0.9775)
[epoch : 11] (l_loss: 0.05752) (t_loss: 0.07887) (accu: 0.9772)
[epoch : 12] (l_loss: 0.05632) (t_loss: 0.07686) (accu: 0.9767)
[epoch : 13] (l_loss: 0.05558) (t_loss: 0.08230) (accu: 0.9751)
[epoch : 14] (l_loss: 0.05560) (t_loss: 0.07763) (accu: 0.9758)
[epoch : 15] (l_loss: 0.05516) (t_loss: 0.07790) (accu: 0.9759)
[epoch : 16] (l_loss: 0.05498) (t_loss: 0.07831) (accu: 0.9770)
[epoch : 17] (l_loss: 0.05429) (t_loss: 0.07935) (accu: 0.9753)
[epoch : 18] (l_loss: 0.05409) (t_loss: 0.07717) (accu: 0.9767)
[epoch : 19] (l_loss: 0.05397) (t_loss: 0.08009) (accu: 0.9759)
[epoch : 20] (l_loss: 0.05403) (t_loss: 0.07705) (accu: 0.9773)
[epoch : 21] (l_loss: 0.05365) (t_loss: 0.07803) (accu: 0.9760)
[epoch : 22] (l_loss: 0.05360) (t_loss: 0.07863) (accu: 0.9759)
[epoch : 23] (l_loss: 0.05408) (t_loss: 0.07564) (accu: 0.9768)
[epoch : 24] (l_loss: 0.05364) (t_loss: 0.07852) (accu: 0.9775)
[epoch : 25] (l_loss: 0.05362) (t_loss: 0.08079) (accu: 0.9761)
[epoch : 26] (l_loss: 0.05332) (t_loss: 0.07801) (accu: 0.9756)
[epoch : 27] (l_loss: 0.05318) (t_loss: 0.07849) (accu: 0.9758)
[epoch : 28] (l_loss: 0.05310) (t_loss: 0.07713) (accu: 0.9766)
[epoch : 29] (l_loss: 0.05346) (t_loss: 0.07592) (accu: 0.9765)
[epoch : 30] (l_loss: 0.05306) (t_loss: 0.07670) (accu: 0.9771)
[epoch : 31] (l_loss: 0.05331) (t_loss: 0.07734) (accu: 0.9767)
[epoch : 32] (l_loss: 0.05331) (t_loss: 0.07693) (accu: 0.9750)
[epoch : 33] (l_loss: 0.05360) (t_loss: 0.08012) (accu: 0.9762)
[epoch : 34] (l_loss: 0.05366) (t_loss: 0.07635) (accu: 0.9775)
[epoch : 35] (l_loss: 0.05304) (t_loss: 0.07721) (accu: 0.9764)
[epoch : 36] (l_loss: 0.05301) (t_loss: 0.07727) (accu: 0.9758)
[epoch : 37] (l_loss: 0.05292) (t_loss: 0.07722) (accu: 0.9758)
[epoch : 38] (l_loss: 0.05318) (t_loss: 0.07643) (accu: 0.9772)
[epoch : 39] (l_loss: 0.05310) (t_loss: 0.07776) (accu: 0.9767)
[epoch : 40] (l_loss: 0.05352) (t_loss: 0.07707) (accu: 0.9773)
[epoch : 41] (l_loss: 0.05289) (t_loss: 0.07897) (accu: 0.9743)
[epoch : 42] (l_loss: 0.05342) (t_loss: 0.07916) (accu: 0.9763)
[epoch : 43] (l_loss: 0.05302) (t_loss: 0.07503) (accu: 0.9771)
[epoch : 44] (l_loss: 0.05327) (t_loss: 0.07720) (accu: 0.9780)
[epoch : 45] (l_loss: 0.05329) (t_loss: 0.07807) (accu: 0.9764)
[epoch : 46] (l_loss: 0.05292) (t_loss: 0.07560) (accu: 0.9770)
[epoch : 47] (l_loss: 0.05321) (t_loss: 0.07657) (accu: 0.9760)
[epoch : 48] (l_loss: 0.05341) (t_loss: 0.07845) (accu: 0.9766)
[epoch : 49] (l_loss: 0.05343) (t_loss: 0.07752) (accu: 0.9765)
[epoch : 50] (l_loss: 0.05334) (t_loss: 0.07725) (accu: 0.9760)
Finish! (Best accu: 0.9780) (Time taken(sec) : 770.67) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (7710 | 207790)          3.58
fc1.weight   :       196000 (6896 | 189104)          3.52
fc2.weight   :        18750 (660 | 18090)            3.52
fcout.weight :          750 (154 | 596)             20.53
------------------------------------------------------------
[Prune_iter : (16/21), Remaining weight : 3.58 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30225) (accu: 0.1271)
[epoch : 1] (l_loss: 0.45114) (t_loss: 0.15722) (accu: 0.9557)
[epoch : 2] (l_loss: 0.13442) (t_loss: 0.11975) (accu: 0.9651)
[epoch : 3] (l_loss: 0.10305) (t_loss: 0.10593) (accu: 0.9680)
[epoch : 4] (l_loss: 0.08808) (t_loss: 0.09780) (accu: 0.9685)
[epoch : 5] (l_loss: 0.07909) (t_loss: 0.09397) (accu: 0.9709)
[epoch : 6] (l_loss: 0.07306) (t_loss: 0.09163) (accu: 0.9714)
[epoch : 7] (l_loss: 0.06863) (t_loss: 0.08858) (accu: 0.9720)
[epoch : 8] (l_loss: 0.06567) (t_loss: 0.08478) (accu: 0.9733)
[epoch : 9] (l_loss: 0.06324) (t_loss: 0.08454) (accu: 0.9745)
[epoch : 10] (l_loss: 0.06154) (t_loss: 0.08174) (accu: 0.9749)
[epoch : 11] (l_loss: 0.06018) (t_loss: 0.08345) (accu: 0.9740)
[epoch : 12] (l_loss: 0.05843) (t_loss: 0.08312) (accu: 0.9739)
[epoch : 13] (l_loss: 0.05782) (t_loss: 0.08050) (accu: 0.9759)
[epoch : 14] (l_loss: 0.05718) (t_loss: 0.07998) (accu: 0.9751)
[epoch : 15] (l_loss: 0.05638) (t_loss: 0.07984) (accu: 0.9764)
[epoch : 16] (l_loss: 0.05562) (t_loss: 0.07918) (accu: 0.9758)
[epoch : 17] (l_loss: 0.05542) (t_loss: 0.07741) (accu: 0.9750)
[epoch : 18] (l_loss: 0.05526) (t_loss: 0.07806) (accu: 0.9755)
[epoch : 19] (l_loss: 0.05498) (t_loss: 0.08005) (accu: 0.9759)
[epoch : 20] (l_loss: 0.05517) (t_loss: 0.07813) (accu: 0.9762)
[epoch : 21] (l_loss: 0.05501) (t_loss: 0.07913) (accu: 0.9751)
[epoch : 22] (l_loss: 0.05466) (t_loss: 0.07957) (accu: 0.9754)
[epoch : 23] (l_loss: 0.05489) (t_loss: 0.07962) (accu: 0.9760)
[epoch : 24] (l_loss: 0.05487) (t_loss: 0.07920) (accu: 0.9753)
[epoch : 25] (l_loss: 0.05441) (t_loss: 0.07920) (accu: 0.9753)
[epoch : 26] (l_loss: 0.05445) (t_loss: 0.07661) (accu: 0.9747)
[epoch : 27] (l_loss: 0.05462) (t_loss: 0.07869) (accu: 0.9751)
[epoch : 28] (l_loss: 0.05417) (t_loss: 0.07704) (accu: 0.9757)
[epoch : 29] (l_loss: 0.05425) (t_loss: 0.07779) (accu: 0.9753)
[epoch : 30] (l_loss: 0.05453) (t_loss: 0.07932) (accu: 0.9758)
[epoch : 31] (l_loss: 0.05423) (t_loss: 0.08084) (accu: 0.9751)
[epoch : 32] (l_loss: 0.05462) (t_loss: 0.07727) (accu: 0.9755)
[epoch : 33] (l_loss: 0.05409) (t_loss: 0.08497) (accu: 0.9743)
[epoch : 34] (l_loss: 0.05386) (t_loss: 0.07877) (accu: 0.9749)
[epoch : 35] (l_loss: 0.05405) (t_loss: 0.07691) (accu: 0.9763)
[epoch : 36] (l_loss: 0.05381) (t_loss: 0.07999) (accu: 0.9759)
[epoch : 37] (l_loss: 0.05413) (t_loss: 0.07703) (accu: 0.9759)
[epoch : 38] (l_loss: 0.05388) (t_loss: 0.08015) (accu: 0.9745)
[epoch : 39] (l_loss: 0.05427) (t_loss: 0.07859) (accu: 0.9747)
[epoch : 40] (l_loss: 0.05452) (t_loss: 0.07978) (accu: 0.9745)
[epoch : 41] (l_loss: 0.05413) (t_loss: 0.08151) (accu: 0.9738)
[epoch : 42] (l_loss: 0.05431) (t_loss: 0.07802) (accu: 0.9750)
[epoch : 43] (l_loss: 0.05407) (t_loss: 0.07807) (accu: 0.9751)
[epoch : 44] (l_loss: 0.05391) (t_loss: 0.07785) (accu: 0.9748)
[epoch : 45] (l_loss: 0.05413) (t_loss: 0.07944) (accu: 0.9756)
[epoch : 46] (l_loss: 0.05490) (t_loss: 0.07816) (accu: 0.9754)
[epoch : 47] (l_loss: 0.05464) (t_loss: 0.07811) (accu: 0.9750)
[epoch : 48] (l_loss: 0.05419) (t_loss: 0.08113) (accu: 0.9756)
[epoch : 49] (l_loss: 0.05441) (t_loss: 0.07788) (accu: 0.9761)
[epoch : 50] (l_loss: 0.05437) (t_loss: 0.07903) (accu: 0.9753)
Finish! (Best accu: 0.9764) (Time taken(sec) : 808.51) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (6184 | 209316)          2.87
fc1.weight   :       196000 (5517 | 190483)          2.81
fc2.weight   :        18750 (528 | 18222)            2.82
fcout.weight :          750 (139 | 611)             18.53
------------------------------------------------------------
[Prune_iter : (17/21), Remaining weight : 2.87 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30015) (accu: 0.1031)
[epoch : 1] (l_loss: 0.45640) (t_loss: 0.16722) (accu: 0.9514)
[epoch : 2] (l_loss: 0.13918) (t_loss: 0.12384) (accu: 0.9627)
[epoch : 3] (l_loss: 0.10626) (t_loss: 0.10758) (accu: 0.9676)
[epoch : 4] (l_loss: 0.09002) (t_loss: 0.09874) (accu: 0.9695)
[epoch : 5] (l_loss: 0.08001) (t_loss: 0.09319) (accu: 0.9707)
[epoch : 6] (l_loss: 0.07396) (t_loss: 0.08770) (accu: 0.9737)
[epoch : 7] (l_loss: 0.07001) (t_loss: 0.08563) (accu: 0.9740)
[epoch : 8] (l_loss: 0.06677) (t_loss: 0.08524) (accu: 0.9735)
[epoch : 9] (l_loss: 0.06421) (t_loss: 0.08503) (accu: 0.9737)
[epoch : 10] (l_loss: 0.06256) (t_loss: 0.08465) (accu: 0.9742)
[epoch : 11] (l_loss: 0.06126) (t_loss: 0.08111) (accu: 0.9754)
[epoch : 12] (l_loss: 0.06015) (t_loss: 0.08368) (accu: 0.9747)
[epoch : 13] (l_loss: 0.05993) (t_loss: 0.08345) (accu: 0.9737)
[epoch : 14] (l_loss: 0.05850) (t_loss: 0.08034) (accu: 0.9745)
[epoch : 15] (l_loss: 0.05812) (t_loss: 0.08110) (accu: 0.9754)
[epoch : 16] (l_loss: 0.05764) (t_loss: 0.08055) (accu: 0.9755)
[epoch : 17] (l_loss: 0.05749) (t_loss: 0.08027) (accu: 0.9745)
[epoch : 18] (l_loss: 0.05707) (t_loss: 0.07717) (accu: 0.9761)
[epoch : 19] (l_loss: 0.05702) (t_loss: 0.08068) (accu: 0.9748)
[epoch : 20] (l_loss: 0.05648) (t_loss: 0.08160) (accu: 0.9744)
[epoch : 21] (l_loss: 0.05697) (t_loss: 0.07723) (accu: 0.9758)
[epoch : 22] (l_loss: 0.05621) (t_loss: 0.08044) (accu: 0.9755)
[epoch : 23] (l_loss: 0.05648) (t_loss: 0.07828) (accu: 0.9766)
[epoch : 24] (l_loss: 0.05598) (t_loss: 0.08114) (accu: 0.9755)
[epoch : 25] (l_loss: 0.05632) (t_loss: 0.07822) (accu: 0.9765)
[epoch : 26] (l_loss: 0.05598) (t_loss: 0.07970) (accu: 0.9759)
[epoch : 27] (l_loss: 0.05614) (t_loss: 0.08086) (accu: 0.9745)
[epoch : 28] (l_loss: 0.05616) (t_loss: 0.07908) (accu: 0.9749)
[epoch : 29] (l_loss: 0.05617) (t_loss: 0.08155) (accu: 0.9743)
[epoch : 30] (l_loss: 0.05590) (t_loss: 0.07843) (accu: 0.9758)
[epoch : 31] (l_loss: 0.05581) (t_loss: 0.07752) (accu: 0.9755)
[epoch : 32] (l_loss: 0.05552) (t_loss: 0.07992) (accu: 0.9750)
[epoch : 33] (l_loss: 0.05582) (t_loss: 0.08427) (accu: 0.9739)
[epoch : 34] (l_loss: 0.05551) (t_loss: 0.08063) (accu: 0.9744)
[epoch : 35] (l_loss: 0.05619) (t_loss: 0.08214) (accu: 0.9748)
[epoch : 36] (l_loss: 0.05550) (t_loss: 0.08086) (accu: 0.9747)
[epoch : 37] (l_loss: 0.05571) (t_loss: 0.08188) (accu: 0.9748)
[epoch : 38] (l_loss: 0.05570) (t_loss: 0.07774) (accu: 0.9751)
[epoch : 39] (l_loss: 0.05553) (t_loss: 0.07972) (accu: 0.9748)
[epoch : 40] (l_loss: 0.05614) (t_loss: 0.07957) (accu: 0.9745)
[epoch : 41] (l_loss: 0.05529) (t_loss: 0.07980) (accu: 0.9753)
[epoch : 42] (l_loss: 0.05562) (t_loss: 0.07700) (accu: 0.9757)
[epoch : 43] (l_loss: 0.05551) (t_loss: 0.07918) (accu: 0.9756)
[epoch : 44] (l_loss: 0.05550) (t_loss: 0.08059) (accu: 0.9751)
[epoch : 45] (l_loss: 0.05531) (t_loss: 0.07818) (accu: 0.9751)
[epoch : 46] (l_loss: 0.05525) (t_loss: 0.08032) (accu: 0.9760)
[epoch : 47] (l_loss: 0.05602) (t_loss: 0.07979) (accu: 0.9745)
[epoch : 48] (l_loss: 0.05519) (t_loss: 0.08116) (accu: 0.9746)
[epoch : 49] (l_loss: 0.05581) (t_loss: 0.08128) (accu: 0.9769)
[epoch : 50] (l_loss: 0.05546) (t_loss: 0.07878) (accu: 0.9759)
Finish! (Best accu: 0.9769) (Time taken(sec) : 806.80) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (4961 | 210539)          2.30
fc1.weight   :       196000 (4414 | 191586)          2.25
fc2.weight   :        18750 (422 | 18328)            2.25
fcout.weight :          750 (125 | 625)             16.67
------------------------------------------------------------
[Prune_iter : (18/21), Remaining weight : 2.3 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29736) (accu: 0.0946)
[epoch : 1] (l_loss: 0.44959) (t_loss: 0.15774) (accu: 0.9551)
[epoch : 2] (l_loss: 0.13270) (t_loss: 0.11761) (accu: 0.9648)
[epoch : 3] (l_loss: 0.10027) (t_loss: 0.10217) (accu: 0.9702)
[epoch : 4] (l_loss: 0.08569) (t_loss: 0.09503) (accu: 0.9717)
[epoch : 5] (l_loss: 0.07678) (t_loss: 0.08991) (accu: 0.9724)
[epoch : 6] (l_loss: 0.07060) (t_loss: 0.08748) (accu: 0.9736)
[epoch : 7] (l_loss: 0.06664) (t_loss: 0.08541) (accu: 0.9730)
[epoch : 8] (l_loss: 0.06393) (t_loss: 0.08444) (accu: 0.9728)
[epoch : 9] (l_loss: 0.06192) (t_loss: 0.08325) (accu: 0.9738)
[epoch : 10] (l_loss: 0.06030) (t_loss: 0.08134) (accu: 0.9750)
[epoch : 11] (l_loss: 0.05938) (t_loss: 0.08178) (accu: 0.9751)
[epoch : 12] (l_loss: 0.05861) (t_loss: 0.07977) (accu: 0.9760)
[epoch : 13] (l_loss: 0.05753) (t_loss: 0.08069) (accu: 0.9755)
[epoch : 14] (l_loss: 0.05744) (t_loss: 0.08103) (accu: 0.9742)
[epoch : 15] (l_loss: 0.05712) (t_loss: 0.07929) (accu: 0.9759)
[epoch : 16] (l_loss: 0.05702) (t_loss: 0.08102) (accu: 0.9765)
[epoch : 17] (l_loss: 0.05616) (t_loss: 0.07954) (accu: 0.9754)
[epoch : 18] (l_loss: 0.05573) (t_loss: 0.07890) (accu: 0.9748)
[epoch : 19] (l_loss: 0.05545) (t_loss: 0.07804) (accu: 0.9753)
[epoch : 20] (l_loss: 0.05563) (t_loss: 0.07927) (accu: 0.9755)
[epoch : 21] (l_loss: 0.05535) (t_loss: 0.07802) (accu: 0.9746)
[epoch : 22] (l_loss: 0.05530) (t_loss: 0.07893) (accu: 0.9744)
[epoch : 23] (l_loss: 0.05530) (t_loss: 0.08077) (accu: 0.9766)
[epoch : 24] (l_loss: 0.05530) (t_loss: 0.07961) (accu: 0.9757)
[epoch : 25] (l_loss: 0.05511) (t_loss: 0.07818) (accu: 0.9750)
[epoch : 26] (l_loss: 0.05521) (t_loss: 0.08353) (accu: 0.9735)
[epoch : 27] (l_loss: 0.05469) (t_loss: 0.08255) (accu: 0.9742)
[epoch : 28] (l_loss: 0.05484) (t_loss: 0.08030) (accu: 0.9744)
[epoch : 29] (l_loss: 0.05503) (t_loss: 0.08109) (accu: 0.9747)
[epoch : 30] (l_loss: 0.05488) (t_loss: 0.07879) (accu: 0.9747)
[epoch : 31] (l_loss: 0.05522) (t_loss: 0.07840) (accu: 0.9740)
[epoch : 32] (l_loss: 0.05500) (t_loss: 0.08058) (accu: 0.9754)
[epoch : 33] (l_loss: 0.05490) (t_loss: 0.07750) (accu: 0.9759)
[epoch : 34] (l_loss: 0.05477) (t_loss: 0.07905) (accu: 0.9766)
[epoch : 35] (l_loss: 0.05502) (t_loss: 0.07962) (accu: 0.9755)
[epoch : 36] (l_loss: 0.05459) (t_loss: 0.07876) (accu: 0.9755)
[epoch : 37] (l_loss: 0.05510) (t_loss: 0.08013) (accu: 0.9748)
[epoch : 38] (l_loss: 0.05487) (t_loss: 0.07629) (accu: 0.9754)
[epoch : 39] (l_loss: 0.05482) (t_loss: 0.07612) (accu: 0.9773)
[epoch : 40] (l_loss: 0.05467) (t_loss: 0.07931) (accu: 0.9751)
[epoch : 41] (l_loss: 0.05441) (t_loss: 0.07798) (accu: 0.9758)
[epoch : 42] (l_loss: 0.05396) (t_loss: 0.07781) (accu: 0.9756)
[epoch : 43] (l_loss: 0.05404) (t_loss: 0.07775) (accu: 0.9756)
[epoch : 44] (l_loss: 0.05395) (t_loss: 0.07939) (accu: 0.9754)
[epoch : 45] (l_loss: 0.05475) (t_loss: 0.08155) (accu: 0.9740)
[epoch : 46] (l_loss: 0.05395) (t_loss: 0.07891) (accu: 0.9753)
[epoch : 47] (l_loss: 0.05387) (t_loss: 0.07790) (accu: 0.9764)
[epoch : 48] (l_loss: 0.05400) (t_loss: 0.07820) (accu: 0.9768)
[epoch : 49] (l_loss: 0.05384) (t_loss: 0.07794) (accu: 0.9752)
[epoch : 50] (l_loss: 0.05396) (t_loss: 0.07611) (accu: 0.9770)
Finish! (Best accu: 0.9773) (Time taken(sec) : 811.80) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (3982 | 211518)          1.85
fc1.weight   :       196000 (3531 | 192469)          1.80
fc2.weight   :        18750 (338 | 18412)            1.80
fcout.weight :          750 (113 | 637)             15.07
------------------------------------------------------------
[Prune_iter : (19/21), Remaining weight : 1.85 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29868) (accu: 0.0993)
[epoch : 1] (l_loss: 0.44858) (t_loss: 0.15922) (accu: 0.9554)
[epoch : 2] (l_loss: 0.13309) (t_loss: 0.11650) (accu: 0.9665)
[epoch : 3] (l_loss: 0.10059) (t_loss: 0.10322) (accu: 0.9687)
[epoch : 4] (l_loss: 0.08513) (t_loss: 0.09236) (accu: 0.9723)
[epoch : 5] (l_loss: 0.07511) (t_loss: 0.09106) (accu: 0.9722)
[epoch : 6] (l_loss: 0.06976) (t_loss: 0.08802) (accu: 0.9736)
[epoch : 7] (l_loss: 0.06558) (t_loss: 0.08313) (accu: 0.9745)
[epoch : 8] (l_loss: 0.06258) (t_loss: 0.08235) (accu: 0.9750)
[epoch : 9] (l_loss: 0.06059) (t_loss: 0.08008) (accu: 0.9755)
[epoch : 10] (l_loss: 0.05868) (t_loss: 0.08161) (accu: 0.9760)
[epoch : 11] (l_loss: 0.05773) (t_loss: 0.08140) (accu: 0.9760)
[epoch : 12] (l_loss: 0.05714) (t_loss: 0.08033) (accu: 0.9758)
[epoch : 13] (l_loss: 0.05644) (t_loss: 0.07924) (accu: 0.9762)
[epoch : 14] (l_loss: 0.05606) (t_loss: 0.08006) (accu: 0.9744)
[epoch : 15] (l_loss: 0.05566) (t_loss: 0.08177) (accu: 0.9754)
[epoch : 16] (l_loss: 0.05567) (t_loss: 0.07626) (accu: 0.9765)
[epoch : 17] (l_loss: 0.05565) (t_loss: 0.07918) (accu: 0.9755)
[epoch : 18] (l_loss: 0.05483) (t_loss: 0.08057) (accu: 0.9748)
[epoch : 19] (l_loss: 0.05504) (t_loss: 0.07962) (accu: 0.9753)
[epoch : 20] (l_loss: 0.05529) (t_loss: 0.07963) (accu: 0.9750)
[epoch : 21] (l_loss: 0.05522) (t_loss: 0.07586) (accu: 0.9755)
[epoch : 22] (l_loss: 0.05494) (t_loss: 0.08205) (accu: 0.9753)
[epoch : 23] (l_loss: 0.05451) (t_loss: 0.08379) (accu: 0.9749)
[epoch : 24] (l_loss: 0.05411) (t_loss: 0.07800) (accu: 0.9758)
[epoch : 25] (l_loss: 0.05470) (t_loss: 0.07944) (accu: 0.9756)
[epoch : 26] (l_loss: 0.05435) (t_loss: 0.08046) (accu: 0.9758)
[epoch : 27] (l_loss: 0.05418) (t_loss: 0.07923) (accu: 0.9756)
[epoch : 28] (l_loss: 0.05450) (t_loss: 0.07871) (accu: 0.9743)
[epoch : 29] (l_loss: 0.05456) (t_loss: 0.07782) (accu: 0.9766)
[epoch : 30] (l_loss: 0.05428) (t_loss: 0.08053) (accu: 0.9749)
[epoch : 31] (l_loss: 0.05406) (t_loss: 0.07962) (accu: 0.9754)
[epoch : 32] (l_loss: 0.05409) (t_loss: 0.07937) (accu: 0.9750)
[epoch : 33] (l_loss: 0.05445) (t_loss: 0.07800) (accu: 0.9755)
[epoch : 34] (l_loss: 0.05421) (t_loss: 0.08018) (accu: 0.9759)
[epoch : 35] (l_loss: 0.05413) (t_loss: 0.08223) (accu: 0.9750)
[epoch : 36] (l_loss: 0.05415) (t_loss: 0.07617) (accu: 0.9764)
[epoch : 37] (l_loss: 0.05389) (t_loss: 0.07671) (accu: 0.9752)
[epoch : 38] (l_loss: 0.05382) (t_loss: 0.07752) (accu: 0.9755)
[epoch : 39] (l_loss: 0.05404) (t_loss: 0.08119) (accu: 0.9748)
[epoch : 40] (l_loss: 0.05388) (t_loss: 0.08007) (accu: 0.9741)
[epoch : 41] (l_loss: 0.05390) (t_loss: 0.07783) (accu: 0.9755)
[epoch : 42] (l_loss: 0.05444) (t_loss: 0.07726) (accu: 0.9751)
[epoch : 43] (l_loss: 0.05417) (t_loss: 0.07856) (accu: 0.9756)
[epoch : 44] (l_loss: 0.05352) (t_loss: 0.07910) (accu: 0.9740)
[epoch : 45] (l_loss: 0.05379) (t_loss: 0.07628) (accu: 0.9766)
[epoch : 46] (l_loss: 0.05415) (t_loss: 0.08025) (accu: 0.9751)
[epoch : 47] (l_loss: 0.05411) (t_loss: 0.07966) (accu: 0.9745)
[epoch : 48] (l_loss: 0.05383) (t_loss: 0.08040) (accu: 0.9752)
[epoch : 49] (l_loss: 0.05385) (t_loss: 0.07751) (accu: 0.9743)
[epoch : 50] (l_loss: 0.05414) (t_loss: 0.07757) (accu: 0.9768)
Finish! (Best accu: 0.9768) (Time taken(sec) : 813.40) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (3196 | 212304)          1.48
fc1.weight   :       196000 (2825 | 193175)          1.44
fc2.weight   :        18750 (270 | 18480)            1.44
fcout.weight :          750 (101 | 649)             13.47
------------------------------------------------------------
[Prune_iter : (20/21), Remaining weight : 1.48 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30043) (accu: 0.0974)
[epoch : 1] (l_loss: 0.45211) (t_loss: 0.16441) (accu: 0.9528)
[epoch : 2] (l_loss: 0.13849) (t_loss: 0.11963) (accu: 0.9631)
[epoch : 3] (l_loss: 0.10432) (t_loss: 0.10575) (accu: 0.9680)
[epoch : 4] (l_loss: 0.08862) (t_loss: 0.09740) (accu: 0.9704)
[epoch : 5] (l_loss: 0.07831) (t_loss: 0.08966) (accu: 0.9718)
[epoch : 6] (l_loss: 0.07263) (t_loss: 0.08753) (accu: 0.9737)
[epoch : 7] (l_loss: 0.06851) (t_loss: 0.08508) (accu: 0.9740)
[epoch : 8] (l_loss: 0.06593) (t_loss: 0.08472) (accu: 0.9732)
[epoch : 9] (l_loss: 0.06367) (t_loss: 0.08369) (accu: 0.9747)
[epoch : 10] (l_loss: 0.06224) (t_loss: 0.08621) (accu: 0.9721)
[epoch : 11] (l_loss: 0.06095) (t_loss: 0.08443) (accu: 0.9733)
[epoch : 12] (l_loss: 0.06015) (t_loss: 0.08075) (accu: 0.9751)
[epoch : 13] (l_loss: 0.05919) (t_loss: 0.08328) (accu: 0.9732)
[epoch : 14] (l_loss: 0.05861) (t_loss: 0.08149) (accu: 0.9751)
[epoch : 15] (l_loss: 0.05820) (t_loss: 0.07871) (accu: 0.9757)
[epoch : 16] (l_loss: 0.05786) (t_loss: 0.08009) (accu: 0.9749)
[epoch : 17] (l_loss: 0.05761) (t_loss: 0.08543) (accu: 0.9744)
[epoch : 18] (l_loss: 0.05771) (t_loss: 0.07967) (accu: 0.9744)
[epoch : 19] (l_loss: 0.05714) (t_loss: 0.07991) (accu: 0.9736)
[epoch : 20] (l_loss: 0.05683) (t_loss: 0.07871) (accu: 0.9753)
[epoch : 21] (l_loss: 0.05709) (t_loss: 0.08126) (accu: 0.9754)
[epoch : 22] (l_loss: 0.05721) (t_loss: 0.07840) (accu: 0.9752)
[epoch : 23] (l_loss: 0.05676) (t_loss: 0.08012) (accu: 0.9757)
[epoch : 24] (l_loss: 0.05708) (t_loss: 0.07740) (accu: 0.9768)
[epoch : 25] (l_loss: 0.05643) (t_loss: 0.07983) (accu: 0.9755)
[epoch : 26] (l_loss: 0.05662) (t_loss: 0.08244) (accu: 0.9737)
[epoch : 27] (l_loss: 0.05645) (t_loss: 0.07860) (accu: 0.9769)
[epoch : 28] (l_loss: 0.05652) (t_loss: 0.08063) (accu: 0.9753)
[epoch : 29] (l_loss: 0.05652) (t_loss: 0.07974) (accu: 0.9751)
[epoch : 30] (l_loss: 0.05624) (t_loss: 0.08135) (accu: 0.9732)
[epoch : 31] (l_loss: 0.05672) (t_loss: 0.07899) (accu: 0.9749)
[epoch : 32] (l_loss: 0.05624) (t_loss: 0.07885) (accu: 0.9756)
[epoch : 33] (l_loss: 0.05655) (t_loss: 0.07825) (accu: 0.9757)
[epoch : 34] (l_loss: 0.05667) (t_loss: 0.08121) (accu: 0.9755)
[epoch : 35] (l_loss: 0.05604) (t_loss: 0.07908) (accu: 0.9749)
[epoch : 36] (l_loss: 0.05605) (t_loss: 0.08133) (accu: 0.9750)
[epoch : 37] (l_loss: 0.05614) (t_loss: 0.08068) (accu: 0.9747)
[epoch : 38] (l_loss: 0.05628) (t_loss: 0.08296) (accu: 0.9735)
[epoch : 39] (l_loss: 0.05617) (t_loss: 0.08014) (accu: 0.9754)
[epoch : 40] (l_loss: 0.05646) (t_loss: 0.07961) (accu: 0.9752)
[epoch : 41] (l_loss: 0.05637) (t_loss: 0.08105) (accu: 0.9741)
[epoch : 42] (l_loss: 0.05650) (t_loss: 0.08289) (accu: 0.9750)
[epoch : 43] (l_loss: 0.05617) (t_loss: 0.08064) (accu: 0.9739)
[epoch : 44] (l_loss: 0.05596) (t_loss: 0.07958) (accu: 0.9749)
[epoch : 45] (l_loss: 0.05618) (t_loss: 0.08360) (accu: 0.9738)
[epoch : 46] (l_loss: 0.05620) (t_loss: 0.08241) (accu: 0.9736)
[epoch : 47] (l_loss: 0.05588) (t_loss: 0.08146) (accu: 0.9739)
[epoch : 48] (l_loss: 0.05624) (t_loss: 0.08021) (accu: 0.9741)
[epoch : 49] (l_loss: 0.05661) (t_loss: 0.08059) (accu: 0.9734)
[epoch : 50] (l_loss: 0.05641) (t_loss: 0.07892) (accu: 0.9745)
Finish! (Best accu: 0.9769) (Time taken(sec) : 807.90) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       215500 (2567 | 212933)          1.19
fc1.weight   :       196000 (2260 | 193740)          1.15
fc2.weight   :        18750 (216 | 18534)            1.15
fcout.weight :           750 (91 | 659)             12.13
------------------------------------------------------------
[Prune_iter : (21/21), Remaining weight : 1.19 %]
[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30005) (accu: 0.0927)
[epoch : 1] (l_loss: 0.44508) (t_loss: 0.16395) (accu: 0.9535)
[epoch : 2] (l_loss: 0.13633) (t_loss: 0.11874) (accu: 0.9665)
[epoch : 3] (l_loss: 0.10353) (t_loss: 0.10530) (accu: 0.9693)
[epoch : 4] (l_loss: 0.08805) (t_loss: 0.09689) (accu: 0.9700)
[epoch : 5] (l_loss: 0.07782) (t_loss: 0.09103) (accu: 0.9722)
[epoch : 6] (l_loss: 0.07162) (t_loss: 0.08634) (accu: 0.9732)
[epoch : 7] (l_loss: 0.06737) (t_loss: 0.08452) (accu: 0.9744)
[epoch : 8] (l_loss: 0.06416) (t_loss: 0.08433) (accu: 0.9739)
[epoch : 9] (l_loss: 0.06244) (t_loss: 0.08160) (accu: 0.9747)
[epoch : 10] (l_loss: 0.06065) (t_loss: 0.08413) (accu: 0.9737)
[epoch : 11] (l_loss: 0.05941) (t_loss: 0.08181) (accu: 0.9744)
[epoch : 12] (l_loss: 0.05858) (t_loss: 0.08060) (accu: 0.9759)
[epoch : 13] (l_loss: 0.05769) (t_loss: 0.08095) (accu: 0.9749)
[epoch : 14] (l_loss: 0.05695) (t_loss: 0.07884) (accu: 0.9762)
[epoch : 15] (l_loss: 0.05666) (t_loss: 0.08036) (accu: 0.9758)
[epoch : 16] (l_loss: 0.05569) (t_loss: 0.07934) (accu: 0.9756)
[epoch : 17] (l_loss: 0.05592) (t_loss: 0.07917) (accu: 0.9754)
[epoch : 18] (l_loss: 0.05585) (t_loss: 0.07805) (accu: 0.9754)
[epoch : 19] (l_loss: 0.05500) (t_loss: 0.07820) (accu: 0.9766)
[epoch : 20] (l_loss: 0.05508) (t_loss: 0.07820) (accu: 0.9760)
[epoch : 21] (l_loss: 0.05453) (t_loss: 0.07830) (accu: 0.9759)
[epoch : 22] (l_loss: 0.05478) (t_loss: 0.07960) (accu: 0.9757)
[epoch : 23] (l_loss: 0.05472) (t_loss: 0.07984) (accu: 0.9761)
[epoch : 24] (l_loss: 0.05427) (t_loss: 0.07703) (accu: 0.9762)
[epoch : 25] (l_loss: 0.05451) (t_loss: 0.07813) (accu: 0.9756)
[epoch : 26] (l_loss: 0.05431) (t_loss: 0.08199) (accu: 0.9743)
[epoch : 27] (l_loss: 0.05461) (t_loss: 0.08344) (accu: 0.9739)
[epoch : 28] (l_loss: 0.05436) (t_loss: 0.07929) (accu: 0.9756)
[epoch : 29] (l_loss: 0.05417) (t_loss: 0.07634) (accu: 0.9751)
[epoch : 30] (l_loss: 0.05480) (t_loss: 0.07817) (accu: 0.9744)
[epoch : 31] (l_loss: 0.05431) (t_loss: 0.08019) (accu: 0.9752)
[epoch : 32] (l_loss: 0.05397) (t_loss: 0.08073) (accu: 0.9746)
[epoch : 33] (l_loss: 0.05402) (t_loss: 0.07847) (accu: 0.9754)
[epoch : 34] (l_loss: 0.05399) (t_loss: 0.07917) (accu: 0.9757)
[epoch : 35] (l_loss: 0.05386) (t_loss: 0.07952) (accu: 0.9753)
[epoch : 36] (l_loss: 0.05414) (t_loss: 0.07720) (accu: 0.9755)
[epoch : 37] (l_loss: 0.05392) (t_loss: 0.08207) (accu: 0.9726)
[epoch : 38] (l_loss: 0.05474) (t_loss: 0.08065) (accu: 0.9772)
[epoch : 39] (l_loss: 0.05381) (t_loss: 0.07860) (accu: 0.9762)
[epoch : 40] (l_loss: 0.05386) (t_loss: 0.07837) (accu: 0.9750)
[epoch : 41] (l_loss: 0.05394) (t_loss: 0.07874) (accu: 0.9743)
[epoch : 42] (l_loss: 0.05392) (t_loss: 0.07774) (accu: 0.9758)
[epoch : 43] (l_loss: 0.05387) (t_loss: 0.07888) (accu: 0.9754)
[epoch : 44] (l_loss: 0.05420) (t_loss: 0.07915) (accu: 0.9753)
[epoch : 45] (l_loss: 0.05403) (t_loss: 0.07888) (accu: 0.9753)
[epoch : 46] (l_loss: 0.05406) (t_loss: 0.07812) (accu: 0.9760)
[epoch : 47] (l_loss: 0.05371) (t_loss: 0.07727) (accu: 0.9761)
[epoch : 48] (l_loss: 0.05418) (t_loss: 0.08142) (accu: 0.9745)
[epoch : 49] (l_loss: 0.05392) (t_loss: 0.07925) (accu: 0.9746)
[epoch : 50] (l_loss: 0.05374) (t_loss: 0.07777) (accu: 0.9766)
Finish! (Best accu: 0.9772) (Time taken(sec) : 792.16) 


Maximum accuracy per weight remaining
Remaining weight 100.0 %  Epoch 22 Accu 0.9763
Remaining weight 80.03 %  Epoch 35 Accu 0.9766
Remaining weight 64.06 %  Epoch 12 Accu 0.9782
Remaining weight 51.28 %  Epoch 31 Accu 0.9781
Remaining weight 41.05 %  Epoch 39 Accu 0.9767
Remaining weight 32.86 %  Epoch 31 Accu 0.9783
Remaining weight 26.31 %  Epoch 47 Accu 0.9773
Remaining weight 21.06 %  Epoch 21 Accu 0.9778
Remaining weight 16.87 %  Epoch 20 Accu 0.9785
Remaining weight 13.51 %  Epoch 33 Accu 0.9780
Remaining weight 10.82 %  Epoch 41 Accu 0.9778
Remaining weight 8.67 %  Epoch 37 Accu 0.9779
Remaining weight 6.95 %  Epoch 32 Accu 0.9778
Remaining weight 5.57 %  Epoch 37 Accu 0.9784
Remaining weight 4.46 %  Epoch 43 Accu 0.9780
Remaining weight 3.58 %  Epoch 14 Accu 0.9764
Remaining weight 2.87 %  Epoch 48 Accu 0.9769
Remaining weight 2.3 %  Epoch 38 Accu 0.9773
Remaining weight 1.85 %  Epoch 49 Accu 0.9768
Remaining weight 1.48 %  Epoch 26 Accu 0.9769
Remaining weight 1.19 %  Epoch 37 Accu 0.9772
Average test data
Remaining weight 100.00 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.715107   0.0803
1     0.445939    0.158919   0.9528
2     0.127749    0.111772   0.9668
3     0.096757    0.107256   0.9667
4     0.082566    0.093780   0.9707
5     0.074711    0.087798   0.9731
6     0.069314    0.084406   0.9737
7     0.065880    0.080864   0.9753
8     0.063206    0.082663   0.9738
9     0.061466    0.078740   0.9751
10     0.060105    0.078783   0.9757
11     0.059346    0.081581   0.9749
12     0.058794    0.079382   0.9758
13     0.057918    0.081775   0.9748
14     0.057418    0.077056   0.9758
15     0.057275    0.080775   0.9752
16     0.056769    0.077710   0.9754
17     0.056578    0.077294   0.9761
18     0.056648    0.079694   0.9754
19     0.056154    0.076949   0.9757
20     0.056036    0.077060   0.9759
21     0.055993    0.077732   0.9756
22     0.055837    0.081762   0.9743
23     0.056094    0.076521   0.9768
24     0.055614    0.078200   0.9755
25     0.055702    0.080322   0.9746
26     0.055248    0.081971   0.9741
27     0.055767    0.079152   0.9753
28     0.055381    0.078955   0.9757
29     0.055443    0.078246   0.9753
30     0.055360    0.077473   0.9756
31     0.055366    0.078168   0.9757
32     0.055374    0.079459   0.9749
33     0.054624    0.076696   0.9761
34     0.055129    0.076213   0.9757
35     0.055142    0.077866   0.9759
36     0.054928    0.075964   0.9765
37     0.054906    0.077085   0.9761
38     0.054964    0.077002   0.9758
39     0.054569    0.077771   0.9759
40     0.055249    0.079157   0.9753
41     0.054845    0.076568   0.9760
42     0.055232    0.078765   0.9754
43     0.054435    0.077041   0.9757
44     0.054771    0.077293   0.9758
45     0.054878    0.077778   0.9760
46     0.055028    0.081651   0.9742
47     0.054844    0.076891   0.9754
48     0.055267    0.079846   0.9756
49     0.054820    0.075851   0.9761
50     0.054260    0.078165   0.9751
Remaining weight 80.03 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.493773   0.1014
1     0.443344    0.162831   0.9513
2     0.131322    0.118002   0.9640
3     0.099574    0.099820   0.9698
4     0.085105    0.094169   0.9710
5     0.076706    0.089321   0.9724
6     0.071477    0.085100   0.9741
7     0.067611    0.085624   0.9734
8     0.065065    0.084143   0.9735
9     0.063550    0.080144   0.9755
10     0.062361    0.082105   0.9743
11     0.060931    0.083995   0.9744
12     0.060583    0.079974   0.9752
13     0.059712    0.079971   0.9752
14     0.059419    0.079673   0.9756
15     0.058558    0.077790   0.9764
16     0.058547    0.079391   0.9751
17     0.058047    0.079422   0.9754
18     0.058143    0.077806   0.9756
19     0.057249    0.078676   0.9755
20     0.057406    0.080328   0.9750
21     0.057199    0.081107   0.9751
22     0.057365    0.079379   0.9751
23     0.056762    0.080519   0.9750
24     0.057300    0.082996   0.9742
25     0.056683    0.079902   0.9746
26     0.057001    0.080871   0.9748
27     0.056280    0.079962   0.9753
28     0.056879    0.079041   0.9755
29     0.056501    0.079467   0.9751
30     0.056576    0.082682   0.9741
31     0.056426    0.078843   0.9752
32     0.056130    0.082998   0.9741
33     0.056604    0.082745   0.9740
34     0.056275    0.084976   0.9736
35     0.056588    0.080552   0.9747
36     0.056261    0.080796   0.9750
37     0.055967    0.079433   0.9753
38     0.056155    0.080208   0.9750
39     0.056325    0.079402   0.9750
40     0.056005    0.081297   0.9748
41     0.056249    0.080130   0.9746
42     0.056122    0.083723   0.9737
43     0.056002    0.080076   0.9749
44     0.056011    0.080870   0.9751
45     0.055603    0.080866   0.9749
46     0.056104    0.081385   0.9747
47     0.055817    0.079691   0.9747
48     0.056243    0.079270   0.9751
49     0.055819    0.081462   0.9744
50     0.056023    0.079991   0.9755
Remaining weight 64.06 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.396330   0.0982
1     0.430819    0.150922   0.9568
2     0.123967    0.107294   0.9681
3     0.093737    0.095462   0.9715
4     0.078898    0.089039   0.9733
5     0.070412    0.084404   0.9750
6     0.064925    0.084175   0.9743
7     0.062088    0.082346   0.9750
8     0.059764    0.080626   0.9753
9     0.057974    0.078151   0.9763
10     0.056640    0.080379   0.9760
11     0.056020    0.078843   0.9757
12     0.054931    0.080281   0.9754
13     0.054825    0.077809   0.9769
14     0.054149    0.075437   0.9771
15     0.053520    0.077079   0.9762
16     0.053640    0.076527   0.9763
17     0.053327    0.077861   0.9761
18     0.052838    0.079506   0.9755
19     0.052940    0.078293   0.9755
20     0.052945    0.074942   0.9771
21     0.052699    0.077525   0.9761
22     0.052942    0.075854   0.9761
23     0.052130    0.074981   0.9771
24     0.052419    0.076648   0.9761
25     0.052745    0.075855   0.9773
26     0.052324    0.076945   0.9766
27     0.051934    0.079046   0.9761
28     0.052484    0.075383   0.9769
29     0.051962    0.076619   0.9766
30     0.051789    0.076748   0.9772
31     0.051877    0.077446   0.9763
32     0.051879    0.076680   0.9764
33     0.051589    0.077213   0.9767
34     0.052108    0.075467   0.9771
35     0.051824    0.077582   0.9757
36     0.051833    0.076547   0.9767
37     0.052076    0.076635   0.9766
38     0.051833    0.075482   0.9764
39     0.051636    0.076521   0.9768
40     0.051682    0.077124   0.9762
41     0.051425    0.075892   0.9771
42     0.051712    0.075231   0.9772
43     0.051589    0.075775   0.9770
44     0.051657    0.075947   0.9766
45     0.051559    0.075338   0.9765
46     0.051577    0.075459   0.9768
47     0.051721    0.077240   0.9768
48     0.051527    0.077204   0.9763
49     0.051595    0.076182   0.9766
50     0.051305    0.078629   0.9756
Remaining weight 51.28 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.349389   0.0959
1     0.432028    0.153211   0.9553
2     0.126618    0.114027   0.9657
3     0.096027    0.097397   0.9711
4     0.081796    0.090921   0.9722
5     0.073295    0.092728   0.9722
6     0.068054    0.085822   0.9742
7     0.063609    0.083231   0.9746
8     0.060731    0.080456   0.9761
9     0.059141    0.081079   0.9758
10     0.057621    0.078256   0.9764
11     0.055987    0.079530   0.9758
12     0.055924    0.077691   0.9766
13     0.055337    0.078159   0.9762
14     0.054769    0.075085   0.9773
15     0.053946    0.078254   0.9760
16     0.054026    0.076918   0.9765
17     0.053577    0.077555   0.9763
18     0.053609    0.076925   0.9769
19     0.053322    0.075789   0.9774
20     0.053257    0.078543   0.9763
21     0.052917    0.075630   0.9778
22     0.052769    0.077320   0.9763
23     0.052656    0.078783   0.9752
24     0.052604    0.081710   0.9751
25     0.052797    0.074188   0.9775
26     0.052494    0.077139   0.9771
27     0.052400    0.075965   0.9764
28     0.052349    0.075644   0.9765
29     0.052275    0.075872   0.9768
30     0.051874    0.078488   0.9762
31     0.052112    0.076411   0.9770
32     0.051753    0.075932   0.9768
33     0.052137    0.076069   0.9765
34     0.051463    0.078468   0.9756
35     0.051592    0.075064   0.9768
36     0.051796    0.077350   0.9759
37     0.051785    0.077177   0.9761
38     0.051158    0.073556   0.9772
39     0.051596    0.075952   0.9768
40     0.051388    0.076472   0.9771
41     0.051115    0.074587   0.9771
42     0.051247    0.073370   0.9775
43     0.051270    0.075362   0.9765
44     0.051369    0.075240   0.9769
45     0.050984    0.073659   0.9771
46     0.051247    0.077493   0.9766
47     0.051268    0.076923   0.9763
48     0.051095    0.076197   0.9761
49     0.050964    0.075287   0.9764
50     0.051191    0.076601   0.9757
Remaining weight 41.05 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.344894   0.1079
1     0.430988    0.161578   0.9515
2     0.132955    0.115119   0.9652
3     0.100290    0.100604   0.9700
4     0.085347    0.094285   0.9714
5     0.076805    0.088848   0.9733
6     0.070838    0.086885   0.9735
7     0.067174    0.084983   0.9742
8     0.063761    0.084074   0.9749
9     0.061840    0.082169   0.9750
10     0.060660    0.082554   0.9748
11     0.058913    0.081164   0.9753
12     0.058234    0.082047   0.9749
13     0.057350    0.080668   0.9754
14     0.057062    0.080236   0.9755
15     0.056522    0.081030   0.9757
16     0.056618    0.080765   0.9752
17     0.056259    0.080269   0.9756
18     0.055716    0.079796   0.9757
19     0.055891    0.079406   0.9760
20     0.055549    0.077956   0.9766
21     0.055057    0.080142   0.9755
22     0.055117    0.080232   0.9755
23     0.054630    0.076003   0.9766
24     0.054798    0.078196   0.9762
25     0.054278    0.077444   0.9760
26     0.054465    0.080666   0.9750
27     0.054433    0.077212   0.9763
28     0.053883    0.079129   0.9765
29     0.054096    0.078807   0.9762
30     0.054141    0.077290   0.9764
31     0.054087    0.078567   0.9754
32     0.053960    0.079508   0.9759
33     0.053726    0.079208   0.9763
34     0.053785    0.077828   0.9762
35     0.053786    0.077876   0.9762
36     0.053501    0.078442   0.9763
37     0.053840    0.080520   0.9749
38     0.053257    0.078720   0.9758
39     0.053310    0.080031   0.9757
40     0.053266    0.077942   0.9765
41     0.052561    0.077876   0.9760
42     0.053407    0.076616   0.9763
43     0.053309    0.077848   0.9762
44     0.052914    0.078043   0.9760
45     0.053225    0.078227   0.9760
46     0.052780    0.077178   0.9766
47     0.053008    0.079467   0.9758
48     0.053032    0.077035   0.9761
49     0.052592    0.078479   0.9760
50     0.053052    0.078298   0.9760
Remaining weight 32.86 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.333692   0.1199
1     0.430713    0.156024   0.9553
2     0.132647    0.116108   0.9659
3     0.100779    0.100861   0.9694
4     0.085974    0.093117   0.9716
5     0.076790    0.089478   0.9728
6     0.071493    0.087652   0.9733
7     0.066547    0.086127   0.9735
8     0.063250    0.081606   0.9749
9     0.060925    0.082272   0.9755
10     0.059067    0.085986   0.9737
11     0.057702    0.080712   0.9755
12     0.056631    0.079650   0.9759
13     0.055707    0.078324   0.9765
14     0.055053    0.081061   0.9751
15     0.054874    0.081460   0.9753
16     0.054924    0.075853   0.9769
17     0.054327    0.079841   0.9756
18     0.053996    0.077530   0.9763
19     0.053638    0.078685   0.9759
20     0.053543    0.076297   0.9767
21     0.053785    0.078307   0.9759
22     0.053277    0.078757   0.9762
23     0.053191    0.079107   0.9757
24     0.052781    0.077885   0.9762
25     0.052626    0.076989   0.9769
26     0.052806    0.076857   0.9770
27     0.052686    0.077880   0.9756
28     0.052825    0.079445   0.9760
29     0.052572    0.077923   0.9762
30     0.052618    0.077045   0.9766
31     0.052335    0.080452   0.9761
32     0.052289    0.075853   0.9773
33     0.052626    0.076715   0.9765
34     0.052224    0.077060   0.9762
35     0.052351    0.075120   0.9771
36     0.052310    0.077539   0.9763
37     0.052560    0.077677   0.9765
38     0.052385    0.077877   0.9765
39     0.052117    0.075631   0.9774
40     0.052278    0.079426   0.9754
41     0.051683    0.077655   0.9760
42     0.052484    0.078014   0.9762
43     0.052098    0.076724   0.9769
44     0.052113    0.077147   0.9766
45     0.052165    0.075688   0.9770
46     0.052107    0.076042   0.9762
47     0.052089    0.078789   0.9761
48     0.051996    0.077280   0.9767
49     0.052249    0.076992   0.9766
50     0.052166    0.075759   0.9766
Remaining weight 26.31 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.309608   0.1334
1     0.419814    0.154737   0.9549
2     0.127603    0.113859   0.9657
3     0.097801    0.103824   0.9687
4     0.082816    0.091536   0.9719
5     0.073403    0.087293   0.9737
6     0.067480    0.083931   0.9742
7     0.063401    0.081589   0.9744
8     0.060656    0.080743   0.9754
9     0.058377    0.079392   0.9753
10     0.056980    0.079715   0.9748
11     0.056136    0.079001   0.9759
12     0.054935    0.078153   0.9762
13     0.054710    0.077614   0.9766
14     0.054165    0.078270   0.9767
15     0.053832    0.075920   0.9764
16     0.053438    0.078569   0.9756
17     0.052959    0.077511   0.9766
18     0.052818    0.076256   0.9767
19     0.052375    0.077079   0.9763
20     0.052408    0.077234   0.9765
21     0.052643    0.076689   0.9764
22     0.052782    0.076051   0.9767
23     0.052327    0.076058   0.9764
24     0.052574    0.074889   0.9769
25     0.052154    0.078365   0.9756
26     0.052284    0.077641   0.9757
27     0.052240    0.076979   0.9763
28     0.052243    0.076634   0.9762
29     0.051740    0.077505   0.9762
30     0.052237    0.075865   0.9765
31     0.052021    0.076993   0.9757
32     0.051634    0.079464   0.9756
33     0.052171    0.076469   0.9760
34     0.051771    0.075798   0.9769
35     0.051886    0.077088   0.9759
36     0.051722    0.079029   0.9756
37     0.051792    0.078545   0.9754
38     0.051728    0.076884   0.9761
39     0.051911    0.076533   0.9766
40     0.051799    0.076042   0.9765
41     0.051851    0.076455   0.9759
42     0.051629    0.075670   0.9767
43     0.051544    0.076224   0.9763
44     0.051689    0.077211   0.9761
45     0.051795    0.075539   0.9766
46     0.051349    0.077637   0.9762
47     0.051724    0.075946   0.9765
48     0.051380    0.077262   0.9759
49     0.051637    0.076949   0.9758
50     0.051723    0.078192   0.9761
Remaining weight 21.06 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.318427   0.1243
1     0.420154    0.157761   0.9542
2     0.131944    0.118642   0.9646
3     0.100191    0.102737   0.9688
4     0.084468    0.091708   0.9720
5     0.074395    0.085463   0.9736
6     0.067800    0.084587   0.9739
7     0.063231    0.081888   0.9756
8     0.060159    0.082660   0.9750
9     0.058352    0.077001   0.9767
10     0.056615    0.078506   0.9761
11     0.055310    0.078268   0.9763
12     0.054618    0.075840   0.9768
13     0.053941    0.079648   0.9751
14     0.053536    0.076491   0.9766
15     0.052882    0.076685   0.9766
16     0.052802    0.074527   0.9772
17     0.052803    0.078295   0.9765
18     0.052390    0.077859   0.9759
19     0.052044    0.075409   0.9767
20     0.051741    0.077465   0.9761
21     0.051915    0.077967   0.9755
22     0.051667    0.075041   0.9774
23     0.051535    0.074850   0.9765
24     0.051813    0.074294   0.9765
25     0.051286    0.077226   0.9755
26     0.052139    0.077544   0.9756
27     0.051108    0.076461   0.9767
28     0.051463    0.075666   0.9762
29     0.051292    0.076365   0.9759
30     0.051371    0.075700   0.9760
31     0.051264    0.082055   0.9745
32     0.051188    0.077224   0.9760
33     0.051513    0.076936   0.9758
34     0.050871    0.075633   0.9759
35     0.051224    0.076153   0.9766
36     0.051072    0.076252   0.9760
37     0.051121    0.076060   0.9761
38     0.050974    0.076105   0.9770
39     0.051107    0.076873   0.9764
40     0.050790    0.073989   0.9762
41     0.051059    0.075532   0.9768
42     0.051015    0.074950   0.9774
43     0.051140    0.077451   0.9764
44     0.050777    0.076031   0.9770
45     0.050924    0.074113   0.9771
46     0.051027    0.076211   0.9765
47     0.051119    0.075547   0.9766
48     0.050788    0.075092   0.9762
49     0.050728    0.076696   0.9763
50     0.051096    0.077355   0.9759
Remaining weight 16.87 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.316024   0.1457
1     0.417479    0.155212   0.9566
2     0.129059    0.112860   0.9671
3     0.099879    0.103820   0.9682
4     0.085361    0.092662   0.9718
5     0.075675    0.088159   0.9725
6     0.069691    0.085590   0.9737
7     0.064888    0.085503   0.9740
8     0.062039    0.081407   0.9751
9     0.059714    0.080918   0.9756
10     0.058244    0.078314   0.9768
11     0.056611    0.078991   0.9763
12     0.055998    0.078595   0.9763
13     0.054799    0.078164   0.9764
14     0.054672    0.077500   0.9762
15     0.054431    0.077968   0.9763
16     0.053675    0.078658   0.9766
17     0.053578    0.076238   0.9769
18     0.053096    0.079223   0.9758
19     0.053167    0.078751   0.9763
20     0.052901    0.076285   0.9767
21     0.052538    0.076866   0.9766
22     0.053003    0.075911   0.9769
23     0.052541    0.075899   0.9768
24     0.052360    0.076710   0.9765
25     0.052263    0.076811   0.9764
26     0.052227    0.076723   0.9765
27     0.052011    0.079363   0.9753
28     0.051984    0.076211   0.9767
29     0.052266    0.079131   0.9757
30     0.051679    0.076344   0.9764
31     0.051811    0.076200   0.9765
32     0.052096    0.075187   0.9770
33     0.051871    0.075898   0.9774
34     0.051827    0.075514   0.9767
35     0.051725    0.075104   0.9766
36     0.051762    0.077184   0.9763
37     0.051903    0.078515   0.9760
38     0.051931    0.076087   0.9765
39     0.051900    0.077320   0.9769
40     0.051873    0.076292   0.9769
41     0.051820    0.075567   0.9767
42     0.051581    0.075876   0.9766
43     0.051793    0.076700   0.9763
44     0.051393    0.076103   0.9762
45     0.052026    0.078816   0.9755
46     0.051488    0.076326   0.9767
47     0.051508    0.077728   0.9760
48     0.051671    0.075569   0.9771
49     0.051561    0.076535   0.9763
50     0.051155    0.077789   0.9754
Remaining weight 13.51 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.303279   0.1482
1     0.422013    0.156718   0.9550
2     0.134048    0.118398   0.9656
3     0.102632    0.102073   0.9695
4     0.086747    0.095920   0.9710
5     0.076837    0.090158   0.9731
6     0.070854    0.086244   0.9740
7     0.066199    0.083767   0.9746
8     0.062680    0.082097   0.9749
9     0.060138    0.080293   0.9756
10     0.058195    0.079823   0.9763
11     0.057226    0.079808   0.9759
12     0.055837    0.078770   0.9759
13     0.055300    0.080383   0.9756
14     0.054806    0.077233   0.9769
15     0.054333    0.079703   0.9758
16     0.053828    0.076375   0.9764
17     0.053750    0.075881   0.9764
18     0.053252    0.077203   0.9758
19     0.052994    0.075394   0.9771
20     0.052987    0.075544   0.9769
21     0.052904    0.076787   0.9767
22     0.052740    0.077759   0.9762
23     0.052328    0.076198   0.9767
24     0.052799    0.078078   0.9766
25     0.052526    0.077364   0.9765
26     0.052521    0.074672   0.9772
27     0.052292    0.078819   0.9761
28     0.052337    0.076168   0.9767
29     0.052232    0.076224   0.9766
30     0.052097    0.076032   0.9767
31     0.052136    0.076151   0.9766
32     0.051853    0.075514   0.9768
33     0.052018    0.077085   0.9765
34     0.051903    0.075623   0.9769
35     0.052171    0.077505   0.9764
36     0.051842    0.076871   0.9767
37     0.052034    0.076990   0.9770
38     0.051843    0.076470   0.9770
39     0.052142    0.076999   0.9761
40     0.051693    0.076804   0.9764
41     0.051893    0.075540   0.9768
42     0.051545    0.078120   0.9759
43     0.051917    0.076505   0.9765
44     0.051747    0.075084   0.9772
45     0.051631    0.078208   0.9762
46     0.051941    0.075942   0.9772
47     0.051868    0.078233   0.9770
48     0.051592    0.075719   0.9766
49     0.051591    0.076663   0.9765
50     0.051884    0.075583   0.9767
Remaining weight 10.82 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.292649   0.1330
1     0.417969    0.156436   0.9546
2     0.130144    0.115500   0.9659
3     0.098052    0.100285   0.9698
4     0.083385    0.092382   0.9725
5     0.074119    0.091371   0.9720
6     0.068044    0.083556   0.9748
7     0.064152    0.081096   0.9751
8     0.060610    0.080721   0.9754
9     0.058867    0.081258   0.9753
10     0.057096    0.081041   0.9758
11     0.055817    0.081917   0.9749
12     0.054683    0.079055   0.9762
13     0.054195    0.076720   0.9769
14     0.053852    0.078597   0.9760
15     0.053410    0.078119   0.9756
16     0.053270    0.076193   0.9765
17     0.052703    0.077325   0.9766
18     0.052572    0.078492   0.9760
19     0.052666    0.077095   0.9762
20     0.052035    0.077913   0.9761
21     0.052106    0.076395   0.9769
22     0.051938    0.077595   0.9760
23     0.051912    0.077904   0.9761
24     0.051697    0.076733   0.9764
25     0.051883    0.076207   0.9764
26     0.051837    0.076755   0.9766
27     0.051610    0.075305   0.9768
28     0.051448    0.077608   0.9759
29     0.051724    0.076509   0.9765
30     0.051212    0.076611   0.9764
31     0.051444    0.075746   0.9769
32     0.051415    0.076626   0.9765
33     0.051671    0.074431   0.9768
34     0.051055    0.076432   0.9766
35     0.051306    0.076403   0.9771
36     0.051548    0.075939   0.9772
37     0.051097    0.077748   0.9757
38     0.051172    0.077033   0.9763
39     0.050997    0.076936   0.9764
40     0.051190    0.076737   0.9760
41     0.051333    0.074799   0.9772
42     0.050853    0.075142   0.9768
43     0.051109    0.075069   0.9770
44     0.051103    0.076031   0.9764
45     0.051227    0.075825   0.9764
46     0.051185    0.075359   0.9769
47     0.051056    0.076498   0.9762
48     0.051105    0.075467   0.9769
49     0.051173    0.076587   0.9765
50     0.050961    0.075253   0.9768
Remaining weight 8.67 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.293192   0.1426
1     0.421341    0.159897   0.9547
2     0.134508    0.118389   0.9645
3     0.102072    0.102944   0.9689
4     0.085627    0.092150   0.9721
5     0.075788    0.089310   0.9728
6     0.069118    0.087281   0.9732
7     0.064725    0.083882   0.9745
8     0.061533    0.081717   0.9752
9     0.059041    0.079453   0.9757
10     0.057429    0.080390   0.9759
11     0.056321    0.079352   0.9766
12     0.055373    0.079340   0.9767
13     0.054654    0.082342   0.9751
14     0.054077    0.077688   0.9767
15     0.053873    0.079137   0.9758
16     0.053624    0.078043   0.9763
17     0.053226    0.077627   0.9763
18     0.053144    0.077164   0.9763
19     0.052737    0.076322   0.9767
20     0.052539    0.075510   0.9773
21     0.052265    0.075853   0.9760
22     0.052375    0.076726   0.9769
23     0.052482    0.076104   0.9771
24     0.052077    0.075966   0.9765
25     0.051939    0.077888   0.9758
26     0.052072    0.077647   0.9759
27     0.051847    0.076102   0.9770
28     0.052184    0.077357   0.9763
29     0.051606    0.078645   0.9760
30     0.051941    0.076550   0.9761
31     0.051958    0.076384   0.9760
32     0.051441    0.077114   0.9762
33     0.051510    0.076768   0.9765
34     0.051738    0.075263   0.9765
35     0.051582    0.076336   0.9766
36     0.051554    0.076733   0.9765
37     0.051517    0.075672   0.9765
38     0.051149    0.076931   0.9767
39     0.051681    0.076926   0.9766
40     0.051480    0.075434   0.9767
41     0.051314    0.076437   0.9766
42     0.051422    0.077594   0.9764
43     0.051218    0.076898   0.9765
44     0.051629    0.079531   0.9759
45     0.051312    0.075378   0.9770
46     0.051388    0.075428   0.9767
47     0.051561    0.077049   0.9760
48     0.051471    0.077946   0.9757
49     0.051129    0.075431   0.9770
50     0.051524    0.076900   0.9767
Remaining weight 6.95 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.298508   0.1462
1     0.423295    0.160287   0.9540
2     0.133460    0.116010   0.9654
3     0.101394    0.102360   0.9691
4     0.085397    0.092653   0.9722
5     0.075710    0.086512   0.9737
6     0.068953    0.086174   0.9742
7     0.064592    0.081749   0.9756
8     0.061444    0.084193   0.9747
9     0.059425    0.080492   0.9758
10     0.057383    0.079910   0.9762
11     0.056176    0.080060   0.9758
12     0.055317    0.078793   0.9765
13     0.054776    0.078427   0.9768
14     0.054131    0.077672   0.9764
15     0.053922    0.077112   0.9766
16     0.053303    0.078226   0.9764
17     0.052906    0.076693   0.9767
18     0.052883    0.076783   0.9763
19     0.052734    0.078295   0.9756
20     0.052446    0.078949   0.9761
21     0.052542    0.078148   0.9764
22     0.051818    0.078979   0.9758
23     0.052319    0.077814   0.9758
24     0.051963    0.077685   0.9764
25     0.052055    0.077188   0.9764
26     0.052099    0.076768   0.9764
27     0.051489    0.077656   0.9765
28     0.051744    0.077331   0.9760
29     0.051766    0.076870   0.9764
30     0.051639    0.076480   0.9766
31     0.051748    0.078466   0.9758
32     0.051610    0.076030   0.9768
33     0.051508    0.076814   0.9767
34     0.051542    0.079138   0.9762
35     0.051351    0.077418   0.9765
36     0.051564    0.079422   0.9757
37     0.051458    0.077521   0.9766
38     0.051327    0.077423   0.9763
39     0.051505    0.075916   0.9771
40     0.051516    0.075542   0.9767
41     0.050936    0.077091   0.9763
42     0.051569    0.076893   0.9768
43     0.051400    0.078038   0.9765
44     0.051361    0.075876   0.9764
45     0.051224    0.077908   0.9764
46     0.051458    0.076403   0.9763
47     0.051259    0.076464   0.9768
48     0.051204    0.076761   0.9759
49     0.051325    0.075790   0.9770
50     0.051420    0.078109   0.9766
Remaining weight 5.57 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.296206   0.1360
1     0.422906    0.160666   0.9539
2     0.130058    0.114713   0.9660
3     0.097975    0.099177   0.9700
4     0.082547    0.093388   0.9714
5     0.073077    0.085941   0.9740
6     0.067104    0.083104   0.9750
7     0.062571    0.081307   0.9750
8     0.059189    0.080248   0.9758
9     0.056890    0.078030   0.9768
10     0.055079    0.079340   0.9759
11     0.053734    0.078061   0.9764
12     0.052914    0.079662   0.9760
13     0.052168    0.076056   0.9767
14     0.051754    0.077326   0.9763
15     0.051327    0.076510   0.9765
16     0.051195    0.076401   0.9766
17     0.050660    0.076395   0.9771
18     0.050695    0.074873   0.9771
19     0.050446    0.075222   0.9764
20     0.050311    0.077432   0.9761
21     0.050291    0.077269   0.9765
22     0.049847    0.074709   0.9764
23     0.049969    0.075592   0.9766
24     0.049800    0.075099   0.9769
25     0.049706    0.075421   0.9772
26     0.049710    0.075311   0.9770
27     0.049877    0.074482   0.9768
28     0.049612    0.074792   0.9771
29     0.049670    0.075423   0.9765
30     0.049652    0.074207   0.9769
31     0.049715    0.075936   0.9771
32     0.049743    0.075149   0.9769
33     0.049719    0.075513   0.9768
34     0.049395    0.076177   0.9768
35     0.049382    0.075647   0.9767
36     0.049531    0.076774   0.9764
37     0.049495    0.075131   0.9772
38     0.049513    0.075749   0.9770
39     0.049461    0.075555   0.9771
40     0.049521    0.074707   0.9774
41     0.049469    0.074995   0.9769
42     0.049475    0.076118   0.9767
43     0.049436    0.076667   0.9762
44     0.049534    0.075600   0.9768
45     0.049591    0.077097   0.9767
46     0.049386    0.075856   0.9767
47     0.049617    0.075729   0.9766
48     0.049408    0.076238   0.9765
49     0.049533    0.075444   0.9767
50     0.049460    0.075974   0.9764
Remaining weight 4.46 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.299838   0.1416
1     0.421859    0.157121   0.9538
2     0.132340    0.116647   0.9654
3     0.100507    0.101872   0.9687
4     0.084976    0.093936   0.9714
5     0.075007    0.089329   0.9722
6     0.068237    0.086740   0.9734
7     0.063853    0.081467   0.9754
8     0.060507    0.081039   0.9755
9     0.058033    0.078790   0.9762
10     0.056396    0.078384   0.9764
11     0.055197    0.079115   0.9759
12     0.054146    0.076886   0.9765
13     0.053668    0.078455   0.9762
14     0.053219    0.077157   0.9766
15     0.052617    0.075811   0.9765
16     0.052536    0.077175   0.9768
17     0.052138    0.077722   0.9762
18     0.052042    0.077905   0.9761
19     0.051654    0.077884   0.9760
20     0.051722    0.076803   0.9766
21     0.051569    0.076963   0.9770
22     0.051343    0.077104   0.9764
23     0.051428    0.075124   0.9776
24     0.051273    0.077168   0.9767
25     0.051224    0.076583   0.9770
26     0.051255    0.076283   0.9762
27     0.051102    0.076701   0.9767
28     0.050866    0.076464   0.9762
29     0.051327    0.075926   0.9765
30     0.050837    0.077227   0.9763
31     0.051103    0.074884   0.9775
32     0.050895    0.076264   0.9761
33     0.051017    0.077044   0.9763
34     0.050809    0.075723   0.9767
35     0.050534    0.076886   0.9764
36     0.050631    0.075400   0.9768
37     0.050653    0.076620   0.9764
38     0.050784    0.074734   0.9772
39     0.050801    0.075286   0.9773
40     0.050565    0.076618   0.9767
41     0.050843    0.077756   0.9756
42     0.050783    0.075410   0.9771
43     0.050505    0.075301   0.9769
44     0.050576    0.075801   0.9774
45     0.050475    0.076523   0.9764
46     0.050614    0.076525   0.9765
47     0.050767    0.076717   0.9763
48     0.050736    0.076325   0.9771
49     0.050856    0.073793   0.9775
50     0.050611    0.076184   0.9767
Remaining weight 3.58 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.298432   0.1163
1     0.424976    0.157592   0.9541
2     0.128466    0.113086   0.9667
3     0.097150    0.101256   0.9695
4     0.082477    0.093398   0.9712
5     0.073308    0.088103   0.9733
6     0.067247    0.086833   0.9732
7     0.062802    0.083826   0.9741
8     0.059864    0.080520   0.9754
9     0.057279    0.079277   0.9756
10     0.055567    0.080687   0.9751
11     0.054446    0.077612   0.9764
12     0.053206    0.076765   0.9765
13     0.052600    0.077488   0.9763
14     0.052130    0.077864   0.9761
15     0.051451    0.077137   0.9767
16     0.051353    0.077352   0.9764
17     0.050922    0.075673   0.9762
18     0.050933    0.075193   0.9769
19     0.050711    0.075989   0.9773
20     0.050624    0.074904   0.9770
21     0.050508    0.076757   0.9764
22     0.050199    0.077353   0.9760
23     0.050232    0.075879   0.9773
24     0.050323    0.075883   0.9761
25     0.050108    0.075819   0.9771
26     0.050176    0.076890   0.9761
27     0.050104    0.075579   0.9771
28     0.050016    0.075910   0.9762
29     0.049963    0.076770   0.9761
30     0.049962    0.076974   0.9760
31     0.049919    0.076010   0.9766
32     0.050048    0.075980   0.9761
33     0.050078    0.077038   0.9756
34     0.049776    0.075165   0.9767
35     0.050064    0.075391   0.9764
36     0.049797    0.075552   0.9767
37     0.049768    0.076858   0.9767
38     0.049766    0.076416   0.9769
39     0.049830    0.075746   0.9766
40     0.050051    0.075826   0.9761
41     0.049942    0.075970   0.9764
42     0.049878    0.076273   0.9767
43     0.049708    0.075073   0.9771
44     0.049783    0.076640   0.9770
45     0.049804    0.077431   0.9763
46     0.050028    0.076908   0.9762
47     0.050030    0.076223   0.9764
48     0.050095    0.075753   0.9766
49     0.049886    0.074657   0.9769
50     0.049568    0.077454   0.9764
Remaining weight 2.87 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.296266   0.1141
1     0.428570    0.158172   0.9542
2     0.131198    0.117396   0.9644
3     0.099261    0.100641   0.9695
4     0.083629    0.092904   0.9715
5     0.074403    0.089298   0.9722
6     0.068685    0.084597   0.9745
7     0.064716    0.082901   0.9748
8     0.061595    0.081691   0.9750
9     0.059332    0.080521   0.9755
10     0.057864    0.080866   0.9748
11     0.056542    0.078828   0.9759
12     0.055436    0.079981   0.9754
13     0.055000    0.079533   0.9755
14     0.054200    0.078023   0.9759
15     0.053640    0.077421   0.9764
16     0.053493    0.077132   0.9763
17     0.053066    0.077441   0.9755
18     0.052921    0.077580   0.9765
19     0.052562    0.078303   0.9757
20     0.052421    0.076916   0.9765
21     0.052447    0.077823   0.9759
22     0.052101    0.077420   0.9761
23     0.052138    0.076663   0.9767
24     0.051998    0.077030   0.9766
25     0.051879    0.076521   0.9760
26     0.051875    0.077360   0.9765
27     0.051657    0.077991   0.9759
28     0.051810    0.077606   0.9761
29     0.051795    0.077638   0.9762
30     0.051389    0.076010   0.9758
31     0.051650    0.076131   0.9762
32     0.051535    0.078001   0.9758
33     0.051764    0.077433   0.9763
34     0.051385    0.078249   0.9755
35     0.051435    0.077354   0.9761
36     0.051616    0.076707   0.9761
37     0.051406    0.077555   0.9762
38     0.051378    0.076291   0.9762
39     0.051622    0.076478   0.9764
40     0.051615    0.076947   0.9760
41     0.051357    0.077676   0.9759
42     0.051624    0.075717   0.9765
43     0.051418    0.075805   0.9772
44     0.051457    0.076764   0.9763
45     0.051359    0.076974   0.9759
46     0.051201    0.077972   0.9759
47     0.051513    0.076476   0.9763
48     0.051366    0.077776   0.9758
49     0.051265    0.076738   0.9768
50     0.051388    0.077513   0.9764
Remaining weight 2.30 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.295082   0.1025
1     0.430057    0.154701   0.9564
2     0.129694    0.115306   0.9657
3     0.098152    0.099564   0.9703
4     0.083669    0.093595   0.9721
5     0.075008    0.089268   0.9725
6     0.068843    0.085332   0.9738
7     0.064764    0.084021   0.9739
8     0.061842    0.082866   0.9743
9     0.059638    0.082783   0.9746
10     0.057990    0.079534   0.9753
11     0.056977    0.080145   0.9759
12     0.056016    0.079773   0.9757
13     0.055455    0.078255   0.9760
14     0.054824    0.078987   0.9757
15     0.054320    0.078163   0.9757
16     0.054130    0.079794   0.9755
17     0.053684    0.078734   0.9760
18     0.053560    0.078424   0.9758
19     0.053105    0.077417   0.9762
20     0.053129    0.077378   0.9760
21     0.053042    0.077851   0.9760
22     0.052874    0.077447   0.9760
23     0.052652    0.079380   0.9754
24     0.052725    0.076772   0.9762
25     0.052619    0.076690   0.9763
26     0.052568    0.078097   0.9759
27     0.052287    0.078155   0.9751
28     0.052327    0.077432   0.9759
29     0.052274    0.077095   0.9761
30     0.052357    0.078658   0.9752
31     0.052319    0.077115   0.9755
32     0.052058    0.077687   0.9759
33     0.052301    0.076848   0.9764
34     0.052238    0.076676   0.9766
35     0.052160    0.077607   0.9758
36     0.052070    0.076932   0.9763
37     0.052143    0.077920   0.9755
38     0.052003    0.076097   0.9764
39     0.052146    0.076704   0.9765
40     0.052126    0.078310   0.9755
41     0.051949    0.075562   0.9762
42     0.051776    0.076841   0.9762
43     0.051656    0.077196   0.9762
44     0.051966    0.078160   0.9759
45     0.052099    0.077746   0.9759
46     0.051892    0.078118   0.9755
47     0.051751    0.076949   0.9762
48     0.051836    0.076534   0.9769
49     0.051708    0.077971   0.9759
50     0.051844    0.077931   0.9759
Remaining weight 1.85 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.296160   0.1064
1     0.434365    0.157521   0.9565
2     0.129891    0.114542   0.9669
3     0.097830    0.100433   0.9701
4     0.082713    0.092327   0.9724
5     0.073457    0.090238   0.9723
6     0.067923    0.086639   0.9735
7     0.063911    0.083219   0.9742
8     0.061188    0.081852   0.9752
9     0.059587    0.082158   0.9748
10     0.057716    0.081106   0.9754
11     0.056596    0.080651   0.9755
12     0.056145    0.079408   0.9758
13     0.055357    0.078724   0.9760
14     0.054859    0.078154   0.9757
15     0.054517    0.079320   0.9756
16     0.054318    0.078505   0.9761
17     0.053938    0.078583   0.9758
18     0.053576    0.079297   0.9756
19     0.053506    0.078860   0.9755
20     0.053477    0.078030   0.9761
21     0.053204    0.077752   0.9756
22     0.053005    0.079567   0.9753
23     0.052894    0.079253   0.9754
24     0.052730    0.077331   0.9755
25     0.053003    0.077203   0.9759
26     0.052731    0.078744   0.9756
27     0.052641    0.077072   0.9759
28     0.052778    0.077457   0.9761
29     0.052760    0.077500   0.9759
30     0.052642    0.077725   0.9757
31     0.052556    0.078751   0.9758
32     0.052416    0.077752   0.9758
33     0.052597    0.076798   0.9761
34     0.052460    0.077900   0.9758
35     0.052673    0.078325   0.9758
36     0.052542    0.076957   0.9761
37     0.052474    0.078190   0.9757
38     0.052367    0.076041   0.9764
39     0.052248    0.077543   0.9756
40     0.052414    0.077879   0.9754
41     0.052467    0.076936   0.9756
42     0.052372    0.076267   0.9760
43     0.052260    0.076976   0.9758
44     0.052136    0.078012   0.9752
45     0.052249    0.076974   0.9759
46     0.052276    0.077444   0.9758
47     0.052403    0.077099   0.9758
48     0.052389    0.078553   0.9750
49     0.052157    0.076195   0.9761
50     0.052347    0.076931   0.9762
Remaining weight 1.48 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.297987   0.0980
1     0.440785    0.159568   0.9544
2     0.132858    0.116352   0.9653
3     0.100332    0.102701   0.9690
4     0.084973    0.093795   0.9717
5     0.075726    0.088925   0.9731
6     0.069957    0.087652   0.9738
7     0.066131    0.084540   0.9743
8     0.063489    0.082871   0.9746
9     0.061743    0.082701   0.9749
10     0.060152    0.083326   0.9745
11     0.059074    0.081592   0.9751
12     0.058304    0.081654   0.9752
13     0.057495    0.080715   0.9756
14     0.056933    0.079766   0.9760
15     0.056551    0.078822   0.9760
16     0.056273    0.079340   0.9760
17     0.055993    0.081024   0.9759
18     0.055846    0.079998   0.9756
19     0.055697    0.080539   0.9749
20     0.055483    0.077929   0.9759
21     0.055476    0.078724   0.9758
22     0.055214    0.079434   0.9757
23     0.055263    0.079660   0.9757
24     0.055160    0.079048   0.9759
25     0.054802    0.078362   0.9760
26     0.054737    0.081001   0.9747
27     0.054798    0.079263   0.9761
28     0.054803    0.078130   0.9760
29     0.054793    0.079205   0.9752
30     0.054527    0.079516   0.9752
31     0.054793    0.079657   0.9754
32     0.054445    0.079240   0.9758
33     0.054632    0.077964   0.9762
34     0.054404    0.078059   0.9759
35     0.054134    0.078426   0.9761
36     0.054134    0.078640   0.9758
37     0.054059    0.079136   0.9758
38     0.054169    0.079425   0.9749
39     0.054198    0.077605   0.9761
40     0.054200    0.079665   0.9757
41     0.054009    0.078964   0.9751
42     0.054209    0.079140   0.9758
43     0.054033    0.078929   0.9755
44     0.053953    0.078432   0.9753
45     0.053978    0.079008   0.9757
46     0.053916    0.078573   0.9755
47     0.054021    0.079124   0.9753
48     0.054064    0.078648   0.9756
49     0.054116    0.078367   0.9755
50     0.054112    0.077966   0.9760
Remaining weight 1.19 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.299806   0.0947
1     0.447736    0.162398   0.9549
2     0.134226    0.119109   0.9654
3     0.101912    0.102619   0.9697
4     0.086156    0.095007   0.9714
5     0.076699    0.090307   0.9722
6     0.070667    0.087978   0.9728
7     0.066677    0.085300   0.9739
8     0.063793    0.084441   0.9739
9     0.061637    0.082657   0.9745
10     0.060091    0.083988   0.9746
11     0.058989    0.081733   0.9747
12     0.057850    0.080649   0.9751
13     0.057201    0.080614   0.9749
14     0.056687    0.079632   0.9752
15     0.056304    0.080283   0.9750
16     0.056031    0.080121   0.9757
17     0.055762    0.078952   0.9752
18     0.055488    0.079292   0.9749
19     0.055236    0.078330   0.9755
20     0.055143    0.079224   0.9759
21     0.054941    0.078444   0.9758
22     0.054968    0.078864   0.9756
23     0.054735    0.080086   0.9758
24     0.054535    0.078489   0.9754
25     0.054775    0.079836   0.9752
26     0.054586    0.079201   0.9753
27     0.054608    0.080966   0.9746
28     0.054295    0.079366   0.9755
29     0.054659    0.080630   0.9746
30     0.054665    0.078578   0.9753
31     0.054525    0.079688   0.9753
32     0.054465    0.078996   0.9752
33     0.054474    0.079540   0.9750
34     0.054407    0.080076   0.9750
35     0.054468    0.079227   0.9756
36     0.054369    0.078585   0.9755
37     0.054214    0.079786   0.9748
38     0.054455    0.078892   0.9761
39     0.054188    0.078441   0.9755
40     0.054188    0.078797   0.9753
41     0.054270    0.078583   0.9751
42     0.054093    0.079348   0.9749
43     0.054152    0.077626   0.9759
44     0.054153    0.078589   0.9755
45     0.054034    0.078029   0.9758
46     0.054132    0.079398   0.9752
47     0.054250    0.077414   0.9759
48     0.054132    0.079108   0.9757
49     0.054124    0.078869   0.9752
50     0.053975    0.079173   0.9754
