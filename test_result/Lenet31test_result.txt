model_type: Lenet_300_100
lr: 0.0012
epochs: 50
batch_size: 60
weight_decay: 0.0012
prune_per_c: 1
prune_per_f: 0.2
prune_per_o: 0.1
test_iter: 1
prune_iter: 20
trainset: Dataset MNIST
    Number of datapoints: 60000
    Root location: ../MNIST_data/
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=(0.1307,), std=(0.3081,))
           )
valset: empty
testset: Dataset MNIST
    Number of datapoints: 10000
    Root location: ../MNIST_data/
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=(0.1307,), std=(0.3081,))
           )
train_loader: <torch.utils.data.dataloader.DataLoader object at 0x7f35ec01a690>
val_loader: empty
test_loader: <torch.utils.data.dataloader.DataLoader object at 0x7f35ec024090> 


Model structure
 Lenet_300_100(
  (fc1): Linear(in_features=784, out_features=300, bias=True)
  (fc2): Linear(in_features=300, out_features=100, bias=True)
  (fcout): Linear(in_features=100, out_features=10, bias=True)
)
===================================================================== 

Test_Iter (1/1)
------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :        266200 (266200 | 0)          100.00
fc1.weight   :        235200 (235200 | 0)          100.00
fc2.weight   :         30000 (30000 | 0)           100.00
fcout.weight :          1000 (1000 | 0)            100.00
------------------------------------------------------------

Learning start! [Prune_iter : (1/20), Remaining weight : 100.0 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.80063) (accu: 0.1075)
[epoch : 1] (l_loss: 0.20361) (t_loss: 0.12240) (accu: 0.9619)
[epoch : 2] (l_loss: 0.10764) (t_loss: 0.10271) (accu: 0.9688)
[epoch : 3] (l_loss: 0.09417) (t_loss: 0.09482) (accu: 0.9713)
[epoch : 4] (l_loss: 0.08688) (t_loss: 0.12156) (accu: 0.9610)
[epoch : 5] (l_loss: 0.08045) (t_loss: 0.10094) (accu: 0.9690)
[epoch : 6] (l_loss: 0.07738) (t_loss: 0.10002) (accu: 0.9676)
[epoch : 7] (l_loss: 0.07409) (t_loss: 0.08075) (accu: 0.9750)
[epoch : 8] (l_loss: 0.07324) (t_loss: 0.08404) (accu: 0.9736)
[epoch : 9] (l_loss: 0.06763) (t_loss: 0.08747) (accu: 0.9726)
[epoch : 10] (l_loss: 0.06835) (t_loss: 0.09146) (accu: 0.9741)
[epoch : 11] (l_loss: 0.06658) (t_loss: 0.08126) (accu: 0.9749)
[epoch : 12] (l_loss: 0.06312) (t_loss: 0.07558) (accu: 0.9771)
[epoch : 13] (l_loss: 0.06243) (t_loss: 0.07468) (accu: 0.9777)
[epoch : 14] (l_loss: 0.06319) (t_loss: 0.08989) (accu: 0.9747)
[epoch : 15] (l_loss: 0.06306) (t_loss: 0.08526) (accu: 0.9738)
[epoch : 16] (l_loss: 0.06145) (t_loss: 0.08167) (accu: 0.9743)
[epoch : 17] (l_loss: 0.06054) (t_loss: 0.08286) (accu: 0.9753)
[epoch : 18] (l_loss: 0.06103) (t_loss: 0.08414) (accu: 0.9737)
[epoch : 19] (l_loss: 0.06108) (t_loss: 0.07945) (accu: 0.9744)
[epoch : 20] (l_loss: 0.05898) (t_loss: 0.09196) (accu: 0.9723)
[epoch : 21] (l_loss: 0.05851) (t_loss: 0.08110) (accu: 0.9751)
[epoch : 22] (l_loss: 0.05859) (t_loss: 0.07219) (accu: 0.9792)
[epoch : 23] (l_loss: 0.05860) (t_loss: 0.08265) (accu: 0.9736)
[epoch : 24] (l_loss: 0.05732) (t_loss: 0.07021) (accu: 0.9795)
[epoch : 25] (l_loss: 0.05700) (t_loss: 0.08040) (accu: 0.9739)
[epoch : 26] (l_loss: 0.05542) (t_loss: 0.07995) (accu: 0.9740)
[epoch : 27] (l_loss: 0.05889) (t_loss: 0.07659) (accu: 0.9736)
[epoch : 28] (l_loss: 0.05875) (t_loss: 0.08010) (accu: 0.9744)
[epoch : 29] (l_loss: 0.05722) (t_loss: 0.07032) (accu: 0.9788)
[epoch : 30] (l_loss: 0.05623) (t_loss: 0.07375) (accu: 0.9785)
[epoch : 31] (l_loss: 0.05787) (t_loss: 0.07603) (accu: 0.9758)
[epoch : 32] (l_loss: 0.05510) (t_loss: 0.07830) (accu: 0.9761)
[epoch : 33] (l_loss: 0.05660) (t_loss: 0.08139) (accu: 0.9743)
[epoch : 34] (l_loss: 0.05635) (t_loss: 0.09068) (accu: 0.9716)
[epoch : 35] (l_loss: 0.05504) (t_loss: 0.07742) (accu: 0.9772)
[epoch : 36] (l_loss: 0.05489) (t_loss: 0.06964) (accu: 0.9786)
[epoch : 37] (l_loss: 0.05691) (t_loss: 0.07306) (accu: 0.9787)
[epoch : 38] (l_loss: 0.05636) (t_loss: 0.07227) (accu: 0.9778)
[epoch : 39] (l_loss: 0.05522) (t_loss: 0.06460) (accu: 0.9789)
[epoch : 40] (l_loss: 0.05605) (t_loss: 0.07672) (accu: 0.9777)
[epoch : 41] (l_loss: 0.05585) (t_loss: 0.07491) (accu: 0.9772)
[epoch : 42] (l_loss: 0.05678) (t_loss: 0.09029) (accu: 0.9724)
[epoch : 43] (l_loss: 0.05552) (t_loss: 0.07497) (accu: 0.9772)
[epoch : 44] (l_loss: 0.05551) (t_loss: 0.07398) (accu: 0.9772)
[epoch : 45] (l_loss: 0.05632) (t_loss: 0.08837) (accu: 0.9730)
[epoch : 46] (l_loss: 0.05467) (t_loss: 0.07287) (accu: 0.9781)
[epoch : 47] (l_loss: 0.05626) (t_loss: 0.08047) (accu: 0.9750)
[epoch : 48] (l_loss: 0.05495) (t_loss: 0.08686) (accu: 0.9723)
[epoch : 49] (l_loss: 0.05588) (t_loss: 0.07804) (accu: 0.9758)
[epoch : 50] (l_loss: 0.05453) (t_loss: 0.08682) (accu: 0.9723)
Finish! (Best accu: 0.9795) (Time taken(sec) : 551.80) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (213060 | 53140)         80.04
fc1.weight   :      235200 (188160 | 47040)         80.00
fc2.weight   :        30000 (24000 | 6000)          80.00
fcout.weight :          1000 (900 | 100)            90.00
------------------------------------------------------------

Learning start! [Prune_iter : (2/20), Remaining weight : 80.04 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.59082) (accu: 0.0960)
[epoch : 1] (l_loss: 0.19762) (t_loss: 0.10634) (accu: 0.9651)
[epoch : 2] (l_loss: 0.10311) (t_loss: 0.09475) (accu: 0.9708)
[epoch : 3] (l_loss: 0.09027) (t_loss: 0.10376) (accu: 0.9646)
[epoch : 4] (l_loss: 0.08073) (t_loss: 0.08399) (accu: 0.9743)
[epoch : 5] (l_loss: 0.07662) (t_loss: 0.08844) (accu: 0.9714)
[epoch : 6] (l_loss: 0.07505) (t_loss: 0.08016) (accu: 0.9757)
[epoch : 7] (l_loss: 0.06799) (t_loss: 0.07800) (accu: 0.9767)
[epoch : 8] (l_loss: 0.06552) (t_loss: 0.08442) (accu: 0.9742)
[epoch : 9] (l_loss: 0.06463) (t_loss: 0.07584) (accu: 0.9751)
[epoch : 10] (l_loss: 0.06174) (t_loss: 0.07457) (accu: 0.9791)
[epoch : 11] (l_loss: 0.06074) (t_loss: 0.07470) (accu: 0.9759)
[epoch : 12] (l_loss: 0.05977) (t_loss: 0.07438) (accu: 0.9755)
[epoch : 13] (l_loss: 0.06002) (t_loss: 0.08123) (accu: 0.9726)
[epoch : 14] (l_loss: 0.05910) (t_loss: 0.07117) (accu: 0.9771)
[epoch : 15] (l_loss: 0.05645) (t_loss: 0.06957) (accu: 0.9780)
[epoch : 16] (l_loss: 0.05823) (t_loss: 0.08819) (accu: 0.9726)
[epoch : 17] (l_loss: 0.05450) (t_loss: 0.08261) (accu: 0.9733)
[epoch : 18] (l_loss: 0.05683) (t_loss: 0.07877) (accu: 0.9752)
[epoch : 19] (l_loss: 0.05523) (t_loss: 0.09111) (accu: 0.9694)
[epoch : 20] (l_loss: 0.05555) (t_loss: 0.09060) (accu: 0.9714)
[epoch : 21] (l_loss: 0.05420) (t_loss: 0.07526) (accu: 0.9764)
[epoch : 22] (l_loss: 0.05534) (t_loss: 0.07708) (accu: 0.9744)
[epoch : 23] (l_loss: 0.05271) (t_loss: 0.07333) (accu: 0.9779)
[epoch : 24] (l_loss: 0.05348) (t_loss: 0.07492) (accu: 0.9762)
[epoch : 25] (l_loss: 0.05551) (t_loss: 0.07805) (accu: 0.9760)
[epoch : 26] (l_loss: 0.05407) (t_loss: 0.07176) (accu: 0.9767)
[epoch : 27] (l_loss: 0.05237) (t_loss: 0.08408) (accu: 0.9745)
[epoch : 28] (l_loss: 0.05358) (t_loss: 0.07976) (accu: 0.9757)
[epoch : 29] (l_loss: 0.05353) (t_loss: 0.07644) (accu: 0.9774)
[epoch : 30] (l_loss: 0.05449) (t_loss: 0.09489) (accu: 0.9708)
[epoch : 31] (l_loss: 0.05412) (t_loss: 0.07669) (accu: 0.9765)
[epoch : 32] (l_loss: 0.05157) (t_loss: 0.08142) (accu: 0.9759)
[epoch : 33] (l_loss: 0.05388) (t_loss: 0.07336) (accu: 0.9759)
[epoch : 34] (l_loss: 0.05237) (t_loss: 0.07318) (accu: 0.9771)
[epoch : 35] (l_loss: 0.05164) (t_loss: 0.07562) (accu: 0.9753)
[epoch : 36] (l_loss: 0.05321) (t_loss: 0.06858) (accu: 0.9779)
[epoch : 37] (l_loss: 0.05124) (t_loss: 0.08149) (accu: 0.9752)
[epoch : 38] (l_loss: 0.05312) (t_loss: 0.07876) (accu: 0.9763)
[epoch : 39] (l_loss: 0.05202) (t_loss: 0.07620) (accu: 0.9774)
[epoch : 40] (l_loss: 0.05162) (t_loss: 0.07488) (accu: 0.9770)
[epoch : 41] (l_loss: 0.05239) (t_loss: 0.07198) (accu: 0.9785)
[epoch : 42] (l_loss: 0.05176) (t_loss: 0.08693) (accu: 0.9741)
[epoch : 43] (l_loss: 0.05045) (t_loss: 0.07925) (accu: 0.9744)
[epoch : 44] (l_loss: 0.05056) (t_loss: 0.07992) (accu: 0.9758)
[epoch : 45] (l_loss: 0.05036) (t_loss: 0.07630) (accu: 0.9758)
[epoch : 46] (l_loss: 0.05250) (t_loss: 0.06866) (accu: 0.9787)
[epoch : 47] (l_loss: 0.05091) (t_loss: 0.07701) (accu: 0.9761)
[epoch : 48] (l_loss: 0.05091) (t_loss: 0.07657) (accu: 0.9772)
[epoch : 49] (l_loss: 0.05144) (t_loss: 0.07027) (accu: 0.9776)
[epoch : 50] (l_loss: 0.05072) (t_loss: 0.07380) (accu: 0.9775)
Finish! (Best accu: 0.9791) (Time taken(sec) : 567.75) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (170538 | 95662)         64.06
fc1.weight   :      235200 (150528 | 84672)         64.00
fc2.weight   :       30000 (19200 | 10800)          64.00
fcout.weight :          1000 (810 | 190)            81.00
------------------------------------------------------------

Learning start! [Prune_iter : (3/20), Remaining weight : 64.06 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.54096) (accu: 0.0832)
[epoch : 1] (l_loss: 0.19928) (t_loss: 0.10591) (accu: 0.9661)
[epoch : 2] (l_loss: 0.10261) (t_loss: 0.10083) (accu: 0.9698)
[epoch : 3] (l_loss: 0.08419) (t_loss: 0.09094) (accu: 0.9708)
[epoch : 4] (l_loss: 0.07703) (t_loss: 0.08523) (accu: 0.9740)
[epoch : 5] (l_loss: 0.07289) (t_loss: 0.08685) (accu: 0.9726)
[epoch : 6] (l_loss: 0.06939) (t_loss: 0.07947) (accu: 0.9754)
[epoch : 7] (l_loss: 0.06611) (t_loss: 0.08826) (accu: 0.9716)
[epoch : 8] (l_loss: 0.06465) (t_loss: 0.08917) (accu: 0.9728)
[epoch : 9] (l_loss: 0.06308) (t_loss: 0.07309) (accu: 0.9780)
[epoch : 10] (l_loss: 0.06214) (t_loss: 0.07921) (accu: 0.9752)
[epoch : 11] (l_loss: 0.06008) (t_loss: 0.07203) (accu: 0.9777)
[epoch : 12] (l_loss: 0.05769) (t_loss: 0.08644) (accu: 0.9724)
[epoch : 13] (l_loss: 0.05903) (t_loss: 0.08303) (accu: 0.9732)
[epoch : 14] (l_loss: 0.05728) (t_loss: 0.07138) (accu: 0.9780)
[epoch : 15] (l_loss: 0.05724) (t_loss: 0.07405) (accu: 0.9769)
[epoch : 16] (l_loss: 0.05723) (t_loss: 0.06852) (accu: 0.9781)
[epoch : 17] (l_loss: 0.05490) (t_loss: 0.09450) (accu: 0.9721)
[epoch : 18] (l_loss: 0.05501) (t_loss: 0.07988) (accu: 0.9729)
[epoch : 19] (l_loss: 0.05516) (t_loss: 0.08096) (accu: 0.9752)
[epoch : 20] (l_loss: 0.05485) (t_loss: 0.06917) (accu: 0.9789)
[epoch : 21] (l_loss: 0.05327) (t_loss: 0.08314) (accu: 0.9748)
[epoch : 22] (l_loss: 0.05377) (t_loss: 0.07702) (accu: 0.9767)
[epoch : 23] (l_loss: 0.05341) (t_loss: 0.07596) (accu: 0.9767)
[epoch : 24] (l_loss: 0.05378) (t_loss: 0.07277) (accu: 0.9783)
[epoch : 25] (l_loss: 0.05367) (t_loss: 0.06984) (accu: 0.9797)
[epoch : 26] (l_loss: 0.05268) (t_loss: 0.07285) (accu: 0.9763)
[epoch : 27] (l_loss: 0.05252) (t_loss: 0.07726) (accu: 0.9777)
[epoch : 28] (l_loss: 0.05215) (t_loss: 0.07905) (accu: 0.9753)
[epoch : 29] (l_loss: 0.05303) (t_loss: 0.07747) (accu: 0.9753)
[epoch : 30] (l_loss: 0.05086) (t_loss: 0.07557) (accu: 0.9778)
[epoch : 31] (l_loss: 0.05194) (t_loss: 0.08716) (accu: 0.9723)
[epoch : 32] (l_loss: 0.05200) (t_loss: 0.07120) (accu: 0.9781)
[epoch : 33] (l_loss: 0.05183) (t_loss: 0.06993) (accu: 0.9783)
[epoch : 34] (l_loss: 0.05095) (t_loss: 0.06940) (accu: 0.9794)
[epoch : 35] (l_loss: 0.05148) (t_loss: 0.07558) (accu: 0.9768)
[epoch : 36] (l_loss: 0.05062) (t_loss: 0.08871) (accu: 0.9717)
[epoch : 37] (l_loss: 0.05131) (t_loss: 0.08124) (accu: 0.9748)
[epoch : 38] (l_loss: 0.05130) (t_loss: 0.07238) (accu: 0.9772)
[epoch : 39] (l_loss: 0.05008) (t_loss: 0.07213) (accu: 0.9777)
[epoch : 40] (l_loss: 0.05063) (t_loss: 0.07365) (accu: 0.9770)
[epoch : 41] (l_loss: 0.05139) (t_loss: 0.06534) (accu: 0.9804)
[epoch : 42] (l_loss: 0.05020) (t_loss: 0.07656) (accu: 0.9758)
[epoch : 43] (l_loss: 0.05068) (t_loss: 0.07476) (accu: 0.9761)
[epoch : 44] (l_loss: 0.04959) (t_loss: 0.07127) (accu: 0.9788)
[epoch : 45] (l_loss: 0.04957) (t_loss: 0.06825) (accu: 0.9787)
[epoch : 46] (l_loss: 0.05075) (t_loss: 0.06554) (accu: 0.9781)
[epoch : 47] (l_loss: 0.04930) (t_loss: 0.07711) (accu: 0.9761)
[epoch : 48] (l_loss: 0.04946) (t_loss: 0.07137) (accu: 0.9784)
[epoch : 49] (l_loss: 0.04993) (t_loss: 0.07654) (accu: 0.9770)
[epoch : 50] (l_loss: 0.05037) (t_loss: 0.07390) (accu: 0.9783)
Finish! (Best accu: 0.9804) (Time taken(sec) : 569.27) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (136511 | 129689)        51.28
fc1.weight   :      235200 (120422 | 114778)        51.20
fc2.weight   :       30000 (15360 | 14640)          51.20
fcout.weight :          1000 (729 | 271)            72.90
------------------------------------------------------------

Learning start! [Prune_iter : (4/20), Remaining weight : 51.28 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.44700) (accu: 0.0808)
[epoch : 1] (l_loss: 0.20626) (t_loss: 0.12851) (accu: 0.9591)
[epoch : 2] (l_loss: 0.10087) (t_loss: 0.10263) (accu: 0.9674)
[epoch : 3] (l_loss: 0.08578) (t_loss: 0.09841) (accu: 0.9700)
[epoch : 4] (l_loss: 0.07889) (t_loss: 0.10649) (accu: 0.9656)
[epoch : 5] (l_loss: 0.07164) (t_loss: 0.09609) (accu: 0.9721)
[epoch : 6] (l_loss: 0.07010) (t_loss: 0.08234) (accu: 0.9754)
[epoch : 7] (l_loss: 0.06566) (t_loss: 0.09020) (accu: 0.9729)
[epoch : 8] (l_loss: 0.06367) (t_loss: 0.08281) (accu: 0.9736)
[epoch : 9] (l_loss: 0.06155) (t_loss: 0.07881) (accu: 0.9746)
[epoch : 10] (l_loss: 0.06065) (t_loss: 0.08344) (accu: 0.9744)
[epoch : 11] (l_loss: 0.05847) (t_loss: 0.08327) (accu: 0.9750)
[epoch : 12] (l_loss: 0.05759) (t_loss: 0.07012) (accu: 0.9786)
[epoch : 13] (l_loss: 0.05702) (t_loss: 0.07640) (accu: 0.9762)
[epoch : 14] (l_loss: 0.05605) (t_loss: 0.07113) (accu: 0.9777)
[epoch : 15] (l_loss: 0.05402) (t_loss: 0.07443) (accu: 0.9764)
[epoch : 16] (l_loss: 0.05368) (t_loss: 0.07191) (accu: 0.9769)
[epoch : 17] (l_loss: 0.05388) (t_loss: 0.07619) (accu: 0.9773)
[epoch : 18] (l_loss: 0.05498) (t_loss: 0.07181) (accu: 0.9772)
[epoch : 19] (l_loss: 0.05294) (t_loss: 0.07638) (accu: 0.9771)
[epoch : 20] (l_loss: 0.05265) (t_loss: 0.07674) (accu: 0.9762)
[epoch : 21] (l_loss: 0.05187) (t_loss: 0.07554) (accu: 0.9773)
[epoch : 22] (l_loss: 0.05105) (t_loss: 0.07135) (accu: 0.9779)
[epoch : 23] (l_loss: 0.05203) (t_loss: 0.07178) (accu: 0.9781)
[epoch : 24] (l_loss: 0.05256) (t_loss: 0.07030) (accu: 0.9781)
[epoch : 25] (l_loss: 0.05276) (t_loss: 0.07480) (accu: 0.9761)
[epoch : 26] (l_loss: 0.05117) (t_loss: 0.08174) (accu: 0.9755)
[epoch : 27] (l_loss: 0.05125) (t_loss: 0.06810) (accu: 0.9791)
[epoch : 28] (l_loss: 0.05109) (t_loss: 0.06787) (accu: 0.9780)
[epoch : 29] (l_loss: 0.05096) (t_loss: 0.07818) (accu: 0.9762)
[epoch : 30] (l_loss: 0.05068) (t_loss: 0.06968) (accu: 0.9785)
[epoch : 31] (l_loss: 0.05132) (t_loss: 0.08287) (accu: 0.9744)
[epoch : 32] (l_loss: 0.05036) (t_loss: 0.07974) (accu: 0.9743)
[epoch : 33] (l_loss: 0.05118) (t_loss: 0.07576) (accu: 0.9760)
[epoch : 34] (l_loss: 0.05162) (t_loss: 0.06763) (accu: 0.9791)
[epoch : 35] (l_loss: 0.04989) (t_loss: 0.08037) (accu: 0.9754)
[epoch : 36] (l_loss: 0.05033) (t_loss: 0.07672) (accu: 0.9766)
[epoch : 37] (l_loss: 0.05041) (t_loss: 0.07048) (accu: 0.9788)
[epoch : 38] (l_loss: 0.04922) (t_loss: 0.07249) (accu: 0.9774)
[epoch : 39] (l_loss: 0.04865) (t_loss: 0.07156) (accu: 0.9782)
[epoch : 40] (l_loss: 0.04918) (t_loss: 0.07408) (accu: 0.9770)
[epoch : 41] (l_loss: 0.05171) (t_loss: 0.07225) (accu: 0.9788)
[epoch : 42] (l_loss: 0.04763) (t_loss: 0.07170) (accu: 0.9796)
[epoch : 43] (l_loss: 0.04929) (t_loss: 0.06684) (accu: 0.9810)
[epoch : 44] (l_loss: 0.04886) (t_loss: 0.06579) (accu: 0.9797)
[epoch : 45] (l_loss: 0.04829) (t_loss: 0.07942) (accu: 0.9747)
[epoch : 46] (l_loss: 0.04969) (t_loss: 0.06855) (accu: 0.9786)
[epoch : 47] (l_loss: 0.04927) (t_loss: 0.07412) (accu: 0.9749)
[epoch : 48] (l_loss: 0.04793) (t_loss: 0.07101) (accu: 0.9776)
[epoch : 49] (l_loss: 0.05172) (t_loss: 0.06957) (accu: 0.9782)
[epoch : 50] (l_loss: 0.04938) (t_loss: 0.08411) (accu: 0.9767)
Finish! (Best accu: 0.9810) (Time taken(sec) : 568.97) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (109282 | 156918)        41.05
fc1.weight   :      235200 (96338 | 138862)         40.96
fc2.weight   :       30000 (12288 | 17712)          40.96
fcout.weight :          1000 (656 | 344)            65.60
------------------------------------------------------------

Learning start! [Prune_iter : (5/20), Remaining weight : 41.05 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.42589) (accu: 0.0938)
[epoch : 1] (l_loss: 0.21178) (t_loss: 0.10898) (accu: 0.9696)
[epoch : 2] (l_loss: 0.09925) (t_loss: 0.09458) (accu: 0.9697)
[epoch : 3] (l_loss: 0.08467) (t_loss: 0.08430) (accu: 0.9737)
[epoch : 4] (l_loss: 0.07443) (t_loss: 0.08183) (accu: 0.9756)
[epoch : 5] (l_loss: 0.06960) (t_loss: 0.07603) (accu: 0.9752)
[epoch : 6] (l_loss: 0.06723) (t_loss: 0.07730) (accu: 0.9737)
[epoch : 7] (l_loss: 0.06469) (t_loss: 0.08012) (accu: 0.9736)
[epoch : 8] (l_loss: 0.06193) (t_loss: 0.07461) (accu: 0.9755)
[epoch : 9] (l_loss: 0.06195) (t_loss: 0.08164) (accu: 0.9748)
[epoch : 10] (l_loss: 0.06018) (t_loss: 0.07263) (accu: 0.9772)
[epoch : 11] (l_loss: 0.05886) (t_loss: 0.07415) (accu: 0.9762)
[epoch : 12] (l_loss: 0.05681) (t_loss: 0.08656) (accu: 0.9732)
[epoch : 13] (l_loss: 0.05726) (t_loss: 0.07071) (accu: 0.9778)
[epoch : 14] (l_loss: 0.05582) (t_loss: 0.08319) (accu: 0.9731)
[epoch : 15] (l_loss: 0.05567) (t_loss: 0.08021) (accu: 0.9743)
[epoch : 16] (l_loss: 0.05277) (t_loss: 0.07745) (accu: 0.9740)
[epoch : 17] (l_loss: 0.05497) (t_loss: 0.07599) (accu: 0.9751)
[epoch : 18] (l_loss: 0.05362) (t_loss: 0.07290) (accu: 0.9770)
[epoch : 19] (l_loss: 0.05250) (t_loss: 0.07776) (accu: 0.9749)
[epoch : 20] (l_loss: 0.05257) (t_loss: 0.07478) (accu: 0.9767)
[epoch : 21] (l_loss: 0.05238) (t_loss: 0.07160) (accu: 0.9783)
[epoch : 22] (l_loss: 0.05171) (t_loss: 0.06846) (accu: 0.9782)
[epoch : 23] (l_loss: 0.05144) (t_loss: 0.07132) (accu: 0.9773)
[epoch : 24] (l_loss: 0.05232) (t_loss: 0.08048) (accu: 0.9727)
[epoch : 25] (l_loss: 0.05089) (t_loss: 0.07620) (accu: 0.9760)
[epoch : 26] (l_loss: 0.05229) (t_loss: 0.07265) (accu: 0.9776)
[epoch : 27] (l_loss: 0.05125) (t_loss: 0.07805) (accu: 0.9758)
[epoch : 28] (l_loss: 0.05149) (t_loss: 0.07773) (accu: 0.9765)
[epoch : 29] (l_loss: 0.04989) (t_loss: 0.07310) (accu: 0.9777)
[epoch : 30] (l_loss: 0.05092) (t_loss: 0.07589) (accu: 0.9748)
[epoch : 31] (l_loss: 0.05013) (t_loss: 0.07343) (accu: 0.9765)
[epoch : 32] (l_loss: 0.05028) (t_loss: 0.07903) (accu: 0.9763)
[epoch : 33] (l_loss: 0.04979) (t_loss: 0.07788) (accu: 0.9766)
[epoch : 34] (l_loss: 0.04888) (t_loss: 0.06950) (accu: 0.9782)
[epoch : 35] (l_loss: 0.04952) (t_loss: 0.07573) (accu: 0.9768)
[epoch : 36] (l_loss: 0.05054) (t_loss: 0.07575) (accu: 0.9776)
[epoch : 37] (l_loss: 0.04954) (t_loss: 0.07320) (accu: 0.9792)
[epoch : 38] (l_loss: 0.04988) (t_loss: 0.07445) (accu: 0.9766)
[epoch : 39] (l_loss: 0.04843) (t_loss: 0.07180) (accu: 0.9772)
[epoch : 40] (l_loss: 0.05048) (t_loss: 0.08161) (accu: 0.9766)
[epoch : 41] (l_loss: 0.04873) (t_loss: 0.07853) (accu: 0.9762)
[epoch : 42] (l_loss: 0.04839) (t_loss: 0.07466) (accu: 0.9766)
[epoch : 43] (l_loss: 0.04907) (t_loss: 0.07156) (accu: 0.9771)
[epoch : 44] (l_loss: 0.04884) (t_loss: 0.06978) (accu: 0.9786)
[epoch : 45] (l_loss: 0.04775) (t_loss: 0.06619) (accu: 0.9799)
[epoch : 46] (l_loss: 0.04897) (t_loss: 0.06855) (accu: 0.9776)
[epoch : 47] (l_loss: 0.04946) (t_loss: 0.08328) (accu: 0.9757)
[epoch : 48] (l_loss: 0.04772) (t_loss: 0.08120) (accu: 0.9750)
[epoch : 49] (l_loss: 0.04905) (t_loss: 0.08229) (accu: 0.9757)
[epoch : 50] (l_loss: 0.04905) (t_loss: 0.07136) (accu: 0.9767)
Finish! (Best accu: 0.9799) (Time taken(sec) : 570.38) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (87490 | 178710)         32.87
fc1.weight   :      235200 (77070 | 158130)         32.77
fc2.weight   :        30000 (9830 | 20170)          32.77
fcout.weight :          1000 (590 | 410)            59.00
------------------------------------------------------------

Learning start! [Prune_iter : (6/20), Remaining weight : 32.87 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.36881) (accu: 0.1173)
[epoch : 1] (l_loss: 0.21849) (t_loss: 0.11936) (accu: 0.9631)
[epoch : 2] (l_loss: 0.10230) (t_loss: 0.09881) (accu: 0.9698)
[epoch : 3] (l_loss: 0.08473) (t_loss: 0.08520) (accu: 0.9739)
[epoch : 4] (l_loss: 0.07626) (t_loss: 0.08486) (accu: 0.9737)
[epoch : 5] (l_loss: 0.07050) (t_loss: 0.09196) (accu: 0.9702)
[epoch : 6] (l_loss: 0.06762) (t_loss: 0.07710) (accu: 0.9750)
[epoch : 7] (l_loss: 0.06340) (t_loss: 0.07682) (accu: 0.9743)
[epoch : 8] (l_loss: 0.06154) (t_loss: 0.08228) (accu: 0.9748)
[epoch : 9] (l_loss: 0.05957) (t_loss: 0.07578) (accu: 0.9785)
[epoch : 10] (l_loss: 0.05874) (t_loss: 0.07855) (accu: 0.9753)
[epoch : 11] (l_loss: 0.05717) (t_loss: 0.07583) (accu: 0.9750)
[epoch : 12] (l_loss: 0.05696) (t_loss: 0.07932) (accu: 0.9736)
[epoch : 13] (l_loss: 0.05533) (t_loss: 0.07364) (accu: 0.9777)
[epoch : 14] (l_loss: 0.05582) (t_loss: 0.07765) (accu: 0.9772)
[epoch : 15] (l_loss: 0.05304) (t_loss: 0.07195) (accu: 0.9788)
[epoch : 16] (l_loss: 0.05495) (t_loss: 0.06624) (accu: 0.9805)
[epoch : 17] (l_loss: 0.05284) (t_loss: 0.07545) (accu: 0.9768)
[epoch : 18] (l_loss: 0.05204) (t_loss: 0.07871) (accu: 0.9742)
[epoch : 19] (l_loss: 0.05174) (t_loss: 0.08103) (accu: 0.9744)
[epoch : 20] (l_loss: 0.05224) (t_loss: 0.08388) (accu: 0.9743)
[epoch : 21] (l_loss: 0.05217) (t_loss: 0.07136) (accu: 0.9775)
[epoch : 22] (l_loss: 0.05132) (t_loss: 0.07185) (accu: 0.9774)
[epoch : 23] (l_loss: 0.05152) (t_loss: 0.07507) (accu: 0.9782)
[epoch : 24] (l_loss: 0.04983) (t_loss: 0.07575) (accu: 0.9767)
[epoch : 25] (l_loss: 0.05078) (t_loss: 0.07335) (accu: 0.9769)
[epoch : 26] (l_loss: 0.05004) (t_loss: 0.07387) (accu: 0.9764)
[epoch : 27] (l_loss: 0.04963) (t_loss: 0.08097) (accu: 0.9753)
[epoch : 28] (l_loss: 0.05167) (t_loss: 0.07555) (accu: 0.9766)
[epoch : 29] (l_loss: 0.04958) (t_loss: 0.07278) (accu: 0.9786)
[epoch : 30] (l_loss: 0.04961) (t_loss: 0.07456) (accu: 0.9754)
[epoch : 31] (l_loss: 0.04862) (t_loss: 0.07761) (accu: 0.9752)
[epoch : 32] (l_loss: 0.04787) (t_loss: 0.08270) (accu: 0.9755)
[epoch : 33] (l_loss: 0.04941) (t_loss: 0.08815) (accu: 0.9738)
[epoch : 34] (l_loss: 0.04923) (t_loss: 0.07684) (accu: 0.9761)
[epoch : 35] (l_loss: 0.04981) (t_loss: 0.06897) (accu: 0.9799)
[epoch : 36] (l_loss: 0.04728) (t_loss: 0.06893) (accu: 0.9811)
[epoch : 37] (l_loss: 0.04848) (t_loss: 0.07146) (accu: 0.9791)
[epoch : 38] (l_loss: 0.05005) (t_loss: 0.07261) (accu: 0.9780)
[epoch : 39] (l_loss: 0.04930) (t_loss: 0.07193) (accu: 0.9787)
[epoch : 40] (l_loss: 0.04782) (t_loss: 0.07711) (accu: 0.9765)
[epoch : 41] (l_loss: 0.04858) (t_loss: 0.07107) (accu: 0.9799)
[epoch : 42] (l_loss: 0.04665) (t_loss: 0.06830) (accu: 0.9802)
[epoch : 43] (l_loss: 0.04701) (t_loss: 0.08008) (accu: 0.9762)
[epoch : 44] (l_loss: 0.04954) (t_loss: 0.07374) (accu: 0.9773)
[epoch : 45] (l_loss: 0.04820) (t_loss: 0.06601) (accu: 0.9795)
[epoch : 46] (l_loss: 0.04675) (t_loss: 0.06916) (accu: 0.9788)
[epoch : 47] (l_loss: 0.04773) (t_loss: 0.07909) (accu: 0.9758)
[epoch : 48] (l_loss: 0.04801) (t_loss: 0.07205) (accu: 0.9778)
[epoch : 49] (l_loss: 0.04684) (t_loss: 0.07492) (accu: 0.9768)
[epoch : 50] (l_loss: 0.04732) (t_loss: 0.07905) (accu: 0.9762)
Finish! (Best accu: 0.9811) (Time taken(sec) : 584.47) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (70051 | 196149)         26.32
fc1.weight   :      235200 (61656 | 173544)         26.21
fc2.weight   :        30000 (7864 | 22136)          26.21
fcout.weight :          1000 (531 | 469)            53.10
------------------------------------------------------------

Learning start! [Prune_iter : (7/20), Remaining weight : 26.32 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.37930) (accu: 0.0929)
[epoch : 1] (l_loss: 0.22725) (t_loss: 0.12207) (accu: 0.9623)
[epoch : 2] (l_loss: 0.10145) (t_loss: 0.09453) (accu: 0.9715)
[epoch : 3] (l_loss: 0.08311) (t_loss: 0.10867) (accu: 0.9671)
[epoch : 4] (l_loss: 0.07414) (t_loss: 0.08511) (accu: 0.9735)
[epoch : 5] (l_loss: 0.06761) (t_loss: 0.08321) (accu: 0.9744)
[epoch : 6] (l_loss: 0.06467) (t_loss: 0.08481) (accu: 0.9736)
[epoch : 7] (l_loss: 0.06284) (t_loss: 0.07564) (accu: 0.9746)
[epoch : 8] (l_loss: 0.05980) (t_loss: 0.07040) (accu: 0.9774)
[epoch : 9] (l_loss: 0.05790) (t_loss: 0.07997) (accu: 0.9755)
[epoch : 10] (l_loss: 0.05585) (t_loss: 0.08093) (accu: 0.9753)
[epoch : 11] (l_loss: 0.05603) (t_loss: 0.07503) (accu: 0.9768)
[epoch : 12] (l_loss: 0.05465) (t_loss: 0.07086) (accu: 0.9775)
[epoch : 13] (l_loss: 0.05364) (t_loss: 0.07339) (accu: 0.9775)
[epoch : 14] (l_loss: 0.05353) (t_loss: 0.07453) (accu: 0.9776)
[epoch : 15] (l_loss: 0.05347) (t_loss: 0.08729) (accu: 0.9728)
[epoch : 16] (l_loss: 0.05279) (t_loss: 0.07949) (accu: 0.9759)
[epoch : 17] (l_loss: 0.05092) (t_loss: 0.06936) (accu: 0.9768)
[epoch : 18] (l_loss: 0.05170) (t_loss: 0.08419) (accu: 0.9738)
[epoch : 19] (l_loss: 0.05177) (t_loss: 0.06825) (accu: 0.9795)
[epoch : 20] (l_loss: 0.05159) (t_loss: 0.08120) (accu: 0.9756)
[epoch : 21] (l_loss: 0.04998) (t_loss: 0.07057) (accu: 0.9776)
[epoch : 22] (l_loss: 0.04859) (t_loss: 0.07830) (accu: 0.9751)
[epoch : 23] (l_loss: 0.04975) (t_loss: 0.07044) (accu: 0.9771)
[epoch : 24] (l_loss: 0.05038) (t_loss: 0.07182) (accu: 0.9785)
[epoch : 25] (l_loss: 0.04900) (t_loss: 0.07736) (accu: 0.9754)
[epoch : 26] (l_loss: 0.05032) (t_loss: 0.07310) (accu: 0.9778)
[epoch : 27] (l_loss: 0.04730) (t_loss: 0.07031) (accu: 0.9780)
[epoch : 28] (l_loss: 0.04981) (t_loss: 0.07626) (accu: 0.9766)
[epoch : 29] (l_loss: 0.04970) (t_loss: 0.07671) (accu: 0.9752)
[epoch : 30] (l_loss: 0.04800) (t_loss: 0.07716) (accu: 0.9764)
[epoch : 31] (l_loss: 0.04921) (t_loss: 0.07786) (accu: 0.9775)
[epoch : 32] (l_loss: 0.04813) (t_loss: 0.07472) (accu: 0.9763)
[epoch : 33] (l_loss: 0.04949) (t_loss: 0.07190) (accu: 0.9786)
[epoch : 34] (l_loss: 0.04911) (t_loss: 0.07003) (accu: 0.9768)
[epoch : 35] (l_loss: 0.04793) (t_loss: 0.07437) (accu: 0.9776)
[epoch : 36] (l_loss: 0.04850) (t_loss: 0.08956) (accu: 0.9741)
[epoch : 37] (l_loss: 0.04950) (t_loss: 0.07027) (accu: 0.9784)
[epoch : 38] (l_loss: 0.04875) (t_loss: 0.07684) (accu: 0.9766)
[epoch : 39] (l_loss: 0.04831) (t_loss: 0.07288) (accu: 0.9752)
[epoch : 40] (l_loss: 0.04857) (t_loss: 0.06833) (accu: 0.9792)
[epoch : 41] (l_loss: 0.04858) (t_loss: 0.07225) (accu: 0.9772)
[epoch : 42] (l_loss: 0.04753) (t_loss: 0.06815) (accu: 0.9795)
[epoch : 43] (l_loss: 0.04984) (t_loss: 0.07358) (accu: 0.9771)
[epoch : 44] (l_loss: 0.04745) (t_loss: 0.06805) (accu: 0.9787)
[epoch : 45] (l_loss: 0.04832) (t_loss: 0.07639) (accu: 0.9773)
[epoch : 46] (l_loss: 0.04813) (t_loss: 0.07207) (accu: 0.9776)
[epoch : 47] (l_loss: 0.04896) (t_loss: 0.07512) (accu: 0.9761)
[epoch : 48] (l_loss: 0.04782) (t_loss: 0.06774) (accu: 0.9781)
[epoch : 49] (l_loss: 0.04911) (t_loss: 0.06745) (accu: 0.9809)
[epoch : 50] (l_loss: 0.04757) (t_loss: 0.07917) (accu: 0.9731)
Finish! (Best accu: 0.9809) (Time taken(sec) : 615.06) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (56094 | 210106)         21.07
fc1.weight   :      235200 (49325 | 185875)         20.97
fc2.weight   :        30000 (6291 | 23709)          20.97
fcout.weight :          1000 (478 | 522)            47.80
------------------------------------------------------------

Learning start! [Prune_iter : (8/20), Remaining weight : 21.07 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.33828) (accu: 0.0893)
[epoch : 1] (l_loss: 0.23103) (t_loss: 0.12300) (accu: 0.9604)
[epoch : 2] (l_loss: 0.10066) (t_loss: 0.10251) (accu: 0.9671)
[epoch : 3] (l_loss: 0.08253) (t_loss: 0.09017) (accu: 0.9709)
[epoch : 4] (l_loss: 0.07475) (t_loss: 0.09997) (accu: 0.9663)
[epoch : 5] (l_loss: 0.06986) (t_loss: 0.07790) (accu: 0.9757)
[epoch : 6] (l_loss: 0.06580) (t_loss: 0.07712) (accu: 0.9767)
[epoch : 7] (l_loss: 0.06292) (t_loss: 0.07947) (accu: 0.9751)
[epoch : 8] (l_loss: 0.06044) (t_loss: 0.06740) (accu: 0.9799)
[epoch : 9] (l_loss: 0.05963) (t_loss: 0.07427) (accu: 0.9769)
[epoch : 10] (l_loss: 0.05790) (t_loss: 0.07443) (accu: 0.9760)
[epoch : 11] (l_loss: 0.05755) (t_loss: 0.06756) (accu: 0.9770)
[epoch : 12] (l_loss: 0.05473) (t_loss: 0.07183) (accu: 0.9785)
[epoch : 13] (l_loss: 0.05483) (t_loss: 0.07685) (accu: 0.9770)
[epoch : 14] (l_loss: 0.05407) (t_loss: 0.08020) (accu: 0.9754)
[epoch : 15] (l_loss: 0.05315) (t_loss: 0.06896) (accu: 0.9786)
[epoch : 16] (l_loss: 0.05274) (t_loss: 0.07793) (accu: 0.9777)
[epoch : 17] (l_loss: 0.05244) (t_loss: 0.07641) (accu: 0.9763)
[epoch : 18] (l_loss: 0.05213) (t_loss: 0.08703) (accu: 0.9726)
[epoch : 19] (l_loss: 0.05011) (t_loss: 0.07149) (accu: 0.9786)
[epoch : 20] (l_loss: 0.05092) (t_loss: 0.07297) (accu: 0.9764)
[epoch : 21] (l_loss: 0.05037) (t_loss: 0.07048) (accu: 0.9765)
[epoch : 22] (l_loss: 0.05108) (t_loss: 0.06747) (accu: 0.9797)
[epoch : 23] (l_loss: 0.05078) (t_loss: 0.07747) (accu: 0.9756)
[epoch : 24] (l_loss: 0.04922) (t_loss: 0.06423) (accu: 0.9800)
[epoch : 25] (l_loss: 0.04926) (t_loss: 0.08259) (accu: 0.9749)
[epoch : 26] (l_loss: 0.04999) (t_loss: 0.06456) (accu: 0.9798)
[epoch : 27] (l_loss: 0.04990) (t_loss: 0.07016) (accu: 0.9802)
[epoch : 28] (l_loss: 0.04890) (t_loss: 0.07678) (accu: 0.9763)
[epoch : 29] (l_loss: 0.04894) (t_loss: 0.07910) (accu: 0.9754)
[epoch : 30] (l_loss: 0.04917) (t_loss: 0.07732) (accu: 0.9745)
[epoch : 31] (l_loss: 0.04889) (t_loss: 0.07130) (accu: 0.9769)
[epoch : 32] (l_loss: 0.04773) (t_loss: 0.07785) (accu: 0.9757)
[epoch : 33] (l_loss: 0.04941) (t_loss: 0.06920) (accu: 0.9786)
[epoch : 34] (l_loss: 0.04748) (t_loss: 0.06662) (accu: 0.9801)
[epoch : 35] (l_loss: 0.04884) (t_loss: 0.08080) (accu: 0.9752)
[epoch : 36] (l_loss: 0.04830) (t_loss: 0.07696) (accu: 0.9756)
[epoch : 37] (l_loss: 0.04866) (t_loss: 0.06942) (accu: 0.9788)
[epoch : 38] (l_loss: 0.04791) (t_loss: 0.07253) (accu: 0.9760)
[epoch : 39] (l_loss: 0.04834) (t_loss: 0.07233) (accu: 0.9780)
[epoch : 40] (l_loss: 0.04777) (t_loss: 0.06830) (accu: 0.9779)
[epoch : 41] (l_loss: 0.04820) (t_loss: 0.07078) (accu: 0.9781)
[epoch : 42] (l_loss: 0.04836) (t_loss: 0.06822) (accu: 0.9788)
[epoch : 43] (l_loss: 0.04777) (t_loss: 0.08019) (accu: 0.9753)
[epoch : 44] (l_loss: 0.04677) (t_loss: 0.06901) (accu: 0.9800)
[epoch : 45] (l_loss: 0.04836) (t_loss: 0.07227) (accu: 0.9767)
[epoch : 46] (l_loss: 0.04835) (t_loss: 0.07648) (accu: 0.9759)
[epoch : 47] (l_loss: 0.04818) (t_loss: 0.06881) (accu: 0.9792)
[epoch : 48] (l_loss: 0.04738) (t_loss: 0.07900) (accu: 0.9762)
[epoch : 49] (l_loss: 0.04870) (t_loss: 0.07432) (accu: 0.9793)
[epoch : 50] (l_loss: 0.04691) (t_loss: 0.07200) (accu: 0.9775)
Finish! (Best accu: 0.9802) (Time taken(sec) : 617.72) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (44923 | 221277)         16.88
fc1.weight   :      235200 (39460 | 195740)         16.78
fc2.weight   :        30000 (5033 | 24967)          16.78
fcout.weight :          1000 (430 | 570)            43.00
------------------------------------------------------------

Learning start! [Prune_iter : (9/20), Remaining weight : 16.88 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.33203) (accu: 0.0812)
[epoch : 1] (l_loss: 0.23686) (t_loss: 0.12841) (accu: 0.9600)
[epoch : 2] (l_loss: 0.10407) (t_loss: 0.09529) (accu: 0.9700)
[epoch : 3] (l_loss: 0.08533) (t_loss: 0.08813) (accu: 0.9733)
[epoch : 4] (l_loss: 0.07473) (t_loss: 0.08240) (accu: 0.9726)
[epoch : 5] (l_loss: 0.06901) (t_loss: 0.08953) (accu: 0.9729)
[epoch : 6] (l_loss: 0.06640) (t_loss: 0.08013) (accu: 0.9760)
[epoch : 7] (l_loss: 0.06272) (t_loss: 0.07383) (accu: 0.9766)
[epoch : 8] (l_loss: 0.06114) (t_loss: 0.07996) (accu: 0.9741)
[epoch : 9] (l_loss: 0.05877) (t_loss: 0.07327) (accu: 0.9765)
[epoch : 10] (l_loss: 0.05807) (t_loss: 0.07049) (accu: 0.9790)
[epoch : 11] (l_loss: 0.05488) (t_loss: 0.07903) (accu: 0.9765)
[epoch : 12] (l_loss: 0.05650) (t_loss: 0.07540) (accu: 0.9761)
[epoch : 13] (l_loss: 0.05471) (t_loss: 0.07054) (accu: 0.9780)
[epoch : 14] (l_loss: 0.05348) (t_loss: 0.07803) (accu: 0.9756)
[epoch : 15] (l_loss: 0.05237) (t_loss: 0.07572) (accu: 0.9761)
[epoch : 16] (l_loss: 0.05175) (t_loss: 0.06593) (accu: 0.9801)
[epoch : 17] (l_loss: 0.05098) (t_loss: 0.07258) (accu: 0.9765)
[epoch : 18] (l_loss: 0.05287) (t_loss: 0.07912) (accu: 0.9747)
[epoch : 19] (l_loss: 0.04977) (t_loss: 0.07005) (accu: 0.9780)
[epoch : 20] (l_loss: 0.05045) (t_loss: 0.07646) (accu: 0.9749)
[epoch : 21] (l_loss: 0.05176) (t_loss: 0.07611) (accu: 0.9763)
[epoch : 22] (l_loss: 0.04897) (t_loss: 0.07511) (accu: 0.9760)
[epoch : 23] (l_loss: 0.04920) (t_loss: 0.07436) (accu: 0.9757)
[epoch : 24] (l_loss: 0.04964) (t_loss: 0.07119) (accu: 0.9775)
[epoch : 25] (l_loss: 0.04974) (t_loss: 0.07603) (accu: 0.9764)
[epoch : 26] (l_loss: 0.05074) (t_loss: 0.06884) (accu: 0.9770)
[epoch : 27] (l_loss: 0.04804) (t_loss: 0.06363) (accu: 0.9784)
[epoch : 28] (l_loss: 0.04736) (t_loss: 0.07257) (accu: 0.9768)
[epoch : 29] (l_loss: 0.05035) (t_loss: 0.07122) (accu: 0.9775)
[epoch : 30] (l_loss: 0.04748) (t_loss: 0.07215) (accu: 0.9773)
[epoch : 31] (l_loss: 0.04982) (t_loss: 0.07143) (accu: 0.9785)
[epoch : 32] (l_loss: 0.04861) (t_loss: 0.07847) (accu: 0.9738)
[epoch : 33] (l_loss: 0.04857) (t_loss: 0.07269) (accu: 0.9771)
[epoch : 34] (l_loss: 0.04906) (t_loss: 0.06958) (accu: 0.9776)
[epoch : 35] (l_loss: 0.04724) (t_loss: 0.07529) (accu: 0.9752)
[epoch : 36] (l_loss: 0.05015) (t_loss: 0.07460) (accu: 0.9754)
[epoch : 37] (l_loss: 0.04882) (t_loss: 0.07123) (accu: 0.9771)
[epoch : 38] (l_loss: 0.04861) (t_loss: 0.07044) (accu: 0.9779)
[epoch : 39] (l_loss: 0.04969) (t_loss: 0.08460) (accu: 0.9704)
[epoch : 40] (l_loss: 0.04775) (t_loss: 0.06958) (accu: 0.9781)
[epoch : 41] (l_loss: 0.04827) (t_loss: 0.07518) (accu: 0.9763)
[epoch : 42] (l_loss: 0.04899) (t_loss: 0.07807) (accu: 0.9749)
[epoch : 43] (l_loss: 0.04686) (t_loss: 0.06699) (accu: 0.9795)
[epoch : 44] (l_loss: 0.04789) (t_loss: 0.06903) (accu: 0.9785)
[epoch : 45] (l_loss: 0.04855) (t_loss: 0.06461) (accu: 0.9796)
[epoch : 46] (l_loss: 0.04698) (t_loss: 0.08001) (accu: 0.9751)
[epoch : 47] (l_loss: 0.04868) (t_loss: 0.07129) (accu: 0.9777)
[epoch : 48] (l_loss: 0.04770) (t_loss: 0.07222) (accu: 0.9780)
[epoch : 49] (l_loss: 0.04796) (t_loss: 0.07864) (accu: 0.9755)
[epoch : 50] (l_loss: 0.04842) (t_loss: 0.06970) (accu: 0.9783)
Finish! (Best accu: 0.9801) (Time taken(sec) : 623.43) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (35982 | 230218)         13.52
fc1.weight   :      235200 (31568 | 203632)         13.42
fc2.weight   :        30000 (4027 | 25973)          13.42
fcout.weight :          1000 (387 | 613)            38.70
------------------------------------------------------------

Learning start! [Prune_iter : (10/20), Remaining weight : 13.52 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30992) (accu: 0.0862)
[epoch : 1] (l_loss: 0.24666) (t_loss: 0.12673) (accu: 0.9608)
[epoch : 2] (l_loss: 0.10792) (t_loss: 0.09495) (accu: 0.9712)
[epoch : 3] (l_loss: 0.08496) (t_loss: 0.08127) (accu: 0.9751)
[epoch : 4] (l_loss: 0.07635) (t_loss: 0.09022) (accu: 0.9711)
[epoch : 5] (l_loss: 0.07014) (t_loss: 0.08583) (accu: 0.9731)
[epoch : 6] (l_loss: 0.06656) (t_loss: 0.07911) (accu: 0.9739)
[epoch : 7] (l_loss: 0.06389) (t_loss: 0.07235) (accu: 0.9773)
[epoch : 8] (l_loss: 0.06133) (t_loss: 0.07756) (accu: 0.9766)
[epoch : 9] (l_loss: 0.05863) (t_loss: 0.07312) (accu: 0.9767)
[epoch : 10] (l_loss: 0.05698) (t_loss: 0.07564) (accu: 0.9765)
[epoch : 11] (l_loss: 0.05723) (t_loss: 0.07510) (accu: 0.9767)
[epoch : 12] (l_loss: 0.05590) (t_loss: 0.08164) (accu: 0.9736)
[epoch : 13] (l_loss: 0.05477) (t_loss: 0.07779) (accu: 0.9770)
[epoch : 14] (l_loss: 0.05466) (t_loss: 0.06772) (accu: 0.9787)
[epoch : 15] (l_loss: 0.05298) (t_loss: 0.08235) (accu: 0.9749)
[epoch : 16] (l_loss: 0.05301) (t_loss: 0.07754) (accu: 0.9758)
[epoch : 17] (l_loss: 0.05394) (t_loss: 0.08671) (accu: 0.9728)
[epoch : 18] (l_loss: 0.05208) (t_loss: 0.08132) (accu: 0.9749)
[epoch : 19] (l_loss: 0.05308) (t_loss: 0.07806) (accu: 0.9766)
[epoch : 20] (l_loss: 0.05129) (t_loss: 0.07276) (accu: 0.9773)
[epoch : 21] (l_loss: 0.05131) (t_loss: 0.07369) (accu: 0.9771)
[epoch : 22] (l_loss: 0.05231) (t_loss: 0.07569) (accu: 0.9762)
[epoch : 23] (l_loss: 0.05174) (t_loss: 0.07924) (accu: 0.9746)
[epoch : 24] (l_loss: 0.05201) (t_loss: 0.06766) (accu: 0.9777)
[epoch : 25] (l_loss: 0.04950) (t_loss: 0.07737) (accu: 0.9759)
[epoch : 26] (l_loss: 0.05164) (t_loss: 0.06905) (accu: 0.9789)
[epoch : 27] (l_loss: 0.04946) (t_loss: 0.07338) (accu: 0.9764)
[epoch : 28] (l_loss: 0.05041) (t_loss: 0.07621) (accu: 0.9761)
[epoch : 29] (l_loss: 0.05055) (t_loss: 0.07273) (accu: 0.9772)
[epoch : 30] (l_loss: 0.04973) (t_loss: 0.07177) (accu: 0.9778)
[epoch : 31] (l_loss: 0.05105) (t_loss: 0.07894) (accu: 0.9748)
[epoch : 32] (l_loss: 0.05052) (t_loss: 0.07507) (accu: 0.9743)
[epoch : 33] (l_loss: 0.04781) (t_loss: 0.06926) (accu: 0.9780)
[epoch : 34] (l_loss: 0.04967) (t_loss: 0.07095) (accu: 0.9788)
[epoch : 35] (l_loss: 0.05005) (t_loss: 0.07438) (accu: 0.9763)
[epoch : 36] (l_loss: 0.04947) (t_loss: 0.06426) (accu: 0.9794)
[epoch : 37] (l_loss: 0.04834) (t_loss: 0.07079) (accu: 0.9778)
[epoch : 38] (l_loss: 0.04986) (t_loss: 0.07014) (accu: 0.9781)
[epoch : 39] (l_loss: 0.04903) (t_loss: 0.06779) (accu: 0.9779)
[epoch : 40] (l_loss: 0.04928) (t_loss: 0.07086) (accu: 0.9786)
[epoch : 41] (l_loss: 0.04944) (t_loss: 0.07190) (accu: 0.9782)
[epoch : 42] (l_loss: 0.04901) (t_loss: 0.07378) (accu: 0.9762)
[epoch : 43] (l_loss: 0.04867) (t_loss: 0.07588) (accu: 0.9762)
[epoch : 44] (l_loss: 0.04965) (t_loss: 0.07878) (accu: 0.9753)
[epoch : 45] (l_loss: 0.04861) (t_loss: 0.07159) (accu: 0.9775)
[epoch : 46] (l_loss: 0.04911) (t_loss: 0.06952) (accu: 0.9777)
[epoch : 47] (l_loss: 0.04928) (t_loss: 0.06950) (accu: 0.9787)
[epoch : 48] (l_loss: 0.04876) (t_loss: 0.07552) (accu: 0.9771)
[epoch : 49] (l_loss: 0.04826) (t_loss: 0.07532) (accu: 0.9765)
[epoch : 50] (l_loss: 0.04923) (t_loss: 0.07407) (accu: 0.9771)
Finish! (Best accu: 0.9794) (Time taken(sec) : 612.86) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (28824 | 237376)         10.83
fc1.weight   :      235200 (25254 | 209946)         10.74
fc2.weight   :        30000 (3221 | 26779)          10.74
fcout.weight :          1000 (349 | 651)            34.90
------------------------------------------------------------

Learning start! [Prune_iter : (11/20), Remaining weight : 10.83 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29366) (accu: 0.1018)
[epoch : 1] (l_loss: 0.25171) (t_loss: 0.13097) (accu: 0.9616)
[epoch : 2] (l_loss: 0.10704) (t_loss: 0.11072) (accu: 0.9637)
[epoch : 3] (l_loss: 0.08640) (t_loss: 0.08873) (accu: 0.9723)
[epoch : 4] (l_loss: 0.07628) (t_loss: 0.08847) (accu: 0.9713)
[epoch : 5] (l_loss: 0.07017) (t_loss: 0.08971) (accu: 0.9722)
[epoch : 6] (l_loss: 0.06610) (t_loss: 0.07591) (accu: 0.9762)
[epoch : 7] (l_loss: 0.06199) (t_loss: 0.08696) (accu: 0.9733)
[epoch : 8] (l_loss: 0.06102) (t_loss: 0.07739) (accu: 0.9758)
[epoch : 9] (l_loss: 0.05924) (t_loss: 0.07265) (accu: 0.9766)
[epoch : 10] (l_loss: 0.05690) (t_loss: 0.06944) (accu: 0.9785)
[epoch : 11] (l_loss: 0.05699) (t_loss: 0.07516) (accu: 0.9773)
[epoch : 12] (l_loss: 0.05518) (t_loss: 0.07522) (accu: 0.9768)
[epoch : 13] (l_loss: 0.05537) (t_loss: 0.07797) (accu: 0.9760)
[epoch : 14] (l_loss: 0.05315) (t_loss: 0.06706) (accu: 0.9781)
[epoch : 15] (l_loss: 0.05255) (t_loss: 0.07296) (accu: 0.9772)
[epoch : 16] (l_loss: 0.05408) (t_loss: 0.07177) (accu: 0.9771)
[epoch : 17] (l_loss: 0.05220) (t_loss: 0.07569) (accu: 0.9747)
[epoch : 18] (l_loss: 0.05190) (t_loss: 0.07050) (accu: 0.9769)
[epoch : 19] (l_loss: 0.05149) (t_loss: 0.06950) (accu: 0.9778)
[epoch : 20] (l_loss: 0.05092) (t_loss: 0.07961) (accu: 0.9749)
[epoch : 21] (l_loss: 0.05034) (t_loss: 0.07061) (accu: 0.9774)
[epoch : 22] (l_loss: 0.05156) (t_loss: 0.07213) (accu: 0.9781)
[epoch : 23] (l_loss: 0.05008) (t_loss: 0.07559) (accu: 0.9754)
[epoch : 24] (l_loss: 0.05134) (t_loss: 0.07344) (accu: 0.9782)
[epoch : 25] (l_loss: 0.05066) (t_loss: 0.06802) (accu: 0.9769)
[epoch : 26] (l_loss: 0.04889) (t_loss: 0.09070) (accu: 0.9707)
[epoch : 27] (l_loss: 0.04920) (t_loss: 0.07192) (accu: 0.9789)
[epoch : 28] (l_loss: 0.04934) (t_loss: 0.07113) (accu: 0.9787)
[epoch : 29] (l_loss: 0.04797) (t_loss: 0.07265) (accu: 0.9773)
[epoch : 30] (l_loss: 0.04922) (t_loss: 0.07586) (accu: 0.9756)
[epoch : 31] (l_loss: 0.04844) (t_loss: 0.07331) (accu: 0.9786)
[epoch : 32] (l_loss: 0.04954) (t_loss: 0.06183) (accu: 0.9801)
[epoch : 33] (l_loss: 0.04849) (t_loss: 0.07679) (accu: 0.9763)
[epoch : 34] (l_loss: 0.04865) (t_loss: 0.07918) (accu: 0.9747)
[epoch : 35] (l_loss: 0.04923) (t_loss: 0.06953) (accu: 0.9788)
[epoch : 36] (l_loss: 0.04923) (t_loss: 0.06626) (accu: 0.9801)
[epoch : 37] (l_loss: 0.04796) (t_loss: 0.07018) (accu: 0.9775)
[epoch : 38] (l_loss: 0.04827) (t_loss: 0.06692) (accu: 0.9791)
[epoch : 39] (l_loss: 0.04855) (t_loss: 0.07854) (accu: 0.9767)
[epoch : 40] (l_loss: 0.04816) (t_loss: 0.06673) (accu: 0.9785)
[epoch : 41] (l_loss: 0.04896) (t_loss: 0.08038) (accu: 0.9758)
[epoch : 42] (l_loss: 0.04976) (t_loss: 0.06943) (accu: 0.9790)
[epoch : 43] (l_loss: 0.04841) (t_loss: 0.08025) (accu: 0.9730)
[epoch : 44] (l_loss: 0.04843) (t_loss: 0.07102) (accu: 0.9775)
[epoch : 45] (l_loss: 0.04720) (t_loss: 0.06770) (accu: 0.9795)
[epoch : 46] (l_loss: 0.04698) (t_loss: 0.07605) (accu: 0.9774)
[epoch : 47] (l_loss: 0.04769) (t_loss: 0.07623) (accu: 0.9758)
[epoch : 48] (l_loss: 0.04841) (t_loss: 0.07664) (accu: 0.9752)
[epoch : 49] (l_loss: 0.04755) (t_loss: 0.06720) (accu: 0.9798)
[epoch : 50] (l_loss: 0.04837) (t_loss: 0.07115) (accu: 0.9780)
Finish! (Best accu: 0.9801) (Time taken(sec) : 630.13) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (23095 | 243105)          8.68
fc1.weight   :      235200 (20204 | 214996)          8.59
fc2.weight   :        30000 (2577 | 27423)           8.59
fcout.weight :          1000 (314 | 686)            31.40
------------------------------------------------------------

Learning start! [Prune_iter : (12/20), Remaining weight : 8.68 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.27607) (accu: 0.1122)
[epoch : 1] (l_loss: 0.26224) (t_loss: 0.13681) (accu: 0.9594)
[epoch : 2] (l_loss: 0.11148) (t_loss: 0.10265) (accu: 0.9679)
[epoch : 3] (l_loss: 0.08652) (t_loss: 0.09702) (accu: 0.9708)
[epoch : 4] (l_loss: 0.07625) (t_loss: 0.08837) (accu: 0.9741)
[epoch : 5] (l_loss: 0.06958) (t_loss: 0.09016) (accu: 0.9703)
[epoch : 6] (l_loss: 0.06685) (t_loss: 0.08263) (accu: 0.9728)
[epoch : 7] (l_loss: 0.06224) (t_loss: 0.08428) (accu: 0.9730)
[epoch : 8] (l_loss: 0.05940) (t_loss: 0.07255) (accu: 0.9779)
[epoch : 9] (l_loss: 0.05702) (t_loss: 0.07552) (accu: 0.9766)
[epoch : 10] (l_loss: 0.05711) (t_loss: 0.07108) (accu: 0.9775)
[epoch : 11] (l_loss: 0.05513) (t_loss: 0.08571) (accu: 0.9718)
[epoch : 12] (l_loss: 0.05519) (t_loss: 0.06580) (accu: 0.9781)
[epoch : 13] (l_loss: 0.05320) (t_loss: 0.08069) (accu: 0.9746)
[epoch : 14] (l_loss: 0.05310) (t_loss: 0.07910) (accu: 0.9765)
[epoch : 15] (l_loss: 0.05241) (t_loss: 0.06563) (accu: 0.9780)
[epoch : 16] (l_loss: 0.05229) (t_loss: 0.07546) (accu: 0.9747)
[epoch : 17] (l_loss: 0.05075) (t_loss: 0.07499) (accu: 0.9751)
[epoch : 18] (l_loss: 0.05025) (t_loss: 0.06799) (accu: 0.9777)
[epoch : 19] (l_loss: 0.05301) (t_loss: 0.06867) (accu: 0.9776)
[epoch : 20] (l_loss: 0.05089) (t_loss: 0.08138) (accu: 0.9734)
[epoch : 21] (l_loss: 0.04931) (t_loss: 0.08267) (accu: 0.9749)
[epoch : 22] (l_loss: 0.05052) (t_loss: 0.07334) (accu: 0.9778)
[epoch : 23] (l_loss: 0.05009) (t_loss: 0.07068) (accu: 0.9760)
[epoch : 24] (l_loss: 0.05047) (t_loss: 0.07347) (accu: 0.9774)
[epoch : 25] (l_loss: 0.04910) (t_loss: 0.06570) (accu: 0.9799)
[epoch : 26] (l_loss: 0.04881) (t_loss: 0.06723) (accu: 0.9779)
[epoch : 27] (l_loss: 0.04784) (t_loss: 0.07297) (accu: 0.9743)
[epoch : 28] (l_loss: 0.04846) (t_loss: 0.06779) (accu: 0.9790)
[epoch : 29] (l_loss: 0.04982) (t_loss: 0.06959) (accu: 0.9767)
[epoch : 30] (l_loss: 0.04914) (t_loss: 0.07886) (accu: 0.9745)
[epoch : 31] (l_loss: 0.04881) (t_loss: 0.07661) (accu: 0.9752)
[epoch : 32] (l_loss: 0.04975) (t_loss: 0.08036) (accu: 0.9740)
[epoch : 33] (l_loss: 0.04743) (t_loss: 0.06975) (accu: 0.9784)
[epoch : 34] (l_loss: 0.04823) (t_loss: 0.07432) (accu: 0.9767)
[epoch : 35] (l_loss: 0.04830) (t_loss: 0.07600) (accu: 0.9757)
[epoch : 36] (l_loss: 0.04855) (t_loss: 0.08419) (accu: 0.9737)
[epoch : 37] (l_loss: 0.04817) (t_loss: 0.06791) (accu: 0.9797)
[epoch : 38] (l_loss: 0.04787) (t_loss: 0.06744) (accu: 0.9794)
[epoch : 39] (l_loss: 0.04824) (t_loss: 0.06554) (accu: 0.9789)
[epoch : 40] (l_loss: 0.04891) (t_loss: 0.07083) (accu: 0.9784)
[epoch : 41] (l_loss: 0.04660) (t_loss: 0.07013) (accu: 0.9770)
[epoch : 42] (l_loss: 0.04712) (t_loss: 0.06720) (accu: 0.9792)
[epoch : 43] (l_loss: 0.04748) (t_loss: 0.07106) (accu: 0.9754)
[epoch : 44] (l_loss: 0.04826) (t_loss: 0.06992) (accu: 0.9787)
[epoch : 45] (l_loss: 0.04715) (t_loss: 0.06510) (accu: 0.9796)
[epoch : 46] (l_loss: 0.04699) (t_loss: 0.07050) (accu: 0.9768)
[epoch : 47] (l_loss: 0.04559) (t_loss: 0.07547) (accu: 0.9758)
[epoch : 48] (l_loss: 0.04695) (t_loss: 0.07270) (accu: 0.9767)
[epoch : 49] (l_loss: 0.04709) (t_loss: 0.07404) (accu: 0.9760)
[epoch : 50] (l_loss: 0.04612) (t_loss: 0.06551) (accu: 0.9795)
Finish! (Best accu: 0.9799) (Time taken(sec) : 632.91) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (18507 | 247693)          6.95
fc1.weight   :      235200 (16163 | 219037)          6.87
fc2.weight   :        30000 (2062 | 27938)           6.87
fcout.weight :          1000 (282 | 718)            28.20
------------------------------------------------------------

Learning start! [Prune_iter : (13/20), Remaining weight : 6.95 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.28389) (accu: 0.1012)
[epoch : 1] (l_loss: 0.27519) (t_loss: 0.15506) (accu: 0.9526)
[epoch : 2] (l_loss: 0.11550) (t_loss: 0.09706) (accu: 0.9701)
[epoch : 3] (l_loss: 0.09002) (t_loss: 0.09875) (accu: 0.9684)
[epoch : 4] (l_loss: 0.07810) (t_loss: 0.08143) (accu: 0.9730)
[epoch : 5] (l_loss: 0.06957) (t_loss: 0.08727) (accu: 0.9748)
[epoch : 6] (l_loss: 0.06640) (t_loss: 0.07561) (accu: 0.9766)
[epoch : 7] (l_loss: 0.06325) (t_loss: 0.08181) (accu: 0.9743)
[epoch : 8] (l_loss: 0.05973) (t_loss: 0.08739) (accu: 0.9708)
[epoch : 9] (l_loss: 0.05863) (t_loss: 0.08364) (accu: 0.9751)
[epoch : 10] (l_loss: 0.05710) (t_loss: 0.07295) (accu: 0.9777)
[epoch : 11] (l_loss: 0.05472) (t_loss: 0.07407) (accu: 0.9770)
[epoch : 12] (l_loss: 0.05579) (t_loss: 0.07810) (accu: 0.9765)
[epoch : 13] (l_loss: 0.05399) (t_loss: 0.07172) (accu: 0.9771)
[epoch : 14] (l_loss: 0.05368) (t_loss: 0.07638) (accu: 0.9778)
[epoch : 15] (l_loss: 0.05253) (t_loss: 0.08655) (accu: 0.9738)
[epoch : 16] (l_loss: 0.05067) (t_loss: 0.08416) (accu: 0.9751)
[epoch : 17] (l_loss: 0.05067) (t_loss: 0.07579) (accu: 0.9756)
[epoch : 18] (l_loss: 0.05116) (t_loss: 0.08062) (accu: 0.9748)
[epoch : 19] (l_loss: 0.05114) (t_loss: 0.07192) (accu: 0.9770)
[epoch : 20] (l_loss: 0.04950) (t_loss: 0.06564) (accu: 0.9811)
[epoch : 21] (l_loss: 0.04940) (t_loss: 0.06957) (accu: 0.9788)
[epoch : 22] (l_loss: 0.04988) (t_loss: 0.07087) (accu: 0.9801)
[epoch : 23] (l_loss: 0.04942) (t_loss: 0.07172) (accu: 0.9782)
[epoch : 24] (l_loss: 0.04741) (t_loss: 0.07850) (accu: 0.9750)
[epoch : 25] (l_loss: 0.04900) (t_loss: 0.06888) (accu: 0.9784)
[epoch : 26] (l_loss: 0.04744) (t_loss: 0.06732) (accu: 0.9798)
[epoch : 27] (l_loss: 0.04783) (t_loss: 0.07461) (accu: 0.9764)
[epoch : 28] (l_loss: 0.04986) (t_loss: 0.06888) (accu: 0.9804)
[epoch : 29] (l_loss: 0.04766) (t_loss: 0.07285) (accu: 0.9783)
[epoch : 30] (l_loss: 0.04873) (t_loss: 0.06869) (accu: 0.9799)
[epoch : 31] (l_loss: 0.04885) (t_loss: 0.07525) (accu: 0.9784)
[epoch : 32] (l_loss: 0.04681) (t_loss: 0.07093) (accu: 0.9793)
[epoch : 33] (l_loss: 0.04771) (t_loss: 0.08632) (accu: 0.9732)
[epoch : 34] (l_loss: 0.04938) (t_loss: 0.07627) (accu: 0.9758)
[epoch : 35] (l_loss: 0.04672) (t_loss: 0.08188) (accu: 0.9748)
[epoch : 36] (l_loss: 0.04811) (t_loss: 0.07700) (accu: 0.9777)
[epoch : 37] (l_loss: 0.04656) (t_loss: 0.07947) (accu: 0.9746)
[epoch : 38] (l_loss: 0.04789) (t_loss: 0.07438) (accu: 0.9786)
[epoch : 39] (l_loss: 0.04667) (t_loss: 0.07681) (accu: 0.9766)
[epoch : 40] (l_loss: 0.04821) (t_loss: 0.07341) (accu: 0.9792)
[epoch : 41] (l_loss: 0.04619) (t_loss: 0.07428) (accu: 0.9779)
[epoch : 42] (l_loss: 0.04711) (t_loss: 0.07203) (accu: 0.9783)
[epoch : 43] (l_loss: 0.04792) (t_loss: 0.07854) (accu: 0.9760)
[epoch : 44] (l_loss: 0.04662) (t_loss: 0.07238) (accu: 0.9788)
[epoch : 45] (l_loss: 0.04619) (t_loss: 0.07376) (accu: 0.9777)
[epoch : 46] (l_loss: 0.04727) (t_loss: 0.07319) (accu: 0.9774)
[epoch : 47] (l_loss: 0.04669) (t_loss: 0.07900) (accu: 0.9752)
[epoch : 48] (l_loss: 0.04735) (t_loss: 0.07015) (accu: 0.9776)
[epoch : 49] (l_loss: 0.04582) (t_loss: 0.07062) (accu: 0.9792)
[epoch : 50] (l_loss: 0.04698) (t_loss: 0.07838) (accu: 0.9773)
Finish! (Best accu: 0.9811) (Time taken(sec) : 638.74) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (14833 | 251367)          5.57
fc1.weight   :      235200 (12930 | 222270)          5.50
fc2.weight   :        30000 (1649 | 28351)           5.50
fcout.weight :          1000 (254 | 746)            25.40
------------------------------------------------------------

Learning start! [Prune_iter : (14/20), Remaining weight : 5.57 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.27288) (accu: 0.1222)
[epoch : 1] (l_loss: 0.28666) (t_loss: 0.13906) (accu: 0.9585)
[epoch : 2] (l_loss: 0.11757) (t_loss: 0.10568) (accu: 0.9695)
[epoch : 3] (l_loss: 0.08985) (t_loss: 0.09161) (accu: 0.9728)
[epoch : 4] (l_loss: 0.07659) (t_loss: 0.08630) (accu: 0.9735)
[epoch : 5] (l_loss: 0.06915) (t_loss: 0.08283) (accu: 0.9743)
[epoch : 6] (l_loss: 0.06332) (t_loss: 0.08455) (accu: 0.9748)
[epoch : 7] (l_loss: 0.06115) (t_loss: 0.07654) (accu: 0.9773)
[epoch : 8] (l_loss: 0.05849) (t_loss: 0.07557) (accu: 0.9775)
[epoch : 9] (l_loss: 0.05669) (t_loss: 0.07635) (accu: 0.9774)
[epoch : 10] (l_loss: 0.05282) (t_loss: 0.07806) (accu: 0.9755)
[epoch : 11] (l_loss: 0.05466) (t_loss: 0.07348) (accu: 0.9775)
[epoch : 12] (l_loss: 0.05166) (t_loss: 0.07922) (accu: 0.9773)
[epoch : 13] (l_loss: 0.05073) (t_loss: 0.07572) (accu: 0.9765)
[epoch : 14] (l_loss: 0.05123) (t_loss: 0.08170) (accu: 0.9743)
[epoch : 15] (l_loss: 0.04995) (t_loss: 0.07873) (accu: 0.9756)
[epoch : 16] (l_loss: 0.05014) (t_loss: 0.07076) (accu: 0.9794)
[epoch : 17] (l_loss: 0.04824) (t_loss: 0.07198) (accu: 0.9770)
[epoch : 18] (l_loss: 0.04928) (t_loss: 0.07317) (accu: 0.9760)
[epoch : 19] (l_loss: 0.04868) (t_loss: 0.07432) (accu: 0.9757)
[epoch : 20] (l_loss: 0.04798) (t_loss: 0.07220) (accu: 0.9778)
[epoch : 21] (l_loss: 0.04749) (t_loss: 0.07368) (accu: 0.9771)
[epoch : 22] (l_loss: 0.04708) (t_loss: 0.07291) (accu: 0.9773)
[epoch : 23] (l_loss: 0.04732) (t_loss: 0.07214) (accu: 0.9798)
[epoch : 24] (l_loss: 0.04801) (t_loss: 0.07042) (accu: 0.9787)
[epoch : 25] (l_loss: 0.04680) (t_loss: 0.07958) (accu: 0.9765)
[epoch : 26] (l_loss: 0.04736) (t_loss: 0.07236) (accu: 0.9765)
[epoch : 27] (l_loss: 0.04694) (t_loss: 0.07549) (accu: 0.9756)
[epoch : 28] (l_loss: 0.04577) (t_loss: 0.06743) (accu: 0.9780)
[epoch : 29] (l_loss: 0.04806) (t_loss: 0.06861) (accu: 0.9788)
[epoch : 30] (l_loss: 0.04534) (t_loss: 0.07720) (accu: 0.9779)
[epoch : 31] (l_loss: 0.04627) (t_loss: 0.07350) (accu: 0.9776)
[epoch : 32] (l_loss: 0.04611) (t_loss: 0.07192) (accu: 0.9779)
[epoch : 33] (l_loss: 0.04543) (t_loss: 0.07272) (accu: 0.9786)
[epoch : 34] (l_loss: 0.04691) (t_loss: 0.07346) (accu: 0.9774)
[epoch : 35] (l_loss: 0.04534) (t_loss: 0.06856) (accu: 0.9785)
[epoch : 36] (l_loss: 0.04630) (t_loss: 0.07242) (accu: 0.9774)
[epoch : 37] (l_loss: 0.04556) (t_loss: 0.07082) (accu: 0.9794)
[epoch : 38] (l_loss: 0.04599) (t_loss: 0.06985) (accu: 0.9798)
[epoch : 39] (l_loss: 0.04630) (t_loss: 0.07562) (accu: 0.9777)
[epoch : 40] (l_loss: 0.04514) (t_loss: 0.07593) (accu: 0.9766)
[epoch : 41] (l_loss: 0.04464) (t_loss: 0.07655) (accu: 0.9762)
[epoch : 42] (l_loss: 0.04622) (t_loss: 0.07576) (accu: 0.9765)
[epoch : 43] (l_loss: 0.04444) (t_loss: 0.07578) (accu: 0.9775)
[epoch : 44] (l_loss: 0.04560) (t_loss: 0.07803) (accu: 0.9770)
[epoch : 45] (l_loss: 0.04552) (t_loss: 0.08644) (accu: 0.9734)
[epoch : 46] (l_loss: 0.04575) (t_loss: 0.07493) (accu: 0.9773)
[epoch : 47] (l_loss: 0.04507) (t_loss: 0.07002) (accu: 0.9783)
[epoch : 48] (l_loss: 0.04413) (t_loss: 0.07730) (accu: 0.9764)
[epoch : 49] (l_loss: 0.04576) (t_loss: 0.06876) (accu: 0.9789)
[epoch : 50] (l_loss: 0.04434) (t_loss: 0.07485) (accu: 0.9775)
Finish! (Best accu: 0.9798) (Time taken(sec) : 627.13) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (11892 | 254308)          4.47
fc1.weight   :      235200 (10344 | 224856)          4.40
fc2.weight   :        30000 (1319 | 28681)           4.40
fcout.weight :          1000 (229 | 771)            22.90
------------------------------------------------------------

Learning start! [Prune_iter : (15/20), Remaining weight : 4.47 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.27652) (accu: 0.1110)
[epoch : 1] (l_loss: 0.29069) (t_loss: 0.13689) (accu: 0.9591)
[epoch : 2] (l_loss: 0.11356) (t_loss: 0.10027) (accu: 0.9710)
[epoch : 3] (l_loss: 0.08779) (t_loss: 0.08977) (accu: 0.9727)
[epoch : 4] (l_loss: 0.07490) (t_loss: 0.07936) (accu: 0.9766)
[epoch : 5] (l_loss: 0.06825) (t_loss: 0.07758) (accu: 0.9771)
[epoch : 6] (l_loss: 0.06190) (t_loss: 0.08231) (accu: 0.9741)
[epoch : 7] (l_loss: 0.05985) (t_loss: 0.07364) (accu: 0.9763)
[epoch : 8] (l_loss: 0.05545) (t_loss: 0.07816) (accu: 0.9767)
[epoch : 9] (l_loss: 0.05394) (t_loss: 0.07744) (accu: 0.9762)
[epoch : 10] (l_loss: 0.05298) (t_loss: 0.06716) (accu: 0.9790)
[epoch : 11] (l_loss: 0.05090) (t_loss: 0.07285) (accu: 0.9768)
[epoch : 12] (l_loss: 0.04934) (t_loss: 0.07341) (accu: 0.9785)
[epoch : 13] (l_loss: 0.04876) (t_loss: 0.08057) (accu: 0.9751)
[epoch : 14] (l_loss: 0.04833) (t_loss: 0.07318) (accu: 0.9789)
[epoch : 15] (l_loss: 0.04577) (t_loss: 0.06619) (accu: 0.9792)
[epoch : 16] (l_loss: 0.04663) (t_loss: 0.07054) (accu: 0.9785)
[epoch : 17] (l_loss: 0.04535) (t_loss: 0.07287) (accu: 0.9773)
[epoch : 18] (l_loss: 0.04443) (t_loss: 0.06837) (accu: 0.9803)
[epoch : 19] (l_loss: 0.04450) (t_loss: 0.07021) (accu: 0.9778)
[epoch : 20] (l_loss: 0.04475) (t_loss: 0.07093) (accu: 0.9774)
[epoch : 21] (l_loss: 0.04369) (t_loss: 0.07492) (accu: 0.9786)
[epoch : 22] (l_loss: 0.04424) (t_loss: 0.06723) (accu: 0.9796)
[epoch : 23] (l_loss: 0.04346) (t_loss: 0.07246) (accu: 0.9777)
[epoch : 24] (l_loss: 0.04425) (t_loss: 0.06909) (accu: 0.9794)
[epoch : 25] (l_loss: 0.04272) (t_loss: 0.07037) (accu: 0.9790)
[epoch : 26] (l_loss: 0.04338) (t_loss: 0.06618) (accu: 0.9783)
[epoch : 27] (l_loss: 0.04275) (t_loss: 0.06941) (accu: 0.9779)
[epoch : 28] (l_loss: 0.04169) (t_loss: 0.07451) (accu: 0.9773)
[epoch : 29] (l_loss: 0.04382) (t_loss: 0.07536) (accu: 0.9776)
[epoch : 30] (l_loss: 0.04257) (t_loss: 0.06903) (accu: 0.9800)
[epoch : 31] (l_loss: 0.04245) (t_loss: 0.08073) (accu: 0.9734)
[epoch : 32] (l_loss: 0.04356) (t_loss: 0.07305) (accu: 0.9780)
[epoch : 33] (l_loss: 0.04271) (t_loss: 0.07789) (accu: 0.9770)
[epoch : 34] (l_loss: 0.04146) (t_loss: 0.06684) (accu: 0.9790)
[epoch : 35] (l_loss: 0.04257) (t_loss: 0.07654) (accu: 0.9761)
[epoch : 36] (l_loss: 0.04250) (t_loss: 0.07192) (accu: 0.9783)
[epoch : 37] (l_loss: 0.04262) (t_loss: 0.07384) (accu: 0.9778)
[epoch : 38] (l_loss: 0.04289) (t_loss: 0.07226) (accu: 0.9779)
[epoch : 39] (l_loss: 0.04202) (t_loss: 0.06912) (accu: 0.9783)
[epoch : 40] (l_loss: 0.04131) (t_loss: 0.07886) (accu: 0.9756)
[epoch : 41] (l_loss: 0.04224) (t_loss: 0.07058) (accu: 0.9785)
[epoch : 42] (l_loss: 0.04289) (t_loss: 0.07935) (accu: 0.9755)
[epoch : 43] (l_loss: 0.04138) (t_loss: 0.07616) (accu: 0.9761)
[epoch : 44] (l_loss: 0.04224) (t_loss: 0.07509) (accu: 0.9766)
[epoch : 45] (l_loss: 0.04215) (t_loss: 0.07013) (accu: 0.9783)
[epoch : 46] (l_loss: 0.04193) (t_loss: 0.06814) (accu: 0.9801)
[epoch : 47] (l_loss: 0.04177) (t_loss: 0.07350) (accu: 0.9777)
[epoch : 48] (l_loss: 0.04185) (t_loss: 0.07400) (accu: 0.9773)
[epoch : 49] (l_loss: 0.04171) (t_loss: 0.07007) (accu: 0.9784)
[epoch : 50] (l_loss: 0.04182) (t_loss: 0.06534) (accu: 0.9793)
Finish! (Best accu: 0.9803) (Time taken(sec) : 636.71) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (9537 | 256663)          3.58
fc1.weight   :       235200 (8275 | 226925)          3.52
fc2.weight   :        30000 (1056 | 28944)           3.52
fcout.weight :          1000 (206 | 794)            20.60
------------------------------------------------------------

Learning start! [Prune_iter : (16/20), Remaining weight : 3.58 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.27150) (accu: 0.1283)
[epoch : 1] (l_loss: 0.30454) (t_loss: 0.13808) (accu: 0.9579)
[epoch : 2] (l_loss: 0.11297) (t_loss: 0.10272) (accu: 0.9693)
[epoch : 3] (l_loss: 0.08268) (t_loss: 0.08516) (accu: 0.9746)
[epoch : 4] (l_loss: 0.06966) (t_loss: 0.07524) (accu: 0.9782)
[epoch : 5] (l_loss: 0.06138) (t_loss: 0.07630) (accu: 0.9758)
[epoch : 6] (l_loss: 0.05679) (t_loss: 0.07058) (accu: 0.9784)
[epoch : 7] (l_loss: 0.05214) (t_loss: 0.06908) (accu: 0.9778)
[epoch : 8] (l_loss: 0.05028) (t_loss: 0.06883) (accu: 0.9786)
[epoch : 9] (l_loss: 0.04867) (t_loss: 0.07301) (accu: 0.9767)
[epoch : 10] (l_loss: 0.04679) (t_loss: 0.07135) (accu: 0.9783)
[epoch : 11] (l_loss: 0.04597) (t_loss: 0.07326) (accu: 0.9765)
[epoch : 12] (l_loss: 0.04581) (t_loss: 0.06736) (accu: 0.9789)
[epoch : 13] (l_loss: 0.04378) (t_loss: 0.07171) (accu: 0.9798)
[epoch : 14] (l_loss: 0.04384) (t_loss: 0.07046) (accu: 0.9795)
[epoch : 15] (l_loss: 0.04276) (t_loss: 0.07044) (accu: 0.9782)
[epoch : 16] (l_loss: 0.04296) (t_loss: 0.07110) (accu: 0.9786)
[epoch : 17] (l_loss: 0.04192) (t_loss: 0.07514) (accu: 0.9787)
[epoch : 18] (l_loss: 0.04215) (t_loss: 0.07186) (accu: 0.9769)
[epoch : 19] (l_loss: 0.04088) (t_loss: 0.07158) (accu: 0.9773)
[epoch : 20] (l_loss: 0.04091) (t_loss: 0.07207) (accu: 0.9766)
[epoch : 21] (l_loss: 0.04121) (t_loss: 0.07437) (accu: 0.9774)
[epoch : 22] (l_loss: 0.04054) (t_loss: 0.06479) (accu: 0.9804)
[epoch : 23] (l_loss: 0.04030) (t_loss: 0.06792) (accu: 0.9791)
[epoch : 24] (l_loss: 0.04063) (t_loss: 0.06853) (accu: 0.9798)
[epoch : 25] (l_loss: 0.04001) (t_loss: 0.06642) (accu: 0.9791)
[epoch : 26] (l_loss: 0.03976) (t_loss: 0.06922) (accu: 0.9789)
[epoch : 27] (l_loss: 0.03968) (t_loss: 0.07039) (accu: 0.9788)
[epoch : 28] (l_loss: 0.04032) (t_loss: 0.07302) (accu: 0.9774)
[epoch : 29] (l_loss: 0.03981) (t_loss: 0.08134) (accu: 0.9761)
[epoch : 30] (l_loss: 0.03941) (t_loss: 0.06719) (accu: 0.9798)
[epoch : 31] (l_loss: 0.04036) (t_loss: 0.06967) (accu: 0.9780)
[epoch : 32] (l_loss: 0.03915) (t_loss: 0.07139) (accu: 0.9780)
[epoch : 33] (l_loss: 0.04065) (t_loss: 0.06829) (accu: 0.9793)
[epoch : 34] (l_loss: 0.03889) (t_loss: 0.07142) (accu: 0.9783)
[epoch : 35] (l_loss: 0.03991) (t_loss: 0.06881) (accu: 0.9789)
[epoch : 36] (l_loss: 0.03972) (t_loss: 0.06964) (accu: 0.9790)
[epoch : 37] (l_loss: 0.04027) (t_loss: 0.07341) (accu: 0.9773)
[epoch : 38] (l_loss: 0.03939) (t_loss: 0.07630) (accu: 0.9763)
[epoch : 39] (l_loss: 0.03910) (t_loss: 0.06966) (accu: 0.9784)
[epoch : 40] (l_loss: 0.03896) (t_loss: 0.07224) (accu: 0.9786)
[epoch : 41] (l_loss: 0.03892) (t_loss: 0.06886) (accu: 0.9797)
[epoch : 42] (l_loss: 0.03953) (t_loss: 0.07440) (accu: 0.9777)
[epoch : 43] (l_loss: 0.03896) (t_loss: 0.07377) (accu: 0.9760)
[epoch : 44] (l_loss: 0.03954) (t_loss: 0.07076) (accu: 0.9786)
[epoch : 45] (l_loss: 0.03966) (t_loss: 0.06894) (accu: 0.9789)
[epoch : 46] (l_loss: 0.03946) (t_loss: 0.07019) (accu: 0.9774)
[epoch : 47] (l_loss: 0.03976) (t_loss: 0.06558) (accu: 0.9807)
[epoch : 48] (l_loss: 0.03894) (t_loss: 0.07516) (accu: 0.9762)
[epoch : 49] (l_loss: 0.04001) (t_loss: 0.06703) (accu: 0.9787)
[epoch : 50] (l_loss: 0.03986) (t_loss: 0.06711) (accu: 0.9805)
Finish! (Best accu: 0.9807) (Time taken(sec) : 637.77) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (7649 | 258551)          2.87
fc1.weight   :       235200 (6620 | 228580)          2.81
fc2.weight   :        30000 (844 | 29156)            2.81
fcout.weight :          1000 (185 | 815)            18.50
------------------------------------------------------------

Learning start! [Prune_iter : (17/20), Remaining weight : 2.87 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.26571) (accu: 0.1492)
[epoch : 1] (l_loss: 0.31668) (t_loss: 0.14149) (accu: 0.9580)
[epoch : 2] (l_loss: 0.11605) (t_loss: 0.09818) (accu: 0.9702)
[epoch : 3] (l_loss: 0.08713) (t_loss: 0.08659) (accu: 0.9735)
[epoch : 4] (l_loss: 0.07294) (t_loss: 0.08251) (accu: 0.9769)
[epoch : 5] (l_loss: 0.06376) (t_loss: 0.07679) (accu: 0.9766)
[epoch : 6] (l_loss: 0.05754) (t_loss: 0.07654) (accu: 0.9771)
[epoch : 7] (l_loss: 0.05471) (t_loss: 0.07327) (accu: 0.9776)
[epoch : 8] (l_loss: 0.05101) (t_loss: 0.06812) (accu: 0.9802)
[epoch : 9] (l_loss: 0.04858) (t_loss: 0.07098) (accu: 0.9783)
[epoch : 10] (l_loss: 0.04651) (t_loss: 0.07179) (accu: 0.9789)
[epoch : 11] (l_loss: 0.04393) (t_loss: 0.07538) (accu: 0.9777)
[epoch : 12] (l_loss: 0.04353) (t_loss: 0.06874) (accu: 0.9789)
[epoch : 13] (l_loss: 0.04203) (t_loss: 0.07135) (accu: 0.9791)
[epoch : 14] (l_loss: 0.04067) (t_loss: 0.07202) (accu: 0.9775)
[epoch : 15] (l_loss: 0.04078) (t_loss: 0.06776) (accu: 0.9800)
[epoch : 16] (l_loss: 0.03897) (t_loss: 0.06752) (accu: 0.9784)
[epoch : 17] (l_loss: 0.03974) (t_loss: 0.06950) (accu: 0.9786)
[epoch : 18] (l_loss: 0.03949) (t_loss: 0.06817) (accu: 0.9788)
[epoch : 19] (l_loss: 0.03902) (t_loss: 0.07323) (accu: 0.9769)
[epoch : 20] (l_loss: 0.03841) (t_loss: 0.06702) (accu: 0.9797)
[epoch : 21] (l_loss: 0.03955) (t_loss: 0.07169) (accu: 0.9792)
[epoch : 22] (l_loss: 0.03870) (t_loss: 0.07490) (accu: 0.9774)
[epoch : 23] (l_loss: 0.03836) (t_loss: 0.07022) (accu: 0.9790)
[epoch : 24] (l_loss: 0.03850) (t_loss: 0.06738) (accu: 0.9794)
[epoch : 25] (l_loss: 0.03867) (t_loss: 0.07055) (accu: 0.9780)
[epoch : 26] (l_loss: 0.03799) (t_loss: 0.06674) (accu: 0.9801)
[epoch : 27] (l_loss: 0.03895) (t_loss: 0.06982) (accu: 0.9780)
[epoch : 28] (l_loss: 0.03800) (t_loss: 0.07344) (accu: 0.9776)
[epoch : 29] (l_loss: 0.03806) (t_loss: 0.07533) (accu: 0.9763)
[epoch : 30] (l_loss: 0.03705) (t_loss: 0.06814) (accu: 0.9794)
[epoch : 31] (l_loss: 0.03848) (t_loss: 0.07031) (accu: 0.9775)
[epoch : 32] (l_loss: 0.03816) (t_loss: 0.07339) (accu: 0.9771)
[epoch : 33] (l_loss: 0.03779) (t_loss: 0.07635) (accu: 0.9772)
[epoch : 34] (l_loss: 0.03736) (t_loss: 0.06875) (accu: 0.9779)
[epoch : 35] (l_loss: 0.03730) (t_loss: 0.06966) (accu: 0.9794)
[epoch : 36] (l_loss: 0.03803) (t_loss: 0.07278) (accu: 0.9761)
[epoch : 37] (l_loss: 0.03731) (t_loss: 0.07160) (accu: 0.9778)
[epoch : 38] (l_loss: 0.03758) (t_loss: 0.06942) (accu: 0.9789)
[epoch : 39] (l_loss: 0.03746) (t_loss: 0.07067) (accu: 0.9776)
[epoch : 40] (l_loss: 0.03756) (t_loss: 0.07429) (accu: 0.9781)
[epoch : 41] (l_loss: 0.03796) (t_loss: 0.07016) (accu: 0.9783)
[epoch : 42] (l_loss: 0.03737) (t_loss: 0.06296) (accu: 0.9804)
[epoch : 43] (l_loss: 0.03818) (t_loss: 0.07012) (accu: 0.9788)
[epoch : 44] (l_loss: 0.03765) (t_loss: 0.07006) (accu: 0.9789)
[epoch : 45] (l_loss: 0.03709) (t_loss: 0.07213) (accu: 0.9770)
[epoch : 46] (l_loss: 0.03801) (t_loss: 0.06933) (accu: 0.9796)
[epoch : 47] (l_loss: 0.03744) (t_loss: 0.07057) (accu: 0.9775)
[epoch : 48] (l_loss: 0.03774) (t_loss: 0.07416) (accu: 0.9779)
[epoch : 49] (l_loss: 0.03703) (t_loss: 0.07215) (accu: 0.9776)
[epoch : 50] (l_loss: 0.03762) (t_loss: 0.06952) (accu: 0.9784)
Finish! (Best accu: 0.9804) (Time taken(sec) : 651.24) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (6139 | 260061)          2.31
fc1.weight   :       235200 (5296 | 229904)          2.25
fc2.weight   :        30000 (676 | 29324)            2.25
fcout.weight :          1000 (167 | 833)            16.70
------------------------------------------------------------

Learning start! [Prune_iter : (18/20), Remaining weight : 2.31 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.27374) (accu: 0.1600)
[epoch : 1] (l_loss: 0.32698) (t_loss: 0.13968) (accu: 0.9585)
[epoch : 2] (l_loss: 0.11447) (t_loss: 0.10167) (accu: 0.9712)
[epoch : 3] (l_loss: 0.08458) (t_loss: 0.08651) (accu: 0.9742)
[epoch : 4] (l_loss: 0.07103) (t_loss: 0.08225) (accu: 0.9761)
[epoch : 5] (l_loss: 0.06295) (t_loss: 0.07964) (accu: 0.9756)
[epoch : 6] (l_loss: 0.05726) (t_loss: 0.07648) (accu: 0.9769)
[epoch : 7] (l_loss: 0.05292) (t_loss: 0.07601) (accu: 0.9763)
[epoch : 8] (l_loss: 0.05008) (t_loss: 0.07298) (accu: 0.9784)
[epoch : 9] (l_loss: 0.04810) (t_loss: 0.07128) (accu: 0.9777)
[epoch : 10] (l_loss: 0.04577) (t_loss: 0.06922) (accu: 0.9791)
[epoch : 11] (l_loss: 0.04423) (t_loss: 0.07066) (accu: 0.9786)
[epoch : 12] (l_loss: 0.04336) (t_loss: 0.07074) (accu: 0.9796)
[epoch : 13] (l_loss: 0.04191) (t_loss: 0.07201) (accu: 0.9796)
[epoch : 14] (l_loss: 0.04204) (t_loss: 0.07003) (accu: 0.9789)
[epoch : 15] (l_loss: 0.04123) (t_loss: 0.07210) (accu: 0.9788)
[epoch : 16] (l_loss: 0.04047) (t_loss: 0.06751) (accu: 0.9792)
[epoch : 17] (l_loss: 0.04024) (t_loss: 0.06828) (accu: 0.9785)
[epoch : 18] (l_loss: 0.04031) (t_loss: 0.06872) (accu: 0.9787)
[epoch : 19] (l_loss: 0.04032) (t_loss: 0.07180) (accu: 0.9779)
[epoch : 20] (l_loss: 0.03974) (t_loss: 0.07146) (accu: 0.9781)
[epoch : 21] (l_loss: 0.03956) (t_loss: 0.06735) (accu: 0.9792)
[epoch : 22] (l_loss: 0.03881) (t_loss: 0.07101) (accu: 0.9782)
[epoch : 23] (l_loss: 0.03888) (t_loss: 0.06759) (accu: 0.9789)
[epoch : 24] (l_loss: 0.03892) (t_loss: 0.07264) (accu: 0.9782)
[epoch : 25] (l_loss: 0.03875) (t_loss: 0.07103) (accu: 0.9783)
[epoch : 26] (l_loss: 0.03917) (t_loss: 0.07039) (accu: 0.9772)
[epoch : 27] (l_loss: 0.03823) (t_loss: 0.07366) (accu: 0.9783)
[epoch : 28] (l_loss: 0.03912) (t_loss: 0.06934) (accu: 0.9787)
[epoch : 29] (l_loss: 0.03892) (t_loss: 0.07273) (accu: 0.9777)
[epoch : 30] (l_loss: 0.03877) (t_loss: 0.06985) (accu: 0.9785)
[epoch : 31] (l_loss: 0.03882) (t_loss: 0.06922) (accu: 0.9779)
[epoch : 32] (l_loss: 0.03822) (t_loss: 0.07526) (accu: 0.9770)
[epoch : 33] (l_loss: 0.03821) (t_loss: 0.07192) (accu: 0.9771)
[epoch : 34] (l_loss: 0.03855) (t_loss: 0.07313) (accu: 0.9779)
[epoch : 35] (l_loss: 0.03820) (t_loss: 0.06975) (accu: 0.9790)
[epoch : 36] (l_loss: 0.03814) (t_loss: 0.06858) (accu: 0.9770)
[epoch : 37] (l_loss: 0.03847) (t_loss: 0.06880) (accu: 0.9785)
[epoch : 38] (l_loss: 0.03809) (t_loss: 0.07184) (accu: 0.9786)
[epoch : 39] (l_loss: 0.03835) (t_loss: 0.07152) (accu: 0.9778)
[epoch : 40] (l_loss: 0.03863) (t_loss: 0.06797) (accu: 0.9775)
[epoch : 41] (l_loss: 0.03821) (t_loss: 0.06926) (accu: 0.9775)
[epoch : 42] (l_loss: 0.03782) (t_loss: 0.07015) (accu: 0.9783)
[epoch : 43] (l_loss: 0.03738) (t_loss: 0.06928) (accu: 0.9781)
[epoch : 44] (l_loss: 0.03712) (t_loss: 0.06875) (accu: 0.9783)
[epoch : 45] (l_loss: 0.03754) (t_loss: 0.06967) (accu: 0.9777)
[epoch : 46] (l_loss: 0.03682) (t_loss: 0.07202) (accu: 0.9781)
[epoch : 47] (l_loss: 0.03748) (t_loss: 0.06857) (accu: 0.9787)
[epoch : 48] (l_loss: 0.03702) (t_loss: 0.06861) (accu: 0.9782)
[epoch : 49] (l_loss: 0.03700) (t_loss: 0.06746) (accu: 0.9798)
[epoch : 50] (l_loss: 0.03661) (t_loss: 0.07457) (accu: 0.9774)
Finish! (Best accu: 0.9798) (Time taken(sec) : 651.75) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (4927 | 261273)          1.85
fc1.weight   :       235200 (4237 | 230963)          1.80
fc2.weight   :        30000 (540 | 29460)            1.80
fcout.weight :          1000 (150 | 850)            15.00
------------------------------------------------------------

Learning start! [Prune_iter : (19/20), Remaining weight : 1.85 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.28937) (accu: 0.1027)
[epoch : 1] (l_loss: 0.35196) (t_loss: 0.14469) (accu: 0.9573)
[epoch : 2] (l_loss: 0.12287) (t_loss: 0.10940) (accu: 0.9664)
[epoch : 3] (l_loss: 0.09312) (t_loss: 0.09498) (accu: 0.9727)
[epoch : 4] (l_loss: 0.07749) (t_loss: 0.08700) (accu: 0.9747)
[epoch : 5] (l_loss: 0.06635) (t_loss: 0.07880) (accu: 0.9771)
[epoch : 6] (l_loss: 0.05947) (t_loss: 0.07900) (accu: 0.9758)
[epoch : 7] (l_loss: 0.05449) (t_loss: 0.07528) (accu: 0.9782)
[epoch : 8] (l_loss: 0.05137) (t_loss: 0.07448) (accu: 0.9778)
[epoch : 9] (l_loss: 0.04967) (t_loss: 0.07794) (accu: 0.9769)
[epoch : 10] (l_loss: 0.04758) (t_loss: 0.07521) (accu: 0.9767)
[epoch : 11] (l_loss: 0.04666) (t_loss: 0.07049) (accu: 0.9782)
[epoch : 12] (l_loss: 0.04569) (t_loss: 0.06953) (accu: 0.9785)
[epoch : 13] (l_loss: 0.04470) (t_loss: 0.07213) (accu: 0.9790)
[epoch : 14] (l_loss: 0.04445) (t_loss: 0.07230) (accu: 0.9784)
[epoch : 15] (l_loss: 0.04327) (t_loss: 0.06915) (accu: 0.9788)
[epoch : 16] (l_loss: 0.04389) (t_loss: 0.07066) (accu: 0.9791)
[epoch : 17] (l_loss: 0.04276) (t_loss: 0.07420) (accu: 0.9785)
[epoch : 18] (l_loss: 0.04286) (t_loss: 0.07305) (accu: 0.9791)
[epoch : 19] (l_loss: 0.04253) (t_loss: 0.07349) (accu: 0.9789)
[epoch : 20] (l_loss: 0.04268) (t_loss: 0.07259) (accu: 0.9784)
[epoch : 21] (l_loss: 0.04235) (t_loss: 0.07087) (accu: 0.9791)
[epoch : 22] (l_loss: 0.04216) (t_loss: 0.07548) (accu: 0.9775)
[epoch : 23] (l_loss: 0.04219) (t_loss: 0.07319) (accu: 0.9774)
[epoch : 24] (l_loss: 0.04150) (t_loss: 0.06849) (accu: 0.9792)
[epoch : 25] (l_loss: 0.04091) (t_loss: 0.06844) (accu: 0.9802)
[epoch : 26] (l_loss: 0.04090) (t_loss: 0.06976) (accu: 0.9794)
[epoch : 27] (l_loss: 0.04124) (t_loss: 0.07142) (accu: 0.9783)
[epoch : 28] (l_loss: 0.04095) (t_loss: 0.07791) (accu: 0.9766)
[epoch : 29] (l_loss: 0.04072) (t_loss: 0.07290) (accu: 0.9766)
[epoch : 30] (l_loss: 0.04019) (t_loss: 0.07018) (accu: 0.9788)
[epoch : 31] (l_loss: 0.03901) (t_loss: 0.07005) (accu: 0.9790)
[epoch : 32] (l_loss: 0.03962) (t_loss: 0.06778) (accu: 0.9792)
[epoch : 33] (l_loss: 0.03980) (t_loss: 0.07085) (accu: 0.9794)
[epoch : 34] (l_loss: 0.03953) (t_loss: 0.07159) (accu: 0.9781)
[epoch : 35] (l_loss: 0.03990) (t_loss: 0.07106) (accu: 0.9775)
[epoch : 36] (l_loss: 0.03938) (t_loss: 0.06842) (accu: 0.9786)
[epoch : 37] (l_loss: 0.03936) (t_loss: 0.07033) (accu: 0.9788)
[epoch : 38] (l_loss: 0.03962) (t_loss: 0.07008) (accu: 0.9770)
[epoch : 39] (l_loss: 0.03919) (t_loss: 0.06689) (accu: 0.9799)
[epoch : 40] (l_loss: 0.03946) (t_loss: 0.07526) (accu: 0.9765)
[epoch : 41] (l_loss: 0.03994) (t_loss: 0.07031) (accu: 0.9792)
[epoch : 42] (l_loss: 0.03929) (t_loss: 0.06825) (accu: 0.9790)
[epoch : 43] (l_loss: 0.03987) (t_loss: 0.07345) (accu: 0.9774)
[epoch : 44] (l_loss: 0.03907) (t_loss: 0.06759) (accu: 0.9796)
[epoch : 45] (l_loss: 0.03911) (t_loss: 0.07014) (accu: 0.9776)
[epoch : 46] (l_loss: 0.04008) (t_loss: 0.06825) (accu: 0.9784)
[epoch : 47] (l_loss: 0.03963) (t_loss: 0.07398) (accu: 0.9782)
[epoch : 48] (l_loss: 0.03916) (t_loss: 0.06933) (accu: 0.9788)
[epoch : 49] (l_loss: 0.03945) (t_loss: 0.06987) (accu: 0.9785)
[epoch : 50] (l_loss: 0.03982) (t_loss: 0.06743) (accu: 0.9794)
Finish! (Best accu: 0.9802) (Time taken(sec) : 653.17) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (3957 | 262243)          1.49
fc1.weight   :       235200 (3390 | 231810)          1.44
fc2.weight   :        30000 (432 | 29568)            1.44
fcout.weight :          1000 (135 | 865)            13.50
------------------------------------------------------------

Learning start! [Prune_iter : (20/20), Remaining weight : 1.49 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29419) (accu: 0.0994)
[epoch : 1] (l_loss: 0.38172) (t_loss: 0.14595) (accu: 0.9571)
[epoch : 2] (l_loss: 0.12533) (t_loss: 0.10798) (accu: 0.9674)
[epoch : 3] (l_loss: 0.09158) (t_loss: 0.09204) (accu: 0.9729)
[epoch : 4] (l_loss: 0.07408) (t_loss: 0.08477) (accu: 0.9744)
[epoch : 5] (l_loss: 0.06314) (t_loss: 0.07596) (accu: 0.9771)
[epoch : 6] (l_loss: 0.05721) (t_loss: 0.07640) (accu: 0.9779)
[epoch : 7] (l_loss: 0.05307) (t_loss: 0.07189) (accu: 0.9794)
[epoch : 8] (l_loss: 0.05023) (t_loss: 0.06925) (accu: 0.9782)
[epoch : 9] (l_loss: 0.04841) (t_loss: 0.06964) (accu: 0.9788)
[epoch : 10] (l_loss: 0.04727) (t_loss: 0.06803) (accu: 0.9792)
[epoch : 11] (l_loss: 0.04695) (t_loss: 0.07303) (accu: 0.9781)
[epoch : 12] (l_loss: 0.04641) (t_loss: 0.07171) (accu: 0.9782)
[epoch : 13] (l_loss: 0.04531) (t_loss: 0.06993) (accu: 0.9785)
[epoch : 14] (l_loss: 0.04503) (t_loss: 0.07067) (accu: 0.9789)
[epoch : 15] (l_loss: 0.04490) (t_loss: 0.07142) (accu: 0.9774)
[epoch : 16] (l_loss: 0.04469) (t_loss: 0.07114) (accu: 0.9759)
[epoch : 17] (l_loss: 0.04452) (t_loss: 0.07076) (accu: 0.9780)
[epoch : 18] (l_loss: 0.04450) (t_loss: 0.06943) (accu: 0.9787)
[epoch : 19] (l_loss: 0.04421) (t_loss: 0.07315) (accu: 0.9768)
[epoch : 20] (l_loss: 0.04424) (t_loss: 0.07259) (accu: 0.9776)
[epoch : 21] (l_loss: 0.04435) (t_loss: 0.07111) (accu: 0.9774)
[epoch : 22] (l_loss: 0.04477) (t_loss: 0.07449) (accu: 0.9768)
[epoch : 23] (l_loss: 0.04334) (t_loss: 0.07076) (accu: 0.9773)
[epoch : 24] (l_loss: 0.04418) (t_loss: 0.06983) (accu: 0.9776)
[epoch : 25] (l_loss: 0.04397) (t_loss: 0.06939) (accu: 0.9764)
[epoch : 26] (l_loss: 0.04417) (t_loss: 0.07300) (accu: 0.9776)
[epoch : 27] (l_loss: 0.04451) (t_loss: 0.07027) (accu: 0.9780)
[epoch : 28] (l_loss: 0.04383) (t_loss: 0.07281) (accu: 0.9773)
[epoch : 29] (l_loss: 0.04381) (t_loss: 0.07164) (accu: 0.9786)
[epoch : 30] (l_loss: 0.04377) (t_loss: 0.07022) (accu: 0.9780)
[epoch : 31] (l_loss: 0.04427) (t_loss: 0.07098) (accu: 0.9772)
[epoch : 32] (l_loss: 0.04384) (t_loss: 0.07234) (accu: 0.9765)
[epoch : 33] (l_loss: 0.04365) (t_loss: 0.06999) (accu: 0.9779)
[epoch : 34] (l_loss: 0.04412) (t_loss: 0.07358) (accu: 0.9777)
[epoch : 35] (l_loss: 0.04426) (t_loss: 0.06874) (accu: 0.9774)
[epoch : 36] (l_loss: 0.04406) (t_loss: 0.07367) (accu: 0.9792)
[epoch : 37] (l_loss: 0.04360) (t_loss: 0.06975) (accu: 0.9778)
[epoch : 38] (l_loss: 0.04410) (t_loss: 0.07115) (accu: 0.9783)
[epoch : 39] (l_loss: 0.04304) (t_loss: 0.07183) (accu: 0.9771)
[epoch : 40] (l_loss: 0.04274) (t_loss: 0.06891) (accu: 0.9798)
[epoch : 41] (l_loss: 0.04314) (t_loss: 0.07132) (accu: 0.9775)
[epoch : 42] (l_loss: 0.04363) (t_loss: 0.06861) (accu: 0.9783)
[epoch : 43] (l_loss: 0.04309) (t_loss: 0.07051) (accu: 0.9776)
[epoch : 44] (l_loss: 0.04317) (t_loss: 0.06942) (accu: 0.9774)
[epoch : 45] (l_loss: 0.04325) (t_loss: 0.07622) (accu: 0.9774)
[epoch : 46] (l_loss: 0.04366) (t_loss: 0.07124) (accu: 0.9778)
[epoch : 47] (l_loss: 0.04306) (t_loss: 0.07402) (accu: 0.9775)
[epoch : 48] (l_loss: 0.04280) (t_loss: 0.06894) (accu: 0.9788)
[epoch : 49] (l_loss: 0.04299) (t_loss: 0.07440) (accu: 0.9750)
[epoch : 50] (l_loss: 0.04269) (t_loss: 0.07357) (accu: 0.9761)
Finish! (Best accu: 0.9798) (Time taken(sec) : 662.70) 


Maximum accuracy per weight remaining
Remaining weight 100.0 %  Epoch 23 Accu 0.9795
Remaining weight 80.04 %  Epoch 9 Accu 0.9791
Remaining weight 64.06 %  Epoch 40 Accu 0.9804
Remaining weight 51.28 %  Epoch 42 Accu 0.9810
Remaining weight 41.05 %  Epoch 44 Accu 0.9799
Remaining weight 32.87 %  Epoch 35 Accu 0.9811
Remaining weight 26.32 %  Epoch 48 Accu 0.9809
Remaining weight 21.07 %  Epoch 26 Accu 0.9802
Remaining weight 16.88 %  Epoch 15 Accu 0.9801
Remaining weight 13.52 %  Epoch 35 Accu 0.9794
Remaining weight 10.83 %  Epoch 35 Accu 0.9801
Remaining weight 8.68 %  Epoch 24 Accu 0.9799
Remaining weight 6.95 %  Epoch 19 Accu 0.9811
Remaining weight 5.57 %  Epoch 37 Accu 0.9798
Remaining weight 4.47 %  Epoch 17 Accu 0.9803
Remaining weight 3.58 %  Epoch 46 Accu 0.9807
Remaining weight 2.87 %  Epoch 41 Accu 0.9804
Remaining weight 2.31 %  Epoch 48 Accu 0.9798
Remaining weight 1.85 %  Epoch 24 Accu 0.9802
Remaining weight 1.49 %  Epoch 39 Accu 0.9798
Average test data
Remaining weight 100.00 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.800631   0.1075
1     0.203610    0.122398   0.9619
2     0.107642    0.102713   0.9688
3     0.094168    0.094820   0.9713
4     0.086881    0.121556   0.9610
5     0.080453    0.100935   0.9690
6     0.077376    0.100022   0.9676
7     0.074087    0.080748   0.9750
8     0.073236    0.084044   0.9736
9     0.067632    0.087465   0.9726
10     0.068354    0.091460   0.9741
11     0.066576    0.081263   0.9749
12     0.063121    0.075580   0.9771
13     0.062431    0.074681   0.9777
14     0.063195    0.089894   0.9747
15     0.063064    0.085256   0.9738
16     0.061450    0.081666   0.9743
17     0.060536    0.082856   0.9753
18     0.061029    0.084137   0.9737
19     0.061076    0.079446   0.9744
20     0.058977    0.091961   0.9723
21     0.058510    0.081100   0.9751
22     0.058593    0.072185   0.9792
23     0.058604    0.082650   0.9736
24     0.057318    0.070213   0.9795
25     0.056995    0.080397   0.9739
26     0.055422    0.079945   0.9740
27     0.058893    0.076587   0.9736
28     0.058750    0.080096   0.9744
29     0.057219    0.070318   0.9788
30     0.056229    0.073755   0.9785
31     0.057868    0.076029   0.9758
32     0.055101    0.078296   0.9761
33     0.056601    0.081385   0.9743
34     0.056348    0.090680   0.9716
35     0.055037    0.077415   0.9772
36     0.054893    0.069641   0.9786
37     0.056909    0.073056   0.9787
38     0.056363    0.072268   0.9778
39     0.055217    0.064602   0.9789
40     0.056054    0.076717   0.9777
41     0.055851    0.074913   0.9772
42     0.056777    0.090290   0.9724
43     0.055523    0.074969   0.9772
44     0.055511    0.073978   0.9772
45     0.056323    0.088367   0.9730
46     0.054672    0.072872   0.9781
47     0.056260    0.080473   0.9750
48     0.054951    0.086856   0.9723
49     0.055880    0.078043   0.9758
50     0.054528    0.086816   0.9723
Remaining weight 80.04 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.590819   0.0960
1     0.197622    0.106335   0.9651
2     0.103112    0.094745   0.9708
3     0.090275    0.103757   0.9646
4     0.080729    0.083987   0.9743
5     0.076618    0.088438   0.9714
6     0.075054    0.080161   0.9757
7     0.067993    0.077997   0.9767
8     0.065518    0.084417   0.9742
9     0.064629    0.075841   0.9751
10     0.061736    0.074566   0.9791
11     0.060741    0.074696   0.9759
12     0.059765    0.074376   0.9755
13     0.060020    0.081226   0.9726
14     0.059105    0.071171   0.9771
15     0.056451    0.069575   0.9780
16     0.058233    0.088195   0.9726
17     0.054498    0.082614   0.9733
18     0.056833    0.078768   0.9752
19     0.055234    0.091107   0.9694
20     0.055554    0.090597   0.9714
21     0.054198    0.075261   0.9764
22     0.055344    0.077076   0.9744
23     0.052711    0.073330   0.9779
24     0.053485    0.074921   0.9762
25     0.055510    0.078053   0.9760
26     0.054070    0.071756   0.9767
27     0.052370    0.084077   0.9745
28     0.053577    0.079762   0.9757
29     0.053532    0.076436   0.9774
30     0.054491    0.094888   0.9708
31     0.054122    0.076695   0.9765
32     0.051574    0.081421   0.9759
33     0.053877    0.073359   0.9759
34     0.052371    0.073182   0.9771
35     0.051638    0.075622   0.9753
36     0.053213    0.068578   0.9779
37     0.051235    0.081488   0.9752
38     0.053119    0.078763   0.9763
39     0.052025    0.076202   0.9774
40     0.051622    0.074882   0.9770
41     0.052394    0.071977   0.9785
42     0.051758    0.086930   0.9741
43     0.050446    0.079253   0.9744
44     0.050559    0.079924   0.9758
45     0.050356    0.076298   0.9758
46     0.052498    0.068663   0.9787
47     0.050908    0.077014   0.9761
48     0.050914    0.076571   0.9772
49     0.051437    0.070274   0.9776
50     0.050720    0.073803   0.9775
Remaining weight 64.06 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.540963   0.0832
1     0.199282    0.105914   0.9661
2     0.102614    0.100832   0.9698
3     0.084187    0.090936   0.9708
4     0.077035    0.085226   0.9740
5     0.072886    0.086846   0.9726
6     0.069387    0.079473   0.9754
7     0.066106    0.088257   0.9716
8     0.064650    0.089173   0.9728
9     0.063076    0.073092   0.9780
10     0.062135    0.079209   0.9752
11     0.060076    0.072035   0.9777
12     0.057690    0.086440   0.9724
13     0.059034    0.083028   0.9732
14     0.057285    0.071379   0.9780
15     0.057242    0.074051   0.9769
16     0.057232    0.068520   0.9781
17     0.054896    0.094499   0.9721
18     0.055009    0.079875   0.9729
19     0.055159    0.080960   0.9752
20     0.054848    0.069175   0.9789
21     0.053271    0.083142   0.9748
22     0.053771    0.077024   0.9767
23     0.053408    0.075956   0.9767
24     0.053780    0.072768   0.9783
25     0.053672    0.069838   0.9797
26     0.052675    0.072848   0.9763
27     0.052516    0.077259   0.9777
28     0.052147    0.079048   0.9753
29     0.053027    0.077466   0.9753
30     0.050857    0.075568   0.9778
31     0.051944    0.087161   0.9723
32     0.051998    0.071202   0.9781
33     0.051833    0.069927   0.9783
34     0.050947    0.069402   0.9794
35     0.051482    0.075584   0.9768
36     0.050624    0.088713   0.9717
37     0.051309    0.081241   0.9748
38     0.051301    0.072376   0.9772
39     0.050078    0.072130   0.9777
40     0.050632    0.073655   0.9770
41     0.051388    0.065340   0.9804
42     0.050203    0.076558   0.9758
43     0.050684    0.074762   0.9761
44     0.049588    0.071272   0.9788
45     0.049568    0.068250   0.9787
46     0.050746    0.065538   0.9781
47     0.049297    0.077110   0.9761
48     0.049457    0.071374   0.9784
49     0.049930    0.076545   0.9770
50     0.050372    0.073899   0.9783
Remaining weight 51.28 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.446997   0.0808
1     0.206259    0.128513   0.9591
2     0.100874    0.102633   0.9674
3     0.085782    0.098410   0.9700
4     0.078893    0.106491   0.9656
5     0.071645    0.096091   0.9721
6     0.070096    0.082344   0.9754
7     0.065663    0.090201   0.9729
8     0.063669    0.082806   0.9736
9     0.061549    0.078808   0.9746
10     0.060654    0.083444   0.9744
11     0.058473    0.083274   0.9750
12     0.057586    0.070122   0.9786
13     0.057025    0.076395   0.9762
14     0.056049    0.071135   0.9777
15     0.054021    0.074433   0.9764
16     0.053680    0.071909   0.9769
17     0.053880    0.076193   0.9773
18     0.054981    0.071814   0.9772
19     0.052939    0.076377   0.9771
20     0.052647    0.076737   0.9762
21     0.051867    0.075535   0.9773
22     0.051053    0.071348   0.9779
23     0.052032    0.071784   0.9781
24     0.052564    0.070298   0.9781
25     0.052761    0.074798   0.9761
26     0.051166    0.081744   0.9755
27     0.051247    0.068104   0.9791
28     0.051089    0.067872   0.9780
29     0.050958    0.078184   0.9762
30     0.050682    0.069684   0.9785
31     0.051322    0.082866   0.9744
32     0.050357    0.079735   0.9743
33     0.051176    0.075760   0.9760
34     0.051620    0.067626   0.9791
35     0.049892    0.080366   0.9754
36     0.050334    0.076716   0.9766
37     0.050407    0.070484   0.9788
38     0.049216    0.072488   0.9774
39     0.048647    0.071563   0.9782
40     0.049183    0.074084   0.9770
41     0.051714    0.072247   0.9788
42     0.047635    0.071702   0.9796
43     0.049285    0.066839   0.9810
44     0.048864    0.065787   0.9797
45     0.048287    0.079421   0.9747
46     0.049692    0.068550   0.9786
47     0.049273    0.074121   0.9749
48     0.047928    0.071007   0.9776
49     0.051723    0.069569   0.9782
50     0.049382    0.084107   0.9767
Remaining weight 41.05 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.425894   0.0938
1     0.211781    0.108977   0.9696
2     0.099252    0.094576   0.9697
3     0.084675    0.084303   0.9737
4     0.074432    0.081829   0.9756
5     0.069605    0.076028   0.9752
6     0.067234    0.077304   0.9737
7     0.064686    0.080119   0.9736
8     0.061929    0.074607   0.9755
9     0.061952    0.081644   0.9748
10     0.060179    0.072635   0.9772
11     0.058862    0.074153   0.9762
12     0.056810    0.086559   0.9732
13     0.057261    0.070710   0.9778
14     0.055817    0.083189   0.9731
15     0.055675    0.080209   0.9743
16     0.052767    0.077449   0.9740
17     0.054969    0.075987   0.9751
18     0.053620    0.072904   0.9770
19     0.052495    0.077763   0.9749
20     0.052569    0.074776   0.9767
21     0.052378    0.071600   0.9783
22     0.051714    0.068456   0.9782
23     0.051443    0.071321   0.9773
24     0.052321    0.080476   0.9727
25     0.050893    0.076204   0.9760
26     0.052288    0.072648   0.9776
27     0.051250    0.078053   0.9758
28     0.051489    0.077730   0.9765
29     0.049889    0.073104   0.9777
30     0.050923    0.075891   0.9748
31     0.050130    0.073427   0.9765
32     0.050277    0.079029   0.9763
33     0.049793    0.077883   0.9766
34     0.048879    0.069495   0.9782
35     0.049520    0.075726   0.9768
36     0.050536    0.075752   0.9776
37     0.049537    0.073199   0.9792
38     0.049880    0.074451   0.9766
39     0.048435    0.071797   0.9772
40     0.050480    0.081612   0.9766
41     0.048731    0.078535   0.9762
42     0.048392    0.074655   0.9766
43     0.049065    0.071561   0.9771
44     0.048843    0.069779   0.9786
45     0.047750    0.066187   0.9799
46     0.048968    0.068550   0.9776
47     0.049462    0.083280   0.9757
48     0.047719    0.081198   0.9750
49     0.049053    0.082286   0.9757
50     0.049051    0.071355   0.9767
Remaining weight 32.87 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.368809   0.1173
1     0.218487    0.119362   0.9631
2     0.102302    0.098807   0.9698
3     0.084732    0.085199   0.9739
4     0.076257    0.084862   0.9737
5     0.070495    0.091962   0.9702
6     0.067618    0.077099   0.9750
7     0.063397    0.076816   0.9743
8     0.061541    0.082281   0.9748
9     0.059570    0.075776   0.9785
10     0.058741    0.078553   0.9753
11     0.057171    0.075826   0.9750
12     0.056960    0.079318   0.9736
13     0.055331    0.073641   0.9777
14     0.055825    0.077653   0.9772
15     0.053044    0.071953   0.9788
16     0.054947    0.066240   0.9805
17     0.052836    0.075448   0.9768
18     0.052036    0.078707   0.9742
19     0.051735    0.081028   0.9744
20     0.052239    0.083878   0.9743
21     0.052170    0.071362   0.9775
22     0.051317    0.071850   0.9774
23     0.051516    0.075074   0.9782
24     0.049832    0.075746   0.9767
25     0.050783    0.073348   0.9769
26     0.050044    0.073873   0.9764
27     0.049633    0.080967   0.9753
28     0.051669    0.075550   0.9766
29     0.049576    0.072780   0.9786
30     0.049610    0.074565   0.9754
31     0.048615    0.077615   0.9752
32     0.047875    0.082700   0.9755
33     0.049406    0.088145   0.9738
34     0.049233    0.076839   0.9761
35     0.049815    0.068970   0.9799
36     0.047281    0.068926   0.9811
37     0.048484    0.071462   0.9791
38     0.050050    0.072605   0.9780
39     0.049301    0.071928   0.9787
40     0.047824    0.077107   0.9765
41     0.048576    0.071072   0.9799
42     0.046650    0.068299   0.9802
43     0.047009    0.080084   0.9762
44     0.049536    0.073736   0.9773
45     0.048199    0.066010   0.9795
46     0.046748    0.069158   0.9788
47     0.047735    0.079090   0.9758
48     0.048012    0.072052   0.9778
49     0.046844    0.074924   0.9768
50     0.047320    0.079049   0.9762
Remaining weight 26.32 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.379295   0.0929
1     0.227251    0.122073   0.9623
2     0.101450    0.094532   0.9715
3     0.083107    0.108672   0.9671
4     0.074138    0.085105   0.9735
5     0.067608    0.083207   0.9744
6     0.064674    0.084811   0.9736
7     0.062839    0.075639   0.9746
8     0.059803    0.070403   0.9774
9     0.057905    0.079974   0.9755
10     0.055850    0.080928   0.9753
11     0.056030    0.075030   0.9768
12     0.054654    0.070857   0.9775
13     0.053643    0.073388   0.9775
14     0.053526    0.074527   0.9776
15     0.053466    0.087290   0.9728
16     0.052788    0.079487   0.9759
17     0.050922    0.069358   0.9768
18     0.051705    0.084191   0.9738
19     0.051769    0.068246   0.9795
20     0.051595    0.081203   0.9756
21     0.049982    0.070572   0.9776
22     0.048590    0.078301   0.9751
23     0.049749    0.070437   0.9771
24     0.050381    0.071819   0.9785
25     0.048997    0.077358   0.9754
26     0.050319    0.073102   0.9778
27     0.047304    0.070314   0.9780
28     0.049806    0.076264   0.9766
29     0.049699    0.076710   0.9752
30     0.047998    0.077157   0.9764
31     0.049211    0.077858   0.9775
32     0.048133    0.074718   0.9763
33     0.049487    0.071898   0.9786
34     0.049110    0.070027   0.9768
35     0.047930    0.074366   0.9776
36     0.048498    0.089556   0.9741
37     0.049504    0.070274   0.9784
38     0.048749    0.076839   0.9766
39     0.048311    0.072877   0.9752
40     0.048569    0.068329   0.9792
41     0.048582    0.072246   0.9772
42     0.047528    0.068147   0.9795
43     0.049844    0.073583   0.9771
44     0.047445    0.068052   0.9787
45     0.048318    0.076394   0.9773
46     0.048135    0.072072   0.9776
47     0.048960    0.075124   0.9761
48     0.047825    0.067744   0.9781
49     0.049108    0.067455   0.9809
50     0.047569    0.079175   0.9731
Remaining weight 21.07 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.338284   0.0893
1     0.231029    0.122995   0.9604
2     0.100659    0.102512   0.9671
3     0.082533    0.090170   0.9709
4     0.074754    0.099967   0.9663
5     0.069857    0.077901   0.9757
6     0.065804    0.077116   0.9767
7     0.062918    0.079465   0.9751
8     0.060442    0.067397   0.9799
9     0.059629    0.074273   0.9769
10     0.057895    0.074431   0.9760
11     0.057546    0.067562   0.9770
12     0.054731    0.071834   0.9785
13     0.054826    0.076852   0.9770
14     0.054072    0.080203   0.9754
15     0.053147    0.068960   0.9786
16     0.052740    0.077927   0.9777
17     0.052443    0.076415   0.9763
18     0.052130    0.087026   0.9726
19     0.050108    0.071491   0.9786
20     0.050918    0.072972   0.9764
21     0.050371    0.070479   0.9765
22     0.051077    0.067467   0.9797
23     0.050779    0.077473   0.9756
24     0.049221    0.064232   0.9800
25     0.049264    0.082594   0.9749
26     0.049989    0.064555   0.9798
27     0.049900    0.070157   0.9802
28     0.048899    0.076781   0.9763
29     0.048940    0.079103   0.9754
30     0.049167    0.077319   0.9745
31     0.048890    0.071305   0.9769
32     0.047726    0.077853   0.9757
33     0.049414    0.069201   0.9786
34     0.047476    0.066621   0.9801
35     0.048837    0.080797   0.9752
36     0.048296    0.076962   0.9756
37     0.048657    0.069418   0.9788
38     0.047913    0.072533   0.9760
39     0.048337    0.072326   0.9780
40     0.047767    0.068303   0.9779
41     0.048198    0.070777   0.9781
42     0.048355    0.068218   0.9788
43     0.047775    0.080187   0.9753
44     0.046770    0.069009   0.9800
45     0.048361    0.072269   0.9767
46     0.048354    0.076478   0.9759
47     0.048185    0.068812   0.9792
48     0.047381    0.079003   0.9762
49     0.048696    0.074323   0.9793
50     0.046905    0.071997   0.9775
Remaining weight 16.88 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.332028   0.0812
1     0.236858    0.128406   0.9600
2     0.104071    0.095290   0.9700
3     0.085331    0.088134   0.9733
4     0.074734    0.082402   0.9726
5     0.069007    0.089530   0.9729
6     0.066397    0.080129   0.9760
7     0.062724    0.073830   0.9766
8     0.061137    0.079957   0.9741
9     0.058774    0.073271   0.9765
10     0.058073    0.070491   0.9790
11     0.054876    0.079027   0.9765
12     0.056503    0.075402   0.9761
13     0.054710    0.070538   0.9780
14     0.053479    0.078031   0.9756
15     0.052373    0.075720   0.9761
16     0.051755    0.065928   0.9801
17     0.050975    0.072579   0.9765
18     0.052875    0.079117   0.9747
19     0.049768    0.070049   0.9780
20     0.050449    0.076463   0.9749
21     0.051755    0.076113   0.9763
22     0.048966    0.075108   0.9760
23     0.049196    0.074357   0.9757
24     0.049644    0.071193   0.9775
25     0.049739    0.076027   0.9764
26     0.050742    0.068839   0.9770
27     0.048041    0.063625   0.9784
28     0.047358    0.072570   0.9768
29     0.050352    0.071219   0.9775
30     0.047483    0.072146   0.9773
31     0.049822    0.071430   0.9785
32     0.048610    0.078469   0.9738
33     0.048567    0.072691   0.9771
34     0.049061    0.069583   0.9776
35     0.047242    0.075287   0.9752
36     0.050148    0.074602   0.9754
37     0.048825    0.071226   0.9771
38     0.048612    0.070435   0.9779
39     0.049692    0.084599   0.9704
40     0.047747    0.069582   0.9781
41     0.048267    0.075181   0.9763
42     0.048995    0.078073   0.9749
43     0.046856    0.066991   0.9795
44     0.047895    0.069029   0.9785
45     0.048545    0.064611   0.9796
46     0.046976    0.080010   0.9751
47     0.048675    0.071285   0.9777
48     0.047695    0.072216   0.9780
49     0.047955    0.078645   0.9755
50     0.048419    0.069697   0.9783
Remaining weight 13.52 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.309917   0.0862
1     0.246658    0.126735   0.9608
2     0.107922    0.094951   0.9712
3     0.084958    0.081274   0.9751
4     0.076350    0.090218   0.9711
5     0.070141    0.085830   0.9731
6     0.066561    0.079115   0.9739
7     0.063892    0.072354   0.9773
8     0.061330    0.077555   0.9766
9     0.058633    0.073119   0.9767
10     0.056982    0.075639   0.9765
11     0.057235    0.075096   0.9767
12     0.055900    0.081638   0.9736
13     0.054775    0.077790   0.9770
14     0.054659    0.067717   0.9787
15     0.052977    0.082350   0.9749
16     0.053014    0.077538   0.9758
17     0.053940    0.086712   0.9728
18     0.052081    0.081323   0.9749
19     0.053083    0.078057   0.9766
20     0.051293    0.072762   0.9773
21     0.051307    0.073693   0.9771
22     0.052308    0.075695   0.9762
23     0.051744    0.079238   0.9746
24     0.052014    0.067660   0.9777
25     0.049503    0.077369   0.9759
26     0.051643    0.069055   0.9789
27     0.049457    0.073384   0.9764
28     0.050412    0.076209   0.9761
29     0.050555    0.072734   0.9772
30     0.049733    0.071766   0.9778
31     0.051048    0.078943   0.9748
32     0.050515    0.075072   0.9743
33     0.047808    0.069263   0.9780
34     0.049673    0.070952   0.9788
35     0.050051    0.074379   0.9763
36     0.049467    0.064257   0.9794
37     0.048340    0.070785   0.9778
38     0.049859    0.070139   0.9781
39     0.049031    0.067791   0.9779
40     0.049282    0.070861   0.9786
41     0.049440    0.071901   0.9782
42     0.049007    0.073781   0.9762
43     0.048665    0.075880   0.9762
44     0.049648    0.078783   0.9753
45     0.048607    0.071593   0.9775
46     0.049109    0.069520   0.9777
47     0.049278    0.069501   0.9787
48     0.048761    0.075516   0.9771
49     0.048259    0.075318   0.9765
50     0.049231    0.074074   0.9771
Remaining weight 10.83 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.293658   0.1018
1     0.251714    0.130970   0.9616
2     0.107037    0.110720   0.9637
3     0.086400    0.088726   0.9723
4     0.076276    0.088467   0.9713
5     0.070171    0.089715   0.9722
6     0.066102    0.075914   0.9762
7     0.061991    0.086955   0.9733
8     0.061024    0.077393   0.9758
9     0.059240    0.072645   0.9766
10     0.056898    0.069442   0.9785
11     0.056990    0.075158   0.9773
12     0.055176    0.075223   0.9768
13     0.055367    0.077968   0.9760
14     0.053147    0.067056   0.9781
15     0.052552    0.072961   0.9772
16     0.054083    0.071767   0.9771
17     0.052199    0.075689   0.9747
18     0.051904    0.070504   0.9769
19     0.051491    0.069505   0.9778
20     0.050916    0.079605   0.9749
21     0.050344    0.070607   0.9774
22     0.051560    0.072135   0.9781
23     0.050078    0.075595   0.9754
24     0.051344    0.073442   0.9782
25     0.050657    0.068015   0.9769
26     0.048892    0.090701   0.9707
27     0.049198    0.071916   0.9789
28     0.049344    0.071131   0.9787
29     0.047974    0.072648   0.9773
30     0.049215    0.075861   0.9756
31     0.048440    0.073314   0.9786
32     0.049541    0.061828   0.9801
33     0.048488    0.076794   0.9763
34     0.048654    0.079180   0.9747
35     0.049227    0.069528   0.9788
36     0.049227    0.066257   0.9801
37     0.047964    0.070182   0.9775
38     0.048274    0.066921   0.9791
39     0.048551    0.078537   0.9767
40     0.048161    0.066734   0.9785
41     0.048959    0.080381   0.9758
42     0.049764    0.069432   0.9790
43     0.048407    0.080246   0.9730
44     0.048429    0.071019   0.9775
45     0.047197    0.067703   0.9795
46     0.046976    0.076047   0.9774
47     0.047688    0.076229   0.9758
48     0.048407    0.076638   0.9752
49     0.047554    0.067200   0.9798
50     0.048371    0.071150   0.9780
Remaining weight 8.68 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.276071   0.1122
1     0.262240    0.136809   0.9594
2     0.111483    0.102653   0.9679
3     0.086516    0.097022   0.9708
4     0.076248    0.088373   0.9741
5     0.069584    0.090161   0.9703
6     0.066854    0.082632   0.9728
7     0.062237    0.084281   0.9730
8     0.059396    0.072547   0.9779
9     0.057020    0.075520   0.9766
10     0.057106    0.071078   0.9775
11     0.055131    0.085713   0.9718
12     0.055188    0.065797   0.9781
13     0.053200    0.080687   0.9746
14     0.053102    0.079095   0.9765
15     0.052410    0.065627   0.9780
16     0.052289    0.075465   0.9747
17     0.050746    0.074991   0.9751
18     0.050253    0.067994   0.9777
19     0.053006    0.068672   0.9776
20     0.050893    0.081385   0.9734
21     0.049310    0.082675   0.9749
22     0.050519    0.073341   0.9778
23     0.050088    0.070679   0.9760
24     0.050470    0.073471   0.9774
25     0.049096    0.065698   0.9799
26     0.048812    0.067229   0.9779
27     0.047842    0.072969   0.9743
28     0.048462    0.067788   0.9790
29     0.049825    0.069594   0.9767
30     0.049144    0.078859   0.9745
31     0.048808    0.076608   0.9752
32     0.049747    0.080361   0.9740
33     0.047427    0.069753   0.9784
34     0.048234    0.074319   0.9767
35     0.048297    0.075996   0.9757
36     0.048548    0.084194   0.9737
37     0.048167    0.067907   0.9797
38     0.047871    0.067439   0.9794
39     0.048239    0.065537   0.9789
40     0.048908    0.070829   0.9784
41     0.046603    0.070133   0.9770
42     0.047120    0.067198   0.9792
43     0.047478    0.071064   0.9754
44     0.048259    0.069920   0.9787
45     0.047147    0.065105   0.9796
46     0.046989    0.070498   0.9768
47     0.045588    0.075471   0.9758
48     0.046953    0.072705   0.9767
49     0.047087    0.074041   0.9760
50     0.046115    0.065512   0.9795
Remaining weight 6.95 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.283891   0.1012
1     0.275188    0.155065   0.9526
2     0.115495    0.097061   0.9701
3     0.090023    0.098746   0.9684
4     0.078099    0.081431   0.9730
5     0.069569    0.087272   0.9748
6     0.066399    0.075612   0.9766
7     0.063252    0.081815   0.9743
8     0.059733    0.087394   0.9708
9     0.058626    0.083641   0.9751
10     0.057099    0.072950   0.9777
11     0.054724    0.074065   0.9770
12     0.055794    0.078096   0.9765
13     0.053992    0.071719   0.9771
14     0.053679    0.076384   0.9778
15     0.052530    0.086553   0.9738
16     0.050674    0.084157   0.9751
17     0.050670    0.075788   0.9756
18     0.051156    0.080624   0.9748
19     0.051140    0.071921   0.9770
20     0.049501    0.065640   0.9811
21     0.049397    0.069570   0.9788
22     0.049882    0.070875   0.9801
23     0.049416    0.071724   0.9782
24     0.047414    0.078505   0.9750
25     0.049005    0.068885   0.9784
26     0.047442    0.067316   0.9798
27     0.047835    0.074608   0.9764
28     0.049865    0.068881   0.9804
29     0.047663    0.072852   0.9783
30     0.048729    0.068688   0.9799
31     0.048851    0.075246   0.9784
32     0.046809    0.070927   0.9793
33     0.047709    0.086319   0.9732
34     0.049384    0.076274   0.9758
35     0.046722    0.081883   0.9748
36     0.048113    0.077000   0.9777
37     0.046563    0.079470   0.9746
38     0.047890    0.074380   0.9786
39     0.046672    0.076810   0.9766
40     0.048207    0.073407   0.9792
41     0.046193    0.074278   0.9779
42     0.047111    0.072026   0.9783
43     0.047924    0.078539   0.9760
44     0.046621    0.072381   0.9788
45     0.046191    0.073759   0.9777
46     0.047270    0.073193   0.9774
47     0.046689    0.079000   0.9752
48     0.047352    0.070152   0.9776
49     0.045823    0.070620   0.9792
50     0.046984    0.078377   0.9773
Remaining weight 5.57 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.272881   0.1222
1     0.286665    0.139061   0.9585
2     0.117572    0.105681   0.9695
3     0.089851    0.091614   0.9728
4     0.076589    0.086299   0.9735
5     0.069152    0.082828   0.9743
6     0.063320    0.084549   0.9748
7     0.061152    0.076544   0.9773
8     0.058486    0.075569   0.9775
9     0.056686    0.076348   0.9774
10     0.052817    0.078057   0.9755
11     0.054664    0.073483   0.9775
12     0.051660    0.079224   0.9773
13     0.050727    0.075716   0.9765
14     0.051235    0.081703   0.9743
15     0.049948    0.078729   0.9756
16     0.050138    0.070756   0.9794
17     0.048242    0.071983   0.9770
18     0.049276    0.073171   0.9760
19     0.048682    0.074318   0.9757
20     0.047982    0.072200   0.9778
21     0.047493    0.073684   0.9771
22     0.047075    0.072914   0.9773
23     0.047320    0.072144   0.9798
24     0.048005    0.070421   0.9787
25     0.046800    0.079583   0.9765
26     0.047360    0.072356   0.9765
27     0.046937    0.075494   0.9756
28     0.045767    0.067434   0.9780
29     0.048057    0.068612   0.9788
30     0.045338    0.077197   0.9779
31     0.046270    0.073497   0.9776
32     0.046105    0.071921   0.9779
33     0.045429    0.072723   0.9786
34     0.046906    0.073458   0.9774
35     0.045345    0.068565   0.9785
36     0.046295    0.072420   0.9774
37     0.045558    0.070823   0.9794
38     0.045993    0.069851   0.9798
39     0.046298    0.075625   0.9777
40     0.045144    0.075933   0.9766
41     0.044636    0.076552   0.9762
42     0.046224    0.075756   0.9765
43     0.044444    0.075784   0.9775
44     0.045597    0.078031   0.9770
45     0.045519    0.086439   0.9734
46     0.045750    0.074930   0.9773
47     0.045068    0.070023   0.9783
48     0.044133    0.077301   0.9764
49     0.045760    0.068762   0.9789
50     0.044345    0.074851   0.9775
Remaining weight 4.47 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.276520   0.1110
1     0.290693    0.136893   0.9591
2     0.113558    0.100267   0.9710
3     0.087790    0.089767   0.9727
4     0.074904    0.079362   0.9766
5     0.068254    0.077582   0.9771
6     0.061904    0.082310   0.9741
7     0.059849    0.073641   0.9763
8     0.055452    0.078164   0.9767
9     0.053938    0.077442   0.9762
10     0.052977    0.067160   0.9790
11     0.050904    0.072853   0.9768
12     0.049339    0.073409   0.9785
13     0.048757    0.080571   0.9751
14     0.048331    0.073183   0.9789
15     0.045769    0.066193   0.9792
16     0.046626    0.070540   0.9785
17     0.045352    0.072868   0.9773
18     0.044432    0.068373   0.9803
19     0.044503    0.070214   0.9778
20     0.044748    0.070926   0.9774
21     0.043689    0.074921   0.9786
22     0.044237    0.067231   0.9796
23     0.043461    0.072457   0.9777
24     0.044253    0.069085   0.9794
25     0.042723    0.070373   0.9790
26     0.043378    0.066178   0.9783
27     0.042752    0.069408   0.9779
28     0.041692    0.074508   0.9773
29     0.043818    0.075364   0.9776
30     0.042570    0.069028   0.9800
31     0.042454    0.080728   0.9734
32     0.043556    0.073055   0.9780
33     0.042713    0.077889   0.9770
34     0.041456    0.066837   0.9790
35     0.042572    0.076538   0.9761
36     0.042501    0.071922   0.9783
37     0.042622    0.073842   0.9778
38     0.042891    0.072259   0.9779
39     0.042023    0.069119   0.9783
40     0.041308    0.078855   0.9756
41     0.042243    0.070584   0.9785
42     0.042888    0.079350   0.9755
43     0.041383    0.076163   0.9761
44     0.042242    0.075092   0.9766
45     0.042154    0.070132   0.9783
46     0.041928    0.068140   0.9801
47     0.041766    0.073495   0.9777
48     0.041847    0.074004   0.9773
49     0.041714    0.070073   0.9784
50     0.041824    0.065336   0.9793
Remaining weight 3.58 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.271504   0.1283
1     0.304535    0.138084   0.9579
2     0.112967    0.102720   0.9693
3     0.082676    0.085163   0.9746
4     0.069663    0.075236   0.9782
5     0.061378    0.076304   0.9758
6     0.056792    0.070584   0.9784
7     0.052140    0.069083   0.9778
8     0.050283    0.068830   0.9786
9     0.048672    0.073013   0.9767
10     0.046790    0.071346   0.9783
11     0.045968    0.073255   0.9765
12     0.045813    0.067365   0.9789
13     0.043776    0.071708   0.9798
14     0.043838    0.070464   0.9795
15     0.042757    0.070440   0.9782
16     0.042958    0.071101   0.9786
17     0.041916    0.075144   0.9787
18     0.042149    0.071862   0.9769
19     0.040879    0.071580   0.9773
20     0.040907    0.072066   0.9766
21     0.041214    0.074368   0.9774
22     0.040535    0.064794   0.9804
23     0.040302    0.067922   0.9791
24     0.040630    0.068535   0.9798
25     0.040009    0.066425   0.9791
26     0.039762    0.069224   0.9789
27     0.039683    0.070387   0.9788
28     0.040319    0.073020   0.9774
29     0.039810    0.081342   0.9761
30     0.039406    0.067190   0.9798
31     0.040359    0.069668   0.9780
32     0.039154    0.071387   0.9780
33     0.040647    0.068286   0.9793
34     0.038886    0.071416   0.9783
35     0.039914    0.068806   0.9789
36     0.039715    0.069642   0.9790
37     0.040266    0.073414   0.9773
38     0.039392    0.076298   0.9763
39     0.039105    0.069661   0.9784
40     0.038961    0.072239   0.9786
41     0.038920    0.068855   0.9797
42     0.039532    0.074405   0.9777
43     0.038962    0.073766   0.9760
44     0.039538    0.070764   0.9786
45     0.039658    0.068942   0.9789
46     0.039463    0.070189   0.9774
47     0.039764    0.065576   0.9807
48     0.038937    0.075158   0.9762
49     0.040010    0.067034   0.9787
50     0.039862    0.067108   0.9805
Remaining weight 2.87 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.265712   0.1492
1     0.316675    0.141487   0.9580
2     0.116051    0.098182   0.9702
3     0.087130    0.086595   0.9735
4     0.072937    0.082513   0.9769
5     0.063763    0.076787   0.9766
6     0.057540    0.076538   0.9771
7     0.054709    0.073268   0.9776
8     0.051014    0.068124   0.9802
9     0.048577    0.070977   0.9783
10     0.046511    0.071786   0.9789
11     0.043932    0.075381   0.9777
12     0.043535    0.068741   0.9789
13     0.042034    0.071347   0.9791
14     0.040666    0.072024   0.9775
15     0.040779    0.067759   0.9800
16     0.038973    0.067516   0.9784
17     0.039741    0.069503   0.9786
18     0.039493    0.068167   0.9788
19     0.039022    0.073231   0.9769
20     0.038415    0.067024   0.9797
21     0.039555    0.071693   0.9792
22     0.038698    0.074896   0.9774
23     0.038365    0.070216   0.9790
24     0.038495    0.067382   0.9794
25     0.038671    0.070553   0.9780
26     0.037989    0.066739   0.9801
27     0.038955    0.069824   0.9780
28     0.038000    0.073438   0.9776
29     0.038061    0.075331   0.9763
30     0.037046    0.068135   0.9794
31     0.038477    0.070308   0.9775
32     0.038155    0.073387   0.9771
33     0.037785    0.076347   0.9772
34     0.037364    0.068747   0.9779
35     0.037303    0.069658   0.9794
36     0.038025    0.072778   0.9761
37     0.037307    0.071604   0.9778
38     0.037579    0.069423   0.9789
39     0.037457    0.070672   0.9776
40     0.037561    0.074293   0.9781
41     0.037955    0.070162   0.9783
42     0.037366    0.062964   0.9804
43     0.038183    0.070122   0.9788
44     0.037647    0.070059   0.9789
45     0.037086    0.072127   0.9770
46     0.038013    0.069332   0.9796
47     0.037442    0.070570   0.9775
48     0.037744    0.074156   0.9779
49     0.037026    0.072152   0.9776
50     0.037617    0.069524   0.9784
Remaining weight 2.31 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.273736   0.1600
1     0.326982    0.139680   0.9585
2     0.114466    0.101665   0.9712
3     0.084579    0.086508   0.9742
4     0.071027    0.082246   0.9761
5     0.062951    0.079643   0.9756
6     0.057263    0.076481   0.9769
7     0.052917    0.076010   0.9763
8     0.050079    0.072982   0.9784
9     0.048099    0.071285   0.9777
10     0.045775    0.069216   0.9791
11     0.044231    0.070658   0.9786
12     0.043355    0.070739   0.9796
13     0.041907    0.072005   0.9796
14     0.042043    0.070030   0.9789
15     0.041235    0.072100   0.9788
16     0.040472    0.067511   0.9792
17     0.040240    0.068280   0.9785
18     0.040307    0.068719   0.9787
19     0.040323    0.071796   0.9779
20     0.039740    0.071464   0.9781
21     0.039562    0.067350   0.9792
22     0.038809    0.071006   0.9782
23     0.038881    0.067589   0.9789
24     0.038924    0.072638   0.9782
25     0.038754    0.071025   0.9783
26     0.039173    0.070387   0.9772
27     0.038231    0.073661   0.9783
28     0.039116    0.069336   0.9787
29     0.038924    0.072725   0.9777
30     0.038772    0.069851   0.9785
31     0.038824    0.069221   0.9779
32     0.038216    0.075262   0.9770
33     0.038210    0.071917   0.9771
34     0.038554    0.073134   0.9779
35     0.038200    0.069748   0.9790
36     0.038139    0.068577   0.9770
37     0.038469    0.068798   0.9785
38     0.038089    0.071841   0.9786
39     0.038347    0.071519   0.9778
40     0.038634    0.067969   0.9775
41     0.038210    0.069264   0.9775
42     0.037820    0.070148   0.9783
43     0.037380    0.069284   0.9781
44     0.037115    0.068752   0.9783
45     0.037539    0.069674   0.9777
46     0.036822    0.072017   0.9781
47     0.037478    0.068568   0.9787
48     0.037020    0.068606   0.9782
49     0.037004    0.067463   0.9798
50     0.036610    0.074574   0.9774
Remaining weight 1.85 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.289365   0.1027
1     0.351959    0.144686   0.9573
2     0.122867    0.109402   0.9664
3     0.093121    0.094976   0.9727
4     0.077491    0.086996   0.9747
5     0.066345    0.078803   0.9771
6     0.059473    0.079004   0.9758
7     0.054490    0.075281   0.9782
8     0.051375    0.074484   0.9778
9     0.049671    0.077938   0.9769
10     0.047584    0.075212   0.9767
11     0.046659    0.070494   0.9782
12     0.045686    0.069532   0.9785
13     0.044703    0.072127   0.9790
14     0.044448    0.072300   0.9784
15     0.043268    0.069154   0.9788
16     0.043889    0.070661   0.9791
17     0.042759    0.074204   0.9785
18     0.042861    0.073055   0.9791
19     0.042526    0.073487   0.9789
20     0.042680    0.072586   0.9784
21     0.042348    0.070875   0.9791
22     0.042160    0.075478   0.9775
23     0.042192    0.073194   0.9774
24     0.041499    0.068488   0.9792
25     0.040910    0.068441   0.9802
26     0.040903    0.069764   0.9794
27     0.041238    0.071421   0.9783
28     0.040946    0.077910   0.9766
29     0.040722    0.072898   0.9766
30     0.040194    0.070178   0.9788
31     0.039007    0.070052   0.9790
32     0.039616    0.067778   0.9792
33     0.039802    0.070855   0.9794
34     0.039528    0.071588   0.9781
35     0.039897    0.071059   0.9775
36     0.039380    0.068420   0.9786
37     0.039362    0.070335   0.9788
38     0.039624    0.070079   0.9770
39     0.039187    0.066887   0.9799
40     0.039465    0.075259   0.9765
41     0.039943    0.070312   0.9792
42     0.039293    0.068247   0.9790
43     0.039867    0.073454   0.9774
44     0.039074    0.067589   0.9796
45     0.039107    0.070139   0.9776
46     0.040081    0.068253   0.9784
47     0.039635    0.073978   0.9782
48     0.039159    0.069332   0.9788
49     0.039445    0.069875   0.9785
50     0.039815    0.067429   0.9794
Remaining weight 1.49 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.294187   0.0994
1     0.381725    0.145948   0.9571
2     0.125327    0.107980   0.9674
3     0.091581    0.092040   0.9729
4     0.074080    0.084774   0.9744
5     0.063139    0.075958   0.9771
6     0.057206    0.076402   0.9779
7     0.053074    0.071887   0.9794
8     0.050230    0.069248   0.9782
9     0.048415    0.069642   0.9788
10     0.047272    0.068035   0.9792
11     0.046950    0.073034   0.9781
12     0.046412    0.071705   0.9782
13     0.045314    0.069926   0.9785
14     0.045035    0.070668   0.9789
15     0.044900    0.071421   0.9774
16     0.044692    0.071145   0.9759
17     0.044517    0.070759   0.9780
18     0.044497    0.069426   0.9787
19     0.044213    0.073149   0.9768
20     0.044241    0.072591   0.9776
21     0.044350    0.071112   0.9774
22     0.044774    0.074493   0.9768
23     0.043336    0.070760   0.9773
24     0.044176    0.069835   0.9776
25     0.043968    0.069394   0.9764
26     0.044169    0.072996   0.9776
27     0.044509    0.070274   0.9780
28     0.043826    0.072815   0.9773
29     0.043808    0.071640   0.9786
30     0.043775    0.070220   0.9780
31     0.044274    0.070984   0.9772
32     0.043839    0.072337   0.9765
33     0.043652    0.069993   0.9779
34     0.044119    0.073580   0.9777
35     0.044256    0.068743   0.9774
36     0.044056    0.073668   0.9792
37     0.043602    0.069752   0.9778
38     0.044100    0.071151   0.9783
39     0.043041    0.071835   0.9771
40     0.042739    0.068909   0.9798
41     0.043136    0.071322   0.9775
42     0.043632    0.068611   0.9783
43     0.043092    0.070506   0.9776
44     0.043174    0.069415   0.9774
45     0.043250    0.076224   0.9774
46     0.043661    0.071241   0.9778
47     0.043059    0.074023   0.9775
48     0.042802    0.068935   0.9788
49     0.042994    0.074404   0.9750
50     0.042687    0.073567   0.9761
