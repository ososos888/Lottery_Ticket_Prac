model_type: Lenet_300_100
lr: 0.0012
epochs: 50
batch_size: 60
weight_decay: 0.0012
prune_per_c: 1
prune_per_f: 0.2
prune_per_o: 0.1
test_iter: 5
prune_iter: 21
trainset: Dataset MNIST
    Number of datapoints: 60000
    Root location: ../MNIST_data/
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=(0.1307,), std=(0.3081,))
           )
valset: empty
testset: Dataset MNIST
    Number of datapoints: 10000
    Root location: ../MNIST_data/
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=(0.1307,), std=(0.3081,))
           )
train_loader: <torch.utils.data.dataloader.DataLoader object at 0x7fb00d4afd50>
val_loader: empty
test_loader: <torch.utils.data.dataloader.DataLoader object at 0x7fb00c921710> 


Model structure
 Lenet_300_100(
  (fc1): Linear(in_features=784, out_features=300, bias=True)
  (fc2): Linear(in_features=300, out_features=100, bias=True)
  (fcout): Linear(in_features=100, out_features=10, bias=True)
)
===================================================================== 

Test_Iter (1/5)
------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :        266200 (266200 | 0)          100.00
fc1.weight   :        235200 (235200 | 0)          100.00
fc2.weight   :         30000 (30000 | 0)           100.00
fcout.weight :          1000 (1000 | 0)            100.00
------------------------------------------------------------

Learning start! [Prune_iter : (1/21), Remaining weight : 100.0 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.84849) (accu: 0.0844)
[epoch : 1] (l_loss: 0.20293) (t_loss: 0.12296) (accu: 0.9612)
[epoch : 2] (l_loss: 0.10836) (t_loss: 0.11074) (accu: 0.9654)
[epoch : 3] (l_loss: 0.09079) (t_loss: 0.11315) (accu: 0.9635)
[epoch : 4] (l_loss: 0.08560) (t_loss: 0.09715) (accu: 0.9677)
[epoch : 5] (l_loss: 0.08116) (t_loss: 0.09338) (accu: 0.9705)
[epoch : 6] (l_loss: 0.07648) (t_loss: 0.08369) (accu: 0.9731)
[epoch : 7] (l_loss: 0.07330) (t_loss: 0.08692) (accu: 0.9733)
[epoch : 8] (l_loss: 0.07032) (t_loss: 0.08587) (accu: 0.9713)
[epoch : 9] (l_loss: 0.06909) (t_loss: 0.08026) (accu: 0.9741)
[epoch : 10] (l_loss: 0.06663) (t_loss: 0.07589) (accu: 0.9772)
[epoch : 11] (l_loss: 0.06375) (t_loss: 0.08114) (accu: 0.9754)
[epoch : 12] (l_loss: 0.06338) (t_loss: 0.08569) (accu: 0.9732)
[epoch : 13] (l_loss: 0.06364) (t_loss: 0.09617) (accu: 0.9698)
[epoch : 14] (l_loss: 0.06208) (t_loss: 0.08080) (accu: 0.9754)
[epoch : 15] (l_loss: 0.06091) (t_loss: 0.07722) (accu: 0.9755)
[epoch : 16] (l_loss: 0.06057) (t_loss: 0.08183) (accu: 0.9764)
[epoch : 17] (l_loss: 0.06100) (t_loss: 0.08749) (accu: 0.9736)
[epoch : 18] (l_loss: 0.06070) (t_loss: 0.07754) (accu: 0.9762)
[epoch : 19] (l_loss: 0.05950) (t_loss: 0.07388) (accu: 0.9798)
[epoch : 20] (l_loss: 0.06097) (t_loss: 0.07595) (accu: 0.9768)
[epoch : 21] (l_loss: 0.05851) (t_loss: 0.08279) (accu: 0.9755)
[epoch : 22] (l_loss: 0.05682) (t_loss: 0.07744) (accu: 0.9751)
[epoch : 23] (l_loss: 0.05868) (t_loss: 0.06852) (accu: 0.9785)
[epoch : 24] (l_loss: 0.05926) (t_loss: 0.07307) (accu: 0.9757)
[epoch : 25] (l_loss: 0.06004) (t_loss: 0.07856) (accu: 0.9760)
[epoch : 26] (l_loss: 0.05814) (t_loss: 0.07530) (accu: 0.9757)
[epoch : 27] (l_loss: 0.05655) (t_loss: 0.07645) (accu: 0.9756)
[epoch : 28] (l_loss: 0.05513) (t_loss: 0.07709) (accu: 0.9772)
[epoch : 29] (l_loss: 0.05666) (t_loss: 0.06927) (accu: 0.9775)
[epoch : 30] (l_loss: 0.05556) (t_loss: 0.08740) (accu: 0.9737)
[epoch : 31] (l_loss: 0.05711) (t_loss: 0.09872) (accu: 0.9682)
[epoch : 32] (l_loss: 0.05552) (t_loss: 0.08133) (accu: 0.9748)
[epoch : 33] (l_loss: 0.05735) (t_loss: 0.07824) (accu: 0.9763)
[epoch : 34] (l_loss: 0.05639) (t_loss: 0.07871) (accu: 0.9737)
[epoch : 35] (l_loss: 0.05547) (t_loss: 0.07294) (accu: 0.9757)
[epoch : 36] (l_loss: 0.05471) (t_loss: 0.08836) (accu: 0.9711)
[epoch : 37] (l_loss: 0.05610) (t_loss: 0.07966) (accu: 0.9750)
[epoch : 38] (l_loss: 0.05506) (t_loss: 0.07036) (accu: 0.9783)
[epoch : 39] (l_loss: 0.05484) (t_loss: 0.07867) (accu: 0.9755)
[epoch : 40] (l_loss: 0.05413) (t_loss: 0.08009) (accu: 0.9740)
[epoch : 41] (l_loss: 0.05476) (t_loss: 0.07946) (accu: 0.9741)
[epoch : 42] (l_loss: 0.05425) (t_loss: 0.07199) (accu: 0.9786)
[epoch : 43] (l_loss: 0.05585) (t_loss: 0.08155) (accu: 0.9752)
[epoch : 44] (l_loss: 0.05315) (t_loss: 0.07856) (accu: 0.9762)
[epoch : 45] (l_loss: 0.05633) (t_loss: 0.08450) (accu: 0.9745)
[epoch : 46] (l_loss: 0.05450) (t_loss: 0.07369) (accu: 0.9766)
[epoch : 47] (l_loss: 0.05238) (t_loss: 0.07038) (accu: 0.9781)
[epoch : 48] (l_loss: 0.05532) (t_loss: 0.07661) (accu: 0.9746)
[epoch : 49] (l_loss: 0.05354) (t_loss: 0.06627) (accu: 0.9802)
[epoch : 50] (l_loss: 0.05545) (t_loss: 0.07603) (accu: 0.9773)
Finish! (Best accu: 0.9802) (Time taken(sec) : 604.09) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (213060 | 53140)         80.04
fc1.weight   :      235200 (188160 | 47040)         80.00
fc2.weight   :        30000 (24000 | 6000)          80.00
fcout.weight :          1000 (900 | 100)            90.00
------------------------------------------------------------

Learning start! [Prune_iter : (2/21), Remaining weight : 80.04 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.56021) (accu: 0.1155)
[epoch : 1] (l_loss: 0.19986) (t_loss: 0.10143) (accu: 0.9699)
[epoch : 2] (l_loss: 0.10291) (t_loss: 0.08995) (accu: 0.9715)
[epoch : 3] (l_loss: 0.08826) (t_loss: 0.08745) (accu: 0.9717)
[epoch : 4] (l_loss: 0.08071) (t_loss: 0.08164) (accu: 0.9740)
[epoch : 5] (l_loss: 0.07577) (t_loss: 0.09313) (accu: 0.9699)
[epoch : 6] (l_loss: 0.07284) (t_loss: 0.07831) (accu: 0.9748)
[epoch : 7] (l_loss: 0.06870) (t_loss: 0.10287) (accu: 0.9678)
[epoch : 8] (l_loss: 0.06750) (t_loss: 0.07870) (accu: 0.9746)
[epoch : 9] (l_loss: 0.06675) (t_loss: 0.07914) (accu: 0.9763)
[epoch : 10] (l_loss: 0.06299) (t_loss: 0.07816) (accu: 0.9760)
[epoch : 11] (l_loss: 0.06312) (t_loss: 0.09430) (accu: 0.9712)
[epoch : 12] (l_loss: 0.06045) (t_loss: 0.07899) (accu: 0.9752)
[epoch : 13] (l_loss: 0.06059) (t_loss: 0.07510) (accu: 0.9772)
[epoch : 14] (l_loss: 0.05858) (t_loss: 0.07134) (accu: 0.9778)
[epoch : 15] (l_loss: 0.05673) (t_loss: 0.07896) (accu: 0.9755)
[epoch : 16] (l_loss: 0.05740) (t_loss: 0.07052) (accu: 0.9799)
[epoch : 17] (l_loss: 0.05846) (t_loss: 0.08151) (accu: 0.9738)
[epoch : 18] (l_loss: 0.05769) (t_loss: 0.07310) (accu: 0.9772)
[epoch : 19] (l_loss: 0.05397) (t_loss: 0.06802) (accu: 0.9797)
[epoch : 20] (l_loss: 0.05609) (t_loss: 0.08388) (accu: 0.9735)
[epoch : 21] (l_loss: 0.05623) (t_loss: 0.06964) (accu: 0.9793)
[epoch : 22] (l_loss: 0.05443) (t_loss: 0.07640) (accu: 0.9741)
[epoch : 23] (l_loss: 0.05562) (t_loss: 0.07635) (accu: 0.9756)
[epoch : 24] (l_loss: 0.05497) (t_loss: 0.07460) (accu: 0.9776)
[epoch : 25] (l_loss: 0.05465) (t_loss: 0.07562) (accu: 0.9767)
[epoch : 26] (l_loss: 0.05421) (t_loss: 0.07328) (accu: 0.9761)
[epoch : 27] (l_loss: 0.05459) (t_loss: 0.07661) (accu: 0.9768)
[epoch : 28] (l_loss: 0.05399) (t_loss: 0.06445) (accu: 0.9802)
[epoch : 29] (l_loss: 0.05386) (t_loss: 0.07829) (accu: 0.9764)
[epoch : 30] (l_loss: 0.05324) (t_loss: 0.07279) (accu: 0.9775)
[epoch : 31] (l_loss: 0.05505) (t_loss: 0.07602) (accu: 0.9772)
[epoch : 32] (l_loss: 0.05363) (t_loss: 0.08844) (accu: 0.9723)
[epoch : 33] (l_loss: 0.05416) (t_loss: 0.08073) (accu: 0.9755)
[epoch : 34] (l_loss: 0.05288) (t_loss: 0.07856) (accu: 0.9742)
[epoch : 35] (l_loss: 0.05449) (t_loss: 0.08019) (accu: 0.9770)
[epoch : 36] (l_loss: 0.05229) (t_loss: 0.06875) (accu: 0.9793)
[epoch : 37] (l_loss: 0.05277) (t_loss: 0.07972) (accu: 0.9749)
[epoch : 38] (l_loss: 0.05385) (t_loss: 0.07027) (accu: 0.9786)
[epoch : 39] (l_loss: 0.05215) (t_loss: 0.07951) (accu: 0.9758)
[epoch : 40] (l_loss: 0.05227) (t_loss: 0.06874) (accu: 0.9786)
[epoch : 41] (l_loss: 0.05203) (t_loss: 0.08173) (accu: 0.9747)
[epoch : 42] (l_loss: 0.05336) (t_loss: 0.07468) (accu: 0.9751)
[epoch : 43] (l_loss: 0.05222) (t_loss: 0.07463) (accu: 0.9746)
[epoch : 44] (l_loss: 0.05121) (t_loss: 0.08362) (accu: 0.9742)
[epoch : 45] (l_loss: 0.05196) (t_loss: 0.07639) (accu: 0.9761)
[epoch : 46] (l_loss: 0.05230) (t_loss: 0.07252) (accu: 0.9777)
[epoch : 47] (l_loss: 0.05257) (t_loss: 0.08527) (accu: 0.9731)
[epoch : 48] (l_loss: 0.04978) (t_loss: 0.08065) (accu: 0.9751)
[epoch : 49] (l_loss: 0.05120) (t_loss: 0.07369) (accu: 0.9773)
[epoch : 50] (l_loss: 0.05253) (t_loss: 0.07044) (accu: 0.9791)
Finish! (Best accu: 0.9802) (Time taken(sec) : 617.89) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (170538 | 95662)         64.06
fc1.weight   :      235200 (150528 | 84672)         64.00
fc2.weight   :       30000 (19200 | 10800)          64.00
fcout.weight :          1000 (810 | 190)            81.00
------------------------------------------------------------

Learning start! [Prune_iter : (3/21), Remaining weight : 64.06 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.43542) (accu: 0.0694)
[epoch : 1] (l_loss: 0.20173) (t_loss: 0.10399) (accu: 0.9668)
[epoch : 2] (l_loss: 0.10264) (t_loss: 0.10843) (accu: 0.9660)
[epoch : 3] (l_loss: 0.08782) (t_loss: 0.09544) (accu: 0.9693)
[epoch : 4] (l_loss: 0.07843) (t_loss: 0.09120) (accu: 0.9710)
[epoch : 5] (l_loss: 0.07435) (t_loss: 0.08172) (accu: 0.9762)
[epoch : 6] (l_loss: 0.06745) (t_loss: 0.09010) (accu: 0.9719)
[epoch : 7] (l_loss: 0.06738) (t_loss: 0.08259) (accu: 0.9756)
[epoch : 8] (l_loss: 0.06411) (t_loss: 0.08760) (accu: 0.9726)
[epoch : 9] (l_loss: 0.06275) (t_loss: 0.09090) (accu: 0.9706)
[epoch : 10] (l_loss: 0.06277) (t_loss: 0.08238) (accu: 0.9731)
[epoch : 11] (l_loss: 0.05947) (t_loss: 0.07459) (accu: 0.9765)
[epoch : 12] (l_loss: 0.05898) (t_loss: 0.07478) (accu: 0.9760)
[epoch : 13] (l_loss: 0.05754) (t_loss: 0.07588) (accu: 0.9761)
[epoch : 14] (l_loss: 0.05646) (t_loss: 0.08546) (accu: 0.9731)
[epoch : 15] (l_loss: 0.05556) (t_loss: 0.07398) (accu: 0.9765)
[epoch : 16] (l_loss: 0.05436) (t_loss: 0.07069) (accu: 0.9783)
[epoch : 17] (l_loss: 0.05567) (t_loss: 0.07916) (accu: 0.9728)
[epoch : 18] (l_loss: 0.05381) (t_loss: 0.06733) (accu: 0.9801)
[epoch : 19] (l_loss: 0.05387) (t_loss: 0.10501) (accu: 0.9669)
[epoch : 20] (l_loss: 0.05474) (t_loss: 0.07325) (accu: 0.9772)
[epoch : 21] (l_loss: 0.05314) (t_loss: 0.07294) (accu: 0.9766)
[epoch : 22] (l_loss: 0.05160) (t_loss: 0.07507) (accu: 0.9762)
[epoch : 23] (l_loss: 0.05183) (t_loss: 0.06877) (accu: 0.9790)
[epoch : 24] (l_loss: 0.05111) (t_loss: 0.07885) (accu: 0.9755)
[epoch : 25] (l_loss: 0.05267) (t_loss: 0.07358) (accu: 0.9774)
[epoch : 26] (l_loss: 0.05270) (t_loss: 0.06835) (accu: 0.9804)
[epoch : 27] (l_loss: 0.05232) (t_loss: 0.06935) (accu: 0.9787)
[epoch : 28] (l_loss: 0.05235) (t_loss: 0.06920) (accu: 0.9795)
[epoch : 29] (l_loss: 0.05159) (t_loss: 0.07254) (accu: 0.9783)
[epoch : 30] (l_loss: 0.05074) (t_loss: 0.07277) (accu: 0.9773)
[epoch : 31] (l_loss: 0.05039) (t_loss: 0.06684) (accu: 0.9798)
[epoch : 32] (l_loss: 0.05072) (t_loss: 0.07892) (accu: 0.9754)
[epoch : 33] (l_loss: 0.04962) (t_loss: 0.06562) (accu: 0.9799)
[epoch : 34] (l_loss: 0.05091) (t_loss: 0.07297) (accu: 0.9769)
[epoch : 35] (l_loss: 0.05059) (t_loss: 0.07209) (accu: 0.9774)
[epoch : 36] (l_loss: 0.04885) (t_loss: 0.07273) (accu: 0.9769)
[epoch : 37] (l_loss: 0.04917) (t_loss: 0.06785) (accu: 0.9789)
[epoch : 38] (l_loss: 0.04816) (t_loss: 0.07788) (accu: 0.9755)
[epoch : 39] (l_loss: 0.04941) (t_loss: 0.08624) (accu: 0.9745)
[epoch : 40] (l_loss: 0.05019) (t_loss: 0.07119) (accu: 0.9774)
[epoch : 41] (l_loss: 0.04894) (t_loss: 0.07360) (accu: 0.9785)
[epoch : 42] (l_loss: 0.04945) (t_loss: 0.07098) (accu: 0.9780)
[epoch : 43] (l_loss: 0.04882) (t_loss: 0.07853) (accu: 0.9740)
[epoch : 44] (l_loss: 0.04852) (t_loss: 0.07051) (accu: 0.9781)
[epoch : 45] (l_loss: 0.04965) (t_loss: 0.06133) (accu: 0.9812)
[epoch : 46] (l_loss: 0.04760) (t_loss: 0.06749) (accu: 0.9787)
[epoch : 47] (l_loss: 0.05044) (t_loss: 0.07470) (accu: 0.9780)
[epoch : 48] (l_loss: 0.04916) (t_loss: 0.08720) (accu: 0.9737)
[epoch : 49] (l_loss: 0.04837) (t_loss: 0.06646) (accu: 0.9787)
[epoch : 50] (l_loss: 0.04854) (t_loss: 0.06769) (accu: 0.9795)
Finish! (Best accu: 0.9812) (Time taken(sec) : 610.00) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (136511 | 129689)        51.28
fc1.weight   :      235200 (120422 | 114778)        51.20
fc2.weight   :       30000 (15360 | 14640)          51.20
fcout.weight :          1000 (729 | 271)            72.90
------------------------------------------------------------

Learning start! [Prune_iter : (4/21), Remaining weight : 51.28 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.33410) (accu: 0.1397)
[epoch : 1] (l_loss: 0.20331) (t_loss: 0.10462) (accu: 0.9677)
[epoch : 2] (l_loss: 0.10042) (t_loss: 0.09729) (accu: 0.9696)
[epoch : 3] (l_loss: 0.08468) (t_loss: 0.08693) (accu: 0.9717)
[epoch : 4] (l_loss: 0.07721) (t_loss: 0.07680) (accu: 0.9761)
[epoch : 5] (l_loss: 0.07339) (t_loss: 0.08259) (accu: 0.9739)
[epoch : 6] (l_loss: 0.06503) (t_loss: 0.08064) (accu: 0.9750)
[epoch : 7] (l_loss: 0.06624) (t_loss: 0.08263) (accu: 0.9742)
[epoch : 8] (l_loss: 0.06311) (t_loss: 0.09526) (accu: 0.9710)
[epoch : 9] (l_loss: 0.06033) (t_loss: 0.07626) (accu: 0.9757)
[epoch : 10] (l_loss: 0.05971) (t_loss: 0.07175) (accu: 0.9788)
[epoch : 11] (l_loss: 0.05791) (t_loss: 0.08695) (accu: 0.9719)
[epoch : 12] (l_loss: 0.05702) (t_loss: 0.07684) (accu: 0.9762)
[epoch : 13] (l_loss: 0.05591) (t_loss: 0.07749) (accu: 0.9757)
[epoch : 14] (l_loss: 0.05509) (t_loss: 0.07439) (accu: 0.9768)
[epoch : 15] (l_loss: 0.05336) (t_loss: 0.07190) (accu: 0.9773)
[epoch : 16] (l_loss: 0.05279) (t_loss: 0.07512) (accu: 0.9769)
[epoch : 17] (l_loss: 0.05495) (t_loss: 0.07621) (accu: 0.9759)
[epoch : 18] (l_loss: 0.05231) (t_loss: 0.08695) (accu: 0.9730)
[epoch : 19] (l_loss: 0.05314) (t_loss: 0.07554) (accu: 0.9773)
[epoch : 20] (l_loss: 0.05185) (t_loss: 0.07586) (accu: 0.9742)
[epoch : 21] (l_loss: 0.05254) (t_loss: 0.07161) (accu: 0.9758)
[epoch : 22] (l_loss: 0.05109) (t_loss: 0.06736) (accu: 0.9792)
[epoch : 23] (l_loss: 0.05149) (t_loss: 0.07138) (accu: 0.9785)
[epoch : 24] (l_loss: 0.04964) (t_loss: 0.07734) (accu: 0.9758)
[epoch : 25] (l_loss: 0.05157) (t_loss: 0.07250) (accu: 0.9766)
[epoch : 26] (l_loss: 0.05231) (t_loss: 0.06983) (accu: 0.9776)
[epoch : 27] (l_loss: 0.05055) (t_loss: 0.06768) (accu: 0.9785)
[epoch : 28] (l_loss: 0.05127) (t_loss: 0.06784) (accu: 0.9792)
[epoch : 29] (l_loss: 0.04975) (t_loss: 0.08832) (accu: 0.9726)
[epoch : 30] (l_loss: 0.05011) (t_loss: 0.07408) (accu: 0.9770)
[epoch : 31] (l_loss: 0.05093) (t_loss: 0.07185) (accu: 0.9772)
[epoch : 32] (l_loss: 0.05030) (t_loss: 0.08693) (accu: 0.9727)
[epoch : 33] (l_loss: 0.05020) (t_loss: 0.07949) (accu: 0.9735)
[epoch : 34] (l_loss: 0.05063) (t_loss: 0.06788) (accu: 0.9785)
[epoch : 35] (l_loss: 0.04887) (t_loss: 0.07597) (accu: 0.9765)
[epoch : 36] (l_loss: 0.05185) (t_loss: 0.08019) (accu: 0.9746)
[epoch : 37] (l_loss: 0.05065) (t_loss: 0.07607) (accu: 0.9761)
[epoch : 38] (l_loss: 0.04929) (t_loss: 0.06603) (accu: 0.9800)
[epoch : 39] (l_loss: 0.04936) (t_loss: 0.07171) (accu: 0.9769)
[epoch : 40] (l_loss: 0.04836) (t_loss: 0.08633) (accu: 0.9729)
[epoch : 41] (l_loss: 0.04848) (t_loss: 0.07802) (accu: 0.9743)
[epoch : 42] (l_loss: 0.04923) (t_loss: 0.06899) (accu: 0.9785)
[epoch : 43] (l_loss: 0.04815) (t_loss: 0.07604) (accu: 0.9780)
[epoch : 44] (l_loss: 0.04909) (t_loss: 0.07435) (accu: 0.9774)
[epoch : 45] (l_loss: 0.04980) (t_loss: 0.07048) (accu: 0.9777)
[epoch : 46] (l_loss: 0.04964) (t_loss: 0.07368) (accu: 0.9776)
[epoch : 47] (l_loss: 0.04888) (t_loss: 0.08480) (accu: 0.9735)
[epoch : 48] (l_loss: 0.04930) (t_loss: 0.08270) (accu: 0.9728)
[epoch : 49] (l_loss: 0.04811) (t_loss: 0.06722) (accu: 0.9787)
[epoch : 50] (l_loss: 0.04805) (t_loss: 0.08067) (accu: 0.9747)
Finish! (Best accu: 0.9800) (Time taken(sec) : 609.60) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (109282 | 156918)        41.05
fc1.weight   :      235200 (96338 | 138862)         40.96
fc2.weight   :       30000 (12288 | 17712)          40.96
fcout.weight :          1000 (656 | 344)            65.60
------------------------------------------------------------

Learning start! [Prune_iter : (5/21), Remaining weight : 41.05 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.32085) (accu: 0.1190)
[epoch : 1] (l_loss: 0.21316) (t_loss: 0.10973) (accu: 0.9666)
[epoch : 2] (l_loss: 0.10232) (t_loss: 0.10769) (accu: 0.9648)
[epoch : 3] (l_loss: 0.08585) (t_loss: 0.09071) (accu: 0.9692)
[epoch : 4] (l_loss: 0.07694) (t_loss: 0.08296) (accu: 0.9753)
[epoch : 5] (l_loss: 0.07116) (t_loss: 0.08437) (accu: 0.9730)
[epoch : 6] (l_loss: 0.06738) (t_loss: 0.07906) (accu: 0.9749)
[epoch : 7] (l_loss: 0.06512) (t_loss: 0.08404) (accu: 0.9712)
[epoch : 8] (l_loss: 0.06344) (t_loss: 0.07548) (accu: 0.9764)
[epoch : 9] (l_loss: 0.05806) (t_loss: 0.07559) (accu: 0.9760)
[epoch : 10] (l_loss: 0.05853) (t_loss: 0.08269) (accu: 0.9725)
[epoch : 11] (l_loss: 0.05688) (t_loss: 0.07636) (accu: 0.9764)
[epoch : 12] (l_loss: 0.05672) (t_loss: 0.07494) (accu: 0.9765)
[epoch : 13] (l_loss: 0.05387) (t_loss: 0.07937) (accu: 0.9757)
[epoch : 14] (l_loss: 0.05440) (t_loss: 0.07512) (accu: 0.9771)
[epoch : 15] (l_loss: 0.05543) (t_loss: 0.07254) (accu: 0.9776)
[epoch : 16] (l_loss: 0.05345) (t_loss: 0.07605) (accu: 0.9773)
[epoch : 17] (l_loss: 0.05206) (t_loss: 0.08365) (accu: 0.9746)
[epoch : 18] (l_loss: 0.05196) (t_loss: 0.07739) (accu: 0.9773)
[epoch : 19] (l_loss: 0.05272) (t_loss: 0.07353) (accu: 0.9781)
[epoch : 20] (l_loss: 0.05269) (t_loss: 0.08125) (accu: 0.9745)
[epoch : 21] (l_loss: 0.05197) (t_loss: 0.07982) (accu: 0.9760)
[epoch : 22] (l_loss: 0.05077) (t_loss: 0.07816) (accu: 0.9757)
[epoch : 23] (l_loss: 0.05169) (t_loss: 0.07842) (accu: 0.9754)
[epoch : 24] (l_loss: 0.05129) (t_loss: 0.06792) (accu: 0.9801)
[epoch : 25] (l_loss: 0.05081) (t_loss: 0.07898) (accu: 0.9750)
[epoch : 26] (l_loss: 0.05116) (t_loss: 0.07402) (accu: 0.9766)
[epoch : 27] (l_loss: 0.05107) (t_loss: 0.07415) (accu: 0.9772)
[epoch : 28] (l_loss: 0.04958) (t_loss: 0.06950) (accu: 0.9782)
[epoch : 29] (l_loss: 0.05002) (t_loss: 0.07225) (accu: 0.9780)
[epoch : 30] (l_loss: 0.04959) (t_loss: 0.07036) (accu: 0.9778)
[epoch : 31] (l_loss: 0.05006) (t_loss: 0.08241) (accu: 0.9760)
[epoch : 32] (l_loss: 0.05039) (t_loss: 0.08015) (accu: 0.9763)
[epoch : 33] (l_loss: 0.05053) (t_loss: 0.06734) (accu: 0.9778)
[epoch : 34] (l_loss: 0.04941) (t_loss: 0.08203) (accu: 0.9751)
[epoch : 35] (l_loss: 0.05005) (t_loss: 0.08949) (accu: 0.9730)
[epoch : 36] (l_loss: 0.04954) (t_loss: 0.07502) (accu: 0.9750)
[epoch : 37] (l_loss: 0.04994) (t_loss: 0.08212) (accu: 0.9736)
[epoch : 38] (l_loss: 0.04914) (t_loss: 0.07168) (accu: 0.9775)
[epoch : 39] (l_loss: 0.04911) (t_loss: 0.07196) (accu: 0.9776)
[epoch : 40] (l_loss: 0.04748) (t_loss: 0.07599) (accu: 0.9769)
[epoch : 41] (l_loss: 0.05023) (t_loss: 0.07316) (accu: 0.9783)
[epoch : 42] (l_loss: 0.04946) (t_loss: 0.07575) (accu: 0.9758)
[epoch : 43] (l_loss: 0.04932) (t_loss: 0.08278) (accu: 0.9733)
[epoch : 44] (l_loss: 0.05026) (t_loss: 0.07672) (accu: 0.9780)
[epoch : 45] (l_loss: 0.04921) (t_loss: 0.08727) (accu: 0.9718)
[epoch : 46] (l_loss: 0.04702) (t_loss: 0.07782) (accu: 0.9757)
[epoch : 47] (l_loss: 0.05019) (t_loss: 0.07148) (accu: 0.9780)
[epoch : 48] (l_loss: 0.04958) (t_loss: 0.06990) (accu: 0.9789)
[epoch : 49] (l_loss: 0.04882) (t_loss: 0.07274) (accu: 0.9785)
[epoch : 50] (l_loss: 0.04716) (t_loss: 0.07188) (accu: 0.9763)
Finish! (Best accu: 0.9801) (Time taken(sec) : 611.47) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (87490 | 178710)         32.87
fc1.weight   :      235200 (77070 | 158130)         32.77
fc2.weight   :        30000 (9830 | 20170)          32.77
fcout.weight :          1000 (590 | 410)            59.00
------------------------------------------------------------

Learning start! [Prune_iter : (6/21), Remaining weight : 32.87 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.34405) (accu: 0.1095)
[epoch : 1] (l_loss: 0.21807) (t_loss: 0.12439) (accu: 0.9588)
[epoch : 2] (l_loss: 0.09850) (t_loss: 0.08844) (accu: 0.9723)
[epoch : 3] (l_loss: 0.08204) (t_loss: 0.07888) (accu: 0.9755)
[epoch : 4] (l_loss: 0.07451) (t_loss: 0.07670) (accu: 0.9757)
[epoch : 5] (l_loss: 0.06984) (t_loss: 0.08331) (accu: 0.9730)
[epoch : 6] (l_loss: 0.06723) (t_loss: 0.08359) (accu: 0.9726)
[epoch : 7] (l_loss: 0.06327) (t_loss: 0.08119) (accu: 0.9742)
[epoch : 8] (l_loss: 0.06088) (t_loss: 0.07407) (accu: 0.9763)
[epoch : 9] (l_loss: 0.05965) (t_loss: 0.07661) (accu: 0.9754)
[epoch : 10] (l_loss: 0.05759) (t_loss: 0.07220) (accu: 0.9766)
[epoch : 11] (l_loss: 0.05742) (t_loss: 0.07784) (accu: 0.9753)
[epoch : 12] (l_loss: 0.05442) (t_loss: 0.07705) (accu: 0.9756)
[epoch : 13] (l_loss: 0.05510) (t_loss: 0.07372) (accu: 0.9770)
[epoch : 14] (l_loss: 0.05401) (t_loss: 0.06489) (accu: 0.9785)
[epoch : 15] (l_loss: 0.05262) (t_loss: 0.08503) (accu: 0.9719)
[epoch : 16] (l_loss: 0.05331) (t_loss: 0.07412) (accu: 0.9769)
[epoch : 17] (l_loss: 0.05341) (t_loss: 0.06932) (accu: 0.9785)
[epoch : 18] (l_loss: 0.05115) (t_loss: 0.07396) (accu: 0.9781)
[epoch : 19] (l_loss: 0.05316) (t_loss: 0.06702) (accu: 0.9783)
[epoch : 20] (l_loss: 0.04960) (t_loss: 0.07312) (accu: 0.9772)
[epoch : 21] (l_loss: 0.05130) (t_loss: 0.07319) (accu: 0.9777)
[epoch : 22] (l_loss: 0.05058) (t_loss: 0.08626) (accu: 0.9733)
[epoch : 23] (l_loss: 0.05055) (t_loss: 0.08384) (accu: 0.9732)
[epoch : 24] (l_loss: 0.04939) (t_loss: 0.07160) (accu: 0.9768)
[epoch : 25] (l_loss: 0.05026) (t_loss: 0.10494) (accu: 0.9671)
[epoch : 26] (l_loss: 0.04966) (t_loss: 0.08534) (accu: 0.9728)
[epoch : 27] (l_loss: 0.05081) (t_loss: 0.07413) (accu: 0.9770)
[epoch : 28] (l_loss: 0.04886) (t_loss: 0.07062) (accu: 0.9771)
[epoch : 29] (l_loss: 0.05050) (t_loss: 0.07163) (accu: 0.9783)
[epoch : 30] (l_loss: 0.04860) (t_loss: 0.06969) (accu: 0.9785)
[epoch : 31] (l_loss: 0.05086) (t_loss: 0.06774) (accu: 0.9780)
[epoch : 32] (l_loss: 0.04894) (t_loss: 0.07820) (accu: 0.9748)
[epoch : 33] (l_loss: 0.04828) (t_loss: 0.06547) (accu: 0.9801)
[epoch : 34] (l_loss: 0.04870) (t_loss: 0.07087) (accu: 0.9778)
[epoch : 35] (l_loss: 0.04970) (t_loss: 0.07171) (accu: 0.9778)
[epoch : 36] (l_loss: 0.04872) (t_loss: 0.07829) (accu: 0.9761)
[epoch : 37] (l_loss: 0.04907) (t_loss: 0.07443) (accu: 0.9767)
[epoch : 38] (l_loss: 0.04914) (t_loss: 0.08546) (accu: 0.9739)
[epoch : 39] (l_loss: 0.05062) (t_loss: 0.08027) (accu: 0.9736)
[epoch : 40] (l_loss: 0.04861) (t_loss: 0.07282) (accu: 0.9784)
[epoch : 41] (l_loss: 0.04856) (t_loss: 0.07185) (accu: 0.9775)
[epoch : 42] (l_loss: 0.04791) (t_loss: 0.06826) (accu: 0.9790)
[epoch : 43] (l_loss: 0.04821) (t_loss: 0.07309) (accu: 0.9760)
[epoch : 44] (l_loss: 0.04780) (t_loss: 0.07538) (accu: 0.9742)
[epoch : 45] (l_loss: 0.04777) (t_loss: 0.08917) (accu: 0.9720)
[epoch : 46] (l_loss: 0.04934) (t_loss: 0.06674) (accu: 0.9794)
[epoch : 47] (l_loss: 0.04895) (t_loss: 0.07779) (accu: 0.9749)
[epoch : 48] (l_loss: 0.04818) (t_loss: 0.07101) (accu: 0.9771)
[epoch : 49] (l_loss: 0.04858) (t_loss: 0.07232) (accu: 0.9756)
[epoch : 50] (l_loss: 0.04914) (t_loss: 0.06983) (accu: 0.9777)
Finish! (Best accu: 0.9801) (Time taken(sec) : 610.97) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (70051 | 196149)         26.32
fc1.weight   :      235200 (61656 | 173544)         26.21
fc2.weight   :        30000 (7864 | 22136)          26.21
fcout.weight :          1000 (531 | 469)            53.10
------------------------------------------------------------

Learning start! [Prune_iter : (7/21), Remaining weight : 26.32 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.36928) (accu: 0.0874)
[epoch : 1] (l_loss: 0.21939) (t_loss: 0.11820) (accu: 0.9643)
[epoch : 2] (l_loss: 0.10240) (t_loss: 0.09561) (accu: 0.9700)
[epoch : 3] (l_loss: 0.08283) (t_loss: 0.08745) (accu: 0.9704)
[epoch : 4] (l_loss: 0.07549) (t_loss: 0.08870) (accu: 0.9725)
[epoch : 5] (l_loss: 0.06996) (t_loss: 0.07857) (accu: 0.9749)
[epoch : 6] (l_loss: 0.06677) (t_loss: 0.09229) (accu: 0.9712)
[epoch : 7] (l_loss: 0.06214) (t_loss: 0.08144) (accu: 0.9753)
[epoch : 8] (l_loss: 0.06095) (t_loss: 0.08241) (accu: 0.9743)
[epoch : 9] (l_loss: 0.06054) (t_loss: 0.07686) (accu: 0.9771)
[epoch : 10] (l_loss: 0.05797) (t_loss: 0.08282) (accu: 0.9740)
[epoch : 11] (l_loss: 0.05702) (t_loss: 0.07424) (accu: 0.9770)
[epoch : 12] (l_loss: 0.05547) (t_loss: 0.08019) (accu: 0.9729)
[epoch : 13] (l_loss: 0.05531) (t_loss: 0.07470) (accu: 0.9758)
[epoch : 14] (l_loss: 0.05379) (t_loss: 0.07910) (accu: 0.9759)
[epoch : 15] (l_loss: 0.05375) (t_loss: 0.07647) (accu: 0.9761)
[epoch : 16] (l_loss: 0.05273) (t_loss: 0.07213) (accu: 0.9785)
[epoch : 17] (l_loss: 0.05392) (t_loss: 0.08606) (accu: 0.9713)
[epoch : 18] (l_loss: 0.05204) (t_loss: 0.07061) (accu: 0.9772)
[epoch : 19] (l_loss: 0.05274) (t_loss: 0.07664) (accu: 0.9759)
[epoch : 20] (l_loss: 0.05213) (t_loss: 0.07263) (accu: 0.9756)
[epoch : 21] (l_loss: 0.05160) (t_loss: 0.07519) (accu: 0.9774)
[epoch : 22] (l_loss: 0.05061) (t_loss: 0.07483) (accu: 0.9760)
[epoch : 23] (l_loss: 0.05094) (t_loss: 0.06975) (accu: 0.9780)
[epoch : 24] (l_loss: 0.04997) (t_loss: 0.07309) (accu: 0.9766)
[epoch : 25] (l_loss: 0.04915) (t_loss: 0.07278) (accu: 0.9774)
[epoch : 26] (l_loss: 0.04989) (t_loss: 0.08773) (accu: 0.9730)
[epoch : 27] (l_loss: 0.05097) (t_loss: 0.08265) (accu: 0.9741)
[epoch : 28] (l_loss: 0.05006) (t_loss: 0.07696) (accu: 0.9763)
[epoch : 29] (l_loss: 0.04883) (t_loss: 0.08673) (accu: 0.9727)
[epoch : 30] (l_loss: 0.04949) (t_loss: 0.07739) (accu: 0.9756)
[epoch : 31] (l_loss: 0.04939) (t_loss: 0.08130) (accu: 0.9757)
[epoch : 32] (l_loss: 0.04938) (t_loss: 0.07289) (accu: 0.9780)
[epoch : 33] (l_loss: 0.04928) (t_loss: 0.07314) (accu: 0.9780)
[epoch : 34] (l_loss: 0.04989) (t_loss: 0.06858) (accu: 0.9793)
[epoch : 35] (l_loss: 0.04846) (t_loss: 0.07263) (accu: 0.9778)
[epoch : 36] (l_loss: 0.04907) (t_loss: 0.07068) (accu: 0.9788)
[epoch : 37] (l_loss: 0.04888) (t_loss: 0.06779) (accu: 0.9786)
[epoch : 38] (l_loss: 0.04767) (t_loss: 0.08277) (accu: 0.9743)
[epoch : 39] (l_loss: 0.04934) (t_loss: 0.06909) (accu: 0.9790)
[epoch : 40] (l_loss: 0.04930) (t_loss: 0.07669) (accu: 0.9759)
[epoch : 41] (l_loss: 0.04843) (t_loss: 0.07298) (accu: 0.9764)
[epoch : 42] (l_loss: 0.04755) (t_loss: 0.07407) (accu: 0.9757)
[epoch : 43] (l_loss: 0.04906) (t_loss: 0.07050) (accu: 0.9769)
[epoch : 44] (l_loss: 0.04788) (t_loss: 0.08328) (accu: 0.9758)
[epoch : 45] (l_loss: 0.04743) (t_loss: 0.06563) (accu: 0.9800)
[epoch : 46] (l_loss: 0.04887) (t_loss: 0.07392) (accu: 0.9766)
[epoch : 47] (l_loss: 0.04742) (t_loss: 0.07489) (accu: 0.9766)
[epoch : 48] (l_loss: 0.04760) (t_loss: 0.07425) (accu: 0.9762)
[epoch : 49] (l_loss: 0.04818) (t_loss: 0.07600) (accu: 0.9757)
[epoch : 50] (l_loss: 0.04822) (t_loss: 0.06785) (accu: 0.9803)
Finish! (Best accu: 0.9803) (Time taken(sec) : 615.65) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (56094 | 210106)         21.07
fc1.weight   :      235200 (49325 | 185875)         20.97
fc2.weight   :        30000 (6291 | 23709)          20.97
fcout.weight :          1000 (478 | 522)            47.80
------------------------------------------------------------

Learning start! [Prune_iter : (8/21), Remaining weight : 21.07 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.36991) (accu: 0.0894)
[epoch : 1] (l_loss: 0.23192) (t_loss: 0.12880) (accu: 0.9594)
[epoch : 2] (l_loss: 0.10465) (t_loss: 0.09790) (accu: 0.9723)
[epoch : 3] (l_loss: 0.08527) (t_loss: 0.08717) (accu: 0.9732)
[epoch : 4] (l_loss: 0.07697) (t_loss: 0.09077) (accu: 0.9704)
[epoch : 5] (l_loss: 0.06927) (t_loss: 0.08481) (accu: 0.9746)
[epoch : 6] (l_loss: 0.06667) (t_loss: 0.07731) (accu: 0.9754)
[epoch : 7] (l_loss: 0.06303) (t_loss: 0.08221) (accu: 0.9735)
[epoch : 8] (l_loss: 0.06147) (t_loss: 0.07838) (accu: 0.9747)
[epoch : 9] (l_loss: 0.06003) (t_loss: 0.08074) (accu: 0.9755)
[epoch : 10] (l_loss: 0.05699) (t_loss: 0.07019) (accu: 0.9776)
[epoch : 11] (l_loss: 0.05564) (t_loss: 0.08598) (accu: 0.9734)
[epoch : 12] (l_loss: 0.05681) (t_loss: 0.07861) (accu: 0.9754)
[epoch : 13] (l_loss: 0.05508) (t_loss: 0.07216) (accu: 0.9769)
[epoch : 14] (l_loss: 0.05436) (t_loss: 0.07295) (accu: 0.9761)
[epoch : 15] (l_loss: 0.05508) (t_loss: 0.07501) (accu: 0.9767)
[epoch : 16] (l_loss: 0.05335) (t_loss: 0.07384) (accu: 0.9760)
[epoch : 17] (l_loss: 0.05253) (t_loss: 0.07467) (accu: 0.9757)
[epoch : 18] (l_loss: 0.05254) (t_loss: 0.07277) (accu: 0.9773)
[epoch : 19] (l_loss: 0.05117) (t_loss: 0.07868) (accu: 0.9767)
[epoch : 20] (l_loss: 0.05011) (t_loss: 0.08029) (accu: 0.9743)
[epoch : 21] (l_loss: 0.05064) (t_loss: 0.07886) (accu: 0.9758)
[epoch : 22] (l_loss: 0.05078) (t_loss: 0.07799) (accu: 0.9757)
[epoch : 23] (l_loss: 0.05002) (t_loss: 0.08051) (accu: 0.9742)
[epoch : 24] (l_loss: 0.05012) (t_loss: 0.08663) (accu: 0.9718)
[epoch : 25] (l_loss: 0.05146) (t_loss: 0.07102) (accu: 0.9780)
[epoch : 26] (l_loss: 0.04868) (t_loss: 0.06962) (accu: 0.9782)
[epoch : 27] (l_loss: 0.05094) (t_loss: 0.07635) (accu: 0.9753)
[epoch : 28] (l_loss: 0.04963) (t_loss: 0.06553) (accu: 0.9790)
[epoch : 29] (l_loss: 0.04987) (t_loss: 0.07464) (accu: 0.9770)
[epoch : 30] (l_loss: 0.04895) (t_loss: 0.07581) (accu: 0.9752)
[epoch : 31] (l_loss: 0.04981) (t_loss: 0.08330) (accu: 0.9740)
[epoch : 32] (l_loss: 0.05075) (t_loss: 0.07604) (accu: 0.9763)
[epoch : 33] (l_loss: 0.04880) (t_loss: 0.08092) (accu: 0.9743)
[epoch : 34] (l_loss: 0.04874) (t_loss: 0.07095) (accu: 0.9770)
[epoch : 35] (l_loss: 0.04783) (t_loss: 0.10688) (accu: 0.9684)
[epoch : 36] (l_loss: 0.04936) (t_loss: 0.07074) (accu: 0.9788)
[epoch : 37] (l_loss: 0.04772) (t_loss: 0.08651) (accu: 0.9734)
[epoch : 38] (l_loss: 0.04867) (t_loss: 0.07906) (accu: 0.9759)
[epoch : 39] (l_loss: 0.04837) (t_loss: 0.06865) (accu: 0.9788)
[epoch : 40] (l_loss: 0.04903) (t_loss: 0.07116) (accu: 0.9777)
[epoch : 41] (l_loss: 0.04801) (t_loss: 0.07231) (accu: 0.9776)
[epoch : 42] (l_loss: 0.04820) (t_loss: 0.07885) (accu: 0.9750)
[epoch : 43] (l_loss: 0.04899) (t_loss: 0.07254) (accu: 0.9787)
[epoch : 44] (l_loss: 0.04757) (t_loss: 0.07274) (accu: 0.9778)
[epoch : 45] (l_loss: 0.04872) (t_loss: 0.07072) (accu: 0.9785)
[epoch : 46] (l_loss: 0.04742) (t_loss: 0.06504) (accu: 0.9787)
[epoch : 47] (l_loss: 0.04670) (t_loss: 0.06566) (accu: 0.9778)
[epoch : 48] (l_loss: 0.04906) (t_loss: 0.06638) (accu: 0.9801)
[epoch : 49] (l_loss: 0.04786) (t_loss: 0.06936) (accu: 0.9788)
[epoch : 50] (l_loss: 0.04739) (t_loss: 0.08318) (accu: 0.9736)
Finish! (Best accu: 0.9801) (Time taken(sec) : 632.17) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (44923 | 221277)         16.88
fc1.weight   :      235200 (39460 | 195740)         16.78
fc2.weight   :        30000 (5033 | 24967)          16.78
fcout.weight :          1000 (430 | 570)            43.00
------------------------------------------------------------

Learning start! [Prune_iter : (9/21), Remaining weight : 16.88 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.34213) (accu: 0.1430)
[epoch : 1] (l_loss: 0.24258) (t_loss: 0.11720) (accu: 0.9650)
[epoch : 2] (l_loss: 0.10812) (t_loss: 0.09705) (accu: 0.9685)
[epoch : 3] (l_loss: 0.08580) (t_loss: 0.09064) (accu: 0.9718)
[epoch : 4] (l_loss: 0.07658) (t_loss: 0.08265) (accu: 0.9721)
[epoch : 5] (l_loss: 0.07031) (t_loss: 0.07903) (accu: 0.9758)
[epoch : 6] (l_loss: 0.06610) (t_loss: 0.08359) (accu: 0.9739)
[epoch : 7] (l_loss: 0.06306) (t_loss: 0.08717) (accu: 0.9730)
[epoch : 8] (l_loss: 0.06062) (t_loss: 0.08464) (accu: 0.9736)
[epoch : 9] (l_loss: 0.05914) (t_loss: 0.08232) (accu: 0.9727)
[epoch : 10] (l_loss: 0.05788) (t_loss: 0.07913) (accu: 0.9749)
[epoch : 11] (l_loss: 0.05804) (t_loss: 0.08723) (accu: 0.9732)
[epoch : 12] (l_loss: 0.05501) (t_loss: 0.07412) (accu: 0.9768)
[epoch : 13] (l_loss: 0.05485) (t_loss: 0.08924) (accu: 0.9717)
[epoch : 14] (l_loss: 0.05462) (t_loss: 0.07543) (accu: 0.9760)
[epoch : 15] (l_loss: 0.05276) (t_loss: 0.07477) (accu: 0.9761)
[epoch : 16] (l_loss: 0.05381) (t_loss: 0.07302) (accu: 0.9763)
[epoch : 17] (l_loss: 0.05107) (t_loss: 0.07990) (accu: 0.9754)
[epoch : 18] (l_loss: 0.05200) (t_loss: 0.07555) (accu: 0.9773)
[epoch : 19] (l_loss: 0.05107) (t_loss: 0.07291) (accu: 0.9760)
[epoch : 20] (l_loss: 0.05123) (t_loss: 0.07566) (accu: 0.9743)
[epoch : 21] (l_loss: 0.05094) (t_loss: 0.07579) (accu: 0.9761)
[epoch : 22] (l_loss: 0.05199) (t_loss: 0.07398) (accu: 0.9766)
[epoch : 23] (l_loss: 0.05070) (t_loss: 0.07185) (accu: 0.9767)
[epoch : 24] (l_loss: 0.05008) (t_loss: 0.08434) (accu: 0.9730)
[epoch : 25] (l_loss: 0.04939) (t_loss: 0.07075) (accu: 0.9766)
[epoch : 26] (l_loss: 0.04901) (t_loss: 0.07341) (accu: 0.9774)
[epoch : 27] (l_loss: 0.05118) (t_loss: 0.07506) (accu: 0.9770)
[epoch : 28] (l_loss: 0.04751) (t_loss: 0.07203) (accu: 0.9764)
[epoch : 29] (l_loss: 0.05029) (t_loss: 0.06848) (accu: 0.9774)
[epoch : 30] (l_loss: 0.04975) (t_loss: 0.06999) (accu: 0.9777)
[epoch : 31] (l_loss: 0.04896) (t_loss: 0.07791) (accu: 0.9749)
[epoch : 32] (l_loss: 0.04884) (t_loss: 0.07689) (accu: 0.9757)
[epoch : 33] (l_loss: 0.04859) (t_loss: 0.08102) (accu: 0.9739)
[epoch : 34] (l_loss: 0.04775) (t_loss: 0.07398) (accu: 0.9762)
[epoch : 35] (l_loss: 0.04813) (t_loss: 0.08524) (accu: 0.9743)
[epoch : 36] (l_loss: 0.04889) (t_loss: 0.08307) (accu: 0.9739)
[epoch : 37] (l_loss: 0.04760) (t_loss: 0.07471) (accu: 0.9764)
[epoch : 38] (l_loss: 0.04726) (t_loss: 0.06940) (accu: 0.9765)
[epoch : 39] (l_loss: 0.04740) (t_loss: 0.07409) (accu: 0.9763)
[epoch : 40] (l_loss: 0.04902) (t_loss: 0.08345) (accu: 0.9741)
[epoch : 41] (l_loss: 0.04688) (t_loss: 0.07025) (accu: 0.9783)
[epoch : 42] (l_loss: 0.04896) (t_loss: 0.07545) (accu: 0.9767)
[epoch : 43] (l_loss: 0.04776) (t_loss: 0.07342) (accu: 0.9779)
[epoch : 44] (l_loss: 0.04680) (t_loss: 0.07277) (accu: 0.9772)
[epoch : 45] (l_loss: 0.04858) (t_loss: 0.07913) (accu: 0.9750)
[epoch : 46] (l_loss: 0.04769) (t_loss: 0.08480) (accu: 0.9734)
[epoch : 47] (l_loss: 0.04846) (t_loss: 0.08048) (accu: 0.9758)
[epoch : 48] (l_loss: 0.04791) (t_loss: 0.07160) (accu: 0.9783)
[epoch : 49] (l_loss: 0.04667) (t_loss: 0.07759) (accu: 0.9749)
[epoch : 50] (l_loss: 0.04645) (t_loss: 0.06768) (accu: 0.9774)
Finish! (Best accu: 0.9783) (Time taken(sec) : 618.87) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (35982 | 230218)         13.52
fc1.weight   :      235200 (31568 | 203632)         13.42
fc2.weight   :        30000 (4027 | 25973)          13.42
fcout.weight :          1000 (387 | 613)            38.70
------------------------------------------------------------

Learning start! [Prune_iter : (10/21), Remaining weight : 13.52 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.32236) (accu: 0.1282)
[epoch : 1] (l_loss: 0.24855) (t_loss: 0.13128) (accu: 0.9583)
[epoch : 2] (l_loss: 0.11157) (t_loss: 0.09685) (accu: 0.9697)
[epoch : 3] (l_loss: 0.08779) (t_loss: 0.08987) (accu: 0.9736)
[epoch : 4] (l_loss: 0.07663) (t_loss: 0.08256) (accu: 0.9747)
[epoch : 5] (l_loss: 0.07018) (t_loss: 0.07917) (accu: 0.9737)
[epoch : 6] (l_loss: 0.06725) (t_loss: 0.07621) (accu: 0.9765)
[epoch : 7] (l_loss: 0.06274) (t_loss: 0.07686) (accu: 0.9766)
[epoch : 8] (l_loss: 0.06015) (t_loss: 0.07532) (accu: 0.9757)
[epoch : 9] (l_loss: 0.05875) (t_loss: 0.07738) (accu: 0.9748)
[epoch : 10] (l_loss: 0.05654) (t_loss: 0.07643) (accu: 0.9766)
[epoch : 11] (l_loss: 0.05446) (t_loss: 0.07667) (accu: 0.9764)
[epoch : 12] (l_loss: 0.05414) (t_loss: 0.07329) (accu: 0.9762)
[epoch : 13] (l_loss: 0.05357) (t_loss: 0.07507) (accu: 0.9761)
[epoch : 14] (l_loss: 0.05276) (t_loss: 0.07665) (accu: 0.9763)
[epoch : 15] (l_loss: 0.05263) (t_loss: 0.06932) (accu: 0.9791)
[epoch : 16] (l_loss: 0.05133) (t_loss: 0.06864) (accu: 0.9793)
[epoch : 17] (l_loss: 0.04972) (t_loss: 0.07675) (accu: 0.9765)
[epoch : 18] (l_loss: 0.05099) (t_loss: 0.07917) (accu: 0.9749)
[epoch : 19] (l_loss: 0.05049) (t_loss: 0.07286) (accu: 0.9766)
[epoch : 20] (l_loss: 0.05029) (t_loss: 0.07408) (accu: 0.9770)
[epoch : 21] (l_loss: 0.04966) (t_loss: 0.07417) (accu: 0.9778)
[epoch : 22] (l_loss: 0.04858) (t_loss: 0.07448) (accu: 0.9756)
[epoch : 23] (l_loss: 0.04931) (t_loss: 0.07355) (accu: 0.9779)
[epoch : 24] (l_loss: 0.04996) (t_loss: 0.06968) (accu: 0.9781)
[epoch : 25] (l_loss: 0.04787) (t_loss: 0.06892) (accu: 0.9774)
[epoch : 26] (l_loss: 0.04907) (t_loss: 0.07050) (accu: 0.9777)
[epoch : 27] (l_loss: 0.04867) (t_loss: 0.06851) (accu: 0.9788)
[epoch : 28] (l_loss: 0.04856) (t_loss: 0.07010) (accu: 0.9770)
[epoch : 29] (l_loss: 0.04872) (t_loss: 0.07164) (accu: 0.9765)
[epoch : 30] (l_loss: 0.04868) (t_loss: 0.07458) (accu: 0.9768)
[epoch : 31] (l_loss: 0.04800) (t_loss: 0.08116) (accu: 0.9749)
[epoch : 32] (l_loss: 0.04799) (t_loss: 0.07199) (accu: 0.9775)
[epoch : 33] (l_loss: 0.04835) (t_loss: 0.07254) (accu: 0.9766)
[epoch : 34] (l_loss: 0.04736) (t_loss: 0.07721) (accu: 0.9761)
[epoch : 35] (l_loss: 0.04642) (t_loss: 0.07111) (accu: 0.9776)
[epoch : 36] (l_loss: 0.04894) (t_loss: 0.08034) (accu: 0.9740)
[epoch : 37] (l_loss: 0.04789) (t_loss: 0.06927) (accu: 0.9773)
[epoch : 38] (l_loss: 0.04831) (t_loss: 0.07420) (accu: 0.9757)
[epoch : 39] (l_loss: 0.04694) (t_loss: 0.06992) (accu: 0.9779)
[epoch : 40] (l_loss: 0.04723) (t_loss: 0.08604) (accu: 0.9730)
[epoch : 41] (l_loss: 0.04734) (t_loss: 0.08205) (accu: 0.9743)
[epoch : 42] (l_loss: 0.04767) (t_loss: 0.07806) (accu: 0.9756)
[epoch : 43] (l_loss: 0.04733) (t_loss: 0.07825) (accu: 0.9753)
[epoch : 44] (l_loss: 0.04624) (t_loss: 0.07137) (accu: 0.9764)
[epoch : 45] (l_loss: 0.04813) (t_loss: 0.07493) (accu: 0.9772)
[epoch : 46] (l_loss: 0.04636) (t_loss: 0.07126) (accu: 0.9791)
[epoch : 47] (l_loss: 0.04713) (t_loss: 0.08690) (accu: 0.9732)
[epoch : 48] (l_loss: 0.04669) (t_loss: 0.07293) (accu: 0.9765)
[epoch : 49] (l_loss: 0.04676) (t_loss: 0.07123) (accu: 0.9771)
[epoch : 50] (l_loss: 0.04561) (t_loss: 0.06914) (accu: 0.9782)
Finish! (Best accu: 0.9793) (Time taken(sec) : 614.81) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (28824 | 237376)         10.83
fc1.weight   :      235200 (25254 | 209946)         10.74
fc2.weight   :        30000 (3221 | 26779)          10.74
fcout.weight :          1000 (349 | 651)            34.90
------------------------------------------------------------

Learning start! [Prune_iter : (11/21), Remaining weight : 10.83 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30604) (accu: 0.1101)
[epoch : 1] (l_loss: 0.25687) (t_loss: 0.12932) (accu: 0.9613)
[epoch : 2] (l_loss: 0.11370) (t_loss: 0.10142) (accu: 0.9685)
[epoch : 3] (l_loss: 0.08816) (t_loss: 0.08768) (accu: 0.9748)
[epoch : 4] (l_loss: 0.07716) (t_loss: 0.08792) (accu: 0.9717)
[epoch : 5] (l_loss: 0.07352) (t_loss: 0.08955) (accu: 0.9717)
[epoch : 6] (l_loss: 0.06660) (t_loss: 0.08374) (accu: 0.9744)
[epoch : 7] (l_loss: 0.06254) (t_loss: 0.09341) (accu: 0.9695)
[epoch : 8] (l_loss: 0.06192) (t_loss: 0.07803) (accu: 0.9775)
[epoch : 9] (l_loss: 0.05758) (t_loss: 0.07411) (accu: 0.9764)
[epoch : 10] (l_loss: 0.05668) (t_loss: 0.08369) (accu: 0.9746)
[epoch : 11] (l_loss: 0.05597) (t_loss: 0.08182) (accu: 0.9747)
[epoch : 12] (l_loss: 0.05415) (t_loss: 0.07815) (accu: 0.9746)
[epoch : 13] (l_loss: 0.05314) (t_loss: 0.08275) (accu: 0.9746)
[epoch : 14] (l_loss: 0.05315) (t_loss: 0.08629) (accu: 0.9725)
[epoch : 15] (l_loss: 0.05202) (t_loss: 0.07885) (accu: 0.9746)
[epoch : 16] (l_loss: 0.05214) (t_loss: 0.07070) (accu: 0.9788)
[epoch : 17] (l_loss: 0.05142) (t_loss: 0.07348) (accu: 0.9774)
[epoch : 18] (l_loss: 0.04985) (t_loss: 0.08400) (accu: 0.9735)
[epoch : 19] (l_loss: 0.04928) (t_loss: 0.07627) (accu: 0.9744)
[epoch : 20] (l_loss: 0.05022) (t_loss: 0.08277) (accu: 0.9738)
[epoch : 21] (l_loss: 0.04871) (t_loss: 0.08368) (accu: 0.9734)
[epoch : 22] (l_loss: 0.05010) (t_loss: 0.08199) (accu: 0.9740)
[epoch : 23] (l_loss: 0.05091) (t_loss: 0.07987) (accu: 0.9739)
[epoch : 24] (l_loss: 0.04799) (t_loss: 0.07585) (accu: 0.9763)
[epoch : 25] (l_loss: 0.04847) (t_loss: 0.07328) (accu: 0.9764)
[epoch : 26] (l_loss: 0.04930) (t_loss: 0.07731) (accu: 0.9759)
[epoch : 27] (l_loss: 0.04805) (t_loss: 0.07519) (accu: 0.9744)
[epoch : 28] (l_loss: 0.04826) (t_loss: 0.08307) (accu: 0.9724)
[epoch : 29] (l_loss: 0.04750) (t_loss: 0.08037) (accu: 0.9746)
[epoch : 30] (l_loss: 0.04794) (t_loss: 0.07322) (accu: 0.9771)
[epoch : 31] (l_loss: 0.04687) (t_loss: 0.07310) (accu: 0.9756)
[epoch : 32] (l_loss: 0.04772) (t_loss: 0.07673) (accu: 0.9745)
[epoch : 33] (l_loss: 0.04824) (t_loss: 0.07595) (accu: 0.9762)
[epoch : 34] (l_loss: 0.04741) (t_loss: 0.07081) (accu: 0.9752)
[epoch : 35] (l_loss: 0.04840) (t_loss: 0.09127) (accu: 0.9716)
[epoch : 36] (l_loss: 0.04617) (t_loss: 0.07124) (accu: 0.9771)
[epoch : 37] (l_loss: 0.04724) (t_loss: 0.07565) (accu: 0.9756)
[epoch : 38] (l_loss: 0.04777) (t_loss: 0.07222) (accu: 0.9766)
[epoch : 39] (l_loss: 0.04684) (t_loss: 0.07272) (accu: 0.9765)
[epoch : 40] (l_loss: 0.04742) (t_loss: 0.07709) (accu: 0.9759)
[epoch : 41] (l_loss: 0.04745) (t_loss: 0.07655) (accu: 0.9758)
[epoch : 42] (l_loss: 0.04803) (t_loss: 0.07438) (accu: 0.9751)
[epoch : 43] (l_loss: 0.04582) (t_loss: 0.07406) (accu: 0.9772)
[epoch : 44] (l_loss: 0.04656) (t_loss: 0.07196) (accu: 0.9764)
[epoch : 45] (l_loss: 0.04669) (t_loss: 0.08354) (accu: 0.9733)
[epoch : 46] (l_loss: 0.04754) (t_loss: 0.07654) (accu: 0.9749)
[epoch : 47] (l_loss: 0.04756) (t_loss: 0.07503) (accu: 0.9762)
[epoch : 48] (l_loss: 0.04666) (t_loss: 0.07196) (accu: 0.9767)
[epoch : 49] (l_loss: 0.04471) (t_loss: 0.07393) (accu: 0.9760)
[epoch : 50] (l_loss: 0.04621) (t_loss: 0.07550) (accu: 0.9757)
Finish! (Best accu: 0.9788) (Time taken(sec) : 627.81) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (23095 | 243105)          8.68
fc1.weight   :      235200 (20204 | 214996)          8.59
fc2.weight   :        30000 (2577 | 27423)           8.59
fcout.weight :          1000 (314 | 686)            31.40
------------------------------------------------------------

Learning start! [Prune_iter : (12/21), Remaining weight : 8.68 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29835) (accu: 0.1361)
[epoch : 1] (l_loss: 0.26530) (t_loss: 0.13120) (accu: 0.9598)
[epoch : 2] (l_loss: 0.11438) (t_loss: 0.10663) (accu: 0.9665)
[epoch : 3] (l_loss: 0.09045) (t_loss: 0.09441) (accu: 0.9709)
[epoch : 4] (l_loss: 0.07727) (t_loss: 0.08452) (accu: 0.9737)
[epoch : 5] (l_loss: 0.06989) (t_loss: 0.08242) (accu: 0.9749)
[epoch : 6] (l_loss: 0.06647) (t_loss: 0.08262) (accu: 0.9732)
[epoch : 7] (l_loss: 0.06312) (t_loss: 0.07743) (accu: 0.9757)
[epoch : 8] (l_loss: 0.06006) (t_loss: 0.07476) (accu: 0.9755)
[epoch : 9] (l_loss: 0.05848) (t_loss: 0.07722) (accu: 0.9762)
[epoch : 10] (l_loss: 0.05569) (t_loss: 0.08388) (accu: 0.9743)
[epoch : 11] (l_loss: 0.05507) (t_loss: 0.08510) (accu: 0.9728)
[epoch : 12] (l_loss: 0.05476) (t_loss: 0.08203) (accu: 0.9739)
[epoch : 13] (l_loss: 0.05359) (t_loss: 0.08492) (accu: 0.9737)
[epoch : 14] (l_loss: 0.05296) (t_loss: 0.07567) (accu: 0.9782)
[epoch : 15] (l_loss: 0.05315) (t_loss: 0.06871) (accu: 0.9784)
[epoch : 16] (l_loss: 0.05201) (t_loss: 0.08111) (accu: 0.9761)
[epoch : 17] (l_loss: 0.05025) (t_loss: 0.08481) (accu: 0.9741)
[epoch : 18] (l_loss: 0.05113) (t_loss: 0.08252) (accu: 0.9748)
[epoch : 19] (l_loss: 0.05083) (t_loss: 0.08089) (accu: 0.9742)
[epoch : 20] (l_loss: 0.04978) (t_loss: 0.08284) (accu: 0.9732)
[epoch : 21] (l_loss: 0.04972) (t_loss: 0.07419) (accu: 0.9775)
[epoch : 22] (l_loss: 0.04924) (t_loss: 0.07407) (accu: 0.9776)
[epoch : 23] (l_loss: 0.04809) (t_loss: 0.08031) (accu: 0.9769)
[epoch : 24] (l_loss: 0.04941) (t_loss: 0.07761) (accu: 0.9751)
[epoch : 25] (l_loss: 0.04919) (t_loss: 0.07376) (accu: 0.9767)
[epoch : 26] (l_loss: 0.04739) (t_loss: 0.07244) (accu: 0.9790)
[epoch : 27] (l_loss: 0.04819) (t_loss: 0.07143) (accu: 0.9767)
[epoch : 28] (l_loss: 0.04701) (t_loss: 0.07975) (accu: 0.9743)
[epoch : 29] (l_loss: 0.04665) (t_loss: 0.07697) (accu: 0.9763)
[epoch : 30] (l_loss: 0.04752) (t_loss: 0.09165) (accu: 0.9703)
[epoch : 31] (l_loss: 0.04865) (t_loss: 0.07227) (accu: 0.9769)
[epoch : 32] (l_loss: 0.04714) (t_loss: 0.07564) (accu: 0.9759)
[epoch : 33] (l_loss: 0.04772) (t_loss: 0.06939) (accu: 0.9776)
[epoch : 34] (l_loss: 0.04679) (t_loss: 0.08019) (accu: 0.9753)
[epoch : 35] (l_loss: 0.04757) (t_loss: 0.07286) (accu: 0.9772)
[epoch : 36] (l_loss: 0.04675) (t_loss: 0.07379) (accu: 0.9763)
[epoch : 37] (l_loss: 0.04811) (t_loss: 0.08526) (accu: 0.9740)
[epoch : 38] (l_loss: 0.04634) (t_loss: 0.07361) (accu: 0.9774)
[epoch : 39] (l_loss: 0.04642) (t_loss: 0.07494) (accu: 0.9770)
[epoch : 40] (l_loss: 0.04725) (t_loss: 0.07145) (accu: 0.9775)
[epoch : 41] (l_loss: 0.04775) (t_loss: 0.07690) (accu: 0.9773)
[epoch : 42] (l_loss: 0.04639) (t_loss: 0.07646) (accu: 0.9762)
[epoch : 43] (l_loss: 0.04701) (t_loss: 0.06928) (accu: 0.9786)
[epoch : 44] (l_loss: 0.04551) (t_loss: 0.07750) (accu: 0.9772)
[epoch : 45] (l_loss: 0.04550) (t_loss: 0.07938) (accu: 0.9750)
[epoch : 46] (l_loss: 0.04664) (t_loss: 0.07021) (accu: 0.9776)
[epoch : 47] (l_loss: 0.04697) (t_loss: 0.08033) (accu: 0.9749)
[epoch : 48] (l_loss: 0.04640) (t_loss: 0.06910) (accu: 0.9779)
[epoch : 49] (l_loss: 0.04556) (t_loss: 0.06886) (accu: 0.9770)
[epoch : 50] (l_loss: 0.04638) (t_loss: 0.07505) (accu: 0.9768)
Finish! (Best accu: 0.9790) (Time taken(sec) : 637.66) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (18507 | 247693)          6.95
fc1.weight   :      235200 (16163 | 219037)          6.87
fc2.weight   :        30000 (2062 | 27938)           6.87
fcout.weight :          1000 (282 | 718)            28.20
------------------------------------------------------------

Learning start! [Prune_iter : (13/21), Remaining weight : 6.95 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30820) (accu: 0.1431)
[epoch : 1] (l_loss: 0.27899) (t_loss: 0.13451) (accu: 0.9605)
[epoch : 2] (l_loss: 0.11788) (t_loss: 0.10339) (accu: 0.9686)
[epoch : 3] (l_loss: 0.09324) (t_loss: 0.11675) (accu: 0.9659)
[epoch : 4] (l_loss: 0.08069) (t_loss: 0.08836) (accu: 0.9734)
[epoch : 5] (l_loss: 0.07230) (t_loss: 0.09342) (accu: 0.9690)
[epoch : 6] (l_loss: 0.06811) (t_loss: 0.07872) (accu: 0.9748)
[epoch : 7] (l_loss: 0.06377) (t_loss: 0.07778) (accu: 0.9758)
[epoch : 8] (l_loss: 0.06105) (t_loss: 0.08575) (accu: 0.9725)
[epoch : 9] (l_loss: 0.05822) (t_loss: 0.07874) (accu: 0.9754)
[epoch : 10] (l_loss: 0.05664) (t_loss: 0.07894) (accu: 0.9747)
[epoch : 11] (l_loss: 0.05470) (t_loss: 0.08727) (accu: 0.9741)
[epoch : 12] (l_loss: 0.05394) (t_loss: 0.08072) (accu: 0.9752)
[epoch : 13] (l_loss: 0.05164) (t_loss: 0.07651) (accu: 0.9773)
[epoch : 14] (l_loss: 0.05139) (t_loss: 0.07843) (accu: 0.9758)
[epoch : 15] (l_loss: 0.05260) (t_loss: 0.07573) (accu: 0.9755)
[epoch : 16] (l_loss: 0.05016) (t_loss: 0.08972) (accu: 0.9710)
[epoch : 17] (l_loss: 0.04983) (t_loss: 0.08145) (accu: 0.9742)
[epoch : 18] (l_loss: 0.04924) (t_loss: 0.07335) (accu: 0.9780)
[epoch : 19] (l_loss: 0.05024) (t_loss: 0.07228) (accu: 0.9763)
[epoch : 20] (l_loss: 0.04866) (t_loss: 0.07523) (accu: 0.9776)
[epoch : 21] (l_loss: 0.04844) (t_loss: 0.08042) (accu: 0.9748)
[epoch : 22] (l_loss: 0.04878) (t_loss: 0.07908) (accu: 0.9744)
[epoch : 23] (l_loss: 0.04936) (t_loss: 0.08450) (accu: 0.9737)
[epoch : 24] (l_loss: 0.04923) (t_loss: 0.07771) (accu: 0.9763)
[epoch : 25] (l_loss: 0.04729) (t_loss: 0.08905) (accu: 0.9721)
[epoch : 26] (l_loss: 0.04831) (t_loss: 0.07869) (accu: 0.9758)
[epoch : 27] (l_loss: 0.04726) (t_loss: 0.07693) (accu: 0.9769)
[epoch : 28] (l_loss: 0.04772) (t_loss: 0.07688) (accu: 0.9764)
[epoch : 29] (l_loss: 0.04834) (t_loss: 0.07071) (accu: 0.9776)
[epoch : 30] (l_loss: 0.04808) (t_loss: 0.07271) (accu: 0.9762)
[epoch : 31] (l_loss: 0.04690) (t_loss: 0.07296) (accu: 0.9763)
[epoch : 32] (l_loss: 0.04734) (t_loss: 0.07636) (accu: 0.9754)
[epoch : 33] (l_loss: 0.04653) (t_loss: 0.07699) (accu: 0.9783)
[epoch : 34] (l_loss: 0.04722) (t_loss: 0.07430) (accu: 0.9772)
[epoch : 35] (l_loss: 0.04661) (t_loss: 0.07486) (accu: 0.9776)
[epoch : 36] (l_loss: 0.04763) (t_loss: 0.07712) (accu: 0.9762)
[epoch : 37] (l_loss: 0.04592) (t_loss: 0.08005) (accu: 0.9750)
[epoch : 38] (l_loss: 0.04650) (t_loss: 0.07401) (accu: 0.9761)
[epoch : 39] (l_loss: 0.04709) (t_loss: 0.06990) (accu: 0.9763)
[epoch : 40] (l_loss: 0.04646) (t_loss: 0.07862) (accu: 0.9741)
[epoch : 41] (l_loss: 0.04648) (t_loss: 0.07830) (accu: 0.9741)
[epoch : 42] (l_loss: 0.04606) (t_loss: 0.08086) (accu: 0.9748)
[epoch : 43] (l_loss: 0.04543) (t_loss: 0.06633) (accu: 0.9796)
[epoch : 44] (l_loss: 0.04607) (t_loss: 0.07826) (accu: 0.9746)
[epoch : 45] (l_loss: 0.04740) (t_loss: 0.07023) (accu: 0.9777)
[epoch : 46] (l_loss: 0.04722) (t_loss: 0.07243) (accu: 0.9775)
[epoch : 47] (l_loss: 0.04568) (t_loss: 0.07464) (accu: 0.9771)
[epoch : 48] (l_loss: 0.04573) (t_loss: 0.07185) (accu: 0.9773)
[epoch : 49] (l_loss: 0.04600) (t_loss: 0.07411) (accu: 0.9777)
[epoch : 50] (l_loss: 0.04608) (t_loss: 0.07397) (accu: 0.9763)
Finish! (Best accu: 0.9796) (Time taken(sec) : 647.90) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (14833 | 251367)          5.57
fc1.weight   :      235200 (12930 | 222270)          5.50
fc2.weight   :        30000 (1649 | 28351)           5.50
fcout.weight :          1000 (254 | 746)            25.40
------------------------------------------------------------

Learning start! [Prune_iter : (14/21), Remaining weight : 5.57 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29786) (accu: 0.1511)
[epoch : 1] (l_loss: 0.28568) (t_loss: 0.12686) (accu: 0.9620)
[epoch : 2] (l_loss: 0.11714) (t_loss: 0.09542) (accu: 0.9707)
[epoch : 3] (l_loss: 0.09174) (t_loss: 0.09342) (accu: 0.9718)
[epoch : 4] (l_loss: 0.07773) (t_loss: 0.08200) (accu: 0.9748)
[epoch : 5] (l_loss: 0.06956) (t_loss: 0.09047) (accu: 0.9715)
[epoch : 6] (l_loss: 0.06543) (t_loss: 0.08055) (accu: 0.9758)
[epoch : 7] (l_loss: 0.06134) (t_loss: 0.07638) (accu: 0.9769)
[epoch : 8] (l_loss: 0.05941) (t_loss: 0.08585) (accu: 0.9735)
[epoch : 9] (l_loss: 0.05653) (t_loss: 0.08373) (accu: 0.9725)
[epoch : 10] (l_loss: 0.05501) (t_loss: 0.07059) (accu: 0.9776)
[epoch : 11] (l_loss: 0.05479) (t_loss: 0.08639) (accu: 0.9713)
[epoch : 12] (l_loss: 0.05242) (t_loss: 0.08241) (accu: 0.9742)
[epoch : 13] (l_loss: 0.05284) (t_loss: 0.06976) (accu: 0.9772)
[epoch : 14] (l_loss: 0.05066) (t_loss: 0.07982) (accu: 0.9747)
[epoch : 15] (l_loss: 0.05083) (t_loss: 0.07392) (accu: 0.9770)
[epoch : 16] (l_loss: 0.04989) (t_loss: 0.07219) (accu: 0.9776)
[epoch : 17] (l_loss: 0.05107) (t_loss: 0.07927) (accu: 0.9759)
[epoch : 18] (l_loss: 0.04864) (t_loss: 0.07878) (accu: 0.9753)
[epoch : 19] (l_loss: 0.04859) (t_loss: 0.07274) (accu: 0.9766)
[epoch : 20] (l_loss: 0.04947) (t_loss: 0.07674) (accu: 0.9753)
[epoch : 21] (l_loss: 0.04741) (t_loss: 0.07730) (accu: 0.9767)
[epoch : 22] (l_loss: 0.04858) (t_loss: 0.07975) (accu: 0.9744)
[epoch : 23] (l_loss: 0.04883) (t_loss: 0.08174) (accu: 0.9740)
[epoch : 24] (l_loss: 0.04697) (t_loss: 0.07398) (accu: 0.9759)
[epoch : 25] (l_loss: 0.04898) (t_loss: 0.07647) (accu: 0.9760)
[epoch : 26] (l_loss: 0.04770) (t_loss: 0.07916) (accu: 0.9759)
[epoch : 27] (l_loss: 0.04771) (t_loss: 0.07582) (accu: 0.9757)
[epoch : 28] (l_loss: 0.04659) (t_loss: 0.06871) (accu: 0.9778)
[epoch : 29] (l_loss: 0.04665) (t_loss: 0.07506) (accu: 0.9782)
[epoch : 30] (l_loss: 0.04645) (t_loss: 0.07837) (accu: 0.9740)
[epoch : 31] (l_loss: 0.04672) (t_loss: 0.07101) (accu: 0.9778)
[epoch : 32] (l_loss: 0.04689) (t_loss: 0.07342) (accu: 0.9765)
[epoch : 33] (l_loss: 0.04644) (t_loss: 0.07523) (accu: 0.9761)
[epoch : 34] (l_loss: 0.04644) (t_loss: 0.07961) (accu: 0.9757)
[epoch : 35] (l_loss: 0.04665) (t_loss: 0.07401) (accu: 0.9779)
[epoch : 36] (l_loss: 0.04508) (t_loss: 0.07302) (accu: 0.9774)
[epoch : 37] (l_loss: 0.04641) (t_loss: 0.07358) (accu: 0.9769)
[epoch : 38] (l_loss: 0.04626) (t_loss: 0.07219) (accu: 0.9769)
[epoch : 39] (l_loss: 0.04605) (t_loss: 0.07422) (accu: 0.9776)
[epoch : 40] (l_loss: 0.04440) (t_loss: 0.07581) (accu: 0.9763)
[epoch : 41] (l_loss: 0.04577) (t_loss: 0.07347) (accu: 0.9772)
[epoch : 42] (l_loss: 0.04572) (t_loss: 0.07132) (accu: 0.9759)
[epoch : 43] (l_loss: 0.04495) (t_loss: 0.07315) (accu: 0.9765)
[epoch : 44] (l_loss: 0.04460) (t_loss: 0.07699) (accu: 0.9751)
[epoch : 45] (l_loss: 0.04529) (t_loss: 0.07723) (accu: 0.9749)
[epoch : 46] (l_loss: 0.04538) (t_loss: 0.06613) (accu: 0.9794)
[epoch : 47] (l_loss: 0.04495) (t_loss: 0.08008) (accu: 0.9758)
[epoch : 48] (l_loss: 0.04497) (t_loss: 0.08159) (accu: 0.9748)
[epoch : 49] (l_loss: 0.04514) (t_loss: 0.07715) (accu: 0.9783)
[epoch : 50] (l_loss: 0.04455) (t_loss: 0.06966) (accu: 0.9773)
Finish! (Best accu: 0.9794) (Time taken(sec) : 663.20) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (11892 | 254308)          4.47
fc1.weight   :      235200 (10344 | 224856)          4.40
fc2.weight   :        30000 (1319 | 28681)           4.40
fcout.weight :          1000 (229 | 771)            22.90
------------------------------------------------------------

Learning start! [Prune_iter : (15/21), Remaining weight : 4.47 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.27319) (accu: 0.1697)
[epoch : 1] (l_loss: 0.29058) (t_loss: 0.13391) (accu: 0.9592)
[epoch : 2] (l_loss: 0.11486) (t_loss: 0.10556) (accu: 0.9697)
[epoch : 3] (l_loss: 0.08867) (t_loss: 0.08854) (accu: 0.9714)
[epoch : 4] (l_loss: 0.07500) (t_loss: 0.08722) (accu: 0.9733)
[epoch : 5] (l_loss: 0.06888) (t_loss: 0.07715) (accu: 0.9763)
[epoch : 6] (l_loss: 0.06264) (t_loss: 0.08203) (accu: 0.9760)
[epoch : 7] (l_loss: 0.05964) (t_loss: 0.07847) (accu: 0.9754)
[epoch : 8] (l_loss: 0.05737) (t_loss: 0.08007) (accu: 0.9741)
[epoch : 9] (l_loss: 0.05547) (t_loss: 0.07839) (accu: 0.9756)
[epoch : 10] (l_loss: 0.05386) (t_loss: 0.07931) (accu: 0.9750)
[epoch : 11] (l_loss: 0.05225) (t_loss: 0.07144) (accu: 0.9788)
[epoch : 12] (l_loss: 0.05136) (t_loss: 0.07663) (accu: 0.9769)
[epoch : 13] (l_loss: 0.05087) (t_loss: 0.07224) (accu: 0.9775)
[epoch : 14] (l_loss: 0.04967) (t_loss: 0.07535) (accu: 0.9789)
[epoch : 15] (l_loss: 0.04816) (t_loss: 0.07040) (accu: 0.9773)
[epoch : 16] (l_loss: 0.04796) (t_loss: 0.08044) (accu: 0.9752)
[epoch : 17] (l_loss: 0.04696) (t_loss: 0.07697) (accu: 0.9761)
[epoch : 18] (l_loss: 0.04673) (t_loss: 0.07451) (accu: 0.9772)
[epoch : 19] (l_loss: 0.04599) (t_loss: 0.08223) (accu: 0.9750)
[epoch : 20] (l_loss: 0.04618) (t_loss: 0.07535) (accu: 0.9767)
[epoch : 21] (l_loss: 0.04514) (t_loss: 0.07543) (accu: 0.9756)
[epoch : 22] (l_loss: 0.04529) (t_loss: 0.07234) (accu: 0.9775)
[epoch : 23] (l_loss: 0.04446) (t_loss: 0.07531) (accu: 0.9759)
[epoch : 24] (l_loss: 0.04408) (t_loss: 0.07012) (accu: 0.9791)
[epoch : 25] (l_loss: 0.04407) (t_loss: 0.06837) (accu: 0.9782)
[epoch : 26] (l_loss: 0.04454) (t_loss: 0.07276) (accu: 0.9765)
[epoch : 27] (l_loss: 0.04411) (t_loss: 0.08502) (accu: 0.9739)
[epoch : 28] (l_loss: 0.04368) (t_loss: 0.07381) (accu: 0.9762)
[epoch : 29] (l_loss: 0.04366) (t_loss: 0.07121) (accu: 0.9770)
[epoch : 30] (l_loss: 0.04362) (t_loss: 0.06735) (accu: 0.9789)
[epoch : 31] (l_loss: 0.04347) (t_loss: 0.07508) (accu: 0.9761)
[epoch : 32] (l_loss: 0.04274) (t_loss: 0.07162) (accu: 0.9778)
[epoch : 33] (l_loss: 0.04322) (t_loss: 0.07442) (accu: 0.9781)
[epoch : 34] (l_loss: 0.04353) (t_loss: 0.07339) (accu: 0.9781)
[epoch : 35] (l_loss: 0.04294) (t_loss: 0.07859) (accu: 0.9746)
[epoch : 36] (l_loss: 0.04288) (t_loss: 0.07331) (accu: 0.9775)
[epoch : 37] (l_loss: 0.04264) (t_loss: 0.07193) (accu: 0.9774)
[epoch : 38] (l_loss: 0.04267) (t_loss: 0.07191) (accu: 0.9778)
[epoch : 39] (l_loss: 0.04279) (t_loss: 0.07576) (accu: 0.9752)
[epoch : 40] (l_loss: 0.04355) (t_loss: 0.07436) (accu: 0.9767)
[epoch : 41] (l_loss: 0.04270) (t_loss: 0.07441) (accu: 0.9756)
[epoch : 42] (l_loss: 0.04224) (t_loss: 0.07074) (accu: 0.9777)
[epoch : 43] (l_loss: 0.04234) (t_loss: 0.07325) (accu: 0.9778)
[epoch : 44] (l_loss: 0.04183) (t_loss: 0.07954) (accu: 0.9740)
[epoch : 45] (l_loss: 0.04211) (t_loss: 0.07945) (accu: 0.9739)
[epoch : 46] (l_loss: 0.04272) (t_loss: 0.06956) (accu: 0.9786)
[epoch : 47] (l_loss: 0.04242) (t_loss: 0.07674) (accu: 0.9758)
[epoch : 48] (l_loss: 0.04117) (t_loss: 0.07919) (accu: 0.9759)
[epoch : 49] (l_loss: 0.04177) (t_loss: 0.06568) (accu: 0.9804)
[epoch : 50] (l_loss: 0.04314) (t_loss: 0.07605) (accu: 0.9763)
Finish! (Best accu: 0.9804) (Time taken(sec) : 662.56) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (9537 | 256663)          3.58
fc1.weight   :       235200 (8275 | 226925)          3.52
fc2.weight   :        30000 (1056 | 28944)           3.52
fcout.weight :          1000 (206 | 794)            20.60
------------------------------------------------------------

Learning start! [Prune_iter : (16/21), Remaining weight : 3.58 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.27933) (accu: 0.1558)
[epoch : 1] (l_loss: 0.30262) (t_loss: 0.13681) (accu: 0.9599)
[epoch : 2] (l_loss: 0.11382) (t_loss: 0.10610) (accu: 0.9665)
[epoch : 3] (l_loss: 0.08562) (t_loss: 0.08668) (accu: 0.9737)
[epoch : 4] (l_loss: 0.07300) (t_loss: 0.08540) (accu: 0.9749)
[epoch : 5] (l_loss: 0.06559) (t_loss: 0.08210) (accu: 0.9744)
[epoch : 6] (l_loss: 0.06095) (t_loss: 0.07856) (accu: 0.9757)
[epoch : 7] (l_loss: 0.05479) (t_loss: 0.07595) (accu: 0.9771)
[epoch : 8] (l_loss: 0.05253) (t_loss: 0.08151) (accu: 0.9740)
[epoch : 9] (l_loss: 0.05010) (t_loss: 0.08234) (accu: 0.9719)
[epoch : 10] (l_loss: 0.04871) (t_loss: 0.07554) (accu: 0.9773)
[epoch : 11] (l_loss: 0.04693) (t_loss: 0.07688) (accu: 0.9757)
[epoch : 12] (l_loss: 0.04536) (t_loss: 0.07229) (accu: 0.9786)
[epoch : 13] (l_loss: 0.04391) (t_loss: 0.07668) (accu: 0.9751)
[epoch : 14] (l_loss: 0.04313) (t_loss: 0.07072) (accu: 0.9783)
[epoch : 15] (l_loss: 0.04372) (t_loss: 0.06976) (accu: 0.9778)
[epoch : 16] (l_loss: 0.04357) (t_loss: 0.07513) (accu: 0.9794)
[epoch : 17] (l_loss: 0.04276) (t_loss: 0.06985) (accu: 0.9783)
[epoch : 18] (l_loss: 0.04197) (t_loss: 0.06567) (accu: 0.9783)
[epoch : 19] (l_loss: 0.04123) (t_loss: 0.07947) (accu: 0.9750)
[epoch : 20] (l_loss: 0.04125) (t_loss: 0.06825) (accu: 0.9780)
[epoch : 21] (l_loss: 0.04124) (t_loss: 0.06910) (accu: 0.9783)
[epoch : 22] (l_loss: 0.04181) (t_loss: 0.07222) (accu: 0.9782)
[epoch : 23] (l_loss: 0.04043) (t_loss: 0.07009) (accu: 0.9769)
[epoch : 24] (l_loss: 0.04151) (t_loss: 0.07074) (accu: 0.9765)
[epoch : 25] (l_loss: 0.04121) (t_loss: 0.07735) (accu: 0.9763)
[epoch : 26] (l_loss: 0.04101) (t_loss: 0.08020) (accu: 0.9751)
[epoch : 27] (l_loss: 0.04067) (t_loss: 0.06930) (accu: 0.9785)
[epoch : 28] (l_loss: 0.04039) (t_loss: 0.07104) (accu: 0.9790)
[epoch : 29] (l_loss: 0.04127) (t_loss: 0.06714) (accu: 0.9788)
[epoch : 30] (l_loss: 0.04015) (t_loss: 0.07154) (accu: 0.9774)
[epoch : 31] (l_loss: 0.04024) (t_loss: 0.07277) (accu: 0.9766)
[epoch : 32] (l_loss: 0.04091) (t_loss: 0.07114) (accu: 0.9761)
[epoch : 33] (l_loss: 0.04060) (t_loss: 0.06876) (accu: 0.9791)
[epoch : 34] (l_loss: 0.04033) (t_loss: 0.07625) (accu: 0.9762)
[epoch : 35] (l_loss: 0.04027) (t_loss: 0.07241) (accu: 0.9774)
[epoch : 36] (l_loss: 0.04019) (t_loss: 0.07112) (accu: 0.9781)
[epoch : 37] (l_loss: 0.04071) (t_loss: 0.07575) (accu: 0.9762)
[epoch : 38] (l_loss: 0.04007) (t_loss: 0.07268) (accu: 0.9780)
[epoch : 39] (l_loss: 0.04087) (t_loss: 0.07481) (accu: 0.9766)
[epoch : 40] (l_loss: 0.04050) (t_loss: 0.07779) (accu: 0.9751)
[epoch : 41] (l_loss: 0.04093) (t_loss: 0.07023) (accu: 0.9773)
[epoch : 42] (l_loss: 0.04084) (t_loss: 0.07324) (accu: 0.9771)
[epoch : 43] (l_loss: 0.04026) (t_loss: 0.07326) (accu: 0.9772)
[epoch : 44] (l_loss: 0.04041) (t_loss: 0.07209) (accu: 0.9778)
[epoch : 45] (l_loss: 0.04098) (t_loss: 0.07195) (accu: 0.9787)
[epoch : 46] (l_loss: 0.03986) (t_loss: 0.07106) (accu: 0.9789)
[epoch : 47] (l_loss: 0.04029) (t_loss: 0.08008) (accu: 0.9758)
[epoch : 48] (l_loss: 0.04026) (t_loss: 0.07652) (accu: 0.9751)
[epoch : 49] (l_loss: 0.04076) (t_loss: 0.06903) (accu: 0.9786)
[epoch : 50] (l_loss: 0.03958) (t_loss: 0.07394) (accu: 0.9760)
Finish! (Best accu: 0.9794) (Time taken(sec) : 654.91) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (7649 | 258551)          2.87
fc1.weight   :       235200 (6620 | 228580)          2.81
fc2.weight   :        30000 (844 | 29156)            2.81
fcout.weight :          1000 (185 | 815)            18.50
------------------------------------------------------------

Learning start! [Prune_iter : (17/21), Remaining weight : 2.87 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.27466) (accu: 0.1680)
[epoch : 1] (l_loss: 0.30906) (t_loss: 0.13216) (accu: 0.9633)
[epoch : 2] (l_loss: 0.11004) (t_loss: 0.09957) (accu: 0.9682)
[epoch : 3] (l_loss: 0.08169) (t_loss: 0.08464) (accu: 0.9740)
[epoch : 4] (l_loss: 0.06819) (t_loss: 0.08348) (accu: 0.9720)
[epoch : 5] (l_loss: 0.06164) (t_loss: 0.07628) (accu: 0.9754)
[epoch : 6] (l_loss: 0.05660) (t_loss: 0.07898) (accu: 0.9763)
[epoch : 7] (l_loss: 0.05232) (t_loss: 0.07313) (accu: 0.9773)
[epoch : 8] (l_loss: 0.05013) (t_loss: 0.07405) (accu: 0.9769)
[epoch : 9] (l_loss: 0.04778) (t_loss: 0.07401) (accu: 0.9780)
[epoch : 10] (l_loss: 0.04689) (t_loss: 0.07587) (accu: 0.9754)
[epoch : 11] (l_loss: 0.04547) (t_loss: 0.07401) (accu: 0.9769)
[epoch : 12] (l_loss: 0.04460) (t_loss: 0.07725) (accu: 0.9762)
[epoch : 13] (l_loss: 0.04455) (t_loss: 0.07022) (accu: 0.9784)
[epoch : 14] (l_loss: 0.04335) (t_loss: 0.07254) (accu: 0.9775)
[epoch : 15] (l_loss: 0.04229) (t_loss: 0.07360) (accu: 0.9777)
[epoch : 16] (l_loss: 0.04260) (t_loss: 0.07179) (accu: 0.9767)
[epoch : 17] (l_loss: 0.04245) (t_loss: 0.06792) (accu: 0.9773)
[epoch : 18] (l_loss: 0.04216) (t_loss: 0.07282) (accu: 0.9767)
[epoch : 19] (l_loss: 0.04161) (t_loss: 0.06906) (accu: 0.9780)
[epoch : 20] (l_loss: 0.04107) (t_loss: 0.07147) (accu: 0.9769)
[epoch : 21] (l_loss: 0.04114) (t_loss: 0.07165) (accu: 0.9761)
[epoch : 22] (l_loss: 0.04103) (t_loss: 0.06870) (accu: 0.9797)
[epoch : 23] (l_loss: 0.04132) (t_loss: 0.07010) (accu: 0.9781)
[epoch : 24] (l_loss: 0.04060) (t_loss: 0.07013) (accu: 0.9801)
[epoch : 25] (l_loss: 0.04038) (t_loss: 0.06726) (accu: 0.9791)
[epoch : 26] (l_loss: 0.04076) (t_loss: 0.06970) (accu: 0.9785)
[epoch : 27] (l_loss: 0.04099) (t_loss: 0.06691) (accu: 0.9792)
[epoch : 28] (l_loss: 0.03947) (t_loss: 0.06881) (accu: 0.9785)
[epoch : 29] (l_loss: 0.04083) (t_loss: 0.07072) (accu: 0.9786)
[epoch : 30] (l_loss: 0.03987) (t_loss: 0.07032) (accu: 0.9782)
[epoch : 31] (l_loss: 0.04102) (t_loss: 0.06580) (accu: 0.9778)
[epoch : 32] (l_loss: 0.04040) (t_loss: 0.07523) (accu: 0.9764)
[epoch : 33] (l_loss: 0.04033) (t_loss: 0.07211) (accu: 0.9766)
[epoch : 34] (l_loss: 0.03980) (t_loss: 0.07355) (accu: 0.9774)
[epoch : 35] (l_loss: 0.04008) (t_loss: 0.07263) (accu: 0.9764)
[epoch : 36] (l_loss: 0.04004) (t_loss: 0.07286) (accu: 0.9768)
[epoch : 37] (l_loss: 0.04025) (t_loss: 0.07116) (accu: 0.9777)
[epoch : 38] (l_loss: 0.04038) (t_loss: 0.06867) (accu: 0.9786)
[epoch : 39] (l_loss: 0.04054) (t_loss: 0.07110) (accu: 0.9774)
[epoch : 40] (l_loss: 0.04078) (t_loss: 0.06854) (accu: 0.9778)
[epoch : 41] (l_loss: 0.03949) (t_loss: 0.06538) (accu: 0.9799)
[epoch : 42] (l_loss: 0.04047) (t_loss: 0.07518) (accu: 0.9775)
[epoch : 43] (l_loss: 0.03996) (t_loss: 0.06679) (accu: 0.9786)
[epoch : 44] (l_loss: 0.04048) (t_loss: 0.06551) (accu: 0.9794)
[epoch : 45] (l_loss: 0.03994) (t_loss: 0.06568) (accu: 0.9793)
[epoch : 46] (l_loss: 0.03979) (t_loss: 0.06840) (accu: 0.9788)
[epoch : 47] (l_loss: 0.04040) (t_loss: 0.06862) (accu: 0.9784)
[epoch : 48] (l_loss: 0.03979) (t_loss: 0.06881) (accu: 0.9783)
[epoch : 49] (l_loss: 0.03900) (t_loss: 0.07025) (accu: 0.9776)
[epoch : 50] (l_loss: 0.04061) (t_loss: 0.07056) (accu: 0.9771)
Finish! (Best accu: 0.9801) (Time taken(sec) : 649.42) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (6139 | 260061)          2.31
fc1.weight   :       235200 (5296 | 229904)          2.25
fc2.weight   :        30000 (676 | 29324)            2.25
fcout.weight :          1000 (167 | 833)            16.70
------------------------------------------------------------

Learning start! [Prune_iter : (18/21), Remaining weight : 2.31 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.27802) (accu: 0.1311)
[epoch : 1] (l_loss: 0.32257) (t_loss: 0.13461) (accu: 0.9612)
[epoch : 2] (l_loss: 0.11305) (t_loss: 0.10477) (accu: 0.9680)
[epoch : 3] (l_loss: 0.08531) (t_loss: 0.08660) (accu: 0.9738)
[epoch : 4] (l_loss: 0.07047) (t_loss: 0.08140) (accu: 0.9755)
[epoch : 5] (l_loss: 0.06092) (t_loss: 0.07295) (accu: 0.9782)
[epoch : 6] (l_loss: 0.05457) (t_loss: 0.07424) (accu: 0.9780)
[epoch : 7] (l_loss: 0.05046) (t_loss: 0.06876) (accu: 0.9789)
[epoch : 8] (l_loss: 0.04611) (t_loss: 0.07467) (accu: 0.9765)
[epoch : 9] (l_loss: 0.04488) (t_loss: 0.06699) (accu: 0.9783)
[epoch : 10] (l_loss: 0.04236) (t_loss: 0.06621) (accu: 0.9785)
[epoch : 11] (l_loss: 0.04176) (t_loss: 0.07517) (accu: 0.9761)
[epoch : 12] (l_loss: 0.04150) (t_loss: 0.06922) (accu: 0.9783)
[epoch : 13] (l_loss: 0.04005) (t_loss: 0.06702) (accu: 0.9785)
[epoch : 14] (l_loss: 0.03991) (t_loss: 0.07770) (accu: 0.9760)
[epoch : 15] (l_loss: 0.03922) (t_loss: 0.06729) (accu: 0.9788)
[epoch : 16] (l_loss: 0.03943) (t_loss: 0.07111) (accu: 0.9777)
[epoch : 17] (l_loss: 0.03959) (t_loss: 0.07017) (accu: 0.9781)
[epoch : 18] (l_loss: 0.03865) (t_loss: 0.06766) (accu: 0.9801)
[epoch : 19] (l_loss: 0.03922) (t_loss: 0.06911) (accu: 0.9779)
[epoch : 20] (l_loss: 0.03914) (t_loss: 0.06924) (accu: 0.9777)
[epoch : 21] (l_loss: 0.03863) (t_loss: 0.06819) (accu: 0.9792)
[epoch : 22] (l_loss: 0.03887) (t_loss: 0.06616) (accu: 0.9789)
[epoch : 23] (l_loss: 0.03890) (t_loss: 0.07013) (accu: 0.9777)
[epoch : 24] (l_loss: 0.03853) (t_loss: 0.07336) (accu: 0.9768)
[epoch : 25] (l_loss: 0.03949) (t_loss: 0.06803) (accu: 0.9799)
[epoch : 26] (l_loss: 0.03861) (t_loss: 0.06848) (accu: 0.9779)
[epoch : 27] (l_loss: 0.03817) (t_loss: 0.06837) (accu: 0.9782)
[epoch : 28] (l_loss: 0.03796) (t_loss: 0.07085) (accu: 0.9777)
[epoch : 29] (l_loss: 0.03831) (t_loss: 0.06903) (accu: 0.9791)
[epoch : 30] (l_loss: 0.03888) (t_loss: 0.08016) (accu: 0.9745)
[epoch : 31] (l_loss: 0.03804) (t_loss: 0.06727) (accu: 0.9784)
[epoch : 32] (l_loss: 0.03879) (t_loss: 0.07121) (accu: 0.9784)
[epoch : 33] (l_loss: 0.03795) (t_loss: 0.06940) (accu: 0.9784)
[epoch : 34] (l_loss: 0.03863) (t_loss: 0.06792) (accu: 0.9790)
[epoch : 35] (l_loss: 0.03811) (t_loss: 0.06768) (accu: 0.9778)
[epoch : 36] (l_loss: 0.03877) (t_loss: 0.06844) (accu: 0.9784)
[epoch : 37] (l_loss: 0.03795) (t_loss: 0.06982) (accu: 0.9782)
[epoch : 38] (l_loss: 0.03860) (t_loss: 0.07152) (accu: 0.9768)
[epoch : 39] (l_loss: 0.03831) (t_loss: 0.07296) (accu: 0.9771)
[epoch : 40] (l_loss: 0.03833) (t_loss: 0.07013) (accu: 0.9779)
[epoch : 41] (l_loss: 0.03940) (t_loss: 0.06744) (accu: 0.9782)
[epoch : 42] (l_loss: 0.03842) (t_loss: 0.07306) (accu: 0.9767)
[epoch : 43] (l_loss: 0.03856) (t_loss: 0.07185) (accu: 0.9762)
[epoch : 44] (l_loss: 0.03808) (t_loss: 0.06990) (accu: 0.9783)
[epoch : 45] (l_loss: 0.03875) (t_loss: 0.06844) (accu: 0.9779)
[epoch : 46] (l_loss: 0.03867) (t_loss: 0.07019) (accu: 0.9783)
[epoch : 47] (l_loss: 0.03778) (t_loss: 0.06885) (accu: 0.9782)
[epoch : 48] (l_loss: 0.03778) (t_loss: 0.06908) (accu: 0.9783)
[epoch : 49] (l_loss: 0.03897) (t_loss: 0.07164) (accu: 0.9776)
[epoch : 50] (l_loss: 0.03817) (t_loss: 0.07109) (accu: 0.9765)
Finish! (Best accu: 0.9801) (Time taken(sec) : 656.99) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (4927 | 261273)          1.85
fc1.weight   :       235200 (4237 | 230963)          1.80
fc2.weight   :        30000 (540 | 29460)            1.80
fcout.weight :          1000 (150 | 850)            15.00
------------------------------------------------------------

Learning start! [Prune_iter : (19/21), Remaining weight : 1.85 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29043) (accu: 0.1215)
[epoch : 1] (l_loss: 0.34787) (t_loss: 0.14551) (accu: 0.9599)
[epoch : 2] (l_loss: 0.12210) (t_loss: 0.10815) (accu: 0.9685)
[epoch : 3] (l_loss: 0.09226) (t_loss: 0.09197) (accu: 0.9727)
[epoch : 4] (l_loss: 0.07563) (t_loss: 0.08693) (accu: 0.9736)
[epoch : 5] (l_loss: 0.06507) (t_loss: 0.08374) (accu: 0.9747)
[epoch : 6] (l_loss: 0.05853) (t_loss: 0.08113) (accu: 0.9750)
[epoch : 7] (l_loss: 0.05337) (t_loss: 0.07242) (accu: 0.9776)
[epoch : 8] (l_loss: 0.05013) (t_loss: 0.07300) (accu: 0.9762)
[epoch : 9] (l_loss: 0.04857) (t_loss: 0.07445) (accu: 0.9777)
[epoch : 10] (l_loss: 0.04521) (t_loss: 0.07071) (accu: 0.9779)
[epoch : 11] (l_loss: 0.04512) (t_loss: 0.07311) (accu: 0.9773)
[epoch : 12] (l_loss: 0.04381) (t_loss: 0.07360) (accu: 0.9771)
[epoch : 13] (l_loss: 0.04290) (t_loss: 0.07221) (accu: 0.9770)
[epoch : 14] (l_loss: 0.04237) (t_loss: 0.07230) (accu: 0.9768)
[epoch : 15] (l_loss: 0.04208) (t_loss: 0.07229) (accu: 0.9772)
[epoch : 16] (l_loss: 0.04138) (t_loss: 0.07260) (accu: 0.9775)
[epoch : 17] (l_loss: 0.04121) (t_loss: 0.06974) (accu: 0.9780)
[epoch : 18] (l_loss: 0.04049) (t_loss: 0.07121) (accu: 0.9776)
[epoch : 19] (l_loss: 0.03989) (t_loss: 0.07140) (accu: 0.9776)
[epoch : 20] (l_loss: 0.04035) (t_loss: 0.06718) (accu: 0.9788)
[epoch : 21] (l_loss: 0.03982) (t_loss: 0.07085) (accu: 0.9791)
[epoch : 22] (l_loss: 0.03978) (t_loss: 0.07438) (accu: 0.9759)
[epoch : 23] (l_loss: 0.03984) (t_loss: 0.07170) (accu: 0.9773)
[epoch : 24] (l_loss: 0.04042) (t_loss: 0.06927) (accu: 0.9764)
[epoch : 25] (l_loss: 0.03998) (t_loss: 0.07034) (accu: 0.9775)
[epoch : 26] (l_loss: 0.03956) (t_loss: 0.07225) (accu: 0.9774)
[epoch : 27] (l_loss: 0.03953) (t_loss: 0.07626) (accu: 0.9761)
[epoch : 28] (l_loss: 0.03995) (t_loss: 0.06993) (accu: 0.9788)
[epoch : 29] (l_loss: 0.03928) (t_loss: 0.06996) (accu: 0.9782)
[epoch : 30] (l_loss: 0.03941) (t_loss: 0.06646) (accu: 0.9804)
[epoch : 31] (l_loss: 0.03938) (t_loss: 0.07271) (accu: 0.9767)
[epoch : 32] (l_loss: 0.03913) (t_loss: 0.07233) (accu: 0.9771)
[epoch : 33] (l_loss: 0.03987) (t_loss: 0.07069) (accu: 0.9772)
[epoch : 34] (l_loss: 0.03935) (t_loss: 0.07282) (accu: 0.9782)
[epoch : 35] (l_loss: 0.03978) (t_loss: 0.06943) (accu: 0.9771)
[epoch : 36] (l_loss: 0.03952) (t_loss: 0.07300) (accu: 0.9766)
[epoch : 37] (l_loss: 0.03928) (t_loss: 0.07207) (accu: 0.9773)
[epoch : 38] (l_loss: 0.04015) (t_loss: 0.06961) (accu: 0.9787)
[epoch : 39] (l_loss: 0.03962) (t_loss: 0.07402) (accu: 0.9761)
[epoch : 40] (l_loss: 0.03929) (t_loss: 0.07213) (accu: 0.9784)
[epoch : 41] (l_loss: 0.03965) (t_loss: 0.06741) (accu: 0.9772)
[epoch : 42] (l_loss: 0.03872) (t_loss: 0.07031) (accu: 0.9778)
[epoch : 43] (l_loss: 0.03841) (t_loss: 0.07210) (accu: 0.9775)
[epoch : 44] (l_loss: 0.03828) (t_loss: 0.07460) (accu: 0.9762)
[epoch : 45] (l_loss: 0.03852) (t_loss: 0.07322) (accu: 0.9777)
[epoch : 46] (l_loss: 0.03866) (t_loss: 0.06973) (accu: 0.9786)
[epoch : 47] (l_loss: 0.03815) (t_loss: 0.06785) (accu: 0.9770)
[epoch : 48] (l_loss: 0.03857) (t_loss: 0.07102) (accu: 0.9785)
[epoch : 49] (l_loss: 0.03879) (t_loss: 0.07178) (accu: 0.9781)
[epoch : 50] (l_loss: 0.03833) (t_loss: 0.07048) (accu: 0.9764)
Finish! (Best accu: 0.9804) (Time taken(sec) : 654.94) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (3957 | 262243)          1.49
fc1.weight   :       235200 (3390 | 231810)          1.44
fc2.weight   :        30000 (432 | 29568)            1.44
fcout.weight :          1000 (135 | 865)            13.50
------------------------------------------------------------

Learning start! [Prune_iter : (20/21), Remaining weight : 1.49 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.28787) (accu: 0.1021)
[epoch : 1] (l_loss: 0.36635) (t_loss: 0.14563) (accu: 0.9590)
[epoch : 2] (l_loss: 0.11966) (t_loss: 0.10699) (accu: 0.9685)
[epoch : 3] (l_loss: 0.09162) (t_loss: 0.09162) (accu: 0.9723)
[epoch : 4] (l_loss: 0.07810) (t_loss: 0.08580) (accu: 0.9745)
[epoch : 5] (l_loss: 0.06847) (t_loss: 0.08052) (accu: 0.9741)
[epoch : 6] (l_loss: 0.06298) (t_loss: 0.07924) (accu: 0.9743)
[epoch : 7] (l_loss: 0.05722) (t_loss: 0.07823) (accu: 0.9755)
[epoch : 8] (l_loss: 0.05417) (t_loss: 0.07295) (accu: 0.9776)
[epoch : 9] (l_loss: 0.05186) (t_loss: 0.07356) (accu: 0.9765)
[epoch : 10] (l_loss: 0.04963) (t_loss: 0.07161) (accu: 0.9781)
[epoch : 11] (l_loss: 0.04798) (t_loss: 0.07237) (accu: 0.9765)
[epoch : 12] (l_loss: 0.04730) (t_loss: 0.07538) (accu: 0.9762)
[epoch : 13] (l_loss: 0.04605) (t_loss: 0.06990) (accu: 0.9792)
[epoch : 14] (l_loss: 0.04478) (t_loss: 0.07095) (accu: 0.9765)
[epoch : 15] (l_loss: 0.04488) (t_loss: 0.07193) (accu: 0.9786)
[epoch : 16] (l_loss: 0.04498) (t_loss: 0.06884) (accu: 0.9779)
[epoch : 17] (l_loss: 0.04454) (t_loss: 0.07354) (accu: 0.9771)
[epoch : 18] (l_loss: 0.04397) (t_loss: 0.07095) (accu: 0.9763)
[epoch : 19] (l_loss: 0.04358) (t_loss: 0.07226) (accu: 0.9770)
[epoch : 20] (l_loss: 0.04323) (t_loss: 0.06818) (accu: 0.9788)
[epoch : 21] (l_loss: 0.04315) (t_loss: 0.06960) (accu: 0.9785)
[epoch : 22] (l_loss: 0.04313) (t_loss: 0.06981) (accu: 0.9780)
[epoch : 23] (l_loss: 0.04275) (t_loss: 0.06910) (accu: 0.9781)
[epoch : 24] (l_loss: 0.04284) (t_loss: 0.06945) (accu: 0.9782)
[epoch : 25] (l_loss: 0.04236) (t_loss: 0.07093) (accu: 0.9783)
[epoch : 26] (l_loss: 0.04225) (t_loss: 0.06638) (accu: 0.9781)
[epoch : 27] (l_loss: 0.04230) (t_loss: 0.07058) (accu: 0.9786)
[epoch : 28] (l_loss: 0.04196) (t_loss: 0.07022) (accu: 0.9790)
[epoch : 29] (l_loss: 0.04249) (t_loss: 0.07416) (accu: 0.9771)
[epoch : 30] (l_loss: 0.04182) (t_loss: 0.06951) (accu: 0.9781)
[epoch : 31] (l_loss: 0.04212) (t_loss: 0.07108) (accu: 0.9767)
[epoch : 32] (l_loss: 0.04202) (t_loss: 0.07081) (accu: 0.9782)
[epoch : 33] (l_loss: 0.04213) (t_loss: 0.06998) (accu: 0.9794)
[epoch : 34] (l_loss: 0.04224) (t_loss: 0.07190) (accu: 0.9772)
[epoch : 35] (l_loss: 0.04153) (t_loss: 0.06862) (accu: 0.9785)
[epoch : 36] (l_loss: 0.04250) (t_loss: 0.07165) (accu: 0.9769)
[epoch : 37] (l_loss: 0.04199) (t_loss: 0.07244) (accu: 0.9774)
[epoch : 38] (l_loss: 0.04179) (t_loss: 0.06767) (accu: 0.9788)
[epoch : 39] (l_loss: 0.04152) (t_loss: 0.07024) (accu: 0.9786)
[epoch : 40] (l_loss: 0.04147) (t_loss: 0.06925) (accu: 0.9775)
[epoch : 41] (l_loss: 0.04187) (t_loss: 0.07138) (accu: 0.9787)
[epoch : 42] (l_loss: 0.04182) (t_loss: 0.07159) (accu: 0.9774)
[epoch : 43] (l_loss: 0.04131) (t_loss: 0.07375) (accu: 0.9765)
[epoch : 44] (l_loss: 0.04192) (t_loss: 0.07466) (accu: 0.9761)
[epoch : 45] (l_loss: 0.04229) (t_loss: 0.06844) (accu: 0.9784)
[epoch : 46] (l_loss: 0.04156) (t_loss: 0.07320) (accu: 0.9776)
[epoch : 47] (l_loss: 0.04216) (t_loss: 0.06841) (accu: 0.9791)
[epoch : 48] (l_loss: 0.04160) (t_loss: 0.06999) (accu: 0.9771)
[epoch : 49] (l_loss: 0.04152) (t_loss: 0.07068) (accu: 0.9789)
[epoch : 50] (l_loss: 0.04182) (t_loss: 0.07028) (accu: 0.9789)
Finish! (Best accu: 0.9794) (Time taken(sec) : 664.60) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (3180 | 263020)          1.19
fc1.weight   :       235200 (2712 | 232488)          1.15
fc2.weight   :        30000 (346 | 29654)            1.15
fcout.weight :          1000 (122 | 878)            12.20
------------------------------------------------------------

Learning start! [Prune_iter : (21/21), Remaining weight : 1.19 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29508) (accu: 0.0974)
[epoch : 1] (l_loss: 0.39946) (t_loss: 0.15260) (accu: 0.9554)
[epoch : 2] (l_loss: 0.12766) (t_loss: 0.11449) (accu: 0.9657)
[epoch : 3] (l_loss: 0.09606) (t_loss: 0.09716) (accu: 0.9697)
[epoch : 4] (l_loss: 0.08032) (t_loss: 0.08703) (accu: 0.9738)
[epoch : 5] (l_loss: 0.07096) (t_loss: 0.08095) (accu: 0.9751)
[epoch : 6] (l_loss: 0.06462) (t_loss: 0.07534) (accu: 0.9764)
[epoch : 7] (l_loss: 0.05880) (t_loss: 0.07558) (accu: 0.9772)
[epoch : 8] (l_loss: 0.05537) (t_loss: 0.07568) (accu: 0.9772)
[epoch : 9] (l_loss: 0.05214) (t_loss: 0.07517) (accu: 0.9773)
[epoch : 10] (l_loss: 0.05024) (t_loss: 0.07413) (accu: 0.9769)
[epoch : 11] (l_loss: 0.04872) (t_loss: 0.07201) (accu: 0.9772)
[epoch : 12] (l_loss: 0.04752) (t_loss: 0.07219) (accu: 0.9775)
[epoch : 13] (l_loss: 0.04649) (t_loss: 0.07273) (accu: 0.9770)
[epoch : 14] (l_loss: 0.04611) (t_loss: 0.07512) (accu: 0.9759)
[epoch : 15] (l_loss: 0.04570) (t_loss: 0.07182) (accu: 0.9766)
[epoch : 16] (l_loss: 0.04538) (t_loss: 0.07336) (accu: 0.9772)
[epoch : 17] (l_loss: 0.04534) (t_loss: 0.07314) (accu: 0.9770)
[epoch : 18] (l_loss: 0.04504) (t_loss: 0.07258) (accu: 0.9771)
[epoch : 19] (l_loss: 0.04423) (t_loss: 0.07443) (accu: 0.9756)
[epoch : 20] (l_loss: 0.04500) (t_loss: 0.07400) (accu: 0.9776)
[epoch : 21] (l_loss: 0.04439) (t_loss: 0.07269) (accu: 0.9775)
[epoch : 22] (l_loss: 0.04446) (t_loss: 0.07074) (accu: 0.9779)
[epoch : 23] (l_loss: 0.04435) (t_loss: 0.07361) (accu: 0.9768)
[epoch : 24] (l_loss: 0.04434) (t_loss: 0.07039) (accu: 0.9779)
[epoch : 25] (l_loss: 0.04376) (t_loss: 0.07679) (accu: 0.9766)
[epoch : 26] (l_loss: 0.04421) (t_loss: 0.07228) (accu: 0.9765)
[epoch : 27] (l_loss: 0.04414) (t_loss: 0.07596) (accu: 0.9744)
[epoch : 28] (l_loss: 0.04477) (t_loss: 0.07450) (accu: 0.9761)
[epoch : 29] (l_loss: 0.04455) (t_loss: 0.07071) (accu: 0.9778)
[epoch : 30] (l_loss: 0.04387) (t_loss: 0.07137) (accu: 0.9774)
[epoch : 31] (l_loss: 0.04410) (t_loss: 0.07186) (accu: 0.9783)
[epoch : 32] (l_loss: 0.04398) (t_loss: 0.07374) (accu: 0.9769)
[epoch : 33] (l_loss: 0.04407) (t_loss: 0.07290) (accu: 0.9768)
[epoch : 34] (l_loss: 0.04427) (t_loss: 0.07097) (accu: 0.9780)
[epoch : 35] (l_loss: 0.04393) (t_loss: 0.07056) (accu: 0.9795)
[epoch : 36] (l_loss: 0.04427) (t_loss: 0.07394) (accu: 0.9764)
[epoch : 37] (l_loss: 0.04419) (t_loss: 0.07330) (accu: 0.9772)
[epoch : 38] (l_loss: 0.04401) (t_loss: 0.07241) (accu: 0.9774)
[epoch : 39] (l_loss: 0.04444) (t_loss: 0.07454) (accu: 0.9767)
[epoch : 40] (l_loss: 0.04429) (t_loss: 0.07246) (accu: 0.9765)
[epoch : 41] (l_loss: 0.04405) (t_loss: 0.07268) (accu: 0.9769)
[epoch : 42] (l_loss: 0.04404) (t_loss: 0.07591) (accu: 0.9753)
[epoch : 43] (l_loss: 0.04430) (t_loss: 0.07615) (accu: 0.9759)
[epoch : 44] (l_loss: 0.04393) (t_loss: 0.07573) (accu: 0.9751)
[epoch : 45] (l_loss: 0.04395) (t_loss: 0.07358) (accu: 0.9774)
[epoch : 46] (l_loss: 0.04363) (t_loss: 0.07371) (accu: 0.9761)
[epoch : 47] (l_loss: 0.04372) (t_loss: 0.07587) (accu: 0.9765)
[epoch : 48] (l_loss: 0.04331) (t_loss: 0.07290) (accu: 0.9770)
[epoch : 49] (l_loss: 0.04327) (t_loss: 0.07149) (accu: 0.9777)
[epoch : 50] (l_loss: 0.04312) (t_loss: 0.07156) (accu: 0.9767)
Finish! (Best accu: 0.9795) (Time taken(sec) : 663.33) 


Maximum accuracy per weight remaining
Remaining weight 100.0 %  Epoch 48 Accu 0.9802
Remaining weight 80.04 %  Epoch 27 Accu 0.9802
Remaining weight 64.06 %  Epoch 44 Accu 0.9812
Remaining weight 51.28 %  Epoch 37 Accu 0.9800
Remaining weight 41.05 %  Epoch 23 Accu 0.9801
Remaining weight 32.87 %  Epoch 32 Accu 0.9801
Remaining weight 26.32 %  Epoch 49 Accu 0.9803
Remaining weight 21.07 %  Epoch 47 Accu 0.9801
Remaining weight 16.88 %  Epoch 47 Accu 0.9783
Remaining weight 13.52 %  Epoch 15 Accu 0.9793
Remaining weight 10.83 %  Epoch 15 Accu 0.9788
Remaining weight 8.68 %  Epoch 25 Accu 0.9790
Remaining weight 6.95 %  Epoch 42 Accu 0.9796
Remaining weight 5.57 %  Epoch 45 Accu 0.9794
Remaining weight 4.47 %  Epoch 48 Accu 0.9804
Remaining weight 3.58 %  Epoch 15 Accu 0.9794
Remaining weight 2.87 %  Epoch 23 Accu 0.9801
Remaining weight 2.31 %  Epoch 17 Accu 0.9801
Remaining weight 1.85 %  Epoch 29 Accu 0.9804
Remaining weight 1.49 %  Epoch 32 Accu 0.9794
Remaining weight 1.19 %  Epoch 34 Accu 0.9795
===================================================================== 

Test_Iter (2/5)
------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :        266200 (266200 | 0)          100.00
fc1.weight   :        235200 (235200 | 0)          100.00
fc2.weight   :         30000 (30000 | 0)           100.00
fcout.weight :          1000 (1000 | 0)            100.00
------------------------------------------------------------

Learning start! [Prune_iter : (1/21), Remaining weight : 100.0 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.83214) (accu: 0.0860)
[epoch : 1] (l_loss: 0.45519) (t_loss: 0.14859) (accu: 0.9568)
[epoch : 2] (l_loss: 0.12338) (t_loss: 0.11035) (accu: 0.9658)
[epoch : 3] (l_loss: 0.09310) (t_loss: 0.09665) (accu: 0.9709)
[epoch : 4] (l_loss: 0.07931) (t_loss: 0.08661) (accu: 0.9733)
[epoch : 5] (l_loss: 0.07013) (t_loss: 0.08250) (accu: 0.9749)
[epoch : 6] (l_loss: 0.06432) (t_loss: 0.07652) (accu: 0.9762)
[epoch : 7] (l_loss: 0.05895) (t_loss: 0.07596) (accu: 0.9768)
[epoch : 8] (l_loss: 0.05523) (t_loss: 0.07585) (accu: 0.9752)
[epoch : 9] (l_loss: 0.05268) (t_loss: 0.07586) (accu: 0.9758)
[epoch : 10] (l_loss: 0.05036) (t_loss: 0.07902) (accu: 0.9752)
[epoch : 11] (l_loss: 0.04918) (t_loss: 0.07548) (accu: 0.9760)
[epoch : 12] (l_loss: 0.04838) (t_loss: 0.07157) (accu: 0.9772)
[epoch : 13] (l_loss: 0.04797) (t_loss: 0.07197) (accu: 0.9764)
[epoch : 14] (l_loss: 0.04696) (t_loss: 0.07139) (accu: 0.9765)
[epoch : 15] (l_loss: 0.04693) (t_loss: 0.07145) (accu: 0.9783)
[epoch : 16] (l_loss: 0.04648) (t_loss: 0.07823) (accu: 0.9762)
[epoch : 17] (l_loss: 0.04614) (t_loss: 0.07276) (accu: 0.9770)
[epoch : 18] (l_loss: 0.04603) (t_loss: 0.07688) (accu: 0.9769)
[epoch : 19] (l_loss: 0.04635) (t_loss: 0.07423) (accu: 0.9754)
[epoch : 20] (l_loss: 0.04560) (t_loss: 0.07462) (accu: 0.9761)
[epoch : 21] (l_loss: 0.04580) (t_loss: 0.07798) (accu: 0.9737)
[epoch : 22] (l_loss: 0.04606) (t_loss: 0.07401) (accu: 0.9771)
[epoch : 23] (l_loss: 0.04568) (t_loss: 0.07214) (accu: 0.9768)
[epoch : 24] (l_loss: 0.04563) (t_loss: 0.07091) (accu: 0.9775)
[epoch : 25] (l_loss: 0.04476) (t_loss: 0.07107) (accu: 0.9779)
[epoch : 26] (l_loss: 0.04559) (t_loss: 0.07181) (accu: 0.9778)
[epoch : 27] (l_loss: 0.04550) (t_loss: 0.07053) (accu: 0.9783)
[epoch : 28] (l_loss: 0.04532) (t_loss: 0.07321) (accu: 0.9761)
[epoch : 29] (l_loss: 0.04537) (t_loss: 0.07306) (accu: 0.9763)
[epoch : 30] (l_loss: 0.04538) (t_loss: 0.07252) (accu: 0.9774)
[epoch : 31] (l_loss: 0.04538) (t_loss: 0.07700) (accu: 0.9766)
[epoch : 32] (l_loss: 0.04492) (t_loss: 0.07324) (accu: 0.9768)
[epoch : 33] (l_loss: 0.04460) (t_loss: 0.07507) (accu: 0.9765)
[epoch : 34] (l_loss: 0.04531) (t_loss: 0.07490) (accu: 0.9759)
[epoch : 35] (l_loss: 0.04529) (t_loss: 0.07473) (accu: 0.9753)
[epoch : 36] (l_loss: 0.04494) (t_loss: 0.07493) (accu: 0.9756)
[epoch : 37] (l_loss: 0.04518) (t_loss: 0.07365) (accu: 0.9760)
[epoch : 38] (l_loss: 0.04518) (t_loss: 0.07281) (accu: 0.9766)
[epoch : 39] (l_loss: 0.04507) (t_loss: 0.07312) (accu: 0.9771)
[epoch : 40] (l_loss: 0.04499) (t_loss: 0.07033) (accu: 0.9788)
[epoch : 41] (l_loss: 0.04493) (t_loss: 0.07326) (accu: 0.9769)
[epoch : 42] (l_loss: 0.04459) (t_loss: 0.07499) (accu: 0.9766)
[epoch : 43] (l_loss: 0.04523) (t_loss: 0.07128) (accu: 0.9783)
[epoch : 44] (l_loss: 0.04461) (t_loss: 0.06982) (accu: 0.9786)
[epoch : 45] (l_loss: 0.04506) (t_loss: 0.07208) (accu: 0.9774)
[epoch : 46] (l_loss: 0.04453) (t_loss: 0.07302) (accu: 0.9770)
[epoch : 47] (l_loss: 0.04473) (t_loss: 0.07024) (accu: 0.9784)
[epoch : 48] (l_loss: 0.04493) (t_loss: 0.07177) (accu: 0.9772)
[epoch : 49] (l_loss: 0.04484) (t_loss: 0.07404) (accu: 0.9774)
[epoch : 50] (l_loss: 0.04523) (t_loss: 0.07540) (accu: 0.9763)
Finish! (Best accu: 0.9788) (Time taken(sec) : 661.89) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (213060 | 53140)         80.04
fc1.weight   :      235200 (188160 | 47040)         80.00
fc2.weight   :        30000 (24000 | 6000)          80.00
fcout.weight :          1000 (900 | 100)            90.00
------------------------------------------------------------

Learning start! [Prune_iter : (2/21), Remaining weight : 80.04 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.57643) (accu: 0.1163)
[epoch : 1] (l_loss: 0.43435) (t_loss: 0.14726) (accu: 0.9583)
[epoch : 2] (l_loss: 0.12052) (t_loss: 0.10579) (accu: 0.9686)
[epoch : 3] (l_loss: 0.08977) (t_loss: 0.09215) (accu: 0.9706)
[epoch : 4] (l_loss: 0.07646) (t_loss: 0.08924) (accu: 0.9729)
[epoch : 5] (l_loss: 0.06809) (t_loss: 0.08152) (accu: 0.9742)
[epoch : 6] (l_loss: 0.06222) (t_loss: 0.07894) (accu: 0.9765)
[epoch : 7] (l_loss: 0.05832) (t_loss: 0.07805) (accu: 0.9759)
[epoch : 8] (l_loss: 0.05433) (t_loss: 0.07439) (accu: 0.9766)
[epoch : 9] (l_loss: 0.05195) (t_loss: 0.07222) (accu: 0.9778)
[epoch : 10] (l_loss: 0.04989) (t_loss: 0.07762) (accu: 0.9754)
[epoch : 11] (l_loss: 0.04888) (t_loss: 0.07350) (accu: 0.9764)
[epoch : 12] (l_loss: 0.04763) (t_loss: 0.07297) (accu: 0.9761)
[epoch : 13] (l_loss: 0.04684) (t_loss: 0.07156) (accu: 0.9763)
[epoch : 14] (l_loss: 0.04638) (t_loss: 0.07298) (accu: 0.9763)
[epoch : 15] (l_loss: 0.04627) (t_loss: 0.07748) (accu: 0.9762)
[epoch : 16] (l_loss: 0.04615) (t_loss: 0.07317) (accu: 0.9761)
[epoch : 17] (l_loss: 0.04493) (t_loss: 0.07226) (accu: 0.9785)
[epoch : 18] (l_loss: 0.04497) (t_loss: 0.07539) (accu: 0.9758)
[epoch : 19] (l_loss: 0.04438) (t_loss: 0.07070) (accu: 0.9772)
[epoch : 20] (l_loss: 0.04468) (t_loss: 0.07225) (accu: 0.9766)
[epoch : 21] (l_loss: 0.04493) (t_loss: 0.07189) (accu: 0.9776)
[epoch : 22] (l_loss: 0.04381) (t_loss: 0.07431) (accu: 0.9768)
[epoch : 23] (l_loss: 0.04430) (t_loss: 0.07148) (accu: 0.9779)
[epoch : 24] (l_loss: 0.04436) (t_loss: 0.07124) (accu: 0.9769)
[epoch : 25] (l_loss: 0.04372) (t_loss: 0.07105) (accu: 0.9783)
[epoch : 26] (l_loss: 0.04422) (t_loss: 0.07416) (accu: 0.9762)
[epoch : 27] (l_loss: 0.04395) (t_loss: 0.07346) (accu: 0.9776)
[epoch : 28] (l_loss: 0.04376) (t_loss: 0.07219) (accu: 0.9780)
[epoch : 29] (l_loss: 0.04416) (t_loss: 0.07310) (accu: 0.9774)
[epoch : 30] (l_loss: 0.04398) (t_loss: 0.07415) (accu: 0.9771)
[epoch : 31] (l_loss: 0.04379) (t_loss: 0.07373) (accu: 0.9767)
[epoch : 32] (l_loss: 0.04370) (t_loss: 0.07152) (accu: 0.9775)
[epoch : 33] (l_loss: 0.04374) (t_loss: 0.07049) (accu: 0.9771)
[epoch : 34] (l_loss: 0.04347) (t_loss: 0.06968) (accu: 0.9775)
[epoch : 35] (l_loss: 0.04396) (t_loss: 0.07281) (accu: 0.9766)
[epoch : 36] (l_loss: 0.04402) (t_loss: 0.07419) (accu: 0.9766)
[epoch : 37] (l_loss: 0.04388) (t_loss: 0.07265) (accu: 0.9778)
[epoch : 38] (l_loss: 0.04375) (t_loss: 0.07132) (accu: 0.9769)
[epoch : 39] (l_loss: 0.04377) (t_loss: 0.07009) (accu: 0.9774)
[epoch : 40] (l_loss: 0.04389) (t_loss: 0.07143) (accu: 0.9762)
[epoch : 41] (l_loss: 0.04400) (t_loss: 0.06939) (accu: 0.9786)
[epoch : 42] (l_loss: 0.04358) (t_loss: 0.07007) (accu: 0.9782)
[epoch : 43] (l_loss: 0.04379) (t_loss: 0.07203) (accu: 0.9777)
[epoch : 44] (l_loss: 0.04379) (t_loss: 0.07352) (accu: 0.9773)
[epoch : 45] (l_loss: 0.04433) (t_loss: 0.07129) (accu: 0.9776)
[epoch : 46] (l_loss: 0.04423) (t_loss: 0.07657) (accu: 0.9753)
[epoch : 47] (l_loss: 0.04375) (t_loss: 0.07436) (accu: 0.9775)
[epoch : 48] (l_loss: 0.04389) (t_loss: 0.07035) (accu: 0.9780)
[epoch : 49] (l_loss: 0.04383) (t_loss: 0.07173) (accu: 0.9776)
[epoch : 50] (l_loss: 0.04345) (t_loss: 0.07496) (accu: 0.9768)
Finish! (Best accu: 0.9786) (Time taken(sec) : 680.96) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (170538 | 95662)         64.06
fc1.weight   :      235200 (150528 | 84672)         64.00
fc2.weight   :       30000 (19200 | 10800)          64.00
fcout.weight :          1000 (810 | 190)            81.00
------------------------------------------------------------

Learning start! [Prune_iter : (3/21), Remaining weight : 64.06 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.43258) (accu: 0.1105)
[epoch : 1] (l_loss: 0.43687) (t_loss: 0.15408) (accu: 0.9554)
[epoch : 2] (l_loss: 0.12904) (t_loss: 0.11323) (accu: 0.9658)
[epoch : 3] (l_loss: 0.09629) (t_loss: 0.10343) (accu: 0.9691)
[epoch : 4] (l_loss: 0.08205) (t_loss: 0.09120) (accu: 0.9718)
[epoch : 5] (l_loss: 0.07302) (t_loss: 0.08559) (accu: 0.9739)
[epoch : 6] (l_loss: 0.06712) (t_loss: 0.08010) (accu: 0.9756)
[epoch : 7] (l_loss: 0.06255) (t_loss: 0.07620) (accu: 0.9763)
[epoch : 8] (l_loss: 0.05872) (t_loss: 0.07619) (accu: 0.9766)
[epoch : 9] (l_loss: 0.05595) (t_loss: 0.07774) (accu: 0.9760)
[epoch : 10] (l_loss: 0.05391) (t_loss: 0.07483) (accu: 0.9777)
[epoch : 11] (l_loss: 0.05171) (t_loss: 0.07335) (accu: 0.9775)
[epoch : 12] (l_loss: 0.05125) (t_loss: 0.07556) (accu: 0.9779)
[epoch : 13] (l_loss: 0.05000) (t_loss: 0.07310) (accu: 0.9786)
[epoch : 14] (l_loss: 0.04965) (t_loss: 0.07288) (accu: 0.9785)
[epoch : 15] (l_loss: 0.04874) (t_loss: 0.07449) (accu: 0.9769)
[epoch : 16] (l_loss: 0.04780) (t_loss: 0.07250) (accu: 0.9777)
[epoch : 17] (l_loss: 0.04798) (t_loss: 0.07408) (accu: 0.9759)
[epoch : 18] (l_loss: 0.04731) (t_loss: 0.07146) (accu: 0.9787)
[epoch : 19] (l_loss: 0.04745) (t_loss: 0.07302) (accu: 0.9778)
[epoch : 20] (l_loss: 0.04736) (t_loss: 0.07026) (accu: 0.9797)
[epoch : 21] (l_loss: 0.04709) (t_loss: 0.07103) (accu: 0.9781)
[epoch : 22] (l_loss: 0.04682) (t_loss: 0.07339) (accu: 0.9772)
[epoch : 23] (l_loss: 0.04677) (t_loss: 0.07371) (accu: 0.9785)
[epoch : 24] (l_loss: 0.04643) (t_loss: 0.07291) (accu: 0.9774)
[epoch : 25] (l_loss: 0.04640) (t_loss: 0.07742) (accu: 0.9766)
[epoch : 26] (l_loss: 0.04601) (t_loss: 0.07264) (accu: 0.9776)
[epoch : 27] (l_loss: 0.04624) (t_loss: 0.07089) (accu: 0.9782)
[epoch : 28] (l_loss: 0.04638) (t_loss: 0.07245) (accu: 0.9783)
[epoch : 29] (l_loss: 0.04648) (t_loss: 0.07129) (accu: 0.9783)
[epoch : 30] (l_loss: 0.04642) (t_loss: 0.07492) (accu: 0.9778)
[epoch : 31] (l_loss: 0.04622) (t_loss: 0.07456) (accu: 0.9770)
[epoch : 32] (l_loss: 0.04625) (t_loss: 0.07329) (accu: 0.9777)
[epoch : 33] (l_loss: 0.04660) (t_loss: 0.07547) (accu: 0.9763)
[epoch : 34] (l_loss: 0.04598) (t_loss: 0.07196) (accu: 0.9788)
[epoch : 35] (l_loss: 0.04657) (t_loss: 0.07242) (accu: 0.9780)
[epoch : 36] (l_loss: 0.04627) (t_loss: 0.07460) (accu: 0.9773)
[epoch : 37] (l_loss: 0.04657) (t_loss: 0.07635) (accu: 0.9767)
[epoch : 38] (l_loss: 0.04626) (t_loss: 0.07503) (accu: 0.9777)
[epoch : 39] (l_loss: 0.04645) (t_loss: 0.07193) (accu: 0.9785)
[epoch : 40] (l_loss: 0.04636) (t_loss: 0.07217) (accu: 0.9776)
[epoch : 41] (l_loss: 0.04622) (t_loss: 0.07348) (accu: 0.9777)
[epoch : 42] (l_loss: 0.04644) (t_loss: 0.07483) (accu: 0.9765)
[epoch : 43] (l_loss: 0.04606) (t_loss: 0.07539) (accu: 0.9761)
[epoch : 44] (l_loss: 0.04654) (t_loss: 0.07286) (accu: 0.9756)
[epoch : 45] (l_loss: 0.04665) (t_loss: 0.06997) (accu: 0.9787)
[epoch : 46] (l_loss: 0.04598) (t_loss: 0.07385) (accu: 0.9781)
[epoch : 47] (l_loss: 0.04577) (t_loss: 0.07254) (accu: 0.9792)
[epoch : 48] (l_loss: 0.04609) (t_loss: 0.07498) (accu: 0.9783)
[epoch : 49] (l_loss: 0.04593) (t_loss: 0.07564) (accu: 0.9774)
[epoch : 50] (l_loss: 0.04612) (t_loss: 0.07258) (accu: 0.9789)
Finish! (Best accu: 0.9797) (Time taken(sec) : 679.13) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (136511 | 129689)        51.28
fc1.weight   :      235200 (120422 | 114778)        51.20
fc2.weight   :       30000 (15360 | 14640)          51.20
fcout.weight :          1000 (729 | 271)            72.90
------------------------------------------------------------

Learning start! [Prune_iter : (4/21), Remaining weight : 51.28 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.40263) (accu: 0.1166)
[epoch : 1] (l_loss: 0.43329) (t_loss: 0.16183) (accu: 0.9535)
[epoch : 2] (l_loss: 0.13190) (t_loss: 0.11252) (accu: 0.9666)
[epoch : 3] (l_loss: 0.09709) (t_loss: 0.09429) (accu: 0.9710)
[epoch : 4] (l_loss: 0.08124) (t_loss: 0.08854) (accu: 0.9738)
[epoch : 5] (l_loss: 0.07256) (t_loss: 0.08618) (accu: 0.9730)
[epoch : 6] (l_loss: 0.06646) (t_loss: 0.08131) (accu: 0.9735)
[epoch : 7] (l_loss: 0.06215) (t_loss: 0.07832) (accu: 0.9754)
[epoch : 8] (l_loss: 0.05818) (t_loss: 0.07581) (accu: 0.9773)
[epoch : 9] (l_loss: 0.05642) (t_loss: 0.07506) (accu: 0.9762)
[epoch : 10] (l_loss: 0.05385) (t_loss: 0.07826) (accu: 0.9750)
[epoch : 11] (l_loss: 0.05201) (t_loss: 0.07584) (accu: 0.9761)
[epoch : 12] (l_loss: 0.05147) (t_loss: 0.07388) (accu: 0.9769)
[epoch : 13] (l_loss: 0.05065) (t_loss: 0.07576) (accu: 0.9761)
[epoch : 14] (l_loss: 0.04973) (t_loss: 0.07495) (accu: 0.9771)
[epoch : 15] (l_loss: 0.04976) (t_loss: 0.07169) (accu: 0.9769)
[epoch : 16] (l_loss: 0.04912) (t_loss: 0.07605) (accu: 0.9760)
[epoch : 17] (l_loss: 0.04936) (t_loss: 0.07267) (accu: 0.9764)
[epoch : 18] (l_loss: 0.04875) (t_loss: 0.07461) (accu: 0.9769)
[epoch : 19] (l_loss: 0.04857) (t_loss: 0.07264) (accu: 0.9778)
[epoch : 20] (l_loss: 0.04839) (t_loss: 0.07150) (accu: 0.9781)
[epoch : 21] (l_loss: 0.04878) (t_loss: 0.07511) (accu: 0.9753)
[epoch : 22] (l_loss: 0.04806) (t_loss: 0.07212) (accu: 0.9779)
[epoch : 23] (l_loss: 0.04843) (t_loss: 0.07299) (accu: 0.9776)
[epoch : 24] (l_loss: 0.04822) (t_loss: 0.07505) (accu: 0.9755)
[epoch : 25] (l_loss: 0.04836) (t_loss: 0.07488) (accu: 0.9772)
[epoch : 26] (l_loss: 0.04859) (t_loss: 0.07367) (accu: 0.9775)
[epoch : 27] (l_loss: 0.04770) (t_loss: 0.07337) (accu: 0.9775)
[epoch : 28] (l_loss: 0.04846) (t_loss: 0.07173) (accu: 0.9774)
[epoch : 29] (l_loss: 0.04756) (t_loss: 0.07480) (accu: 0.9768)
[epoch : 30] (l_loss: 0.04842) (t_loss: 0.07639) (accu: 0.9769)
[epoch : 31] (l_loss: 0.04748) (t_loss: 0.07922) (accu: 0.9753)
[epoch : 32] (l_loss: 0.04805) (t_loss: 0.07347) (accu: 0.9772)
[epoch : 33] (l_loss: 0.04771) (t_loss: 0.07444) (accu: 0.9770)
[epoch : 34] (l_loss: 0.04767) (t_loss: 0.07486) (accu: 0.9763)
[epoch : 35] (l_loss: 0.04699) (t_loss: 0.07215) (accu: 0.9783)
[epoch : 36] (l_loss: 0.04706) (t_loss: 0.07627) (accu: 0.9768)
[epoch : 37] (l_loss: 0.04759) (t_loss: 0.07444) (accu: 0.9763)
[epoch : 38] (l_loss: 0.04683) (t_loss: 0.07401) (accu: 0.9769)
[epoch : 39] (l_loss: 0.04722) (t_loss: 0.07523) (accu: 0.9773)
[epoch : 40] (l_loss: 0.04740) (t_loss: 0.07332) (accu: 0.9766)
[epoch : 41] (l_loss: 0.04722) (t_loss: 0.07350) (accu: 0.9754)
[epoch : 42] (l_loss: 0.04728) (t_loss: 0.07300) (accu: 0.9765)
[epoch : 43] (l_loss: 0.04703) (t_loss: 0.07113) (accu: 0.9787)
[epoch : 44] (l_loss: 0.04745) (t_loss: 0.07305) (accu: 0.9768)
[epoch : 45] (l_loss: 0.04723) (t_loss: 0.07646) (accu: 0.9769)
[epoch : 46] (l_loss: 0.04701) (t_loss: 0.07254) (accu: 0.9774)
[epoch : 47] (l_loss: 0.04690) (t_loss: 0.07295) (accu: 0.9763)
[epoch : 48] (l_loss: 0.04721) (t_loss: 0.07316) (accu: 0.9766)
[epoch : 49] (l_loss: 0.04675) (t_loss: 0.07736) (accu: 0.9753)
[epoch : 50] (l_loss: 0.04723) (t_loss: 0.07232) (accu: 0.9782)
Finish! (Best accu: 0.9787) (Time taken(sec) : 666.34) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (109282 | 156918)        41.05
fc1.weight   :      235200 (96338 | 138862)         40.96
fc2.weight   :       30000 (12288 | 17712)          40.96
fcout.weight :          1000 (656 | 344)            65.60
------------------------------------------------------------

Learning start! [Prune_iter : (5/21), Remaining weight : 41.05 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.37172) (accu: 0.1552)
[epoch : 1] (l_loss: 0.43354) (t_loss: 0.16178) (accu: 0.9534)
[epoch : 2] (l_loss: 0.13455) (t_loss: 0.11837) (accu: 0.9642)
[epoch : 3] (l_loss: 0.10041) (t_loss: 0.09934) (accu: 0.9692)
[epoch : 4] (l_loss: 0.08494) (t_loss: 0.09177) (accu: 0.9725)
[epoch : 5] (l_loss: 0.07405) (t_loss: 0.08639) (accu: 0.9740)
[epoch : 6] (l_loss: 0.06713) (t_loss: 0.08622) (accu: 0.9719)
[epoch : 7] (l_loss: 0.06283) (t_loss: 0.08058) (accu: 0.9731)
[epoch : 8] (l_loss: 0.05921) (t_loss: 0.08213) (accu: 0.9747)
[epoch : 9] (l_loss: 0.05599) (t_loss: 0.07877) (accu: 0.9763)
[epoch : 10] (l_loss: 0.05358) (t_loss: 0.07856) (accu: 0.9742)
[epoch : 11] (l_loss: 0.05132) (t_loss: 0.07509) (accu: 0.9762)
[epoch : 12] (l_loss: 0.05017) (t_loss: 0.07353) (accu: 0.9770)
[epoch : 13] (l_loss: 0.04973) (t_loss: 0.07381) (accu: 0.9770)
[epoch : 14] (l_loss: 0.04870) (t_loss: 0.07358) (accu: 0.9760)
[epoch : 15] (l_loss: 0.04880) (t_loss: 0.07843) (accu: 0.9749)
[epoch : 16] (l_loss: 0.04842) (t_loss: 0.07275) (accu: 0.9773)
[epoch : 17] (l_loss: 0.04833) (t_loss: 0.07593) (accu: 0.9774)
[epoch : 18] (l_loss: 0.04806) (t_loss: 0.07718) (accu: 0.9769)
[epoch : 19] (l_loss: 0.04784) (t_loss: 0.07529) (accu: 0.9757)
[epoch : 20] (l_loss: 0.04726) (t_loss: 0.07356) (accu: 0.9775)
[epoch : 21] (l_loss: 0.04741) (t_loss: 0.07155) (accu: 0.9776)
[epoch : 22] (l_loss: 0.04757) (t_loss: 0.07247) (accu: 0.9769)
[epoch : 23] (l_loss: 0.04702) (t_loss: 0.07121) (accu: 0.9768)
[epoch : 24] (l_loss: 0.04713) (t_loss: 0.07457) (accu: 0.9770)
[epoch : 25] (l_loss: 0.04679) (t_loss: 0.07173) (accu: 0.9777)
[epoch : 26] (l_loss: 0.04733) (t_loss: 0.07338) (accu: 0.9776)
[epoch : 27] (l_loss: 0.04699) (t_loss: 0.07316) (accu: 0.9769)
[epoch : 28] (l_loss: 0.04682) (t_loss: 0.07273) (accu: 0.9772)
[epoch : 29] (l_loss: 0.04664) (t_loss: 0.07277) (accu: 0.9770)
[epoch : 30] (l_loss: 0.04687) (t_loss: 0.07550) (accu: 0.9772)
[epoch : 31] (l_loss: 0.04712) (t_loss: 0.07552) (accu: 0.9764)
[epoch : 32] (l_loss: 0.04725) (t_loss: 0.07359) (accu: 0.9774)
[epoch : 33] (l_loss: 0.04645) (t_loss: 0.07638) (accu: 0.9769)
[epoch : 34] (l_loss: 0.04686) (t_loss: 0.07567) (accu: 0.9761)
[epoch : 35] (l_loss: 0.04595) (t_loss: 0.07795) (accu: 0.9763)
[epoch : 36] (l_loss: 0.04661) (t_loss: 0.07234) (accu: 0.9774)
[epoch : 37] (l_loss: 0.04657) (t_loss: 0.07597) (accu: 0.9772)
[epoch : 38] (l_loss: 0.04672) (t_loss: 0.07525) (accu: 0.9766)
[epoch : 39] (l_loss: 0.04659) (t_loss: 0.07435) (accu: 0.9768)
[epoch : 40] (l_loss: 0.04639) (t_loss: 0.07500) (accu: 0.9773)
[epoch : 41] (l_loss: 0.04685) (t_loss: 0.07143) (accu: 0.9789)
[epoch : 42] (l_loss: 0.04625) (t_loss: 0.07203) (accu: 0.9785)
[epoch : 43] (l_loss: 0.04691) (t_loss: 0.07252) (accu: 0.9773)
[epoch : 44] (l_loss: 0.04689) (t_loss: 0.07229) (accu: 0.9776)
[epoch : 45] (l_loss: 0.04663) (t_loss: 0.07797) (accu: 0.9749)
[epoch : 46] (l_loss: 0.04641) (t_loss: 0.07580) (accu: 0.9768)
[epoch : 47] (l_loss: 0.04664) (t_loss: 0.07436) (accu: 0.9765)
[epoch : 48] (l_loss: 0.04601) (t_loss: 0.07356) (accu: 0.9767)
[epoch : 49] (l_loss: 0.04629) (t_loss: 0.07257) (accu: 0.9775)
[epoch : 50] (l_loss: 0.04641) (t_loss: 0.07269) (accu: 0.9775)
Finish! (Best accu: 0.9789) (Time taken(sec) : 677.50) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (87490 | 178710)         32.87
fc1.weight   :      235200 (77070 | 158130)         32.77
fc2.weight   :        30000 (9830 | 20170)          32.77
fcout.weight :          1000 (590 | 410)            59.00
------------------------------------------------------------

Learning start! [Prune_iter : (6/21), Remaining weight : 32.87 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.35495) (accu: 0.1007)
[epoch : 1] (l_loss: 0.41865) (t_loss: 0.14886) (accu: 0.9573)
[epoch : 2] (l_loss: 0.12181) (t_loss: 0.10617) (accu: 0.9694)
[epoch : 3] (l_loss: 0.09216) (t_loss: 0.09240) (accu: 0.9708)
[epoch : 4] (l_loss: 0.07792) (t_loss: 0.08393) (accu: 0.9750)
[epoch : 5] (l_loss: 0.06973) (t_loss: 0.08097) (accu: 0.9753)
[epoch : 6] (l_loss: 0.06448) (t_loss: 0.07704) (accu: 0.9768)
[epoch : 7] (l_loss: 0.06047) (t_loss: 0.07844) (accu: 0.9759)
[epoch : 8] (l_loss: 0.05741) (t_loss: 0.07762) (accu: 0.9761)
[epoch : 9] (l_loss: 0.05484) (t_loss: 0.08149) (accu: 0.9741)
[epoch : 10] (l_loss: 0.05300) (t_loss: 0.07361) (accu: 0.9768)
[epoch : 11] (l_loss: 0.05167) (t_loss: 0.07318) (accu: 0.9774)
[epoch : 12] (l_loss: 0.05081) (t_loss: 0.07242) (accu: 0.9782)
[epoch : 13] (l_loss: 0.04959) (t_loss: 0.07407) (accu: 0.9761)
[epoch : 14] (l_loss: 0.04905) (t_loss: 0.07162) (accu: 0.9787)
[epoch : 15] (l_loss: 0.04913) (t_loss: 0.07406) (accu: 0.9766)
[epoch : 16] (l_loss: 0.04846) (t_loss: 0.07438) (accu: 0.9769)
[epoch : 17] (l_loss: 0.04852) (t_loss: 0.07679) (accu: 0.9753)
[epoch : 18] (l_loss: 0.04818) (t_loss: 0.07456) (accu: 0.9766)
[epoch : 19] (l_loss: 0.04842) (t_loss: 0.07185) (accu: 0.9786)
[epoch : 20] (l_loss: 0.04804) (t_loss: 0.07525) (accu: 0.9775)
[epoch : 21] (l_loss: 0.04747) (t_loss: 0.07584) (accu: 0.9747)
[epoch : 22] (l_loss: 0.04745) (t_loss: 0.07342) (accu: 0.9784)
[epoch : 23] (l_loss: 0.04774) (t_loss: 0.08098) (accu: 0.9733)
[epoch : 24] (l_loss: 0.04798) (t_loss: 0.07710) (accu: 0.9752)
[epoch : 25] (l_loss: 0.04765) (t_loss: 0.07437) (accu: 0.9775)
[epoch : 26] (l_loss: 0.04761) (t_loss: 0.08395) (accu: 0.9737)
[epoch : 27] (l_loss: 0.04759) (t_loss: 0.07506) (accu: 0.9760)
[epoch : 28] (l_loss: 0.04752) (t_loss: 0.07253) (accu: 0.9766)
[epoch : 29] (l_loss: 0.04686) (t_loss: 0.07724) (accu: 0.9742)
[epoch : 30] (l_loss: 0.04685) (t_loss: 0.07557) (accu: 0.9760)
[epoch : 31] (l_loss: 0.04652) (t_loss: 0.07667) (accu: 0.9761)
[epoch : 32] (l_loss: 0.04655) (t_loss: 0.07447) (accu: 0.9769)
[epoch : 33] (l_loss: 0.04594) (t_loss: 0.07326) (accu: 0.9762)
[epoch : 34] (l_loss: 0.04642) (t_loss: 0.07269) (accu: 0.9777)
[epoch : 35] (l_loss: 0.04629) (t_loss: 0.07083) (accu: 0.9780)
[epoch : 36] (l_loss: 0.04609) (t_loss: 0.07451) (accu: 0.9762)
[epoch : 37] (l_loss: 0.04629) (t_loss: 0.07170) (accu: 0.9784)
[epoch : 38] (l_loss: 0.04646) (t_loss: 0.07689) (accu: 0.9766)
[epoch : 39] (l_loss: 0.04633) (t_loss: 0.07723) (accu: 0.9749)
[epoch : 40] (l_loss: 0.04630) (t_loss: 0.07634) (accu: 0.9762)
[epoch : 41] (l_loss: 0.04621) (t_loss: 0.07745) (accu: 0.9757)
[epoch : 42] (l_loss: 0.04601) (t_loss: 0.07273) (accu: 0.9771)
[epoch : 43] (l_loss: 0.04628) (t_loss: 0.07657) (accu: 0.9759)
[epoch : 44] (l_loss: 0.04615) (t_loss: 0.07221) (accu: 0.9780)
[epoch : 45] (l_loss: 0.04657) (t_loss: 0.07586) (accu: 0.9757)
[epoch : 46] (l_loss: 0.04624) (t_loss: 0.07347) (accu: 0.9780)
[epoch : 47] (l_loss: 0.04576) (t_loss: 0.07411) (accu: 0.9773)
[epoch : 48] (l_loss: 0.04677) (t_loss: 0.07318) (accu: 0.9765)
[epoch : 49] (l_loss: 0.04622) (t_loss: 0.07443) (accu: 0.9761)
[epoch : 50] (l_loss: 0.04637) (t_loss: 0.07432) (accu: 0.9770)
Finish! (Best accu: 0.9787) (Time taken(sec) : 680.47) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (70051 | 196149)         26.32
fc1.weight   :      235200 (61656 | 173544)         26.21
fc2.weight   :        30000 (7864 | 22136)          26.21
fcout.weight :          1000 (531 | 469)            53.10
------------------------------------------------------------

Learning start! [Prune_iter : (7/21), Remaining weight : 26.32 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.32495) (accu: 0.1167)
[epoch : 1] (l_loss: 0.42361) (t_loss: 0.15500) (accu: 0.9567)
[epoch : 2] (l_loss: 0.12760) (t_loss: 0.11210) (accu: 0.9655)
[epoch : 3] (l_loss: 0.09627) (t_loss: 0.09499) (accu: 0.9703)
[epoch : 4] (l_loss: 0.08062) (t_loss: 0.08816) (accu: 0.9731)
[epoch : 5] (l_loss: 0.07144) (t_loss: 0.07989) (accu: 0.9761)
[epoch : 6] (l_loss: 0.06535) (t_loss: 0.07699) (accu: 0.9771)
[epoch : 7] (l_loss: 0.06006) (t_loss: 0.07743) (accu: 0.9759)
[epoch : 8] (l_loss: 0.05603) (t_loss: 0.07850) (accu: 0.9772)
[epoch : 9] (l_loss: 0.05304) (t_loss: 0.07282) (accu: 0.9782)
[epoch : 10] (l_loss: 0.05117) (t_loss: 0.07582) (accu: 0.9758)
[epoch : 11] (l_loss: 0.04958) (t_loss: 0.07653) (accu: 0.9759)
[epoch : 12] (l_loss: 0.04849) (t_loss: 0.07290) (accu: 0.9772)
[epoch : 13] (l_loss: 0.04769) (t_loss: 0.07372) (accu: 0.9777)
[epoch : 14] (l_loss: 0.04751) (t_loss: 0.07228) (accu: 0.9783)
[epoch : 15] (l_loss: 0.04706) (t_loss: 0.07506) (accu: 0.9763)
[epoch : 16] (l_loss: 0.04631) (t_loss: 0.07510) (accu: 0.9777)
[epoch : 17] (l_loss: 0.04566) (t_loss: 0.07266) (accu: 0.9770)
[epoch : 18] (l_loss: 0.04550) (t_loss: 0.07618) (accu: 0.9755)
[epoch : 19] (l_loss: 0.04559) (t_loss: 0.07181) (accu: 0.9784)
[epoch : 20] (l_loss: 0.04564) (t_loss: 0.07640) (accu: 0.9770)
[epoch : 21] (l_loss: 0.04529) (t_loss: 0.07164) (accu: 0.9781)
[epoch : 22] (l_loss: 0.04513) (t_loss: 0.07135) (accu: 0.9782)
[epoch : 23] (l_loss: 0.04474) (t_loss: 0.07069) (accu: 0.9772)
[epoch : 24] (l_loss: 0.04445) (t_loss: 0.07223) (accu: 0.9765)
[epoch : 25] (l_loss: 0.04431) (t_loss: 0.07100) (accu: 0.9778)
[epoch : 26] (l_loss: 0.04401) (t_loss: 0.07072) (accu: 0.9787)
[epoch : 27] (l_loss: 0.04369) (t_loss: 0.07132) (accu: 0.9771)
[epoch : 28] (l_loss: 0.04361) (t_loss: 0.07751) (accu: 0.9751)
[epoch : 29] (l_loss: 0.04421) (t_loss: 0.07139) (accu: 0.9774)
[epoch : 30] (l_loss: 0.04406) (t_loss: 0.07117) (accu: 0.9785)
[epoch : 31] (l_loss: 0.04357) (t_loss: 0.07079) (accu: 0.9793)
[epoch : 32] (l_loss: 0.04377) (t_loss: 0.07313) (accu: 0.9781)
[epoch : 33] (l_loss: 0.04336) (t_loss: 0.06947) (accu: 0.9777)
[epoch : 34] (l_loss: 0.04347) (t_loss: 0.07258) (accu: 0.9776)
[epoch : 35] (l_loss: 0.04351) (t_loss: 0.07647) (accu: 0.9770)
[epoch : 36] (l_loss: 0.04377) (t_loss: 0.07131) (accu: 0.9765)
[epoch : 37] (l_loss: 0.04418) (t_loss: 0.07130) (accu: 0.9773)
[epoch : 38] (l_loss: 0.04382) (t_loss: 0.07212) (accu: 0.9773)
[epoch : 39] (l_loss: 0.04312) (t_loss: 0.07138) (accu: 0.9782)
[epoch : 40] (l_loss: 0.04357) (t_loss: 0.07069) (accu: 0.9780)
[epoch : 41] (l_loss: 0.04398) (t_loss: 0.07174) (accu: 0.9779)
[epoch : 42] (l_loss: 0.04346) (t_loss: 0.07131) (accu: 0.9788)
[epoch : 43] (l_loss: 0.04343) (t_loss: 0.07064) (accu: 0.9772)
[epoch : 44] (l_loss: 0.04354) (t_loss: 0.07375) (accu: 0.9790)
[epoch : 45] (l_loss: 0.04349) (t_loss: 0.07219) (accu: 0.9774)
[epoch : 46] (l_loss: 0.04381) (t_loss: 0.07295) (accu: 0.9782)
[epoch : 47] (l_loss: 0.04344) (t_loss: 0.07121) (accu: 0.9783)
[epoch : 48] (l_loss: 0.04346) (t_loss: 0.06985) (accu: 0.9787)
[epoch : 49] (l_loss: 0.04361) (t_loss: 0.07187) (accu: 0.9784)
[epoch : 50] (l_loss: 0.04320) (t_loss: 0.07104) (accu: 0.9782)
Finish! (Best accu: 0.9793) (Time taken(sec) : 695.21) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (56094 | 210106)         21.07
fc1.weight   :      235200 (49325 | 185875)         20.97
fc2.weight   :        30000 (6291 | 23709)          20.97
fcout.weight :          1000 (478 | 522)            47.80
------------------------------------------------------------

Learning start! [Prune_iter : (8/21), Remaining weight : 21.07 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.31577) (accu: 0.1089)
[epoch : 1] (l_loss: 0.41650) (t_loss: 0.15651) (accu: 0.9543)
[epoch : 2] (l_loss: 0.12560) (t_loss: 0.11044) (accu: 0.9659)
[epoch : 3] (l_loss: 0.09346) (t_loss: 0.09571) (accu: 0.9711)
[epoch : 4] (l_loss: 0.08009) (t_loss: 0.08649) (accu: 0.9741)
[epoch : 5] (l_loss: 0.07044) (t_loss: 0.07942) (accu: 0.9760)
[epoch : 6] (l_loss: 0.06364) (t_loss: 0.08048) (accu: 0.9748)
[epoch : 7] (l_loss: 0.05906) (t_loss: 0.07730) (accu: 0.9773)
[epoch : 8] (l_loss: 0.05527) (t_loss: 0.07550) (accu: 0.9766)
[epoch : 9] (l_loss: 0.05221) (t_loss: 0.07351) (accu: 0.9771)
[epoch : 10] (l_loss: 0.05043) (t_loss: 0.07513) (accu: 0.9758)
[epoch : 11] (l_loss: 0.04961) (t_loss: 0.07336) (accu: 0.9773)
[epoch : 12] (l_loss: 0.04827) (t_loss: 0.07294) (accu: 0.9774)
[epoch : 13] (l_loss: 0.04775) (t_loss: 0.07233) (accu: 0.9786)
[epoch : 14] (l_loss: 0.04766) (t_loss: 0.07491) (accu: 0.9772)
[epoch : 15] (l_loss: 0.04770) (t_loss: 0.07285) (accu: 0.9781)
[epoch : 16] (l_loss: 0.04713) (t_loss: 0.07272) (accu: 0.9781)
[epoch : 17] (l_loss: 0.04664) (t_loss: 0.07139) (accu: 0.9781)
[epoch : 18] (l_loss: 0.04572) (t_loss: 0.07378) (accu: 0.9773)
[epoch : 19] (l_loss: 0.04510) (t_loss: 0.07075) (accu: 0.9777)
[epoch : 20] (l_loss: 0.04441) (t_loss: 0.07362) (accu: 0.9761)
[epoch : 21] (l_loss: 0.04491) (t_loss: 0.07040) (accu: 0.9788)
[epoch : 22] (l_loss: 0.04488) (t_loss: 0.06900) (accu: 0.9784)
[epoch : 23] (l_loss: 0.04422) (t_loss: 0.07221) (accu: 0.9770)
[epoch : 24] (l_loss: 0.04416) (t_loss: 0.06812) (accu: 0.9789)
[epoch : 25] (l_loss: 0.04376) (t_loss: 0.06943) (accu: 0.9785)
[epoch : 26] (l_loss: 0.04366) (t_loss: 0.07230) (accu: 0.9773)
[epoch : 27] (l_loss: 0.04364) (t_loss: 0.07322) (accu: 0.9773)
[epoch : 28] (l_loss: 0.04377) (t_loss: 0.06984) (accu: 0.9790)
[epoch : 29] (l_loss: 0.04356) (t_loss: 0.07217) (accu: 0.9765)
[epoch : 30] (l_loss: 0.04360) (t_loss: 0.07227) (accu: 0.9786)
[epoch : 31] (l_loss: 0.04408) (t_loss: 0.07072) (accu: 0.9778)
[epoch : 32] (l_loss: 0.04357) (t_loss: 0.07120) (accu: 0.9769)
[epoch : 33] (l_loss: 0.04348) (t_loss: 0.07209) (accu: 0.9780)
[epoch : 34] (l_loss: 0.04426) (t_loss: 0.06912) (accu: 0.9796)
[epoch : 35] (l_loss: 0.04352) (t_loss: 0.07237) (accu: 0.9772)
[epoch : 36] (l_loss: 0.04386) (t_loss: 0.07176) (accu: 0.9779)
[epoch : 37] (l_loss: 0.04361) (t_loss: 0.06978) (accu: 0.9782)
[epoch : 38] (l_loss: 0.04379) (t_loss: 0.07051) (accu: 0.9791)
[epoch : 39] (l_loss: 0.04372) (t_loss: 0.07287) (accu: 0.9782)
[epoch : 40] (l_loss: 0.04357) (t_loss: 0.07092) (accu: 0.9769)
[epoch : 41] (l_loss: 0.04365) (t_loss: 0.07251) (accu: 0.9780)
[epoch : 42] (l_loss: 0.04341) (t_loss: 0.07218) (accu: 0.9776)
[epoch : 43] (l_loss: 0.04387) (t_loss: 0.07144) (accu: 0.9772)
[epoch : 44] (l_loss: 0.04366) (t_loss: 0.07461) (accu: 0.9766)
[epoch : 45] (l_loss: 0.04359) (t_loss: 0.07023) (accu: 0.9793)
[epoch : 46] (l_loss: 0.04377) (t_loss: 0.07280) (accu: 0.9764)
[epoch : 47] (l_loss: 0.04341) (t_loss: 0.07294) (accu: 0.9774)
[epoch : 48] (l_loss: 0.04344) (t_loss: 0.07173) (accu: 0.9782)
[epoch : 49] (l_loss: 0.04333) (t_loss: 0.07001) (accu: 0.9791)
[epoch : 50] (l_loss: 0.04375) (t_loss: 0.07193) (accu: 0.9777)
Finish! (Best accu: 0.9796) (Time taken(sec) : 681.19) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (44923 | 221277)         16.88
fc1.weight   :      235200 (39460 | 195740)         16.78
fc2.weight   :        30000 (5033 | 24967)          16.78
fcout.weight :          1000 (430 | 570)            43.00
------------------------------------------------------------

Learning start! [Prune_iter : (9/21), Remaining weight : 16.88 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30116) (accu: 0.1154)
[epoch : 1] (l_loss: 0.40800) (t_loss: 0.15330) (accu: 0.9555)
[epoch : 2] (l_loss: 0.12350) (t_loss: 0.10750) (accu: 0.9665)
[epoch : 3] (l_loss: 0.09163) (t_loss: 0.09310) (accu: 0.9701)
[epoch : 4] (l_loss: 0.07757) (t_loss: 0.08478) (accu: 0.9748)
[epoch : 5] (l_loss: 0.06911) (t_loss: 0.07814) (accu: 0.9765)
[epoch : 6] (l_loss: 0.06411) (t_loss: 0.08258) (accu: 0.9752)
[epoch : 7] (l_loss: 0.05967) (t_loss: 0.07572) (accu: 0.9766)
[epoch : 8] (l_loss: 0.05615) (t_loss: 0.07480) (accu: 0.9775)
[epoch : 9] (l_loss: 0.05277) (t_loss: 0.07261) (accu: 0.9772)
[epoch : 10] (l_loss: 0.05009) (t_loss: 0.07135) (accu: 0.9779)
[epoch : 11] (l_loss: 0.04845) (t_loss: 0.07399) (accu: 0.9776)
[epoch : 12] (l_loss: 0.04806) (t_loss: 0.07003) (accu: 0.9779)
[epoch : 13] (l_loss: 0.04680) (t_loss: 0.07107) (accu: 0.9781)
[epoch : 14] (l_loss: 0.04623) (t_loss: 0.06947) (accu: 0.9784)
[epoch : 15] (l_loss: 0.04568) (t_loss: 0.06988) (accu: 0.9774)
[epoch : 16] (l_loss: 0.04527) (t_loss: 0.07210) (accu: 0.9773)
[epoch : 17] (l_loss: 0.04493) (t_loss: 0.07132) (accu: 0.9788)
[epoch : 18] (l_loss: 0.04483) (t_loss: 0.07183) (accu: 0.9786)
[epoch : 19] (l_loss: 0.04440) (t_loss: 0.07073) (accu: 0.9775)
[epoch : 20] (l_loss: 0.04401) (t_loss: 0.07065) (accu: 0.9789)
[epoch : 21] (l_loss: 0.04449) (t_loss: 0.07146) (accu: 0.9781)
[epoch : 22] (l_loss: 0.04394) (t_loss: 0.07141) (accu: 0.9777)
[epoch : 23] (l_loss: 0.04357) (t_loss: 0.07272) (accu: 0.9770)
[epoch : 24] (l_loss: 0.04383) (t_loss: 0.07003) (accu: 0.9788)
[epoch : 25] (l_loss: 0.04395) (t_loss: 0.07089) (accu: 0.9783)
[epoch : 26] (l_loss: 0.04354) (t_loss: 0.06984) (accu: 0.9781)
[epoch : 27] (l_loss: 0.04312) (t_loss: 0.07071) (accu: 0.9768)
[epoch : 28] (l_loss: 0.04323) (t_loss: 0.07068) (accu: 0.9776)
[epoch : 29] (l_loss: 0.04350) (t_loss: 0.07421) (accu: 0.9763)
[epoch : 30] (l_loss: 0.04299) (t_loss: 0.07196) (accu: 0.9763)
[epoch : 31] (l_loss: 0.04289) (t_loss: 0.07049) (accu: 0.9780)
[epoch : 32] (l_loss: 0.04314) (t_loss: 0.07042) (accu: 0.9781)
[epoch : 33] (l_loss: 0.04278) (t_loss: 0.07021) (accu: 0.9786)
[epoch : 34] (l_loss: 0.04294) (t_loss: 0.07243) (accu: 0.9780)
[epoch : 35] (l_loss: 0.04319) (t_loss: 0.06926) (accu: 0.9781)
[epoch : 36] (l_loss: 0.04324) (t_loss: 0.07064) (accu: 0.9785)
[epoch : 37] (l_loss: 0.04313) (t_loss: 0.07140) (accu: 0.9764)
[epoch : 38] (l_loss: 0.04335) (t_loss: 0.06992) (accu: 0.9778)
[epoch : 39] (l_loss: 0.04256) (t_loss: 0.07483) (accu: 0.9770)
[epoch : 40] (l_loss: 0.04286) (t_loss: 0.07328) (accu: 0.9764)
[epoch : 41] (l_loss: 0.04289) (t_loss: 0.07235) (accu: 0.9772)
[epoch : 42] (l_loss: 0.04340) (t_loss: 0.06954) (accu: 0.9787)
[epoch : 43] (l_loss: 0.04277) (t_loss: 0.06953) (accu: 0.9780)
[epoch : 44] (l_loss: 0.04319) (t_loss: 0.06926) (accu: 0.9788)
[epoch : 45] (l_loss: 0.04347) (t_loss: 0.07001) (accu: 0.9786)
[epoch : 46] (l_loss: 0.04328) (t_loss: 0.07092) (accu: 0.9769)
[epoch : 47] (l_loss: 0.04309) (t_loss: 0.07165) (accu: 0.9779)
[epoch : 48] (l_loss: 0.04293) (t_loss: 0.07192) (accu: 0.9781)
[epoch : 49] (l_loss: 0.04275) (t_loss: 0.07046) (accu: 0.9780)
[epoch : 50] (l_loss: 0.04276) (t_loss: 0.07187) (accu: 0.9776)
Finish! (Best accu: 0.9789) (Time taken(sec) : 685.59) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (35982 | 230218)         13.52
fc1.weight   :      235200 (31568 | 203632)         13.42
fc2.weight   :        30000 (4027 | 25973)          13.42
fcout.weight :          1000 (387 | 613)            38.70
------------------------------------------------------------

Learning start! [Prune_iter : (10/21), Remaining weight : 13.52 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29780) (accu: 0.0840)
[epoch : 1] (l_loss: 0.40389) (t_loss: 0.15355) (accu: 0.9552)
[epoch : 2] (l_loss: 0.12752) (t_loss: 0.10997) (accu: 0.9662)
[epoch : 3] (l_loss: 0.09684) (t_loss: 0.09488) (accu: 0.9706)
[epoch : 4] (l_loss: 0.08174) (t_loss: 0.08821) (accu: 0.9746)
[epoch : 5] (l_loss: 0.07252) (t_loss: 0.08441) (accu: 0.9738)
[epoch : 6] (l_loss: 0.06602) (t_loss: 0.08129) (accu: 0.9748)
[epoch : 7] (l_loss: 0.06046) (t_loss: 0.07857) (accu: 0.9762)
[epoch : 8] (l_loss: 0.05743) (t_loss: 0.07448) (accu: 0.9775)
[epoch : 9] (l_loss: 0.05436) (t_loss: 0.07331) (accu: 0.9775)
[epoch : 10] (l_loss: 0.05211) (t_loss: 0.07674) (accu: 0.9755)
[epoch : 11] (l_loss: 0.05041) (t_loss: 0.07371) (accu: 0.9773)
[epoch : 12] (l_loss: 0.04964) (t_loss: 0.07437) (accu: 0.9758)
[epoch : 13] (l_loss: 0.04838) (t_loss: 0.07124) (accu: 0.9781)
[epoch : 14] (l_loss: 0.04780) (t_loss: 0.07005) (accu: 0.9784)
[epoch : 15] (l_loss: 0.04750) (t_loss: 0.07635) (accu: 0.9750)
[epoch : 16] (l_loss: 0.04654) (t_loss: 0.07362) (accu: 0.9764)
[epoch : 17] (l_loss: 0.04575) (t_loss: 0.07129) (accu: 0.9773)
[epoch : 18] (l_loss: 0.04557) (t_loss: 0.07042) (accu: 0.9781)
[epoch : 19] (l_loss: 0.04509) (t_loss: 0.07112) (accu: 0.9784)
[epoch : 20] (l_loss: 0.04521) (t_loss: 0.07128) (accu: 0.9795)
[epoch : 21] (l_loss: 0.04451) (t_loss: 0.07198) (accu: 0.9788)
[epoch : 22] (l_loss: 0.04463) (t_loss: 0.07327) (accu: 0.9761)
[epoch : 23] (l_loss: 0.04462) (t_loss: 0.07111) (accu: 0.9765)
[epoch : 24] (l_loss: 0.04465) (t_loss: 0.07098) (accu: 0.9784)
[epoch : 25] (l_loss: 0.04457) (t_loss: 0.07458) (accu: 0.9762)
[epoch : 26] (l_loss: 0.04407) (t_loss: 0.07515) (accu: 0.9764)
[epoch : 27] (l_loss: 0.04426) (t_loss: 0.07041) (accu: 0.9771)
[epoch : 28] (l_loss: 0.04424) (t_loss: 0.07299) (accu: 0.9762)
[epoch : 29] (l_loss: 0.04419) (t_loss: 0.07460) (accu: 0.9746)
[epoch : 30] (l_loss: 0.04418) (t_loss: 0.07024) (accu: 0.9777)
[epoch : 31] (l_loss: 0.04463) (t_loss: 0.07143) (accu: 0.9776)
[epoch : 32] (l_loss: 0.04450) (t_loss: 0.07113) (accu: 0.9782)
[epoch : 33] (l_loss: 0.04432) (t_loss: 0.07295) (accu: 0.9764)
[epoch : 34] (l_loss: 0.04434) (t_loss: 0.07590) (accu: 0.9766)
[epoch : 35] (l_loss: 0.04441) (t_loss: 0.07323) (accu: 0.9762)
[epoch : 36] (l_loss: 0.04403) (t_loss: 0.07144) (accu: 0.9781)
[epoch : 37] (l_loss: 0.04425) (t_loss: 0.07275) (accu: 0.9772)
[epoch : 38] (l_loss: 0.04440) (t_loss: 0.07810) (accu: 0.9757)
[epoch : 39] (l_loss: 0.04432) (t_loss: 0.07539) (accu: 0.9774)
[epoch : 40] (l_loss: 0.04433) (t_loss: 0.07432) (accu: 0.9769)
[epoch : 41] (l_loss: 0.04397) (t_loss: 0.07224) (accu: 0.9772)
[epoch : 42] (l_loss: 0.04436) (t_loss: 0.07160) (accu: 0.9765)
[epoch : 43] (l_loss: 0.04431) (t_loss: 0.07273) (accu: 0.9762)
[epoch : 44] (l_loss: 0.04407) (t_loss: 0.07114) (accu: 0.9779)
[epoch : 45] (l_loss: 0.04455) (t_loss: 0.07189) (accu: 0.9777)
[epoch : 46] (l_loss: 0.04383) (t_loss: 0.07332) (accu: 0.9762)
[epoch : 47] (l_loss: 0.04466) (t_loss: 0.07387) (accu: 0.9772)
[epoch : 48] (l_loss: 0.04438) (t_loss: 0.07176) (accu: 0.9770)
[epoch : 49] (l_loss: 0.04438) (t_loss: 0.06909) (accu: 0.9771)
[epoch : 50] (l_loss: 0.04432) (t_loss: 0.07472) (accu: 0.9773)
Finish! (Best accu: 0.9795) (Time taken(sec) : 687.44) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (28824 | 237376)         10.83
fc1.weight   :      235200 (25254 | 209946)         10.74
fc2.weight   :        30000 (3221 | 26779)          10.74
fcout.weight :          1000 (349 | 651)            34.90
------------------------------------------------------------

Learning start! [Prune_iter : (11/21), Remaining weight : 10.83 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29989) (accu: 0.0998)
[epoch : 1] (l_loss: 0.40308) (t_loss: 0.15330) (accu: 0.9544)
[epoch : 2] (l_loss: 0.12534) (t_loss: 0.10675) (accu: 0.9669)
[epoch : 3] (l_loss: 0.09477) (t_loss: 0.09761) (accu: 0.9682)
[epoch : 4] (l_loss: 0.08076) (t_loss: 0.08720) (accu: 0.9740)
[epoch : 5] (l_loss: 0.07168) (t_loss: 0.07919) (accu: 0.9754)
[epoch : 6] (l_loss: 0.06636) (t_loss: 0.08083) (accu: 0.9742)
[epoch : 7] (l_loss: 0.06152) (t_loss: 0.08148) (accu: 0.9739)
[epoch : 8] (l_loss: 0.05762) (t_loss: 0.07735) (accu: 0.9750)
[epoch : 9] (l_loss: 0.05454) (t_loss: 0.07667) (accu: 0.9750)
[epoch : 10] (l_loss: 0.05315) (t_loss: 0.07467) (accu: 0.9777)
[epoch : 11] (l_loss: 0.05166) (t_loss: 0.07420) (accu: 0.9777)
[epoch : 12] (l_loss: 0.05117) (t_loss: 0.07336) (accu: 0.9768)
[epoch : 13] (l_loss: 0.05026) (t_loss: 0.07524) (accu: 0.9765)
[epoch : 14] (l_loss: 0.04960) (t_loss: 0.07520) (accu: 0.9770)
[epoch : 15] (l_loss: 0.04916) (t_loss: 0.07527) (accu: 0.9766)
[epoch : 16] (l_loss: 0.04858) (t_loss: 0.07307) (accu: 0.9769)
[epoch : 17] (l_loss: 0.04800) (t_loss: 0.07300) (accu: 0.9770)
[epoch : 18] (l_loss: 0.04787) (t_loss: 0.07269) (accu: 0.9768)
[epoch : 19] (l_loss: 0.04810) (t_loss: 0.07416) (accu: 0.9773)
[epoch : 20] (l_loss: 0.04770) (t_loss: 0.07556) (accu: 0.9775)
[epoch : 21] (l_loss: 0.04741) (t_loss: 0.07273) (accu: 0.9762)
[epoch : 22] (l_loss: 0.04687) (t_loss: 0.07297) (accu: 0.9768)
[epoch : 23] (l_loss: 0.04684) (t_loss: 0.07179) (accu: 0.9774)
[epoch : 24] (l_loss: 0.04661) (t_loss: 0.07353) (accu: 0.9765)
[epoch : 25] (l_loss: 0.04635) (t_loss: 0.07293) (accu: 0.9779)
[epoch : 26] (l_loss: 0.04620) (t_loss: 0.07151) (accu: 0.9779)
[epoch : 27] (l_loss: 0.04613) (t_loss: 0.07230) (accu: 0.9785)
[epoch : 28] (l_loss: 0.04610) (t_loss: 0.07062) (accu: 0.9778)
[epoch : 29] (l_loss: 0.04649) (t_loss: 0.07358) (accu: 0.9768)
[epoch : 30] (l_loss: 0.04575) (t_loss: 0.07280) (accu: 0.9781)
[epoch : 31] (l_loss: 0.04537) (t_loss: 0.06983) (accu: 0.9788)
[epoch : 32] (l_loss: 0.04592) (t_loss: 0.07239) (accu: 0.9772)
[epoch : 33] (l_loss: 0.04625) (t_loss: 0.06988) (accu: 0.9781)
[epoch : 34] (l_loss: 0.04569) (t_loss: 0.07072) (accu: 0.9778)
[epoch : 35] (l_loss: 0.04611) (t_loss: 0.06941) (accu: 0.9783)
[epoch : 36] (l_loss: 0.04603) (t_loss: 0.07143) (accu: 0.9775)
[epoch : 37] (l_loss: 0.04561) (t_loss: 0.06877) (accu: 0.9784)
[epoch : 38] (l_loss: 0.04587) (t_loss: 0.07788) (accu: 0.9752)
[epoch : 39] (l_loss: 0.04634) (t_loss: 0.07139) (accu: 0.9788)
[epoch : 40] (l_loss: 0.04611) (t_loss: 0.07472) (accu: 0.9767)
[epoch : 41] (l_loss: 0.04647) (t_loss: 0.07094) (accu: 0.9773)
[epoch : 42] (l_loss: 0.04574) (t_loss: 0.07093) (accu: 0.9767)
[epoch : 43] (l_loss: 0.04581) (t_loss: 0.07129) (accu: 0.9788)
[epoch : 44] (l_loss: 0.04605) (t_loss: 0.07117) (accu: 0.9771)
[epoch : 45] (l_loss: 0.04607) (t_loss: 0.07246) (accu: 0.9778)
[epoch : 46] (l_loss: 0.04599) (t_loss: 0.07212) (accu: 0.9773)
[epoch : 47] (l_loss: 0.04566) (t_loss: 0.07510) (accu: 0.9763)
[epoch : 48] (l_loss: 0.04551) (t_loss: 0.06965) (accu: 0.9788)
[epoch : 49] (l_loss: 0.04575) (t_loss: 0.07080) (accu: 0.9778)
[epoch : 50] (l_loss: 0.04584) (t_loss: 0.07062) (accu: 0.9780)
Finish! (Best accu: 0.9788) (Time taken(sec) : 712.80) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (23095 | 243105)          8.68
fc1.weight   :      235200 (20204 | 214996)          8.59
fc2.weight   :        30000 (2577 | 27423)           8.59
fcout.weight :          1000 (314 | 686)            31.40
------------------------------------------------------------

Learning start! [Prune_iter : (12/21), Remaining weight : 8.68 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30267) (accu: 0.0976)
[epoch : 1] (l_loss: 0.40103) (t_loss: 0.14667) (accu: 0.9569)
[epoch : 2] (l_loss: 0.12328) (t_loss: 0.10871) (accu: 0.9679)
[epoch : 3] (l_loss: 0.09366) (t_loss: 0.09440) (accu: 0.9697)
[epoch : 4] (l_loss: 0.08023) (t_loss: 0.08524) (accu: 0.9733)
[epoch : 5] (l_loss: 0.07104) (t_loss: 0.08234) (accu: 0.9747)
[epoch : 6] (l_loss: 0.06505) (t_loss: 0.07828) (accu: 0.9775)
[epoch : 7] (l_loss: 0.06098) (t_loss: 0.07608) (accu: 0.9760)
[epoch : 8] (l_loss: 0.05741) (t_loss: 0.07373) (accu: 0.9775)
[epoch : 9] (l_loss: 0.05434) (t_loss: 0.07295) (accu: 0.9773)
[epoch : 10] (l_loss: 0.05217) (t_loss: 0.07193) (accu: 0.9762)
[epoch : 11] (l_loss: 0.05002) (t_loss: 0.07325) (accu: 0.9775)
[epoch : 12] (l_loss: 0.04850) (t_loss: 0.07595) (accu: 0.9780)
[epoch : 13] (l_loss: 0.04801) (t_loss: 0.07314) (accu: 0.9767)
[epoch : 14] (l_loss: 0.04687) (t_loss: 0.07543) (accu: 0.9771)
[epoch : 15] (l_loss: 0.04682) (t_loss: 0.07323) (accu: 0.9769)
[epoch : 16] (l_loss: 0.04644) (t_loss: 0.07347) (accu: 0.9759)
[epoch : 17] (l_loss: 0.04666) (t_loss: 0.07528) (accu: 0.9770)
[epoch : 18] (l_loss: 0.04625) (t_loss: 0.07411) (accu: 0.9767)
[epoch : 19] (l_loss: 0.04574) (t_loss: 0.07387) (accu: 0.9767)
[epoch : 20] (l_loss: 0.04524) (t_loss: 0.07191) (accu: 0.9774)
[epoch : 21] (l_loss: 0.04511) (t_loss: 0.07230) (accu: 0.9776)
[epoch : 22] (l_loss: 0.04560) (t_loss: 0.07442) (accu: 0.9767)
[epoch : 23] (l_loss: 0.04538) (t_loss: 0.07268) (accu: 0.9784)
[epoch : 24] (l_loss: 0.04514) (t_loss: 0.07632) (accu: 0.9764)
[epoch : 25] (l_loss: 0.04489) (t_loss: 0.07269) (accu: 0.9769)
[epoch : 26] (l_loss: 0.04497) (t_loss: 0.07050) (accu: 0.9764)
[epoch : 27] (l_loss: 0.04496) (t_loss: 0.07463) (accu: 0.9764)
[epoch : 28] (l_loss: 0.04521) (t_loss: 0.07251) (accu: 0.9789)
[epoch : 29] (l_loss: 0.04530) (t_loss: 0.07357) (accu: 0.9762)
[epoch : 30] (l_loss: 0.04464) (t_loss: 0.07214) (accu: 0.9772)
[epoch : 31] (l_loss: 0.04448) (t_loss: 0.07424) (accu: 0.9771)
[epoch : 32] (l_loss: 0.04434) (t_loss: 0.07441) (accu: 0.9774)
[epoch : 33] (l_loss: 0.04491) (t_loss: 0.07102) (accu: 0.9779)
[epoch : 34] (l_loss: 0.04503) (t_loss: 0.07040) (accu: 0.9787)
[epoch : 35] (l_loss: 0.04466) (t_loss: 0.07620) (accu: 0.9763)
[epoch : 36] (l_loss: 0.04443) (t_loss: 0.06974) (accu: 0.9780)
[epoch : 37] (l_loss: 0.04524) (t_loss: 0.07380) (accu: 0.9762)
[epoch : 38] (l_loss: 0.04470) (t_loss: 0.07391) (accu: 0.9769)
[epoch : 39] (l_loss: 0.04436) (t_loss: 0.07389) (accu: 0.9771)
[epoch : 40] (l_loss: 0.04487) (t_loss: 0.07372) (accu: 0.9765)
[epoch : 41] (l_loss: 0.04459) (t_loss: 0.07342) (accu: 0.9761)
[epoch : 42] (l_loss: 0.04491) (t_loss: 0.07548) (accu: 0.9763)
[epoch : 43] (l_loss: 0.04503) (t_loss: 0.07429) (accu: 0.9769)
[epoch : 44] (l_loss: 0.04494) (t_loss: 0.07260) (accu: 0.9771)
[epoch : 45] (l_loss: 0.04447) (t_loss: 0.07307) (accu: 0.9762)
[epoch : 46] (l_loss: 0.04439) (t_loss: 0.07425) (accu: 0.9782)
[epoch : 47] (l_loss: 0.04505) (t_loss: 0.07213) (accu: 0.9767)
[epoch : 48] (l_loss: 0.04474) (t_loss: 0.07268) (accu: 0.9778)
[epoch : 49] (l_loss: 0.04477) (t_loss: 0.07458) (accu: 0.9751)
[epoch : 50] (l_loss: 0.04420) (t_loss: 0.07361) (accu: 0.9766)
Finish! (Best accu: 0.9789) (Time taken(sec) : 667.12) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (18507 | 247693)          6.95
fc1.weight   :      235200 (16163 | 219037)          6.87
fc2.weight   :        30000 (2062 | 27938)           6.87
fcout.weight :          1000 (282 | 718)            28.20
------------------------------------------------------------

Learning start! [Prune_iter : (13/21), Remaining weight : 6.95 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30018) (accu: 0.0990)
[epoch : 1] (l_loss: 0.39833) (t_loss: 0.15356) (accu: 0.9547)
[epoch : 2] (l_loss: 0.12223) (t_loss: 0.10525) (accu: 0.9681)
[epoch : 3] (l_loss: 0.09266) (t_loss: 0.09281) (accu: 0.9724)
[epoch : 4] (l_loss: 0.07827) (t_loss: 0.08628) (accu: 0.9736)
[epoch : 5] (l_loss: 0.07041) (t_loss: 0.08119) (accu: 0.9759)
[epoch : 6] (l_loss: 0.06459) (t_loss: 0.07749) (accu: 0.9749)
[epoch : 7] (l_loss: 0.05979) (t_loss: 0.07549) (accu: 0.9769)
[epoch : 8] (l_loss: 0.05659) (t_loss: 0.07444) (accu: 0.9775)
[epoch : 9] (l_loss: 0.05305) (t_loss: 0.07574) (accu: 0.9757)
[epoch : 10] (l_loss: 0.05060) (t_loss: 0.07590) (accu: 0.9756)
[epoch : 11] (l_loss: 0.04978) (t_loss: 0.07250) (accu: 0.9783)
[epoch : 12] (l_loss: 0.04879) (t_loss: 0.07377) (accu: 0.9754)
[epoch : 13] (l_loss: 0.04777) (t_loss: 0.07272) (accu: 0.9785)
[epoch : 14] (l_loss: 0.04728) (t_loss: 0.07692) (accu: 0.9760)
[epoch : 15] (l_loss: 0.04670) (t_loss: 0.07286) (accu: 0.9767)
[epoch : 16] (l_loss: 0.04663) (t_loss: 0.07562) (accu: 0.9765)
[epoch : 17] (l_loss: 0.04623) (t_loss: 0.07378) (accu: 0.9764)
[epoch : 18] (l_loss: 0.04620) (t_loss: 0.07290) (accu: 0.9769)
[epoch : 19] (l_loss: 0.04544) (t_loss: 0.07373) (accu: 0.9761)
[epoch : 20] (l_loss: 0.04582) (t_loss: 0.07546) (accu: 0.9776)
[epoch : 21] (l_loss: 0.04524) (t_loss: 0.07180) (accu: 0.9770)
[epoch : 22] (l_loss: 0.04492) (t_loss: 0.07663) (accu: 0.9767)
[epoch : 23] (l_loss: 0.04485) (t_loss: 0.07220) (accu: 0.9772)
[epoch : 24] (l_loss: 0.04472) (t_loss: 0.07274) (accu: 0.9784)
[epoch : 25] (l_loss: 0.04455) (t_loss: 0.07400) (accu: 0.9786)
[epoch : 26] (l_loss: 0.04477) (t_loss: 0.06987) (accu: 0.9783)
[epoch : 27] (l_loss: 0.04455) (t_loss: 0.07393) (accu: 0.9777)
[epoch : 28] (l_loss: 0.04504) (t_loss: 0.06904) (accu: 0.9788)
[epoch : 29] (l_loss: 0.04375) (t_loss: 0.07626) (accu: 0.9768)
[epoch : 30] (l_loss: 0.04456) (t_loss: 0.07230) (accu: 0.9770)
[epoch : 31] (l_loss: 0.04404) (t_loss: 0.06976) (accu: 0.9785)
[epoch : 32] (l_loss: 0.04428) (t_loss: 0.07158) (accu: 0.9780)
[epoch : 33] (l_loss: 0.04486) (t_loss: 0.07194) (accu: 0.9771)
[epoch : 34] (l_loss: 0.04452) (t_loss: 0.06977) (accu: 0.9784)
[epoch : 35] (l_loss: 0.04442) (t_loss: 0.06900) (accu: 0.9798)
[epoch : 36] (l_loss: 0.04410) (t_loss: 0.07634) (accu: 0.9769)
[epoch : 37] (l_loss: 0.04433) (t_loss: 0.07413) (accu: 0.9773)
[epoch : 38] (l_loss: 0.04438) (t_loss: 0.07359) (accu: 0.9765)
[epoch : 39] (l_loss: 0.04388) (t_loss: 0.07588) (accu: 0.9772)
[epoch : 40] (l_loss: 0.04390) (t_loss: 0.07048) (accu: 0.9781)
[epoch : 41] (l_loss: 0.04482) (t_loss: 0.07261) (accu: 0.9772)
[epoch : 42] (l_loss: 0.04437) (t_loss: 0.07443) (accu: 0.9772)
[epoch : 43] (l_loss: 0.04406) (t_loss: 0.07782) (accu: 0.9754)
[epoch : 44] (l_loss: 0.04425) (t_loss: 0.07380) (accu: 0.9774)
[epoch : 45] (l_loss: 0.04408) (t_loss: 0.07259) (accu: 0.9770)
[epoch : 46] (l_loss: 0.04438) (t_loss: 0.07131) (accu: 0.9777)
[epoch : 47] (l_loss: 0.04468) (t_loss: 0.07506) (accu: 0.9772)
[epoch : 48] (l_loss: 0.04432) (t_loss: 0.07342) (accu: 0.9769)
[epoch : 49] (l_loss: 0.04410) (t_loss: 0.07177) (accu: 0.9782)
[epoch : 50] (l_loss: 0.04442) (t_loss: 0.07280) (accu: 0.9775)
Finish! (Best accu: 0.9798) (Time taken(sec) : 669.25) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (14833 | 251367)          5.57
fc1.weight   :      235200 (12930 | 222270)          5.50
fc2.weight   :        30000 (1649 | 28351)           5.50
fcout.weight :          1000 (254 | 746)            25.40
------------------------------------------------------------

Learning start! [Prune_iter : (14/21), Remaining weight : 5.57 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29953) (accu: 0.0993)
[epoch : 1] (l_loss: 0.39883) (t_loss: 0.15161) (accu: 0.9571)
[epoch : 2] (l_loss: 0.12761) (t_loss: 0.11199) (accu: 0.9667)
[epoch : 3] (l_loss: 0.09738) (t_loss: 0.09545) (accu: 0.9716)
[epoch : 4] (l_loss: 0.08323) (t_loss: 0.09028) (accu: 0.9738)
[epoch : 5] (l_loss: 0.07420) (t_loss: 0.08202) (accu: 0.9762)
[epoch : 6] (l_loss: 0.06742) (t_loss: 0.08053) (accu: 0.9757)
[epoch : 7] (l_loss: 0.06326) (t_loss: 0.07811) (accu: 0.9758)
[epoch : 8] (l_loss: 0.05965) (t_loss: 0.07719) (accu: 0.9759)
[epoch : 9] (l_loss: 0.05678) (t_loss: 0.07584) (accu: 0.9768)
[epoch : 10] (l_loss: 0.05354) (t_loss: 0.07359) (accu: 0.9776)
[epoch : 11] (l_loss: 0.05076) (t_loss: 0.07439) (accu: 0.9770)
[epoch : 12] (l_loss: 0.05007) (t_loss: 0.07178) (accu: 0.9780)
[epoch : 13] (l_loss: 0.04888) (t_loss: 0.07190) (accu: 0.9779)
[epoch : 14] (l_loss: 0.04779) (t_loss: 0.07300) (accu: 0.9780)
[epoch : 15] (l_loss: 0.04750) (t_loss: 0.07495) (accu: 0.9765)
[epoch : 16] (l_loss: 0.04709) (t_loss: 0.07313) (accu: 0.9777)
[epoch : 17] (l_loss: 0.04681) (t_loss: 0.07215) (accu: 0.9772)
[epoch : 18] (l_loss: 0.04634) (t_loss: 0.07575) (accu: 0.9760)
[epoch : 19] (l_loss: 0.04669) (t_loss: 0.07804) (accu: 0.9759)
[epoch : 20] (l_loss: 0.04641) (t_loss: 0.07316) (accu: 0.9772)
[epoch : 21] (l_loss: 0.04632) (t_loss: 0.07170) (accu: 0.9776)
[epoch : 22] (l_loss: 0.04616) (t_loss: 0.07300) (accu: 0.9771)
[epoch : 23] (l_loss: 0.04610) (t_loss: 0.07477) (accu: 0.9765)
[epoch : 24] (l_loss: 0.04583) (t_loss: 0.07303) (accu: 0.9770)
[epoch : 25] (l_loss: 0.04532) (t_loss: 0.07401) (accu: 0.9758)
[epoch : 26] (l_loss: 0.04526) (t_loss: 0.07054) (accu: 0.9779)
[epoch : 27] (l_loss: 0.04549) (t_loss: 0.07089) (accu: 0.9772)
[epoch : 28] (l_loss: 0.04544) (t_loss: 0.07350) (accu: 0.9765)
[epoch : 29] (l_loss: 0.04531) (t_loss: 0.06938) (accu: 0.9782)
[epoch : 30] (l_loss: 0.04506) (t_loss: 0.07297) (accu: 0.9767)
[epoch : 31] (l_loss: 0.04483) (t_loss: 0.07005) (accu: 0.9779)
[epoch : 32] (l_loss: 0.04511) (t_loss: 0.07423) (accu: 0.9764)
[epoch : 33] (l_loss: 0.04474) (t_loss: 0.07485) (accu: 0.9766)
[epoch : 34] (l_loss: 0.04478) (t_loss: 0.07342) (accu: 0.9763)
[epoch : 35] (l_loss: 0.04467) (t_loss: 0.07367) (accu: 0.9773)
[epoch : 36] (l_loss: 0.04551) (t_loss: 0.07263) (accu: 0.9772)
[epoch : 37] (l_loss: 0.04457) (t_loss: 0.07241) (accu: 0.9758)
[epoch : 38] (l_loss: 0.04503) (t_loss: 0.07030) (accu: 0.9797)
[epoch : 39] (l_loss: 0.04499) (t_loss: 0.07398) (accu: 0.9772)
[epoch : 40] (l_loss: 0.04507) (t_loss: 0.07025) (accu: 0.9767)
[epoch : 41] (l_loss: 0.04497) (t_loss: 0.07545) (accu: 0.9767)
[epoch : 42] (l_loss: 0.04500) (t_loss: 0.07215) (accu: 0.9774)
[epoch : 43] (l_loss: 0.04470) (t_loss: 0.07048) (accu: 0.9787)
[epoch : 44] (l_loss: 0.04483) (t_loss: 0.07362) (accu: 0.9777)
[epoch : 45] (l_loss: 0.04496) (t_loss: 0.06977) (accu: 0.9783)
[epoch : 46] (l_loss: 0.04455) (t_loss: 0.07399) (accu: 0.9764)
[epoch : 47] (l_loss: 0.04466) (t_loss: 0.07129) (accu: 0.9757)
[epoch : 48] (l_loss: 0.04450) (t_loss: 0.07286) (accu: 0.9772)
[epoch : 49] (l_loss: 0.04418) (t_loss: 0.07158) (accu: 0.9782)
[epoch : 50] (l_loss: 0.04407) (t_loss: 0.06987) (accu: 0.9780)
Finish! (Best accu: 0.9797) (Time taken(sec) : 651.62) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (11892 | 254308)          4.47
fc1.weight   :      235200 (10344 | 224856)          4.40
fc2.weight   :        30000 (1319 | 28681)           4.40
fcout.weight :          1000 (229 | 771)            22.90
------------------------------------------------------------

Learning start! [Prune_iter : (15/21), Remaining weight : 4.47 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29645) (accu: 0.1032)
[epoch : 1] (l_loss: 0.39861) (t_loss: 0.15461) (accu: 0.9557)
[epoch : 2] (l_loss: 0.12796) (t_loss: 0.10987) (accu: 0.9679)
[epoch : 3] (l_loss: 0.09757) (t_loss: 0.09353) (accu: 0.9716)
[epoch : 4] (l_loss: 0.08323) (t_loss: 0.08884) (accu: 0.9734)
[epoch : 5] (l_loss: 0.07495) (t_loss: 0.08318) (accu: 0.9746)
[epoch : 6] (l_loss: 0.06878) (t_loss: 0.08339) (accu: 0.9754)
[epoch : 7] (l_loss: 0.06450) (t_loss: 0.07838) (accu: 0.9762)
[epoch : 8] (l_loss: 0.06005) (t_loss: 0.07704) (accu: 0.9770)
[epoch : 9] (l_loss: 0.05695) (t_loss: 0.07726) (accu: 0.9774)
[epoch : 10] (l_loss: 0.05443) (t_loss: 0.07533) (accu: 0.9770)
[epoch : 11] (l_loss: 0.05271) (t_loss: 0.07352) (accu: 0.9777)
[epoch : 12] (l_loss: 0.05160) (t_loss: 0.07545) (accu: 0.9771)
[epoch : 13] (l_loss: 0.05028) (t_loss: 0.07294) (accu: 0.9766)
[epoch : 14] (l_loss: 0.04919) (t_loss: 0.07471) (accu: 0.9782)
[epoch : 15] (l_loss: 0.04838) (t_loss: 0.07696) (accu: 0.9752)
[epoch : 16] (l_loss: 0.04838) (t_loss: 0.07490) (accu: 0.9762)
[epoch : 17] (l_loss: 0.04723) (t_loss: 0.07444) (accu: 0.9763)
[epoch : 18] (l_loss: 0.04735) (t_loss: 0.07590) (accu: 0.9773)
[epoch : 19] (l_loss: 0.04725) (t_loss: 0.07311) (accu: 0.9771)
[epoch : 20] (l_loss: 0.04693) (t_loss: 0.07621) (accu: 0.9753)
[epoch : 21] (l_loss: 0.04730) (t_loss: 0.07394) (accu: 0.9768)
[epoch : 22] (l_loss: 0.04662) (t_loss: 0.07892) (accu: 0.9759)
[epoch : 23] (l_loss: 0.04706) (t_loss: 0.07591) (accu: 0.9773)
[epoch : 24] (l_loss: 0.04637) (t_loss: 0.07450) (accu: 0.9768)
[epoch : 25] (l_loss: 0.04674) (t_loss: 0.07310) (accu: 0.9771)
[epoch : 26] (l_loss: 0.04654) (t_loss: 0.07587) (accu: 0.9757)
[epoch : 27] (l_loss: 0.04632) (t_loss: 0.07556) (accu: 0.9766)
[epoch : 28] (l_loss: 0.04594) (t_loss: 0.07423) (accu: 0.9766)
[epoch : 29] (l_loss: 0.04595) (t_loss: 0.07315) (accu: 0.9782)
[epoch : 30] (l_loss: 0.04619) (t_loss: 0.07469) (accu: 0.9757)
[epoch : 31] (l_loss: 0.04643) (t_loss: 0.07937) (accu: 0.9753)
[epoch : 32] (l_loss: 0.04521) (t_loss: 0.07371) (accu: 0.9760)
[epoch : 33] (l_loss: 0.04555) (t_loss: 0.07120) (accu: 0.9784)
[epoch : 34] (l_loss: 0.04572) (t_loss: 0.07298) (accu: 0.9763)
[epoch : 35] (l_loss: 0.04595) (t_loss: 0.07552) (accu: 0.9771)
[epoch : 36] (l_loss: 0.04549) (t_loss: 0.07159) (accu: 0.9773)
[epoch : 37] (l_loss: 0.04586) (t_loss: 0.07193) (accu: 0.9782)
[epoch : 38] (l_loss: 0.04561) (t_loss: 0.07373) (accu: 0.9766)
[epoch : 39] (l_loss: 0.04548) (t_loss: 0.07312) (accu: 0.9766)
[epoch : 40] (l_loss: 0.04503) (t_loss: 0.07334) (accu: 0.9778)
[epoch : 41] (l_loss: 0.04457) (t_loss: 0.07223) (accu: 0.9777)
[epoch : 42] (l_loss: 0.04498) (t_loss: 0.07043) (accu: 0.9783)
[epoch : 43] (l_loss: 0.04423) (t_loss: 0.07363) (accu: 0.9764)
[epoch : 44] (l_loss: 0.04503) (t_loss: 0.06987) (accu: 0.9797)
[epoch : 45] (l_loss: 0.04487) (t_loss: 0.07078) (accu: 0.9777)
[epoch : 46] (l_loss: 0.04449) (t_loss: 0.07354) (accu: 0.9762)
[epoch : 47] (l_loss: 0.04475) (t_loss: 0.06943) (accu: 0.9783)
[epoch : 48] (l_loss: 0.04460) (t_loss: 0.07142) (accu: 0.9776)
[epoch : 49] (l_loss: 0.04459) (t_loss: 0.07106) (accu: 0.9773)
[epoch : 50] (l_loss: 0.04472) (t_loss: 0.07299) (accu: 0.9780)
Finish! (Best accu: 0.9797) (Time taken(sec) : 653.94) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (9537 | 256663)          3.58
fc1.weight   :       235200 (8275 | 226925)          3.52
fc2.weight   :        30000 (1056 | 28944)           3.52
fcout.weight :          1000 (206 | 794)            20.60
------------------------------------------------------------

Learning start! [Prune_iter : (16/21), Remaining weight : 3.58 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29409) (accu: 0.0987)
[epoch : 1] (l_loss: 0.40226) (t_loss: 0.15491) (accu: 0.9544)
[epoch : 2] (l_loss: 0.12750) (t_loss: 0.10961) (accu: 0.9677)
[epoch : 3] (l_loss: 0.09683) (t_loss: 0.09327) (accu: 0.9722)
[epoch : 4] (l_loss: 0.08276) (t_loss: 0.08885) (accu: 0.9732)
[epoch : 5] (l_loss: 0.07306) (t_loss: 0.08227) (accu: 0.9744)
[epoch : 6] (l_loss: 0.06639) (t_loss: 0.08114) (accu: 0.9745)
[epoch : 7] (l_loss: 0.06082) (t_loss: 0.07909) (accu: 0.9743)
[epoch : 8] (l_loss: 0.05643) (t_loss: 0.07624) (accu: 0.9772)
[epoch : 9] (l_loss: 0.05344) (t_loss: 0.07953) (accu: 0.9742)
[epoch : 10] (l_loss: 0.05070) (t_loss: 0.07414) (accu: 0.9769)
[epoch : 11] (l_loss: 0.04883) (t_loss: 0.07394) (accu: 0.9771)
[epoch : 12] (l_loss: 0.04812) (t_loss: 0.07670) (accu: 0.9768)
[epoch : 13] (l_loss: 0.04711) (t_loss: 0.07139) (accu: 0.9769)
[epoch : 14] (l_loss: 0.04731) (t_loss: 0.07172) (accu: 0.9784)
[epoch : 15] (l_loss: 0.04657) (t_loss: 0.07424) (accu: 0.9759)
[epoch : 16] (l_loss: 0.04656) (t_loss: 0.07468) (accu: 0.9763)
[epoch : 17] (l_loss: 0.04598) (t_loss: 0.07351) (accu: 0.9777)
[epoch : 18] (l_loss: 0.04590) (t_loss: 0.07212) (accu: 0.9774)
[epoch : 19] (l_loss: 0.04561) (t_loss: 0.07360) (accu: 0.9769)
[epoch : 20] (l_loss: 0.04585) (t_loss: 0.07264) (accu: 0.9780)
[epoch : 21] (l_loss: 0.04519) (t_loss: 0.07269) (accu: 0.9779)
[epoch : 22] (l_loss: 0.04456) (t_loss: 0.07374) (accu: 0.9773)
[epoch : 23] (l_loss: 0.04515) (t_loss: 0.07290) (accu: 0.9776)
[epoch : 24] (l_loss: 0.04543) (t_loss: 0.07207) (accu: 0.9781)
[epoch : 25] (l_loss: 0.04469) (t_loss: 0.07182) (accu: 0.9778)
[epoch : 26] (l_loss: 0.04483) (t_loss: 0.07128) (accu: 0.9778)
[epoch : 27] (l_loss: 0.04490) (t_loss: 0.07358) (accu: 0.9777)
[epoch : 28] (l_loss: 0.04485) (t_loss: 0.07281) (accu: 0.9779)
[epoch : 29] (l_loss: 0.04467) (t_loss: 0.07287) (accu: 0.9780)
[epoch : 30] (l_loss: 0.04490) (t_loss: 0.07515) (accu: 0.9757)
[epoch : 31] (l_loss: 0.04497) (t_loss: 0.07353) (accu: 0.9767)
[epoch : 32] (l_loss: 0.04489) (t_loss: 0.07270) (accu: 0.9772)
[epoch : 33] (l_loss: 0.04468) (t_loss: 0.07277) (accu: 0.9777)
[epoch : 34] (l_loss: 0.04459) (t_loss: 0.07468) (accu: 0.9760)
[epoch : 35] (l_loss: 0.04445) (t_loss: 0.07296) (accu: 0.9772)
[epoch : 36] (l_loss: 0.04460) (t_loss: 0.07809) (accu: 0.9750)
[epoch : 37] (l_loss: 0.04476) (t_loss: 0.07396) (accu: 0.9773)
[epoch : 38] (l_loss: 0.04475) (t_loss: 0.07357) (accu: 0.9777)
[epoch : 39] (l_loss: 0.04421) (t_loss: 0.07441) (accu: 0.9772)
[epoch : 40] (l_loss: 0.04375) (t_loss: 0.07410) (accu: 0.9766)
[epoch : 41] (l_loss: 0.04385) (t_loss: 0.07063) (accu: 0.9770)
[epoch : 42] (l_loss: 0.04377) (t_loss: 0.07426) (accu: 0.9773)
[epoch : 43] (l_loss: 0.04364) (t_loss: 0.07258) (accu: 0.9767)
[epoch : 44] (l_loss: 0.04362) (t_loss: 0.07077) (accu: 0.9775)
[epoch : 45] (l_loss: 0.04364) (t_loss: 0.06970) (accu: 0.9783)
[epoch : 46] (l_loss: 0.04364) (t_loss: 0.07201) (accu: 0.9775)
[epoch : 47] (l_loss: 0.04417) (t_loss: 0.07068) (accu: 0.9787)
[epoch : 48] (l_loss: 0.04368) (t_loss: 0.07160) (accu: 0.9763)
[epoch : 49] (l_loss: 0.04374) (t_loss: 0.07094) (accu: 0.9782)
[epoch : 50] (l_loss: 0.04357) (t_loss: 0.07247) (accu: 0.9783)
Finish! (Best accu: 0.9787) (Time taken(sec) : 662.11) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (7649 | 258551)          2.87
fc1.weight   :       235200 (6620 | 228580)          2.81
fc2.weight   :        30000 (844 | 29156)            2.81
fcout.weight :          1000 (185 | 815)            18.50
------------------------------------------------------------

Learning start! [Prune_iter : (17/21), Remaining weight : 2.87 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29822) (accu: 0.0974)
[epoch : 1] (l_loss: 0.40067) (t_loss: 0.15050) (accu: 0.9546)
[epoch : 2] (l_loss: 0.12544) (t_loss: 0.10467) (accu: 0.9699)
[epoch : 3] (l_loss: 0.09589) (t_loss: 0.09199) (accu: 0.9731)
[epoch : 4] (l_loss: 0.08186) (t_loss: 0.08745) (accu: 0.9735)
[epoch : 5] (l_loss: 0.07318) (t_loss: 0.08841) (accu: 0.9734)
[epoch : 6] (l_loss: 0.06802) (t_loss: 0.07879) (accu: 0.9756)
[epoch : 7] (l_loss: 0.06323) (t_loss: 0.07977) (accu: 0.9742)
[epoch : 8] (l_loss: 0.05955) (t_loss: 0.07843) (accu: 0.9756)
[epoch : 9] (l_loss: 0.05687) (t_loss: 0.07281) (accu: 0.9770)
[epoch : 10] (l_loss: 0.05366) (t_loss: 0.07819) (accu: 0.9765)
[epoch : 11] (l_loss: 0.05220) (t_loss: 0.07216) (accu: 0.9789)
[epoch : 12] (l_loss: 0.05095) (t_loss: 0.07383) (accu: 0.9778)
[epoch : 13] (l_loss: 0.04923) (t_loss: 0.07178) (accu: 0.9786)
[epoch : 14] (l_loss: 0.04808) (t_loss: 0.07378) (accu: 0.9764)
[epoch : 15] (l_loss: 0.04784) (t_loss: 0.07586) (accu: 0.9767)
[epoch : 16] (l_loss: 0.04699) (t_loss: 0.07359) (accu: 0.9765)
[epoch : 17] (l_loss: 0.04694) (t_loss: 0.07217) (accu: 0.9778)
[epoch : 18] (l_loss: 0.04632) (t_loss: 0.07429) (accu: 0.9769)
[epoch : 19] (l_loss: 0.04663) (t_loss: 0.07499) (accu: 0.9768)
[epoch : 20] (l_loss: 0.04598) (t_loss: 0.07311) (accu: 0.9778)
[epoch : 21] (l_loss: 0.04581) (t_loss: 0.07400) (accu: 0.9772)
[epoch : 22] (l_loss: 0.04572) (t_loss: 0.07267) (accu: 0.9778)
[epoch : 23] (l_loss: 0.04449) (t_loss: 0.07047) (accu: 0.9779)
[epoch : 24] (l_loss: 0.04466) (t_loss: 0.06883) (accu: 0.9783)
[epoch : 25] (l_loss: 0.04430) (t_loss: 0.07430) (accu: 0.9760)
[epoch : 26] (l_loss: 0.04421) (t_loss: 0.07404) (accu: 0.9773)
[epoch : 27] (l_loss: 0.04446) (t_loss: 0.07192) (accu: 0.9773)
[epoch : 28] (l_loss: 0.04399) (t_loss: 0.07073) (accu: 0.9774)
[epoch : 29] (l_loss: 0.04434) (t_loss: 0.07400) (accu: 0.9773)
[epoch : 30] (l_loss: 0.04434) (t_loss: 0.07149) (accu: 0.9788)
[epoch : 31] (l_loss: 0.04416) (t_loss: 0.07297) (accu: 0.9769)
[epoch : 32] (l_loss: 0.04414) (t_loss: 0.07086) (accu: 0.9782)
[epoch : 33] (l_loss: 0.04420) (t_loss: 0.07079) (accu: 0.9776)
[epoch : 34] (l_loss: 0.04413) (t_loss: 0.06939) (accu: 0.9782)
[epoch : 35] (l_loss: 0.04407) (t_loss: 0.07058) (accu: 0.9792)
[epoch : 36] (l_loss: 0.04450) (t_loss: 0.07272) (accu: 0.9780)
[epoch : 37] (l_loss: 0.04395) (t_loss: 0.07133) (accu: 0.9777)
[epoch : 38] (l_loss: 0.04390) (t_loss: 0.07001) (accu: 0.9791)
[epoch : 39] (l_loss: 0.04433) (t_loss: 0.07014) (accu: 0.9779)
[epoch : 40] (l_loss: 0.04394) (t_loss: 0.07412) (accu: 0.9764)
[epoch : 41] (l_loss: 0.04442) (t_loss: 0.07096) (accu: 0.9784)
[epoch : 42] (l_loss: 0.04429) (t_loss: 0.07076) (accu: 0.9782)
[epoch : 43] (l_loss: 0.04401) (t_loss: 0.07207) (accu: 0.9771)
[epoch : 44] (l_loss: 0.04401) (t_loss: 0.07272) (accu: 0.9782)
[epoch : 45] (l_loss: 0.04366) (t_loss: 0.06890) (accu: 0.9793)
[epoch : 46] (l_loss: 0.04404) (t_loss: 0.07158) (accu: 0.9768)
[epoch : 47] (l_loss: 0.04395) (t_loss: 0.07234) (accu: 0.9773)
[epoch : 48] (l_loss: 0.04443) (t_loss: 0.07564) (accu: 0.9770)
[epoch : 49] (l_loss: 0.04410) (t_loss: 0.07392) (accu: 0.9761)
[epoch : 50] (l_loss: 0.04449) (t_loss: 0.07082) (accu: 0.9775)
Finish! (Best accu: 0.9793) (Time taken(sec) : 662.16) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (6139 | 260061)          2.31
fc1.weight   :       235200 (5296 | 229904)          2.25
fc2.weight   :        30000 (676 | 29324)            2.25
fcout.weight :          1000 (167 | 833)            16.70
------------------------------------------------------------

Learning start! [Prune_iter : (18/21), Remaining weight : 2.31 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29848) (accu: 0.0974)
[epoch : 1] (l_loss: 0.40148) (t_loss: 0.15053) (accu: 0.9571)
[epoch : 2] (l_loss: 0.12552) (t_loss: 0.11029) (accu: 0.9679)
[epoch : 3] (l_loss: 0.09452) (t_loss: 0.09390) (accu: 0.9718)
[epoch : 4] (l_loss: 0.08040) (t_loss: 0.08489) (accu: 0.9746)
[epoch : 5] (l_loss: 0.07211) (t_loss: 0.08094) (accu: 0.9758)
[epoch : 6] (l_loss: 0.06579) (t_loss: 0.08064) (accu: 0.9757)
[epoch : 7] (l_loss: 0.06118) (t_loss: 0.07579) (accu: 0.9772)
[epoch : 8] (l_loss: 0.05679) (t_loss: 0.07663) (accu: 0.9765)
[epoch : 9] (l_loss: 0.05384) (t_loss: 0.07815) (accu: 0.9756)
[epoch : 10] (l_loss: 0.05144) (t_loss: 0.07543) (accu: 0.9767)
[epoch : 11] (l_loss: 0.05038) (t_loss: 0.07306) (accu: 0.9775)
[epoch : 12] (l_loss: 0.04914) (t_loss: 0.07401) (accu: 0.9772)
[epoch : 13] (l_loss: 0.04856) (t_loss: 0.07266) (accu: 0.9784)
[epoch : 14] (l_loss: 0.04737) (t_loss: 0.07535) (accu: 0.9770)
[epoch : 15] (l_loss: 0.04700) (t_loss: 0.07398) (accu: 0.9770)
[epoch : 16] (l_loss: 0.04704) (t_loss: 0.07537) (accu: 0.9763)
[epoch : 17] (l_loss: 0.04698) (t_loss: 0.07248) (accu: 0.9780)
[epoch : 18] (l_loss: 0.04661) (t_loss: 0.07199) (accu: 0.9769)
[epoch : 19] (l_loss: 0.04619) (t_loss: 0.07472) (accu: 0.9766)
[epoch : 20] (l_loss: 0.04613) (t_loss: 0.07328) (accu: 0.9772)
[epoch : 21] (l_loss: 0.04630) (t_loss: 0.07263) (accu: 0.9778)
[epoch : 22] (l_loss: 0.04560) (t_loss: 0.07297) (accu: 0.9782)
[epoch : 23] (l_loss: 0.04527) (t_loss: 0.07405) (accu: 0.9772)
[epoch : 24] (l_loss: 0.04582) (t_loss: 0.07620) (accu: 0.9770)
[epoch : 25] (l_loss: 0.04498) (t_loss: 0.07344) (accu: 0.9773)
[epoch : 26] (l_loss: 0.04570) (t_loss: 0.07836) (accu: 0.9762)
[epoch : 27] (l_loss: 0.04554) (t_loss: 0.07220) (accu: 0.9776)
[epoch : 28] (l_loss: 0.04557) (t_loss: 0.07162) (accu: 0.9771)
[epoch : 29] (l_loss: 0.04542) (t_loss: 0.07217) (accu: 0.9765)
[epoch : 30] (l_loss: 0.04518) (t_loss: 0.07285) (accu: 0.9765)
[epoch : 31] (l_loss: 0.04485) (t_loss: 0.07144) (accu: 0.9783)
[epoch : 32] (l_loss: 0.04502) (t_loss: 0.07597) (accu: 0.9766)
[epoch : 33] (l_loss: 0.04459) (t_loss: 0.07454) (accu: 0.9771)
[epoch : 34] (l_loss: 0.04418) (t_loss: 0.07635) (accu: 0.9760)
[epoch : 35] (l_loss: 0.04396) (t_loss: 0.07522) (accu: 0.9772)
[epoch : 36] (l_loss: 0.04453) (t_loss: 0.07181) (accu: 0.9785)
[epoch : 37] (l_loss: 0.04441) (t_loss: 0.07212) (accu: 0.9779)
[epoch : 38] (l_loss: 0.04398) (t_loss: 0.07062) (accu: 0.9790)
[epoch : 39] (l_loss: 0.04389) (t_loss: 0.07058) (accu: 0.9782)
[epoch : 40] (l_loss: 0.04456) (t_loss: 0.07477) (accu: 0.9772)
[epoch : 41] (l_loss: 0.04386) (t_loss: 0.07188) (accu: 0.9788)
[epoch : 42] (l_loss: 0.04412) (t_loss: 0.07027) (accu: 0.9783)
[epoch : 43] (l_loss: 0.04417) (t_loss: 0.07178) (accu: 0.9791)
[epoch : 44] (l_loss: 0.04397) (t_loss: 0.07147) (accu: 0.9778)
[epoch : 45] (l_loss: 0.04396) (t_loss: 0.07159) (accu: 0.9778)
[epoch : 46] (l_loss: 0.04378) (t_loss: 0.07201) (accu: 0.9768)
[epoch : 47] (l_loss: 0.04428) (t_loss: 0.07167) (accu: 0.9771)
[epoch : 48] (l_loss: 0.04375) (t_loss: 0.06783) (accu: 0.9787)
[epoch : 49] (l_loss: 0.04379) (t_loss: 0.07029) (accu: 0.9776)
[epoch : 50] (l_loss: 0.04387) (t_loss: 0.07463) (accu: 0.9782)
Finish! (Best accu: 0.9791) (Time taken(sec) : 688.74) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (4927 | 261273)          1.85
fc1.weight   :       235200 (4237 | 230963)          1.80
fc2.weight   :        30000 (540 | 29460)            1.80
fcout.weight :          1000 (150 | 850)            15.00
------------------------------------------------------------

Learning start! [Prune_iter : (19/21), Remaining weight : 1.85 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29584) (accu: 0.0974)
[epoch : 1] (l_loss: 0.40348) (t_loss: 0.15303) (accu: 0.9572)
[epoch : 2] (l_loss: 0.12704) (t_loss: 0.10947) (accu: 0.9683)
[epoch : 3] (l_loss: 0.09615) (t_loss: 0.09396) (accu: 0.9718)
[epoch : 4] (l_loss: 0.08088) (t_loss: 0.08895) (accu: 0.9736)
[epoch : 5] (l_loss: 0.07276) (t_loss: 0.08434) (accu: 0.9740)
[epoch : 6] (l_loss: 0.06614) (t_loss: 0.07926) (accu: 0.9772)
[epoch : 7] (l_loss: 0.06139) (t_loss: 0.08292) (accu: 0.9733)
[epoch : 8] (l_loss: 0.05782) (t_loss: 0.07754) (accu: 0.9764)
[epoch : 9] (l_loss: 0.05481) (t_loss: 0.07898) (accu: 0.9754)
[epoch : 10] (l_loss: 0.05175) (t_loss: 0.07382) (accu: 0.9766)
[epoch : 11] (l_loss: 0.04962) (t_loss: 0.07248) (accu: 0.9771)
[epoch : 12] (l_loss: 0.04835) (t_loss: 0.07335) (accu: 0.9774)
[epoch : 13] (l_loss: 0.04738) (t_loss: 0.07070) (accu: 0.9781)
[epoch : 14] (l_loss: 0.04669) (t_loss: 0.07108) (accu: 0.9777)
[epoch : 15] (l_loss: 0.04637) (t_loss: 0.07097) (accu: 0.9781)
[epoch : 16] (l_loss: 0.04563) (t_loss: 0.07206) (accu: 0.9766)
[epoch : 17] (l_loss: 0.04525) (t_loss: 0.07084) (accu: 0.9776)
[epoch : 18] (l_loss: 0.04536) (t_loss: 0.07119) (accu: 0.9802)
[epoch : 19] (l_loss: 0.04478) (t_loss: 0.07135) (accu: 0.9779)
[epoch : 20] (l_loss: 0.04458) (t_loss: 0.06925) (accu: 0.9785)
[epoch : 21] (l_loss: 0.04432) (t_loss: 0.07013) (accu: 0.9776)
[epoch : 22] (l_loss: 0.04394) (t_loss: 0.06888) (accu: 0.9792)
[epoch : 23] (l_loss: 0.04389) (t_loss: 0.07271) (accu: 0.9773)
[epoch : 24] (l_loss: 0.04443) (t_loss: 0.07080) (accu: 0.9779)
[epoch : 25] (l_loss: 0.04364) (t_loss: 0.07369) (accu: 0.9771)
[epoch : 26] (l_loss: 0.04407) (t_loss: 0.07079) (accu: 0.9782)
[epoch : 27] (l_loss: 0.04338) (t_loss: 0.07219) (accu: 0.9777)
[epoch : 28] (l_loss: 0.04384) (t_loss: 0.07181) (accu: 0.9789)
[epoch : 29] (l_loss: 0.04406) (t_loss: 0.07167) (accu: 0.9778)
[epoch : 30] (l_loss: 0.04373) (t_loss: 0.07185) (accu: 0.9784)
[epoch : 31] (l_loss: 0.04321) (t_loss: 0.07168) (accu: 0.9783)
[epoch : 32] (l_loss: 0.04353) (t_loss: 0.07336) (accu: 0.9785)
[epoch : 33] (l_loss: 0.04380) (t_loss: 0.07482) (accu: 0.9766)
[epoch : 34] (l_loss: 0.04363) (t_loss: 0.07225) (accu: 0.9774)
[epoch : 35] (l_loss: 0.04386) (t_loss: 0.07098) (accu: 0.9784)
[epoch : 36] (l_loss: 0.04366) (t_loss: 0.07111) (accu: 0.9783)
[epoch : 37] (l_loss: 0.04345) (t_loss: 0.07422) (accu: 0.9768)
[epoch : 38] (l_loss: 0.04389) (t_loss: 0.06980) (accu: 0.9788)
[epoch : 39] (l_loss: 0.04355) (t_loss: 0.07226) (accu: 0.9775)
[epoch : 40] (l_loss: 0.04376) (t_loss: 0.07312) (accu: 0.9782)
[epoch : 41] (l_loss: 0.04331) (t_loss: 0.07388) (accu: 0.9777)
[epoch : 42] (l_loss: 0.04366) (t_loss: 0.07260) (accu: 0.9768)
[epoch : 43] (l_loss: 0.04324) (t_loss: 0.07169) (accu: 0.9770)
[epoch : 44] (l_loss: 0.04369) (t_loss: 0.06953) (accu: 0.9783)
[epoch : 45] (l_loss: 0.04375) (t_loss: 0.06833) (accu: 0.9790)
[epoch : 46] (l_loss: 0.04387) (t_loss: 0.07240) (accu: 0.9770)
[epoch : 47] (l_loss: 0.04355) (t_loss: 0.06999) (accu: 0.9778)
[epoch : 48] (l_loss: 0.04376) (t_loss: 0.07098) (accu: 0.9788)
[epoch : 49] (l_loss: 0.04369) (t_loss: 0.07300) (accu: 0.9773)
[epoch : 50] (l_loss: 0.04378) (t_loss: 0.07206) (accu: 0.9774)
Finish! (Best accu: 0.9802) (Time taken(sec) : 687.42) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (3957 | 262243)          1.49
fc1.weight   :       235200 (3390 | 231810)          1.44
fc2.weight   :        30000 (432 | 29568)            1.44
fcout.weight :          1000 (135 | 865)            13.50
------------------------------------------------------------

Learning start! [Prune_iter : (20/21), Remaining weight : 1.49 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29704) (accu: 0.0974)
[epoch : 1] (l_loss: 0.40181) (t_loss: 0.15379) (accu: 0.9546)
[epoch : 2] (l_loss: 0.12761) (t_loss: 0.10940) (accu: 0.9679)
[epoch : 3] (l_loss: 0.09722) (t_loss: 0.09338) (accu: 0.9728)
[epoch : 4] (l_loss: 0.08295) (t_loss: 0.08976) (accu: 0.9736)
[epoch : 5] (l_loss: 0.07415) (t_loss: 0.08279) (accu: 0.9758)
[epoch : 6] (l_loss: 0.06899) (t_loss: 0.07869) (accu: 0.9754)
[epoch : 7] (l_loss: 0.06445) (t_loss: 0.08024) (accu: 0.9771)
[epoch : 8] (l_loss: 0.06054) (t_loss: 0.07642) (accu: 0.9765)
[epoch : 9] (l_loss: 0.05735) (t_loss: 0.07491) (accu: 0.9767)
[epoch : 10] (l_loss: 0.05391) (t_loss: 0.07567) (accu: 0.9783)
[epoch : 11] (l_loss: 0.05168) (t_loss: 0.07226) (accu: 0.9775)
[epoch : 12] (l_loss: 0.05004) (t_loss: 0.07155) (accu: 0.9785)
[epoch : 13] (l_loss: 0.04891) (t_loss: 0.07365) (accu: 0.9780)
[epoch : 14] (l_loss: 0.04876) (t_loss: 0.07158) (accu: 0.9775)
[epoch : 15] (l_loss: 0.04802) (t_loss: 0.07261) (accu: 0.9773)
[epoch : 16] (l_loss: 0.04775) (t_loss: 0.07602) (accu: 0.9757)
[epoch : 17] (l_loss: 0.04704) (t_loss: 0.07369) (accu: 0.9766)
[epoch : 18] (l_loss: 0.04660) (t_loss: 0.07550) (accu: 0.9759)
[epoch : 19] (l_loss: 0.04682) (t_loss: 0.07237) (accu: 0.9765)
[epoch : 20] (l_loss: 0.04677) (t_loss: 0.07190) (accu: 0.9782)
[epoch : 21] (l_loss: 0.04604) (t_loss: 0.07301) (accu: 0.9779)
[epoch : 22] (l_loss: 0.04633) (t_loss: 0.07237) (accu: 0.9776)
[epoch : 23] (l_loss: 0.04622) (t_loss: 0.07275) (accu: 0.9767)
[epoch : 24] (l_loss: 0.04572) (t_loss: 0.07363) (accu: 0.9769)
[epoch : 25] (l_loss: 0.04616) (t_loss: 0.07264) (accu: 0.9777)
[epoch : 26] (l_loss: 0.04577) (t_loss: 0.07457) (accu: 0.9762)
[epoch : 27] (l_loss: 0.04498) (t_loss: 0.07115) (accu: 0.9779)
[epoch : 28] (l_loss: 0.04516) (t_loss: 0.07197) (accu: 0.9772)
[epoch : 29] (l_loss: 0.04496) (t_loss: 0.07283) (accu: 0.9766)
[epoch : 30] (l_loss: 0.04490) (t_loss: 0.07189) (accu: 0.9781)
[epoch : 31] (l_loss: 0.04480) (t_loss: 0.07410) (accu: 0.9761)
[epoch : 32] (l_loss: 0.04489) (t_loss: 0.07189) (accu: 0.9784)
[epoch : 33] (l_loss: 0.04495) (t_loss: 0.07356) (accu: 0.9768)
[epoch : 34] (l_loss: 0.04440) (t_loss: 0.07468) (accu: 0.9780)
[epoch : 35] (l_loss: 0.04505) (t_loss: 0.07132) (accu: 0.9765)
[epoch : 36] (l_loss: 0.04474) (t_loss: 0.07092) (accu: 0.9780)
[epoch : 37] (l_loss: 0.04415) (t_loss: 0.07162) (accu: 0.9776)
[epoch : 38] (l_loss: 0.04386) (t_loss: 0.07315) (accu: 0.9768)
[epoch : 39] (l_loss: 0.04396) (t_loss: 0.07061) (accu: 0.9794)
[epoch : 40] (l_loss: 0.04428) (t_loss: 0.07056) (accu: 0.9789)
[epoch : 41] (l_loss: 0.04367) (t_loss: 0.07109) (accu: 0.9767)
[epoch : 42] (l_loss: 0.04391) (t_loss: 0.07134) (accu: 0.9778)
[epoch : 43] (l_loss: 0.04395) (t_loss: 0.07221) (accu: 0.9776)
[epoch : 44] (l_loss: 0.04374) (t_loss: 0.07182) (accu: 0.9770)
[epoch : 45] (l_loss: 0.04372) (t_loss: 0.07339) (accu: 0.9769)
[epoch : 46] (l_loss: 0.04384) (t_loss: 0.07091) (accu: 0.9780)
[epoch : 47] (l_loss: 0.04363) (t_loss: 0.07096) (accu: 0.9780)
[epoch : 48] (l_loss: 0.04376) (t_loss: 0.07212) (accu: 0.9787)
[epoch : 49] (l_loss: 0.04379) (t_loss: 0.06916) (accu: 0.9786)
[epoch : 50] (l_loss: 0.04325) (t_loss: 0.07002) (accu: 0.9780)
Finish! (Best accu: 0.9794) (Time taken(sec) : 702.02) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (3180 | 263020)          1.19
fc1.weight   :       235200 (2712 | 232488)          1.15
fc2.weight   :        30000 (346 | 29654)            1.15
fcout.weight :          1000 (122 | 878)            12.20
------------------------------------------------------------

Learning start! [Prune_iter : (21/21), Remaining weight : 1.19 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29785) (accu: 0.0974)
[epoch : 1] (l_loss: 0.40122) (t_loss: 0.15713) (accu: 0.9534)
[epoch : 2] (l_loss: 0.12998) (t_loss: 0.11290) (accu: 0.9660)
[epoch : 3] (l_loss: 0.09808) (t_loss: 0.09734) (accu: 0.9699)
[epoch : 4] (l_loss: 0.08244) (t_loss: 0.08848) (accu: 0.9734)
[epoch : 5] (l_loss: 0.07368) (t_loss: 0.08380) (accu: 0.9758)
[epoch : 6] (l_loss: 0.06806) (t_loss: 0.08135) (accu: 0.9751)
[epoch : 7] (l_loss: 0.06352) (t_loss: 0.07864) (accu: 0.9766)
[epoch : 8] (l_loss: 0.05912) (t_loss: 0.07636) (accu: 0.9770)
[epoch : 9] (l_loss: 0.05627) (t_loss: 0.07950) (accu: 0.9748)
[epoch : 10] (l_loss: 0.05393) (t_loss: 0.07263) (accu: 0.9784)
[epoch : 11] (l_loss: 0.05198) (t_loss: 0.07820) (accu: 0.9752)
[epoch : 12] (l_loss: 0.05039) (t_loss: 0.07474) (accu: 0.9773)
[epoch : 13] (l_loss: 0.04926) (t_loss: 0.07352) (accu: 0.9759)
[epoch : 14] (l_loss: 0.04836) (t_loss: 0.07432) (accu: 0.9765)
[epoch : 15] (l_loss: 0.04811) (t_loss: 0.07801) (accu: 0.9763)
[epoch : 16] (l_loss: 0.04707) (t_loss: 0.07799) (accu: 0.9757)
[epoch : 17] (l_loss: 0.04657) (t_loss: 0.07611) (accu: 0.9773)
[epoch : 18] (l_loss: 0.04637) (t_loss: 0.07493) (accu: 0.9764)
[epoch : 19] (l_loss: 0.04637) (t_loss: 0.07495) (accu: 0.9753)
[epoch : 20] (l_loss: 0.04601) (t_loss: 0.07403) (accu: 0.9761)
[epoch : 21] (l_loss: 0.04616) (t_loss: 0.07310) (accu: 0.9768)
[epoch : 22] (l_loss: 0.04555) (t_loss: 0.07381) (accu: 0.9779)
[epoch : 23] (l_loss: 0.04562) (t_loss: 0.07114) (accu: 0.9774)
[epoch : 24] (l_loss: 0.04522) (t_loss: 0.07349) (accu: 0.9765)
[epoch : 25] (l_loss: 0.04560) (t_loss: 0.07245) (accu: 0.9775)
[epoch : 26] (l_loss: 0.04571) (t_loss: 0.07064) (accu: 0.9779)
[epoch : 27] (l_loss: 0.04504) (t_loss: 0.07407) (accu: 0.9763)
[epoch : 28] (l_loss: 0.04599) (t_loss: 0.07160) (accu: 0.9771)
[epoch : 29] (l_loss: 0.04552) (t_loss: 0.07669) (accu: 0.9750)
[epoch : 30] (l_loss: 0.04482) (t_loss: 0.07145) (accu: 0.9775)
[epoch : 31] (l_loss: 0.04488) (t_loss: 0.07073) (accu: 0.9771)
[epoch : 32] (l_loss: 0.04449) (t_loss: 0.07312) (accu: 0.9769)
[epoch : 33] (l_loss: 0.04425) (t_loss: 0.07074) (accu: 0.9779)
[epoch : 34] (l_loss: 0.04408) (t_loss: 0.07223) (accu: 0.9761)
[epoch : 35] (l_loss: 0.04407) (t_loss: 0.07215) (accu: 0.9769)
[epoch : 36] (l_loss: 0.04441) (t_loss: 0.07260) (accu: 0.9777)
[epoch : 37] (l_loss: 0.04373) (t_loss: 0.07364) (accu: 0.9769)
[epoch : 38] (l_loss: 0.04366) (t_loss: 0.07269) (accu: 0.9769)
[epoch : 39] (l_loss: 0.04419) (t_loss: 0.07099) (accu: 0.9784)
[epoch : 40] (l_loss: 0.04439) (t_loss: 0.07274) (accu: 0.9772)
[epoch : 41] (l_loss: 0.04365) (t_loss: 0.07153) (accu: 0.9776)
[epoch : 42] (l_loss: 0.04428) (t_loss: 0.07208) (accu: 0.9776)
[epoch : 43] (l_loss: 0.04397) (t_loss: 0.07046) (accu: 0.9777)
[epoch : 44] (l_loss: 0.04372) (t_loss: 0.06934) (accu: 0.9775)
[epoch : 45] (l_loss: 0.04432) (t_loss: 0.07309) (accu: 0.9774)
[epoch : 46] (l_loss: 0.04406) (t_loss: 0.07572) (accu: 0.9758)
[epoch : 47] (l_loss: 0.04385) (t_loss: 0.07229) (accu: 0.9780)
[epoch : 48] (l_loss: 0.04376) (t_loss: 0.07291) (accu: 0.9769)
[epoch : 49] (l_loss: 0.04390) (t_loss: 0.06940) (accu: 0.9778)
[epoch : 50] (l_loss: 0.04399) (t_loss: 0.07545) (accu: 0.9767)
Finish! (Best accu: 0.9784) (Time taken(sec) : 718.99) 


Maximum accuracy per weight remaining
Remaining weight 100.0 %  Epoch 39 Accu 0.9788
Remaining weight 80.04 %  Epoch 40 Accu 0.9786
Remaining weight 64.06 %  Epoch 19 Accu 0.9797
Remaining weight 51.28 %  Epoch 42 Accu 0.9787
Remaining weight 41.05 %  Epoch 40 Accu 0.9789
Remaining weight 32.87 %  Epoch 13 Accu 0.9787
Remaining weight 26.32 %  Epoch 30 Accu 0.9793
Remaining weight 21.07 %  Epoch 33 Accu 0.9796
Remaining weight 16.88 %  Epoch 19 Accu 0.9789
Remaining weight 13.52 %  Epoch 19 Accu 0.9795
Remaining weight 10.83 %  Epoch 47 Accu 0.9788
Remaining weight 8.68 %  Epoch 27 Accu 0.9789
Remaining weight 6.95 %  Epoch 34 Accu 0.9798
Remaining weight 5.57 %  Epoch 37 Accu 0.9797
Remaining weight 4.47 %  Epoch 43 Accu 0.9797
Remaining weight 3.58 %  Epoch 46 Accu 0.9787
Remaining weight 2.87 %  Epoch 44 Accu 0.9793
Remaining weight 2.31 %  Epoch 42 Accu 0.9791
Remaining weight 1.85 %  Epoch 17 Accu 0.9802
Remaining weight 1.49 %  Epoch 38 Accu 0.9794
Remaining weight 1.19 %  Epoch 38 Accu 0.9784
===================================================================== 

Test_Iter (3/5)
------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :        266200 (266200 | 0)          100.00
fc1.weight   :        235200 (235200 | 0)          100.00
fc2.weight   :         30000 (30000 | 0)           100.00
fcout.weight :          1000 (1000 | 0)            100.00
------------------------------------------------------------

Learning start! [Prune_iter : (1/21), Remaining weight : 100.0 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.83409) (accu: 0.0850)
[epoch : 1] (l_loss: 0.44399) (t_loss: 0.14556) (accu: 0.9575)
[epoch : 2] (l_loss: 0.12103) (t_loss: 0.10917) (accu: 0.9664)
[epoch : 3] (l_loss: 0.09234) (t_loss: 0.09534) (accu: 0.9718)
[epoch : 4] (l_loss: 0.07872) (t_loss: 0.08645) (accu: 0.9733)
[epoch : 5] (l_loss: 0.07018) (t_loss: 0.08082) (accu: 0.9754)
[epoch : 6] (l_loss: 0.06354) (t_loss: 0.07729) (accu: 0.9756)
[epoch : 7] (l_loss: 0.05830) (t_loss: 0.07293) (accu: 0.9765)
[epoch : 8] (l_loss: 0.05444) (t_loss: 0.07553) (accu: 0.9765)
[epoch : 9] (l_loss: 0.05225) (t_loss: 0.07280) (accu: 0.9784)
[epoch : 10] (l_loss: 0.05062) (t_loss: 0.07232) (accu: 0.9760)
[epoch : 11] (l_loss: 0.04925) (t_loss: 0.07256) (accu: 0.9756)
[epoch : 12] (l_loss: 0.04792) (t_loss: 0.07274) (accu: 0.9765)
[epoch : 13] (l_loss: 0.04748) (t_loss: 0.07558) (accu: 0.9761)
[epoch : 14] (l_loss: 0.04714) (t_loss: 0.07297) (accu: 0.9762)
[epoch : 15] (l_loss: 0.04622) (t_loss: 0.07231) (accu: 0.9773)
[epoch : 16] (l_loss: 0.04624) (t_loss: 0.06938) (accu: 0.9780)
[epoch : 17] (l_loss: 0.04556) (t_loss: 0.07253) (accu: 0.9776)
[epoch : 18] (l_loss: 0.04513) (t_loss: 0.07196) (accu: 0.9768)
[epoch : 19] (l_loss: 0.04600) (t_loss: 0.07146) (accu: 0.9769)
[epoch : 20] (l_loss: 0.04535) (t_loss: 0.07140) (accu: 0.9777)
[epoch : 21] (l_loss: 0.04518) (t_loss: 0.07396) (accu: 0.9766)
[epoch : 22] (l_loss: 0.04488) (t_loss: 0.07063) (accu: 0.9774)
[epoch : 23] (l_loss: 0.04487) (t_loss: 0.07212) (accu: 0.9776)
[epoch : 24] (l_loss: 0.04544) (t_loss: 0.07279) (accu: 0.9764)
[epoch : 25] (l_loss: 0.04485) (t_loss: 0.07181) (accu: 0.9778)
[epoch : 26] (l_loss: 0.04462) (t_loss: 0.07171) (accu: 0.9780)
[epoch : 27] (l_loss: 0.04464) (t_loss: 0.07342) (accu: 0.9771)
[epoch : 28] (l_loss: 0.04480) (t_loss: 0.07370) (accu: 0.9763)
[epoch : 29] (l_loss: 0.04502) (t_loss: 0.07260) (accu: 0.9779)
[epoch : 30] (l_loss: 0.04487) (t_loss: 0.07200) (accu: 0.9787)
[epoch : 31] (l_loss: 0.04452) (t_loss: 0.07404) (accu: 0.9765)
[epoch : 32] (l_loss: 0.04455) (t_loss: 0.07017) (accu: 0.9767)
[epoch : 33] (l_loss: 0.04461) (t_loss: 0.07296) (accu: 0.9768)
[epoch : 34] (l_loss: 0.04412) (t_loss: 0.07582) (accu: 0.9757)
[epoch : 35] (l_loss: 0.04498) (t_loss: 0.07248) (accu: 0.9765)
[epoch : 36] (l_loss: 0.04443) (t_loss: 0.07165) (accu: 0.9779)
[epoch : 37] (l_loss: 0.04448) (t_loss: 0.07233) (accu: 0.9773)
[epoch : 38] (l_loss: 0.04426) (t_loss: 0.07590) (accu: 0.9754)
[epoch : 39] (l_loss: 0.04472) (t_loss: 0.07352) (accu: 0.9762)
[epoch : 40] (l_loss: 0.04427) (t_loss: 0.07066) (accu: 0.9781)
[epoch : 41] (l_loss: 0.04507) (t_loss: 0.07650) (accu: 0.9761)
[epoch : 42] (l_loss: 0.04446) (t_loss: 0.07296) (accu: 0.9767)
[epoch : 43] (l_loss: 0.04490) (t_loss: 0.06992) (accu: 0.9781)
[epoch : 44] (l_loss: 0.04450) (t_loss: 0.07042) (accu: 0.9777)
[epoch : 45] (l_loss: 0.04460) (t_loss: 0.06993) (accu: 0.9779)
[epoch : 46] (l_loss: 0.04426) (t_loss: 0.07275) (accu: 0.9764)
[epoch : 47] (l_loss: 0.04449) (t_loss: 0.07584) (accu: 0.9747)
[epoch : 48] (l_loss: 0.04436) (t_loss: 0.06991) (accu: 0.9772)
[epoch : 49] (l_loss: 0.04461) (t_loss: 0.07569) (accu: 0.9753)
[epoch : 50] (l_loss: 0.04497) (t_loss: 0.07149) (accu: 0.9776)
Finish! (Best accu: 0.9787) (Time taken(sec) : 693.15) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (213060 | 53140)         80.04
fc1.weight   :      235200 (188160 | 47040)         80.00
fc2.weight   :        30000 (24000 | 6000)          80.00
fcout.weight :          1000 (900 | 100)            90.00
------------------------------------------------------------

Learning start! [Prune_iter : (2/21), Remaining weight : 80.04 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.58485) (accu: 0.1096)
[epoch : 1] (l_loss: 0.43084) (t_loss: 0.14834) (accu: 0.9575)
[epoch : 2] (l_loss: 0.12254) (t_loss: 0.10976) (accu: 0.9677)
[epoch : 3] (l_loss: 0.09203) (t_loss: 0.08887) (accu: 0.9733)
[epoch : 4] (l_loss: 0.07866) (t_loss: 0.08678) (accu: 0.9720)
[epoch : 5] (l_loss: 0.07011) (t_loss: 0.07903) (accu: 0.9751)
[epoch : 6] (l_loss: 0.06423) (t_loss: 0.07787) (accu: 0.9759)
[epoch : 7] (l_loss: 0.06004) (t_loss: 0.07801) (accu: 0.9759)
[epoch : 8] (l_loss: 0.05691) (t_loss: 0.07896) (accu: 0.9756)
[epoch : 9] (l_loss: 0.05325) (t_loss: 0.07897) (accu: 0.9759)
[epoch : 10] (l_loss: 0.05171) (t_loss: 0.07312) (accu: 0.9769)
[epoch : 11] (l_loss: 0.04997) (t_loss: 0.07018) (accu: 0.9781)
[epoch : 12] (l_loss: 0.04847) (t_loss: 0.07362) (accu: 0.9766)
[epoch : 13] (l_loss: 0.04743) (t_loss: 0.07488) (accu: 0.9769)
[epoch : 14] (l_loss: 0.04756) (t_loss: 0.07404) (accu: 0.9785)
[epoch : 15] (l_loss: 0.04657) (t_loss: 0.07170) (accu: 0.9769)
[epoch : 16] (l_loss: 0.04627) (t_loss: 0.07342) (accu: 0.9762)
[epoch : 17] (l_loss: 0.04665) (t_loss: 0.07187) (accu: 0.9777)
[epoch : 18] (l_loss: 0.04617) (t_loss: 0.07202) (accu: 0.9776)
[epoch : 19] (l_loss: 0.04601) (t_loss: 0.07188) (accu: 0.9774)
[epoch : 20] (l_loss: 0.04543) (t_loss: 0.07269) (accu: 0.9776)
[epoch : 21] (l_loss: 0.04544) (t_loss: 0.07264) (accu: 0.9771)
[epoch : 22] (l_loss: 0.04484) (t_loss: 0.07486) (accu: 0.9757)
[epoch : 23] (l_loss: 0.04453) (t_loss: 0.07246) (accu: 0.9774)
[epoch : 24] (l_loss: 0.04440) (t_loss: 0.07174) (accu: 0.9765)
[epoch : 25] (l_loss: 0.04453) (t_loss: 0.07321) (accu: 0.9769)
[epoch : 26] (l_loss: 0.04433) (t_loss: 0.07193) (accu: 0.9786)
[epoch : 27] (l_loss: 0.04471) (t_loss: 0.07463) (accu: 0.9764)
[epoch : 28] (l_loss: 0.04454) (t_loss: 0.07286) (accu: 0.9765)
[epoch : 29] (l_loss: 0.04414) (t_loss: 0.06920) (accu: 0.9779)
[epoch : 30] (l_loss: 0.04421) (t_loss: 0.07147) (accu: 0.9768)
[epoch : 31] (l_loss: 0.04446) (t_loss: 0.07598) (accu: 0.9765)
[epoch : 32] (l_loss: 0.04400) (t_loss: 0.07006) (accu: 0.9778)
[epoch : 33] (l_loss: 0.04453) (t_loss: 0.07265) (accu: 0.9788)
[epoch : 34] (l_loss: 0.04430) (t_loss: 0.07136) (accu: 0.9778)
[epoch : 35] (l_loss: 0.04418) (t_loss: 0.07291) (accu: 0.9775)
[epoch : 36] (l_loss: 0.04403) (t_loss: 0.07055) (accu: 0.9790)
[epoch : 37] (l_loss: 0.04453) (t_loss: 0.07036) (accu: 0.9785)
[epoch : 38] (l_loss: 0.04411) (t_loss: 0.07104) (accu: 0.9777)
[epoch : 39] (l_loss: 0.04427) (t_loss: 0.07215) (accu: 0.9762)
[epoch : 40] (l_loss: 0.04415) (t_loss: 0.07151) (accu: 0.9762)
[epoch : 41] (l_loss: 0.04438) (t_loss: 0.07541) (accu: 0.9765)
[epoch : 42] (l_loss: 0.04458) (t_loss: 0.07012) (accu: 0.9779)
[epoch : 43] (l_loss: 0.04399) (t_loss: 0.07084) (accu: 0.9776)
[epoch : 44] (l_loss: 0.04411) (t_loss: 0.07150) (accu: 0.9773)
[epoch : 45] (l_loss: 0.04430) (t_loss: 0.07278) (accu: 0.9775)
[epoch : 46] (l_loss: 0.04441) (t_loss: 0.07391) (accu: 0.9780)
[epoch : 47] (l_loss: 0.04380) (t_loss: 0.07098) (accu: 0.9776)
[epoch : 48] (l_loss: 0.04434) (t_loss: 0.06984) (accu: 0.9775)
[epoch : 49] (l_loss: 0.04437) (t_loss: 0.07099) (accu: 0.9776)
[epoch : 50] (l_loss: 0.04427) (t_loss: 0.06972) (accu: 0.9781)
Finish! (Best accu: 0.9790) (Time taken(sec) : 683.71) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (170538 | 95662)         64.06
fc1.weight   :      235200 (150528 | 84672)         64.00
fc2.weight   :       30000 (19200 | 10800)          64.00
fcout.weight :          1000 (810 | 190)            81.00
------------------------------------------------------------

Learning start! [Prune_iter : (3/21), Remaining weight : 64.06 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.43185) (accu: 0.1084)
[epoch : 1] (l_loss: 0.42923) (t_loss: 0.14963) (accu: 0.9573)
[epoch : 2] (l_loss: 0.12423) (t_loss: 0.10594) (accu: 0.9691)
[epoch : 3] (l_loss: 0.09367) (t_loss: 0.09663) (accu: 0.9699)
[epoch : 4] (l_loss: 0.07895) (t_loss: 0.08834) (accu: 0.9733)
[epoch : 5] (l_loss: 0.07053) (t_loss: 0.08389) (accu: 0.9755)
[epoch : 6] (l_loss: 0.06544) (t_loss: 0.07929) (accu: 0.9756)
[epoch : 7] (l_loss: 0.06112) (t_loss: 0.07827) (accu: 0.9750)
[epoch : 8] (l_loss: 0.05750) (t_loss: 0.07566) (accu: 0.9764)
[epoch : 9] (l_loss: 0.05412) (t_loss: 0.07480) (accu: 0.9770)
[epoch : 10] (l_loss: 0.05209) (t_loss: 0.07309) (accu: 0.9786)
[epoch : 11] (l_loss: 0.04973) (t_loss: 0.07131) (accu: 0.9779)
[epoch : 12] (l_loss: 0.04903) (t_loss: 0.07545) (accu: 0.9764)
[epoch : 13] (l_loss: 0.04807) (t_loss: 0.07503) (accu: 0.9757)
[epoch : 14] (l_loss: 0.04739) (t_loss: 0.07411) (accu: 0.9759)
[epoch : 15] (l_loss: 0.04762) (t_loss: 0.07206) (accu: 0.9766)
[epoch : 16] (l_loss: 0.04662) (t_loss: 0.07265) (accu: 0.9779)
[epoch : 17] (l_loss: 0.04690) (t_loss: 0.07305) (accu: 0.9776)
[epoch : 18] (l_loss: 0.04619) (t_loss: 0.07426) (accu: 0.9771)
[epoch : 19] (l_loss: 0.04590) (t_loss: 0.07458) (accu: 0.9774)
[epoch : 20] (l_loss: 0.04611) (t_loss: 0.07328) (accu: 0.9779)
[epoch : 21] (l_loss: 0.04608) (t_loss: 0.07315) (accu: 0.9764)
[epoch : 22] (l_loss: 0.04611) (t_loss: 0.07502) (accu: 0.9760)
[epoch : 23] (l_loss: 0.04595) (t_loss: 0.07061) (accu: 0.9761)
[epoch : 24] (l_loss: 0.04563) (t_loss: 0.07293) (accu: 0.9760)
[epoch : 25] (l_loss: 0.04601) (t_loss: 0.07838) (accu: 0.9753)
[epoch : 26] (l_loss: 0.04553) (t_loss: 0.07289) (accu: 0.9783)
[epoch : 27] (l_loss: 0.04551) (t_loss: 0.07532) (accu: 0.9763)
[epoch : 28] (l_loss: 0.04520) (t_loss: 0.07415) (accu: 0.9768)
[epoch : 29] (l_loss: 0.04570) (t_loss: 0.07299) (accu: 0.9775)
[epoch : 30] (l_loss: 0.04561) (t_loss: 0.07423) (accu: 0.9774)
[epoch : 31] (l_loss: 0.04591) (t_loss: 0.07966) (accu: 0.9742)
[epoch : 32] (l_loss: 0.04561) (t_loss: 0.07369) (accu: 0.9769)
[epoch : 33] (l_loss: 0.04532) (t_loss: 0.07267) (accu: 0.9768)
[epoch : 34] (l_loss: 0.04561) (t_loss: 0.07372) (accu: 0.9762)
[epoch : 35] (l_loss: 0.04579) (t_loss: 0.07048) (accu: 0.9774)
[epoch : 36] (l_loss: 0.04531) (t_loss: 0.07262) (accu: 0.9778)
[epoch : 37] (l_loss: 0.04524) (t_loss: 0.07167) (accu: 0.9774)
[epoch : 38] (l_loss: 0.04530) (t_loss: 0.07286) (accu: 0.9776)
[epoch : 39] (l_loss: 0.04490) (t_loss: 0.07363) (accu: 0.9783)
[epoch : 40] (l_loss: 0.04498) (t_loss: 0.07812) (accu: 0.9768)
[epoch : 41] (l_loss: 0.04544) (t_loss: 0.07170) (accu: 0.9780)
[epoch : 42] (l_loss: 0.04511) (t_loss: 0.07102) (accu: 0.9774)
[epoch : 43] (l_loss: 0.04565) (t_loss: 0.07094) (accu: 0.9778)
[epoch : 44] (l_loss: 0.04506) (t_loss: 0.07007) (accu: 0.9772)
[epoch : 45] (l_loss: 0.04502) (t_loss: 0.07481) (accu: 0.9755)
[epoch : 46] (l_loss: 0.04517) (t_loss: 0.07210) (accu: 0.9776)
[epoch : 47] (l_loss: 0.04516) (t_loss: 0.07474) (accu: 0.9759)
[epoch : 48] (l_loss: 0.04498) (t_loss: 0.07022) (accu: 0.9775)
[epoch : 49] (l_loss: 0.04490) (t_loss: 0.07357) (accu: 0.9767)
[epoch : 50] (l_loss: 0.04511) (t_loss: 0.07214) (accu: 0.9773)
Finish! (Best accu: 0.9786) (Time taken(sec) : 663.41) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (136511 | 129689)        51.28
fc1.weight   :      235200 (120422 | 114778)        51.20
fc2.weight   :       30000 (15360 | 14640)          51.20
fcout.weight :          1000 (729 | 271)            72.90
------------------------------------------------------------

Learning start! [Prune_iter : (4/21), Remaining weight : 51.28 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.39590) (accu: 0.1243)
[epoch : 1] (l_loss: 0.43046) (t_loss: 0.15341) (accu: 0.9542)
[epoch : 2] (l_loss: 0.12680) (t_loss: 0.10763) (accu: 0.9673)
[epoch : 3] (l_loss: 0.09477) (t_loss: 0.09302) (accu: 0.9717)
[epoch : 4] (l_loss: 0.07992) (t_loss: 0.08702) (accu: 0.9736)
[epoch : 5] (l_loss: 0.07079) (t_loss: 0.08339) (accu: 0.9747)
[epoch : 6] (l_loss: 0.06542) (t_loss: 0.07794) (accu: 0.9765)
[epoch : 7] (l_loss: 0.06085) (t_loss: 0.07908) (accu: 0.9740)
[epoch : 8] (l_loss: 0.05817) (t_loss: 0.08198) (accu: 0.9762)
[epoch : 9] (l_loss: 0.05530) (t_loss: 0.07431) (accu: 0.9769)
[epoch : 10] (l_loss: 0.05396) (t_loss: 0.07629) (accu: 0.9760)
[epoch : 11] (l_loss: 0.05210) (t_loss: 0.07464) (accu: 0.9752)
[epoch : 12] (l_loss: 0.05134) (t_loss: 0.07203) (accu: 0.9773)
[epoch : 13] (l_loss: 0.05103) (t_loss: 0.07363) (accu: 0.9767)
[epoch : 14] (l_loss: 0.05063) (t_loss: 0.07355) (accu: 0.9769)
[epoch : 15] (l_loss: 0.04968) (t_loss: 0.07583) (accu: 0.9771)
[epoch : 16] (l_loss: 0.04975) (t_loss: 0.07481) (accu: 0.9765)
[epoch : 17] (l_loss: 0.04953) (t_loss: 0.07705) (accu: 0.9752)
[epoch : 18] (l_loss: 0.04913) (t_loss: 0.07348) (accu: 0.9765)
[epoch : 19] (l_loss: 0.04872) (t_loss: 0.07753) (accu: 0.9767)
[epoch : 20] (l_loss: 0.04892) (t_loss: 0.07323) (accu: 0.9778)
[epoch : 21] (l_loss: 0.04876) (t_loss: 0.07211) (accu: 0.9783)
[epoch : 22] (l_loss: 0.04839) (t_loss: 0.07280) (accu: 0.9765)
[epoch : 23] (l_loss: 0.04844) (t_loss: 0.07470) (accu: 0.9763)
[epoch : 24] (l_loss: 0.04814) (t_loss: 0.07379) (accu: 0.9770)
[epoch : 25] (l_loss: 0.04807) (t_loss: 0.07306) (accu: 0.9782)
[epoch : 26] (l_loss: 0.04808) (t_loss: 0.07502) (accu: 0.9767)
[epoch : 27] (l_loss: 0.04793) (t_loss: 0.07183) (accu: 0.9760)
[epoch : 28] (l_loss: 0.04771) (t_loss: 0.07369) (accu: 0.9777)
[epoch : 29] (l_loss: 0.04784) (t_loss: 0.07658) (accu: 0.9757)
[epoch : 30] (l_loss: 0.04794) (t_loss: 0.07769) (accu: 0.9755)
[epoch : 31] (l_loss: 0.04795) (t_loss: 0.07456) (accu: 0.9772)
[epoch : 32] (l_loss: 0.04796) (t_loss: 0.07518) (accu: 0.9755)
[epoch : 33] (l_loss: 0.04773) (t_loss: 0.07077) (accu: 0.9784)
[epoch : 34] (l_loss: 0.04806) (t_loss: 0.07337) (accu: 0.9772)
[epoch : 35] (l_loss: 0.04780) (t_loss: 0.07342) (accu: 0.9773)
[epoch : 36] (l_loss: 0.04792) (t_loss: 0.07393) (accu: 0.9774)
[epoch : 37] (l_loss: 0.04823) (t_loss: 0.07548) (accu: 0.9765)
[epoch : 38] (l_loss: 0.04751) (t_loss: 0.07406) (accu: 0.9770)
[epoch : 39] (l_loss: 0.04797) (t_loss: 0.07832) (accu: 0.9743)
[epoch : 40] (l_loss: 0.04761) (t_loss: 0.07352) (accu: 0.9756)
[epoch : 41] (l_loss: 0.04797) (t_loss: 0.07580) (accu: 0.9768)
[epoch : 42] (l_loss: 0.04761) (t_loss: 0.07364) (accu: 0.9770)
[epoch : 43] (l_loss: 0.04742) (t_loss: 0.07213) (accu: 0.9766)
[epoch : 44] (l_loss: 0.04840) (t_loss: 0.07138) (accu: 0.9775)
[epoch : 45] (l_loss: 0.04747) (t_loss: 0.07347) (accu: 0.9769)
[epoch : 46] (l_loss: 0.04808) (t_loss: 0.07569) (accu: 0.9761)
[epoch : 47] (l_loss: 0.04842) (t_loss: 0.07805) (accu: 0.9765)
[epoch : 48] (l_loss: 0.04756) (t_loss: 0.07529) (accu: 0.9758)
[epoch : 49] (l_loss: 0.04766) (t_loss: 0.07509) (accu: 0.9763)
[epoch : 50] (l_loss: 0.04747) (t_loss: 0.07620) (accu: 0.9745)
Finish! (Best accu: 0.9784) (Time taken(sec) : 665.79) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (109282 | 156918)        41.05
fc1.weight   :      235200 (96338 | 138862)         40.96
fc2.weight   :       30000 (12288 | 17712)          40.96
fcout.weight :          1000 (656 | 344)            65.60
------------------------------------------------------------

Learning start! [Prune_iter : (5/21), Remaining weight : 41.05 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.37283) (accu: 0.1638)
[epoch : 1] (l_loss: 0.43506) (t_loss: 0.15733) (accu: 0.9558)
[epoch : 2] (l_loss: 0.13051) (t_loss: 0.11164) (accu: 0.9659)
[epoch : 3] (l_loss: 0.09728) (t_loss: 0.09492) (accu: 0.9704)
[epoch : 4] (l_loss: 0.08144) (t_loss: 0.08815) (accu: 0.9729)
[epoch : 5] (l_loss: 0.07214) (t_loss: 0.08373) (accu: 0.9751)
[epoch : 6] (l_loss: 0.06599) (t_loss: 0.08516) (accu: 0.9732)
[epoch : 7] (l_loss: 0.06143) (t_loss: 0.08186) (accu: 0.9741)
[epoch : 8] (l_loss: 0.05811) (t_loss: 0.08017) (accu: 0.9756)
[epoch : 9] (l_loss: 0.05519) (t_loss: 0.07622) (accu: 0.9781)
[epoch : 10] (l_loss: 0.05324) (t_loss: 0.07593) (accu: 0.9763)
[epoch : 11] (l_loss: 0.05143) (t_loss: 0.07614) (accu: 0.9762)
[epoch : 12] (l_loss: 0.05082) (t_loss: 0.07462) (accu: 0.9781)
[epoch : 13] (l_loss: 0.04958) (t_loss: 0.07512) (accu: 0.9779)
[epoch : 14] (l_loss: 0.04920) (t_loss: 0.07490) (accu: 0.9776)
[epoch : 15] (l_loss: 0.04925) (t_loss: 0.07851) (accu: 0.9760)
[epoch : 16] (l_loss: 0.04877) (t_loss: 0.07393) (accu: 0.9784)
[epoch : 17] (l_loss: 0.04779) (t_loss: 0.07450) (accu: 0.9778)
[epoch : 18] (l_loss: 0.04779) (t_loss: 0.07768) (accu: 0.9766)
[epoch : 19] (l_loss: 0.04766) (t_loss: 0.07405) (accu: 0.9775)
[epoch : 20] (l_loss: 0.04727) (t_loss: 0.07693) (accu: 0.9770)
[epoch : 21] (l_loss: 0.04721) (t_loss: 0.07330) (accu: 0.9765)
[epoch : 22] (l_loss: 0.04732) (t_loss: 0.08003) (accu: 0.9755)
[epoch : 23] (l_loss: 0.04727) (t_loss: 0.07231) (accu: 0.9792)
[epoch : 24] (l_loss: 0.04631) (t_loss: 0.07410) (accu: 0.9784)
[epoch : 25] (l_loss: 0.04646) (t_loss: 0.07306) (accu: 0.9776)
[epoch : 26] (l_loss: 0.04579) (t_loss: 0.07484) (accu: 0.9778)
[epoch : 27] (l_loss: 0.04631) (t_loss: 0.07627) (accu: 0.9760)
[epoch : 28] (l_loss: 0.04599) (t_loss: 0.07279) (accu: 0.9786)
[epoch : 29] (l_loss: 0.04574) (t_loss: 0.07247) (accu: 0.9788)
[epoch : 30] (l_loss: 0.04581) (t_loss: 0.07343) (accu: 0.9774)
[epoch : 31] (l_loss: 0.04589) (t_loss: 0.07067) (accu: 0.9787)
[epoch : 32] (l_loss: 0.04568) (t_loss: 0.07361) (accu: 0.9777)
[epoch : 33] (l_loss: 0.04618) (t_loss: 0.07390) (accu: 0.9782)
[epoch : 34] (l_loss: 0.04524) (t_loss: 0.07188) (accu: 0.9788)
[epoch : 35] (l_loss: 0.04542) (t_loss: 0.07219) (accu: 0.9779)
[epoch : 36] (l_loss: 0.04620) (t_loss: 0.07375) (accu: 0.9779)
[epoch : 37] (l_loss: 0.04525) (t_loss: 0.07339) (accu: 0.9777)
[epoch : 38] (l_loss: 0.04600) (t_loss: 0.07432) (accu: 0.9764)
[epoch : 39] (l_loss: 0.04609) (t_loss: 0.07250) (accu: 0.9781)
[epoch : 40] (l_loss: 0.04576) (t_loss: 0.06996) (accu: 0.9792)
[epoch : 41] (l_loss: 0.04550) (t_loss: 0.07059) (accu: 0.9791)
[epoch : 42] (l_loss: 0.04551) (t_loss: 0.07274) (accu: 0.9775)
[epoch : 43] (l_loss: 0.04570) (t_loss: 0.07445) (accu: 0.9780)
[epoch : 44] (l_loss: 0.04562) (t_loss: 0.07140) (accu: 0.9787)
[epoch : 45] (l_loss: 0.04541) (t_loss: 0.07285) (accu: 0.9783)
[epoch : 46] (l_loss: 0.04555) (t_loss: 0.07245) (accu: 0.9787)
[epoch : 47] (l_loss: 0.04583) (t_loss: 0.07168) (accu: 0.9780)
[epoch : 48] (l_loss: 0.04539) (t_loss: 0.07161) (accu: 0.9796)
[epoch : 49] (l_loss: 0.04568) (t_loss: 0.07085) (accu: 0.9772)
[epoch : 50] (l_loss: 0.04556) (t_loss: 0.07131) (accu: 0.9790)
Finish! (Best accu: 0.9796) (Time taken(sec) : 676.44) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (87490 | 178710)         32.87
fc1.weight   :      235200 (77070 | 158130)         32.77
fc2.weight   :        30000 (9830 | 20170)          32.77
fcout.weight :          1000 (590 | 410)            59.00
------------------------------------------------------------

Learning start! [Prune_iter : (6/21), Remaining weight : 32.87 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.35431) (accu: 0.1123)
[epoch : 1] (l_loss: 0.41923) (t_loss: 0.14864) (accu: 0.9561)
[epoch : 2] (l_loss: 0.12602) (t_loss: 0.11174) (accu: 0.9660)
[epoch : 3] (l_loss: 0.09431) (t_loss: 0.09360) (accu: 0.9730)
[epoch : 4] (l_loss: 0.08012) (t_loss: 0.08833) (accu: 0.9717)
[epoch : 5] (l_loss: 0.07158) (t_loss: 0.08342) (accu: 0.9732)
[epoch : 6] (l_loss: 0.06618) (t_loss: 0.08087) (accu: 0.9752)
[epoch : 7] (l_loss: 0.06187) (t_loss: 0.07828) (accu: 0.9760)
[epoch : 8] (l_loss: 0.05918) (t_loss: 0.08070) (accu: 0.9750)
[epoch : 9] (l_loss: 0.05711) (t_loss: 0.07692) (accu: 0.9760)
[epoch : 10] (l_loss: 0.05512) (t_loss: 0.07982) (accu: 0.9759)
[epoch : 11] (l_loss: 0.05370) (t_loss: 0.07651) (accu: 0.9762)
[epoch : 12] (l_loss: 0.05210) (t_loss: 0.08253) (accu: 0.9750)
[epoch : 13] (l_loss: 0.05208) (t_loss: 0.07585) (accu: 0.9762)
[epoch : 14] (l_loss: 0.05097) (t_loss: 0.07520) (accu: 0.9768)
[epoch : 15] (l_loss: 0.05114) (t_loss: 0.07812) (accu: 0.9753)
[epoch : 16] (l_loss: 0.05000) (t_loss: 0.07691) (accu: 0.9765)
[epoch : 17] (l_loss: 0.05044) (t_loss: 0.07561) (accu: 0.9769)
[epoch : 18] (l_loss: 0.05008) (t_loss: 0.07911) (accu: 0.9766)
[epoch : 19] (l_loss: 0.05002) (t_loss: 0.07538) (accu: 0.9777)
[epoch : 20] (l_loss: 0.04968) (t_loss: 0.07462) (accu: 0.9777)
[epoch : 21] (l_loss: 0.04947) (t_loss: 0.07564) (accu: 0.9769)
[epoch : 22] (l_loss: 0.04945) (t_loss: 0.07328) (accu: 0.9771)
[epoch : 23] (l_loss: 0.04970) (t_loss: 0.07810) (accu: 0.9758)
[epoch : 24] (l_loss: 0.04946) (t_loss: 0.07506) (accu: 0.9777)
[epoch : 25] (l_loss: 0.04920) (t_loss: 0.07432) (accu: 0.9772)
[epoch : 26] (l_loss: 0.04970) (t_loss: 0.07700) (accu: 0.9762)
[epoch : 27] (l_loss: 0.04906) (t_loss: 0.07433) (accu: 0.9777)
[epoch : 28] (l_loss: 0.04902) (t_loss: 0.07666) (accu: 0.9767)
[epoch : 29] (l_loss: 0.04933) (t_loss: 0.07414) (accu: 0.9790)
[epoch : 30] (l_loss: 0.04876) (t_loss: 0.07505) (accu: 0.9768)
[epoch : 31] (l_loss: 0.04949) (t_loss: 0.07529) (accu: 0.9781)
[epoch : 32] (l_loss: 0.04873) (t_loss: 0.07307) (accu: 0.9768)
[epoch : 33] (l_loss: 0.04863) (t_loss: 0.07300) (accu: 0.9786)
[epoch : 34] (l_loss: 0.04814) (t_loss: 0.07163) (accu: 0.9779)
[epoch : 35] (l_loss: 0.04835) (t_loss: 0.07460) (accu: 0.9775)
[epoch : 36] (l_loss: 0.04812) (t_loss: 0.07413) (accu: 0.9776)
[epoch : 37] (l_loss: 0.04831) (t_loss: 0.07429) (accu: 0.9762)
[epoch : 38] (l_loss: 0.04796) (t_loss: 0.07178) (accu: 0.9783)
[epoch : 39] (l_loss: 0.04805) (t_loss: 0.07350) (accu: 0.9772)
[epoch : 40] (l_loss: 0.04829) (t_loss: 0.07392) (accu: 0.9774)
[epoch : 41] (l_loss: 0.04817) (t_loss: 0.07653) (accu: 0.9776)
[epoch : 42] (l_loss: 0.04796) (t_loss: 0.07289) (accu: 0.9776)
[epoch : 43] (l_loss: 0.04807) (t_loss: 0.07310) (accu: 0.9778)
[epoch : 44] (l_loss: 0.04759) (t_loss: 0.07533) (accu: 0.9763)
[epoch : 45] (l_loss: 0.04840) (t_loss: 0.07580) (accu: 0.9773)
[epoch : 46] (l_loss: 0.04822) (t_loss: 0.07602) (accu: 0.9775)
[epoch : 47] (l_loss: 0.04765) (t_loss: 0.07335) (accu: 0.9781)
[epoch : 48] (l_loss: 0.04797) (t_loss: 0.07426) (accu: 0.9781)
[epoch : 49] (l_loss: 0.04817) (t_loss: 0.07602) (accu: 0.9763)
[epoch : 50] (l_loss: 0.04817) (t_loss: 0.07368) (accu: 0.9776)
Finish! (Best accu: 0.9790) (Time taken(sec) : 675.03) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (70051 | 196149)         26.32
fc1.weight   :      235200 (61656 | 173544)         26.21
fc2.weight   :        30000 (7864 | 22136)          26.21
fcout.weight :          1000 (531 | 469)            53.10
------------------------------------------------------------

Learning start! [Prune_iter : (7/21), Remaining weight : 26.32 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.32623) (accu: 0.1121)
[epoch : 1] (l_loss: 0.42166) (t_loss: 0.15360) (accu: 0.9576)
[epoch : 2] (l_loss: 0.12581) (t_loss: 0.11163) (accu: 0.9665)
[epoch : 3] (l_loss: 0.09430) (t_loss: 0.09638) (accu: 0.9700)
[epoch : 4] (l_loss: 0.07868) (t_loss: 0.08252) (accu: 0.9744)
[epoch : 5] (l_loss: 0.06900) (t_loss: 0.07808) (accu: 0.9765)
[epoch : 6] (l_loss: 0.06313) (t_loss: 0.07658) (accu: 0.9774)
[epoch : 7] (l_loss: 0.05888) (t_loss: 0.07511) (accu: 0.9776)
[epoch : 8] (l_loss: 0.05569) (t_loss: 0.07317) (accu: 0.9781)
[epoch : 9] (l_loss: 0.05250) (t_loss: 0.07486) (accu: 0.9768)
[epoch : 10] (l_loss: 0.04958) (t_loss: 0.07139) (accu: 0.9768)
[epoch : 11] (l_loss: 0.04834) (t_loss: 0.06948) (accu: 0.9785)
[epoch : 12] (l_loss: 0.04805) (t_loss: 0.07152) (accu: 0.9780)
[epoch : 13] (l_loss: 0.04645) (t_loss: 0.07302) (accu: 0.9775)
[epoch : 14] (l_loss: 0.04629) (t_loss: 0.07114) (accu: 0.9793)
[epoch : 15] (l_loss: 0.04571) (t_loss: 0.07190) (accu: 0.9788)
[epoch : 16] (l_loss: 0.04525) (t_loss: 0.07408) (accu: 0.9777)
[epoch : 17] (l_loss: 0.04530) (t_loss: 0.07103) (accu: 0.9776)
[epoch : 18] (l_loss: 0.04500) (t_loss: 0.07174) (accu: 0.9779)
[epoch : 19] (l_loss: 0.04494) (t_loss: 0.07271) (accu: 0.9780)
[epoch : 20] (l_loss: 0.04512) (t_loss: 0.06905) (accu: 0.9799)
[epoch : 21] (l_loss: 0.04466) (t_loss: 0.07042) (accu: 0.9790)
[epoch : 22] (l_loss: 0.04478) (t_loss: 0.07486) (accu: 0.9766)
[epoch : 23] (l_loss: 0.04445) (t_loss: 0.06954) (accu: 0.9790)
[epoch : 24] (l_loss: 0.04429) (t_loss: 0.07696) (accu: 0.9768)
[epoch : 25] (l_loss: 0.04431) (t_loss: 0.07126) (accu: 0.9788)
[epoch : 26] (l_loss: 0.04428) (t_loss: 0.07205) (accu: 0.9786)
[epoch : 27] (l_loss: 0.04437) (t_loss: 0.07146) (accu: 0.9782)
[epoch : 28] (l_loss: 0.04434) (t_loss: 0.07084) (accu: 0.9777)
[epoch : 29] (l_loss: 0.04442) (t_loss: 0.07275) (accu: 0.9775)
[epoch : 30] (l_loss: 0.04458) (t_loss: 0.07231) (accu: 0.9774)
[epoch : 31] (l_loss: 0.04397) (t_loss: 0.07287) (accu: 0.9781)
[epoch : 32] (l_loss: 0.04431) (t_loss: 0.07160) (accu: 0.9782)
[epoch : 33] (l_loss: 0.04405) (t_loss: 0.06927) (accu: 0.9797)
[epoch : 34] (l_loss: 0.04407) (t_loss: 0.07362) (accu: 0.9775)
[epoch : 35] (l_loss: 0.04398) (t_loss: 0.07128) (accu: 0.9773)
[epoch : 36] (l_loss: 0.04414) (t_loss: 0.07141) (accu: 0.9772)
[epoch : 37] (l_loss: 0.04441) (t_loss: 0.06974) (accu: 0.9790)
[epoch : 38] (l_loss: 0.04403) (t_loss: 0.07679) (accu: 0.9766)
[epoch : 39] (l_loss: 0.04394) (t_loss: 0.07602) (accu: 0.9760)
[epoch : 40] (l_loss: 0.04434) (t_loss: 0.07180) (accu: 0.9780)
[epoch : 41] (l_loss: 0.04372) (t_loss: 0.07513) (accu: 0.9762)
[epoch : 42] (l_loss: 0.04400) (t_loss: 0.07210) (accu: 0.9774)
[epoch : 43] (l_loss: 0.04398) (t_loss: 0.06935) (accu: 0.9784)
[epoch : 44] (l_loss: 0.04416) (t_loss: 0.07002) (accu: 0.9792)
[epoch : 45] (l_loss: 0.04350) (t_loss: 0.07251) (accu: 0.9770)
[epoch : 46] (l_loss: 0.04406) (t_loss: 0.07443) (accu: 0.9765)
[epoch : 47] (l_loss: 0.04405) (t_loss: 0.06981) (accu: 0.9790)
[epoch : 48] (l_loss: 0.04399) (t_loss: 0.07413) (accu: 0.9768)
[epoch : 49] (l_loss: 0.04431) (t_loss: 0.07072) (accu: 0.9783)
[epoch : 50] (l_loss: 0.04370) (t_loss: 0.07246) (accu: 0.9788)
Finish! (Best accu: 0.9799) (Time taken(sec) : 679.18) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (56094 | 210106)         21.07
fc1.weight   :      235200 (49325 | 185875)         20.97
fc2.weight   :        30000 (6291 | 23709)          20.97
fcout.weight :          1000 (478 | 522)            47.80
------------------------------------------------------------

Learning start! [Prune_iter : (8/21), Remaining weight : 21.07 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.31584) (accu: 0.1164)
[epoch : 1] (l_loss: 0.41297) (t_loss: 0.15197) (accu: 0.9556)
[epoch : 2] (l_loss: 0.12598) (t_loss: 0.10874) (accu: 0.9666)
[epoch : 3] (l_loss: 0.09507) (t_loss: 0.09367) (accu: 0.9723)
[epoch : 4] (l_loss: 0.07957) (t_loss: 0.08228) (accu: 0.9740)
[epoch : 5] (l_loss: 0.07018) (t_loss: 0.07945) (accu: 0.9763)
[epoch : 6] (l_loss: 0.06470) (t_loss: 0.07781) (accu: 0.9760)
[epoch : 7] (l_loss: 0.06040) (t_loss: 0.07956) (accu: 0.9744)
[epoch : 8] (l_loss: 0.05690) (t_loss: 0.07481) (accu: 0.9771)
[epoch : 9] (l_loss: 0.05379) (t_loss: 0.07321) (accu: 0.9774)
[epoch : 10] (l_loss: 0.05169) (t_loss: 0.06997) (accu: 0.9797)
[epoch : 11] (l_loss: 0.05010) (t_loss: 0.07629) (accu: 0.9759)
[epoch : 12] (l_loss: 0.04903) (t_loss: 0.07396) (accu: 0.9773)
[epoch : 13] (l_loss: 0.04888) (t_loss: 0.07407) (accu: 0.9785)
[epoch : 14] (l_loss: 0.04756) (t_loss: 0.07232) (accu: 0.9786)
[epoch : 15] (l_loss: 0.04754) (t_loss: 0.07302) (accu: 0.9786)
[epoch : 16] (l_loss: 0.04705) (t_loss: 0.07289) (accu: 0.9786)
[epoch : 17] (l_loss: 0.04682) (t_loss: 0.07126) (accu: 0.9772)
[epoch : 18] (l_loss: 0.04669) (t_loss: 0.07451) (accu: 0.9775)
[epoch : 19] (l_loss: 0.04669) (t_loss: 0.07381) (accu: 0.9774)
[epoch : 20] (l_loss: 0.04662) (t_loss: 0.07451) (accu: 0.9785)
[epoch : 21] (l_loss: 0.04601) (t_loss: 0.07401) (accu: 0.9768)
[epoch : 22] (l_loss: 0.04631) (t_loss: 0.07497) (accu: 0.9766)
[epoch : 23] (l_loss: 0.04575) (t_loss: 0.07299) (accu: 0.9799)
[epoch : 24] (l_loss: 0.04605) (t_loss: 0.07481) (accu: 0.9764)
[epoch : 25] (l_loss: 0.04574) (t_loss: 0.06804) (accu: 0.9795)
[epoch : 26] (l_loss: 0.04515) (t_loss: 0.06954) (accu: 0.9795)
[epoch : 27] (l_loss: 0.04511) (t_loss: 0.07126) (accu: 0.9786)
[epoch : 28] (l_loss: 0.04492) (t_loss: 0.07121) (accu: 0.9780)
[epoch : 29] (l_loss: 0.04484) (t_loss: 0.07089) (accu: 0.9787)
[epoch : 30] (l_loss: 0.04524) (t_loss: 0.06997) (accu: 0.9782)
[epoch : 31] (l_loss: 0.04456) (t_loss: 0.06927) (accu: 0.9785)
[epoch : 32] (l_loss: 0.04453) (t_loss: 0.07112) (accu: 0.9777)
[epoch : 33] (l_loss: 0.04553) (t_loss: 0.07594) (accu: 0.9780)
[epoch : 34] (l_loss: 0.04528) (t_loss: 0.06889) (accu: 0.9797)
[epoch : 35] (l_loss: 0.04465) (t_loss: 0.07518) (accu: 0.9774)
[epoch : 36] (l_loss: 0.04489) (t_loss: 0.07106) (accu: 0.9788)
[epoch : 37] (l_loss: 0.04464) (t_loss: 0.06946) (accu: 0.9789)
[epoch : 38] (l_loss: 0.04522) (t_loss: 0.07487) (accu: 0.9788)
[epoch : 39] (l_loss: 0.04441) (t_loss: 0.06981) (accu: 0.9788)
[epoch : 40] (l_loss: 0.04458) (t_loss: 0.07452) (accu: 0.9787)
[epoch : 41] (l_loss: 0.04421) (t_loss: 0.07317) (accu: 0.9777)
[epoch : 42] (l_loss: 0.04489) (t_loss: 0.07493) (accu: 0.9773)
[epoch : 43] (l_loss: 0.04447) (t_loss: 0.07305) (accu: 0.9775)
[epoch : 44] (l_loss: 0.04506) (t_loss: 0.07336) (accu: 0.9774)
[epoch : 45] (l_loss: 0.04447) (t_loss: 0.07495) (accu: 0.9773)
[epoch : 46] (l_loss: 0.04455) (t_loss: 0.07244) (accu: 0.9784)
[epoch : 47] (l_loss: 0.04461) (t_loss: 0.07212) (accu: 0.9783)
[epoch : 48] (l_loss: 0.04476) (t_loss: 0.07209) (accu: 0.9783)
[epoch : 49] (l_loss: 0.04444) (t_loss: 0.07424) (accu: 0.9778)
[epoch : 50] (l_loss: 0.04514) (t_loss: 0.07021) (accu: 0.9783)
Finish! (Best accu: 0.9799) (Time taken(sec) : 733.01) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (44923 | 221277)         16.88
fc1.weight   :      235200 (39460 | 195740)         16.78
fc2.weight   :        30000 (5033 | 24967)          16.78
fcout.weight :          1000 (430 | 570)            43.00
------------------------------------------------------------

Learning start! [Prune_iter : (9/21), Remaining weight : 16.88 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30269) (accu: 0.1350)
[epoch : 1] (l_loss: 0.41035) (t_loss: 0.15279) (accu: 0.9567)
[epoch : 2] (l_loss: 0.12540) (t_loss: 0.11065) (accu: 0.9669)
[epoch : 3] (l_loss: 0.09411) (t_loss: 0.09523) (accu: 0.9710)
[epoch : 4] (l_loss: 0.07970) (t_loss: 0.08675) (accu: 0.9730)
[epoch : 5] (l_loss: 0.07079) (t_loss: 0.08013) (accu: 0.9762)
[epoch : 6] (l_loss: 0.06498) (t_loss: 0.07842) (accu: 0.9764)
[epoch : 7] (l_loss: 0.06061) (t_loss: 0.07758) (accu: 0.9759)
[epoch : 8] (l_loss: 0.05779) (t_loss: 0.07500) (accu: 0.9772)
[epoch : 9] (l_loss: 0.05367) (t_loss: 0.07398) (accu: 0.9775)
[epoch : 10] (l_loss: 0.05133) (t_loss: 0.07169) (accu: 0.9788)
[epoch : 11] (l_loss: 0.04982) (t_loss: 0.07297) (accu: 0.9782)
[epoch : 12] (l_loss: 0.04883) (t_loss: 0.07222) (accu: 0.9770)
[epoch : 13] (l_loss: 0.04791) (t_loss: 0.07599) (accu: 0.9766)
[epoch : 14] (l_loss: 0.04764) (t_loss: 0.07323) (accu: 0.9768)
[epoch : 15] (l_loss: 0.04750) (t_loss: 0.07571) (accu: 0.9766)
[epoch : 16] (l_loss: 0.04688) (t_loss: 0.07533) (accu: 0.9767)
[epoch : 17] (l_loss: 0.04615) (t_loss: 0.07416) (accu: 0.9777)
[epoch : 18] (l_loss: 0.04607) (t_loss: 0.07544) (accu: 0.9779)
[epoch : 19] (l_loss: 0.04658) (t_loss: 0.07455) (accu: 0.9763)
[epoch : 20] (l_loss: 0.04564) (t_loss: 0.07373) (accu: 0.9774)
[epoch : 21] (l_loss: 0.04632) (t_loss: 0.07477) (accu: 0.9771)
[epoch : 22] (l_loss: 0.04593) (t_loss: 0.07420) (accu: 0.9767)
[epoch : 23] (l_loss: 0.04579) (t_loss: 0.07178) (accu: 0.9785)
[epoch : 24] (l_loss: 0.04579) (t_loss: 0.07225) (accu: 0.9779)
[epoch : 25] (l_loss: 0.04529) (t_loss: 0.08183) (accu: 0.9756)
[epoch : 26] (l_loss: 0.04556) (t_loss: 0.07451) (accu: 0.9765)
[epoch : 27] (l_loss: 0.04557) (t_loss: 0.07613) (accu: 0.9758)
[epoch : 28] (l_loss: 0.04616) (t_loss: 0.07484) (accu: 0.9766)
[epoch : 29] (l_loss: 0.04559) (t_loss: 0.07208) (accu: 0.9778)
[epoch : 30] (l_loss: 0.04578) (t_loss: 0.07322) (accu: 0.9775)
[epoch : 31] (l_loss: 0.04540) (t_loss: 0.07496) (accu: 0.9770)
[epoch : 32] (l_loss: 0.04559) (t_loss: 0.07621) (accu: 0.9747)
[epoch : 33] (l_loss: 0.04563) (t_loss: 0.07035) (accu: 0.9789)
[epoch : 34] (l_loss: 0.04527) (t_loss: 0.07357) (accu: 0.9775)
[epoch : 35] (l_loss: 0.04538) (t_loss: 0.07429) (accu: 0.9783)
[epoch : 36] (l_loss: 0.04536) (t_loss: 0.07342) (accu: 0.9780)
[epoch : 37] (l_loss: 0.04594) (t_loss: 0.07564) (accu: 0.9767)
[epoch : 38] (l_loss: 0.04494) (t_loss: 0.07154) (accu: 0.9792)
[epoch : 39] (l_loss: 0.04590) (t_loss: 0.07397) (accu: 0.9776)
[epoch : 40] (l_loss: 0.04553) (t_loss: 0.07707) (accu: 0.9763)
[epoch : 41] (l_loss: 0.04555) (t_loss: 0.07439) (accu: 0.9768)
[epoch : 42] (l_loss: 0.04478) (t_loss: 0.07434) (accu: 0.9766)
[epoch : 43] (l_loss: 0.04552) (t_loss: 0.07690) (accu: 0.9766)
[epoch : 44] (l_loss: 0.04534) (t_loss: 0.07392) (accu: 0.9768)
[epoch : 45] (l_loss: 0.04492) (t_loss: 0.07461) (accu: 0.9772)
[epoch : 46] (l_loss: 0.04546) (t_loss: 0.07208) (accu: 0.9782)
[epoch : 47] (l_loss: 0.04565) (t_loss: 0.07149) (accu: 0.9786)
[epoch : 48] (l_loss: 0.04548) (t_loss: 0.07447) (accu: 0.9772)
[epoch : 49] (l_loss: 0.04559) (t_loss: 0.07488) (accu: 0.9790)
[epoch : 50] (l_loss: 0.04531) (t_loss: 0.06807) (accu: 0.9787)
Finish! (Best accu: 0.9792) (Time taken(sec) : 721.35) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (35982 | 230218)         13.52
fc1.weight   :      235200 (31568 | 203632)         13.42
fc2.weight   :        30000 (4027 | 25973)          13.42
fcout.weight :          1000 (387 | 613)            38.70
------------------------------------------------------------

Learning start! [Prune_iter : (10/21), Remaining weight : 13.52 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29787) (accu: 0.0842)
[epoch : 1] (l_loss: 0.40326) (t_loss: 0.15020) (accu: 0.9575)
[epoch : 2] (l_loss: 0.12377) (t_loss: 0.10686) (accu: 0.9680)
[epoch : 3] (l_loss: 0.09349) (t_loss: 0.09440) (accu: 0.9717)
[epoch : 4] (l_loss: 0.07843) (t_loss: 0.08620) (accu: 0.9744)
[epoch : 5] (l_loss: 0.06955) (t_loss: 0.07913) (accu: 0.9764)
[epoch : 6] (l_loss: 0.06412) (t_loss: 0.07775) (accu: 0.9762)
[epoch : 7] (l_loss: 0.05967) (t_loss: 0.07732) (accu: 0.9783)
[epoch : 8] (l_loss: 0.05613) (t_loss: 0.07622) (accu: 0.9778)
[epoch : 9] (l_loss: 0.05384) (t_loss: 0.07586) (accu: 0.9768)
[epoch : 10] (l_loss: 0.05171) (t_loss: 0.07579) (accu: 0.9777)
[epoch : 11] (l_loss: 0.05070) (t_loss: 0.07193) (accu: 0.9777)
[epoch : 12] (l_loss: 0.04901) (t_loss: 0.07358) (accu: 0.9775)
[epoch : 13] (l_loss: 0.04785) (t_loss: 0.07418) (accu: 0.9773)
[epoch : 14] (l_loss: 0.04672) (t_loss: 0.07508) (accu: 0.9761)
[epoch : 15] (l_loss: 0.04668) (t_loss: 0.07121) (accu: 0.9793)
[epoch : 16] (l_loss: 0.04637) (t_loss: 0.07346) (accu: 0.9778)
[epoch : 17] (l_loss: 0.04564) (t_loss: 0.07310) (accu: 0.9777)
[epoch : 18] (l_loss: 0.04559) (t_loss: 0.07249) (accu: 0.9762)
[epoch : 19] (l_loss: 0.04509) (t_loss: 0.07323) (accu: 0.9770)
[epoch : 20] (l_loss: 0.04563) (t_loss: 0.07501) (accu: 0.9764)
[epoch : 21] (l_loss: 0.04511) (t_loss: 0.07393) (accu: 0.9759)
[epoch : 22] (l_loss: 0.04518) (t_loss: 0.07126) (accu: 0.9778)
[epoch : 23] (l_loss: 0.04524) (t_loss: 0.07187) (accu: 0.9779)
[epoch : 24] (l_loss: 0.04483) (t_loss: 0.07485) (accu: 0.9760)
[epoch : 25] (l_loss: 0.04515) (t_loss: 0.07306) (accu: 0.9769)
[epoch : 26] (l_loss: 0.04500) (t_loss: 0.07427) (accu: 0.9757)
[epoch : 27] (l_loss: 0.04473) (t_loss: 0.07115) (accu: 0.9773)
[epoch : 28] (l_loss: 0.04531) (t_loss: 0.07108) (accu: 0.9770)
[epoch : 29] (l_loss: 0.04478) (t_loss: 0.07339) (accu: 0.9782)
[epoch : 30] (l_loss: 0.04476) (t_loss: 0.07407) (accu: 0.9774)
[epoch : 31] (l_loss: 0.04460) (t_loss: 0.07381) (accu: 0.9767)
[epoch : 32] (l_loss: 0.04507) (t_loss: 0.07447) (accu: 0.9777)
[epoch : 33] (l_loss: 0.04513) (t_loss: 0.07549) (accu: 0.9757)
[epoch : 34] (l_loss: 0.04450) (t_loss: 0.07187) (accu: 0.9775)
[epoch : 35] (l_loss: 0.04496) (t_loss: 0.07177) (accu: 0.9788)
[epoch : 36] (l_loss: 0.04437) (t_loss: 0.07727) (accu: 0.9751)
[epoch : 37] (l_loss: 0.04450) (t_loss: 0.07512) (accu: 0.9764)
[epoch : 38] (l_loss: 0.04479) (t_loss: 0.07272) (accu: 0.9770)
[epoch : 39] (l_loss: 0.04478) (t_loss: 0.07538) (accu: 0.9749)
[epoch : 40] (l_loss: 0.04515) (t_loss: 0.07151) (accu: 0.9780)
[epoch : 41] (l_loss: 0.04498) (t_loss: 0.07374) (accu: 0.9774)
[epoch : 42] (l_loss: 0.04373) (t_loss: 0.07557) (accu: 0.9766)
[epoch : 43] (l_loss: 0.04406) (t_loss: 0.06977) (accu: 0.9786)
[epoch : 44] (l_loss: 0.04377) (t_loss: 0.07083) (accu: 0.9790)
[epoch : 45] (l_loss: 0.04390) (t_loss: 0.07333) (accu: 0.9778)
[epoch : 46] (l_loss: 0.04365) (t_loss: 0.07093) (accu: 0.9780)
[epoch : 47] (l_loss: 0.04448) (t_loss: 0.07124) (accu: 0.9781)
[epoch : 48] (l_loss: 0.04348) (t_loss: 0.07532) (accu: 0.9762)
[epoch : 49] (l_loss: 0.04411) (t_loss: 0.07072) (accu: 0.9783)
[epoch : 50] (l_loss: 0.04366) (t_loss: 0.07244) (accu: 0.9775)
Finish! (Best accu: 0.9793) (Time taken(sec) : 726.60) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (28824 | 237376)         10.83
fc1.weight   :      235200 (25254 | 209946)         10.74
fc2.weight   :        30000 (3221 | 26779)          10.74
fcout.weight :          1000 (349 | 651)            34.90
------------------------------------------------------------

Learning start! [Prune_iter : (11/21), Remaining weight : 10.83 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30110) (accu: 0.1102)
[epoch : 1] (l_loss: 0.40234) (t_loss: 0.15022) (accu: 0.9566)
[epoch : 2] (l_loss: 0.12488) (t_loss: 0.10811) (accu: 0.9671)
[epoch : 3] (l_loss: 0.09490) (t_loss: 0.09470) (accu: 0.9706)
[epoch : 4] (l_loss: 0.08094) (t_loss: 0.08825) (accu: 0.9733)
[epoch : 5] (l_loss: 0.07264) (t_loss: 0.08725) (accu: 0.9735)
[epoch : 6] (l_loss: 0.06705) (t_loss: 0.07997) (accu: 0.9757)
[epoch : 7] (l_loss: 0.06263) (t_loss: 0.07841) (accu: 0.9765)
[epoch : 8] (l_loss: 0.05945) (t_loss: 0.07613) (accu: 0.9771)
[epoch : 9] (l_loss: 0.05604) (t_loss: 0.07466) (accu: 0.9770)
[epoch : 10] (l_loss: 0.05347) (t_loss: 0.07404) (accu: 0.9774)
[epoch : 11] (l_loss: 0.05100) (t_loss: 0.07702) (accu: 0.9769)
[epoch : 12] (l_loss: 0.04992) (t_loss: 0.07676) (accu: 0.9759)
[epoch : 13] (l_loss: 0.04864) (t_loss: 0.07568) (accu: 0.9759)
[epoch : 14] (l_loss: 0.04825) (t_loss: 0.07462) (accu: 0.9765)
[epoch : 15] (l_loss: 0.04782) (t_loss: 0.07531) (accu: 0.9753)
[epoch : 16] (l_loss: 0.04739) (t_loss: 0.07095) (accu: 0.9777)
[epoch : 17] (l_loss: 0.04711) (t_loss: 0.07749) (accu: 0.9759)
[epoch : 18] (l_loss: 0.04718) (t_loss: 0.07289) (accu: 0.9760)
[epoch : 19] (l_loss: 0.04671) (t_loss: 0.07502) (accu: 0.9775)
[epoch : 20] (l_loss: 0.04663) (t_loss: 0.07679) (accu: 0.9765)
[epoch : 21] (l_loss: 0.04604) (t_loss: 0.07710) (accu: 0.9756)
[epoch : 22] (l_loss: 0.04614) (t_loss: 0.07568) (accu: 0.9745)
[epoch : 23] (l_loss: 0.04584) (t_loss: 0.07442) (accu: 0.9766)
[epoch : 24] (l_loss: 0.04577) (t_loss: 0.07423) (accu: 0.9770)
[epoch : 25] (l_loss: 0.04657) (t_loss: 0.07128) (accu: 0.9768)
[epoch : 26] (l_loss: 0.04621) (t_loss: 0.07287) (accu: 0.9775)
[epoch : 27] (l_loss: 0.04557) (t_loss: 0.07316) (accu: 0.9764)
[epoch : 28] (l_loss: 0.04612) (t_loss: 0.07810) (accu: 0.9749)
[epoch : 29] (l_loss: 0.04579) (t_loss: 0.07207) (accu: 0.9775)
[epoch : 30] (l_loss: 0.04532) (t_loss: 0.07324) (accu: 0.9767)
[epoch : 31] (l_loss: 0.04592) (t_loss: 0.07407) (accu: 0.9777)
[epoch : 32] (l_loss: 0.04578) (t_loss: 0.07327) (accu: 0.9752)
[epoch : 33] (l_loss: 0.04563) (t_loss: 0.07507) (accu: 0.9757)
[epoch : 34] (l_loss: 0.04558) (t_loss: 0.07723) (accu: 0.9752)
[epoch : 35] (l_loss: 0.04589) (t_loss: 0.07519) (accu: 0.9771)
[epoch : 36] (l_loss: 0.04518) (t_loss: 0.07995) (accu: 0.9745)
[epoch : 37] (l_loss: 0.04589) (t_loss: 0.07272) (accu: 0.9773)
[epoch : 38] (l_loss: 0.04544) (t_loss: 0.07453) (accu: 0.9770)
[epoch : 39] (l_loss: 0.04491) (t_loss: 0.07679) (accu: 0.9745)
[epoch : 40] (l_loss: 0.04437) (t_loss: 0.07338) (accu: 0.9768)
[epoch : 41] (l_loss: 0.04473) (t_loss: 0.06995) (accu: 0.9783)
[epoch : 42] (l_loss: 0.04467) (t_loss: 0.07028) (accu: 0.9790)
[epoch : 43] (l_loss: 0.04432) (t_loss: 0.07394) (accu: 0.9774)
[epoch : 44] (l_loss: 0.04366) (t_loss: 0.07596) (accu: 0.9763)
[epoch : 45] (l_loss: 0.04389) (t_loss: 0.07213) (accu: 0.9763)
[epoch : 46] (l_loss: 0.04390) (t_loss: 0.07353) (accu: 0.9770)
[epoch : 47] (l_loss: 0.04407) (t_loss: 0.07220) (accu: 0.9773)
[epoch : 48] (l_loss: 0.04412) (t_loss: 0.07178) (accu: 0.9782)
[epoch : 49] (l_loss: 0.04408) (t_loss: 0.07161) (accu: 0.9772)
[epoch : 50] (l_loss: 0.04412) (t_loss: 0.07207) (accu: 0.9766)
Finish! (Best accu: 0.9790) (Time taken(sec) : 720.42) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (23095 | 243105)          8.68
fc1.weight   :      235200 (20204 | 214996)          8.59
fc2.weight   :        30000 (2577 | 27423)           8.59
fcout.weight :          1000 (314 | 686)            31.40
------------------------------------------------------------

Learning start! [Prune_iter : (12/21), Remaining weight : 8.68 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29910) (accu: 0.0996)
[epoch : 1] (l_loss: 0.40121) (t_loss: 0.15513) (accu: 0.9525)
[epoch : 2] (l_loss: 0.12754) (t_loss: 0.11178) (accu: 0.9670)
[epoch : 3] (l_loss: 0.09675) (t_loss: 0.09227) (accu: 0.9709)
[epoch : 4] (l_loss: 0.08258) (t_loss: 0.08716) (accu: 0.9737)
[epoch : 5] (l_loss: 0.07298) (t_loss: 0.08034) (accu: 0.9759)
[epoch : 6] (l_loss: 0.06655) (t_loss: 0.07908) (accu: 0.9773)
[epoch : 7] (l_loss: 0.06149) (t_loss: 0.07652) (accu: 0.9779)
[epoch : 8] (l_loss: 0.05740) (t_loss: 0.07682) (accu: 0.9764)
[epoch : 9] (l_loss: 0.05389) (t_loss: 0.07617) (accu: 0.9769)
[epoch : 10] (l_loss: 0.05205) (t_loss: 0.07758) (accu: 0.9766)
[epoch : 11] (l_loss: 0.04993) (t_loss: 0.07859) (accu: 0.9769)
[epoch : 12] (l_loss: 0.04942) (t_loss: 0.07633) (accu: 0.9759)
[epoch : 13] (l_loss: 0.04872) (t_loss: 0.07192) (accu: 0.9779)
[epoch : 14] (l_loss: 0.04773) (t_loss: 0.07219) (accu: 0.9776)
[epoch : 15] (l_loss: 0.04690) (t_loss: 0.07835) (accu: 0.9757)
[epoch : 16] (l_loss: 0.04699) (t_loss: 0.07640) (accu: 0.9763)
[epoch : 17] (l_loss: 0.04663) (t_loss: 0.07434) (accu: 0.9763)
[epoch : 18] (l_loss: 0.04642) (t_loss: 0.07109) (accu: 0.9779)
[epoch : 19] (l_loss: 0.04638) (t_loss: 0.07284) (accu: 0.9766)
[epoch : 20] (l_loss: 0.04630) (t_loss: 0.07280) (accu: 0.9765)
[epoch : 21] (l_loss: 0.04569) (t_loss: 0.07464) (accu: 0.9762)
[epoch : 22] (l_loss: 0.04602) (t_loss: 0.07300) (accu: 0.9773)
[epoch : 23] (l_loss: 0.04644) (t_loss: 0.07497) (accu: 0.9774)
[epoch : 24] (l_loss: 0.04580) (t_loss: 0.07345) (accu: 0.9778)
[epoch : 25] (l_loss: 0.04527) (t_loss: 0.07326) (accu: 0.9780)
[epoch : 26] (l_loss: 0.04546) (t_loss: 0.07389) (accu: 0.9765)
[epoch : 27] (l_loss: 0.04594) (t_loss: 0.07078) (accu: 0.9782)
[epoch : 28] (l_loss: 0.04487) (t_loss: 0.07199) (accu: 0.9769)
[epoch : 29] (l_loss: 0.04542) (t_loss: 0.07230) (accu: 0.9770)
[epoch : 30] (l_loss: 0.04520) (t_loss: 0.07192) (accu: 0.9771)
[epoch : 31] (l_loss: 0.04520) (t_loss: 0.07635) (accu: 0.9761)
[epoch : 32] (l_loss: 0.04532) (t_loss: 0.07456) (accu: 0.9782)
[epoch : 33] (l_loss: 0.04492) (t_loss: 0.07432) (accu: 0.9767)
[epoch : 34] (l_loss: 0.04485) (t_loss: 0.07298) (accu: 0.9785)
[epoch : 35] (l_loss: 0.04536) (t_loss: 0.07317) (accu: 0.9774)
[epoch : 36] (l_loss: 0.04500) (t_loss: 0.07483) (accu: 0.9762)
[epoch : 37] (l_loss: 0.04505) (t_loss: 0.07495) (accu: 0.9773)
[epoch : 38] (l_loss: 0.04497) (t_loss: 0.07086) (accu: 0.9776)
[epoch : 39] (l_loss: 0.04466) (t_loss: 0.07453) (accu: 0.9768)
[epoch : 40] (l_loss: 0.04514) (t_loss: 0.07299) (accu: 0.9769)
[epoch : 41] (l_loss: 0.04516) (t_loss: 0.07107) (accu: 0.9778)
[epoch : 42] (l_loss: 0.04469) (t_loss: 0.07564) (accu: 0.9764)
[epoch : 43] (l_loss: 0.04475) (t_loss: 0.07418) (accu: 0.9774)
[epoch : 44] (l_loss: 0.04492) (t_loss: 0.07613) (accu: 0.9772)
[epoch : 45] (l_loss: 0.04479) (t_loss: 0.07210) (accu: 0.9785)
[epoch : 46] (l_loss: 0.04437) (t_loss: 0.07161) (accu: 0.9782)
[epoch : 47] (l_loss: 0.04436) (t_loss: 0.07383) (accu: 0.9782)
[epoch : 48] (l_loss: 0.04398) (t_loss: 0.07262) (accu: 0.9767)
[epoch : 49] (l_loss: 0.04434) (t_loss: 0.07121) (accu: 0.9769)
[epoch : 50] (l_loss: 0.04433) (t_loss: 0.06835) (accu: 0.9788)
Finish! (Best accu: 0.9788) (Time taken(sec) : 725.91) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (18507 | 247693)          6.95
fc1.weight   :      235200 (16163 | 219037)          6.87
fc2.weight   :        30000 (2062 | 27938)           6.87
fcout.weight :          1000 (282 | 718)            28.20
------------------------------------------------------------

Learning start! [Prune_iter : (13/21), Remaining weight : 6.95 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29931) (accu: 0.0997)
[epoch : 1] (l_loss: 0.40295) (t_loss: 0.15324) (accu: 0.9550)
[epoch : 2] (l_loss: 0.12744) (t_loss: 0.10820) (accu: 0.9664)
[epoch : 3] (l_loss: 0.09671) (t_loss: 0.09326) (accu: 0.9714)
[epoch : 4] (l_loss: 0.08140) (t_loss: 0.08520) (accu: 0.9754)
[epoch : 5] (l_loss: 0.07259) (t_loss: 0.08469) (accu: 0.9754)
[epoch : 6] (l_loss: 0.06691) (t_loss: 0.08349) (accu: 0.9756)
[epoch : 7] (l_loss: 0.06230) (t_loss: 0.07651) (accu: 0.9763)
[epoch : 8] (l_loss: 0.05907) (t_loss: 0.07260) (accu: 0.9780)
[epoch : 9] (l_loss: 0.05592) (t_loss: 0.07289) (accu: 0.9782)
[epoch : 10] (l_loss: 0.05357) (t_loss: 0.07275) (accu: 0.9782)
[epoch : 11] (l_loss: 0.05120) (t_loss: 0.07328) (accu: 0.9781)
[epoch : 12] (l_loss: 0.05023) (t_loss: 0.07413) (accu: 0.9774)
[epoch : 13] (l_loss: 0.04837) (t_loss: 0.07946) (accu: 0.9747)
[epoch : 14] (l_loss: 0.04766) (t_loss: 0.07896) (accu: 0.9749)
[epoch : 15] (l_loss: 0.04733) (t_loss: 0.07715) (accu: 0.9745)
[epoch : 16] (l_loss: 0.04646) (t_loss: 0.07382) (accu: 0.9768)
[epoch : 17] (l_loss: 0.04607) (t_loss: 0.07230) (accu: 0.9778)
[epoch : 18] (l_loss: 0.04597) (t_loss: 0.07297) (accu: 0.9762)
[epoch : 19] (l_loss: 0.04560) (t_loss: 0.07150) (accu: 0.9773)
[epoch : 20] (l_loss: 0.04511) (t_loss: 0.07329) (accu: 0.9768)
[epoch : 21] (l_loss: 0.04518) (t_loss: 0.07312) (accu: 0.9771)
[epoch : 22] (l_loss: 0.04506) (t_loss: 0.07054) (accu: 0.9784)
[epoch : 23] (l_loss: 0.04513) (t_loss: 0.07312) (accu: 0.9785)
[epoch : 24] (l_loss: 0.04492) (t_loss: 0.07135) (accu: 0.9782)
[epoch : 25] (l_loss: 0.04534) (t_loss: 0.07399) (accu: 0.9763)
[epoch : 26] (l_loss: 0.04501) (t_loss: 0.07324) (accu: 0.9775)
[epoch : 27] (l_loss: 0.04476) (t_loss: 0.07301) (accu: 0.9773)
[epoch : 28] (l_loss: 0.04480) (t_loss: 0.07373) (accu: 0.9769)
[epoch : 29] (l_loss: 0.04454) (t_loss: 0.07390) (accu: 0.9766)
[epoch : 30] (l_loss: 0.04500) (t_loss: 0.07238) (accu: 0.9770)
[epoch : 31] (l_loss: 0.04484) (t_loss: 0.07454) (accu: 0.9768)
[epoch : 32] (l_loss: 0.04454) (t_loss: 0.07254) (accu: 0.9771)
[epoch : 33] (l_loss: 0.04473) (t_loss: 0.07438) (accu: 0.9764)
[epoch : 34] (l_loss: 0.04434) (t_loss: 0.07374) (accu: 0.9770)
[epoch : 35] (l_loss: 0.04485) (t_loss: 0.07283) (accu: 0.9770)
[epoch : 36] (l_loss: 0.04462) (t_loss: 0.07261) (accu: 0.9777)
[epoch : 37] (l_loss: 0.04482) (t_loss: 0.07205) (accu: 0.9780)
[epoch : 38] (l_loss: 0.04419) (t_loss: 0.07448) (accu: 0.9770)
[epoch : 39] (l_loss: 0.04396) (t_loss: 0.07058) (accu: 0.9782)
[epoch : 40] (l_loss: 0.04427) (t_loss: 0.07612) (accu: 0.9756)
[epoch : 41] (l_loss: 0.04378) (t_loss: 0.06958) (accu: 0.9774)
[epoch : 42] (l_loss: 0.04364) (t_loss: 0.07368) (accu: 0.9779)
[epoch : 43] (l_loss: 0.04366) (t_loss: 0.07217) (accu: 0.9770)
[epoch : 44] (l_loss: 0.04378) (t_loss: 0.06967) (accu: 0.9793)
[epoch : 45] (l_loss: 0.04356) (t_loss: 0.07265) (accu: 0.9777)
[epoch : 46] (l_loss: 0.04348) (t_loss: 0.07474) (accu: 0.9760)
[epoch : 47] (l_loss: 0.04381) (t_loss: 0.07221) (accu: 0.9772)
[epoch : 48] (l_loss: 0.04342) (t_loss: 0.07152) (accu: 0.9787)
[epoch : 49] (l_loss: 0.04327) (t_loss: 0.07206) (accu: 0.9767)
[epoch : 50] (l_loss: 0.04379) (t_loss: 0.07416) (accu: 0.9764)
Finish! (Best accu: 0.9793) (Time taken(sec) : 722.99) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (14833 | 251367)          5.57
fc1.weight   :      235200 (12930 | 222270)          5.50
fc2.weight   :        30000 (1649 | 28351)           5.50
fcout.weight :          1000 (254 | 746)            25.40
------------------------------------------------------------

Learning start! [Prune_iter : (14/21), Remaining weight : 5.57 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30175) (accu: 0.1071)
[epoch : 1] (l_loss: 0.40706) (t_loss: 0.15887) (accu: 0.9540)
[epoch : 2] (l_loss: 0.13022) (t_loss: 0.11193) (accu: 0.9681)
[epoch : 3] (l_loss: 0.09921) (t_loss: 0.09800) (accu: 0.9712)
[epoch : 4] (l_loss: 0.08345) (t_loss: 0.09143) (accu: 0.9725)
[epoch : 5] (l_loss: 0.07427) (t_loss: 0.08718) (accu: 0.9736)
[epoch : 6] (l_loss: 0.06839) (t_loss: 0.07916) (accu: 0.9763)
[epoch : 7] (l_loss: 0.06344) (t_loss: 0.08090) (accu: 0.9744)
[epoch : 8] (l_loss: 0.06047) (t_loss: 0.07532) (accu: 0.9769)
[epoch : 9] (l_loss: 0.05756) (t_loss: 0.07639) (accu: 0.9771)
[epoch : 10] (l_loss: 0.05475) (t_loss: 0.07553) (accu: 0.9768)
[epoch : 11] (l_loss: 0.05221) (t_loss: 0.07681) (accu: 0.9766)
[epoch : 12] (l_loss: 0.05115) (t_loss: 0.07641) (accu: 0.9759)
[epoch : 13] (l_loss: 0.04983) (t_loss: 0.07500) (accu: 0.9785)
[epoch : 14] (l_loss: 0.04915) (t_loss: 0.07516) (accu: 0.9757)
[epoch : 15] (l_loss: 0.04812) (t_loss: 0.07330) (accu: 0.9777)
[epoch : 16] (l_loss: 0.04775) (t_loss: 0.07486) (accu: 0.9775)
[epoch : 17] (l_loss: 0.04727) (t_loss: 0.07154) (accu: 0.9771)
[epoch : 18] (l_loss: 0.04695) (t_loss: 0.07299) (accu: 0.9759)
[epoch : 19] (l_loss: 0.04711) (t_loss: 0.07306) (accu: 0.9778)
[epoch : 20] (l_loss: 0.04672) (t_loss: 0.07216) (accu: 0.9780)
[epoch : 21] (l_loss: 0.04573) (t_loss: 0.07517) (accu: 0.9769)
[epoch : 22] (l_loss: 0.04506) (t_loss: 0.07114) (accu: 0.9774)
[epoch : 23] (l_loss: 0.04517) (t_loss: 0.07184) (accu: 0.9780)
[epoch : 24] (l_loss: 0.04479) (t_loss: 0.07484) (accu: 0.9771)
[epoch : 25] (l_loss: 0.04504) (t_loss: 0.07047) (accu: 0.9786)
[epoch : 26] (l_loss: 0.04497) (t_loss: 0.07232) (accu: 0.9764)
[epoch : 27] (l_loss: 0.04482) (t_loss: 0.07091) (accu: 0.9785)
[epoch : 28] (l_loss: 0.04436) (t_loss: 0.07280) (accu: 0.9785)
[epoch : 29] (l_loss: 0.04497) (t_loss: 0.07487) (accu: 0.9762)
[epoch : 30] (l_loss: 0.04406) (t_loss: 0.07131) (accu: 0.9769)
[epoch : 31] (l_loss: 0.04467) (t_loss: 0.06991) (accu: 0.9778)
[epoch : 32] (l_loss: 0.04429) (t_loss: 0.07028) (accu: 0.9781)
[epoch : 33] (l_loss: 0.04433) (t_loss: 0.07258) (accu: 0.9769)
[epoch : 34] (l_loss: 0.04390) (t_loss: 0.07088) (accu: 0.9771)
[epoch : 35] (l_loss: 0.04439) (t_loss: 0.07284) (accu: 0.9773)
[epoch : 36] (l_loss: 0.04436) (t_loss: 0.07145) (accu: 0.9781)
[epoch : 37] (l_loss: 0.04461) (t_loss: 0.07090) (accu: 0.9771)
[epoch : 38] (l_loss: 0.04433) (t_loss: 0.07332) (accu: 0.9774)
[epoch : 39] (l_loss: 0.04438) (t_loss: 0.07352) (accu: 0.9773)
[epoch : 40] (l_loss: 0.04471) (t_loss: 0.07045) (accu: 0.9783)
[epoch : 41] (l_loss: 0.04454) (t_loss: 0.07296) (accu: 0.9770)
[epoch : 42] (l_loss: 0.04430) (t_loss: 0.06986) (accu: 0.9769)
[epoch : 43] (l_loss: 0.04416) (t_loss: 0.07172) (accu: 0.9779)
[epoch : 44] (l_loss: 0.04405) (t_loss: 0.07148) (accu: 0.9776)
[epoch : 45] (l_loss: 0.04376) (t_loss: 0.07333) (accu: 0.9774)
[epoch : 46] (l_loss: 0.04420) (t_loss: 0.07332) (accu: 0.9770)
[epoch : 47] (l_loss: 0.04421) (t_loss: 0.07135) (accu: 0.9773)
[epoch : 48] (l_loss: 0.04378) (t_loss: 0.07153) (accu: 0.9775)
[epoch : 49] (l_loss: 0.04439) (t_loss: 0.07103) (accu: 0.9768)
[epoch : 50] (l_loss: 0.04456) (t_loss: 0.07006) (accu: 0.9793)
Finish! (Best accu: 0.9793) (Time taken(sec) : 718.92) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (11892 | 254308)          4.47
fc1.weight   :      235200 (10344 | 224856)          4.40
fc2.weight   :        30000 (1319 | 28681)           4.40
fcout.weight :          1000 (229 | 771)            22.90
------------------------------------------------------------

Learning start! [Prune_iter : (15/21), Remaining weight : 4.47 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29836) (accu: 0.1078)
[epoch : 1] (l_loss: 0.40250) (t_loss: 0.15597) (accu: 0.9562)
[epoch : 2] (l_loss: 0.13027) (t_loss: 0.11723) (accu: 0.9648)
[epoch : 3] (l_loss: 0.09884) (t_loss: 0.09428) (accu: 0.9717)
[epoch : 4] (l_loss: 0.08449) (t_loss: 0.08795) (accu: 0.9732)
[epoch : 5] (l_loss: 0.07564) (t_loss: 0.08592) (accu: 0.9740)
[epoch : 6] (l_loss: 0.06916) (t_loss: 0.08299) (accu: 0.9759)
[epoch : 7] (l_loss: 0.06459) (t_loss: 0.08106) (accu: 0.9758)
[epoch : 8] (l_loss: 0.06098) (t_loss: 0.07831) (accu: 0.9759)
[epoch : 9] (l_loss: 0.05838) (t_loss: 0.07405) (accu: 0.9777)
[epoch : 10] (l_loss: 0.05529) (t_loss: 0.07675) (accu: 0.9770)
[epoch : 11] (l_loss: 0.05289) (t_loss: 0.07538) (accu: 0.9770)
[epoch : 12] (l_loss: 0.05066) (t_loss: 0.07232) (accu: 0.9779)
[epoch : 13] (l_loss: 0.04966) (t_loss: 0.07239) (accu: 0.9770)
[epoch : 14] (l_loss: 0.04822) (t_loss: 0.07212) (accu: 0.9779)
[epoch : 15] (l_loss: 0.04671) (t_loss: 0.07392) (accu: 0.9766)
[epoch : 16] (l_loss: 0.04647) (t_loss: 0.07647) (accu: 0.9773)
[epoch : 17] (l_loss: 0.04577) (t_loss: 0.07269) (accu: 0.9755)
[epoch : 18] (l_loss: 0.04535) (t_loss: 0.07183) (accu: 0.9782)
[epoch : 19] (l_loss: 0.04496) (t_loss: 0.07082) (accu: 0.9776)
[epoch : 20] (l_loss: 0.04465) (t_loss: 0.07180) (accu: 0.9766)
[epoch : 21] (l_loss: 0.04454) (t_loss: 0.07194) (accu: 0.9774)
[epoch : 22] (l_loss: 0.04464) (t_loss: 0.07039) (accu: 0.9781)
[epoch : 23] (l_loss: 0.04457) (t_loss: 0.07159) (accu: 0.9778)
[epoch : 24] (l_loss: 0.04436) (t_loss: 0.06912) (accu: 0.9787)
[epoch : 25] (l_loss: 0.04431) (t_loss: 0.07130) (accu: 0.9778)
[epoch : 26] (l_loss: 0.04429) (t_loss: 0.06935) (accu: 0.9789)
[epoch : 27] (l_loss: 0.04421) (t_loss: 0.07045) (accu: 0.9784)
[epoch : 28] (l_loss: 0.04420) (t_loss: 0.06947) (accu: 0.9790)
[epoch : 29] (l_loss: 0.04390) (t_loss: 0.07275) (accu: 0.9778)
[epoch : 30] (l_loss: 0.04350) (t_loss: 0.07194) (accu: 0.9778)
[epoch : 31] (l_loss: 0.04400) (t_loss: 0.06972) (accu: 0.9781)
[epoch : 32] (l_loss: 0.04363) (t_loss: 0.07544) (accu: 0.9759)
[epoch : 33] (l_loss: 0.04384) (t_loss: 0.07155) (accu: 0.9782)
[epoch : 34] (l_loss: 0.04379) (t_loss: 0.06925) (accu: 0.9793)
[epoch : 35] (l_loss: 0.04364) (t_loss: 0.07351) (accu: 0.9765)
[epoch : 36] (l_loss: 0.04338) (t_loss: 0.07123) (accu: 0.9784)
[epoch : 37] (l_loss: 0.04306) (t_loss: 0.07142) (accu: 0.9785)
[epoch : 38] (l_loss: 0.04401) (t_loss: 0.07093) (accu: 0.9761)
[epoch : 39] (l_loss: 0.04345) (t_loss: 0.07657) (accu: 0.9779)
[epoch : 40] (l_loss: 0.04367) (t_loss: 0.07185) (accu: 0.9766)
[epoch : 41] (l_loss: 0.04358) (t_loss: 0.07032) (accu: 0.9779)
[epoch : 42] (l_loss: 0.04366) (t_loss: 0.06969) (accu: 0.9784)
[epoch : 43] (l_loss: 0.04331) (t_loss: 0.07722) (accu: 0.9760)
[epoch : 44] (l_loss: 0.04353) (t_loss: 0.07308) (accu: 0.9774)
[epoch : 45] (l_loss: 0.04345) (t_loss: 0.06973) (accu: 0.9785)
[epoch : 46] (l_loss: 0.04358) (t_loss: 0.07469) (accu: 0.9774)
[epoch : 47] (l_loss: 0.04370) (t_loss: 0.07059) (accu: 0.9776)
[epoch : 48] (l_loss: 0.04334) (t_loss: 0.07054) (accu: 0.9774)
[epoch : 49] (l_loss: 0.04350) (t_loss: 0.07045) (accu: 0.9786)
[epoch : 50] (l_loss: 0.04349) (t_loss: 0.07042) (accu: 0.9790)
Finish! (Best accu: 0.9793) (Time taken(sec) : 724.77) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (9537 | 256663)          3.58
fc1.weight   :       235200 (8275 | 226925)          3.52
fc2.weight   :        30000 (1056 | 28944)           3.52
fcout.weight :          1000 (206 | 794)            20.60
------------------------------------------------------------

Learning start! [Prune_iter : (16/21), Remaining weight : 3.58 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29398) (accu: 0.1019)
[epoch : 1] (l_loss: 0.40105) (t_loss: 0.15148) (accu: 0.9561)
[epoch : 2] (l_loss: 0.12744) (t_loss: 0.10787) (accu: 0.9680)
[epoch : 3] (l_loss: 0.09671) (t_loss: 0.09305) (accu: 0.9715)
[epoch : 4] (l_loss: 0.08234) (t_loss: 0.08773) (accu: 0.9735)
[epoch : 5] (l_loss: 0.07351) (t_loss: 0.08474) (accu: 0.9741)
[epoch : 6] (l_loss: 0.06788) (t_loss: 0.07893) (accu: 0.9764)
[epoch : 7] (l_loss: 0.06346) (t_loss: 0.07632) (accu: 0.9762)
[epoch : 8] (l_loss: 0.05973) (t_loss: 0.07763) (accu: 0.9769)
[epoch : 9] (l_loss: 0.05659) (t_loss: 0.07394) (accu: 0.9767)
[epoch : 10] (l_loss: 0.05385) (t_loss: 0.07282) (accu: 0.9779)
[epoch : 11] (l_loss: 0.05144) (t_loss: 0.07288) (accu: 0.9775)
[epoch : 12] (l_loss: 0.04941) (t_loss: 0.07213) (accu: 0.9781)
[epoch : 13] (l_loss: 0.04808) (t_loss: 0.07561) (accu: 0.9765)
[epoch : 14] (l_loss: 0.04796) (t_loss: 0.07015) (accu: 0.9787)
[epoch : 15] (l_loss: 0.04747) (t_loss: 0.07574) (accu: 0.9767)
[epoch : 16] (l_loss: 0.04655) (t_loss: 0.07369) (accu: 0.9766)
[epoch : 17] (l_loss: 0.04665) (t_loss: 0.07491) (accu: 0.9772)
[epoch : 18] (l_loss: 0.04581) (t_loss: 0.07198) (accu: 0.9783)
[epoch : 19] (l_loss: 0.04608) (t_loss: 0.07458) (accu: 0.9762)
[epoch : 20] (l_loss: 0.04569) (t_loss: 0.07289) (accu: 0.9770)
[epoch : 21] (l_loss: 0.04544) (t_loss: 0.07164) (accu: 0.9780)
[epoch : 22] (l_loss: 0.04599) (t_loss: 0.07483) (accu: 0.9758)
[epoch : 23] (l_loss: 0.04489) (t_loss: 0.07654) (accu: 0.9762)
[epoch : 24] (l_loss: 0.04494) (t_loss: 0.07576) (accu: 0.9769)
[epoch : 25] (l_loss: 0.04529) (t_loss: 0.07329) (accu: 0.9775)
[epoch : 26] (l_loss: 0.04522) (t_loss: 0.07298) (accu: 0.9783)
[epoch : 27] (l_loss: 0.04464) (t_loss: 0.07384) (accu: 0.9771)
[epoch : 28] (l_loss: 0.04459) (t_loss: 0.07314) (accu: 0.9780)
[epoch : 29] (l_loss: 0.04490) (t_loss: 0.07304) (accu: 0.9765)
[epoch : 30] (l_loss: 0.04513) (t_loss: 0.07593) (accu: 0.9759)
[epoch : 31] (l_loss: 0.04490) (t_loss: 0.07325) (accu: 0.9777)
[epoch : 32] (l_loss: 0.04499) (t_loss: 0.07461) (accu: 0.9771)
[epoch : 33] (l_loss: 0.04457) (t_loss: 0.07463) (accu: 0.9768)
[epoch : 34] (l_loss: 0.04482) (t_loss: 0.07517) (accu: 0.9766)
[epoch : 35] (l_loss: 0.04449) (t_loss: 0.07518) (accu: 0.9765)
[epoch : 36] (l_loss: 0.04445) (t_loss: 0.07456) (accu: 0.9766)
[epoch : 37] (l_loss: 0.04489) (t_loss: 0.07445) (accu: 0.9770)
[epoch : 38] (l_loss: 0.04492) (t_loss: 0.07239) (accu: 0.9772)
[epoch : 39] (l_loss: 0.04466) (t_loss: 0.07363) (accu: 0.9781)
[epoch : 40] (l_loss: 0.04500) (t_loss: 0.07147) (accu: 0.9778)
[epoch : 41] (l_loss: 0.04502) (t_loss: 0.07340) (accu: 0.9770)
[epoch : 42] (l_loss: 0.04478) (t_loss: 0.07302) (accu: 0.9780)
[epoch : 43] (l_loss: 0.04408) (t_loss: 0.07396) (accu: 0.9775)
[epoch : 44] (l_loss: 0.04410) (t_loss: 0.07127) (accu: 0.9778)
[epoch : 45] (l_loss: 0.04324) (t_loss: 0.07057) (accu: 0.9783)
[epoch : 46] (l_loss: 0.04345) (t_loss: 0.06981) (accu: 0.9783)
[epoch : 47] (l_loss: 0.04367) (t_loss: 0.07456) (accu: 0.9775)
[epoch : 48] (l_loss: 0.04393) (t_loss: 0.07079) (accu: 0.9762)
[epoch : 49] (l_loss: 0.04330) (t_loss: 0.07109) (accu: 0.9798)
[epoch : 50] (l_loss: 0.04362) (t_loss: 0.07200) (accu: 0.9776)
Finish! (Best accu: 0.9798) (Time taken(sec) : 711.38) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (7649 | 258551)          2.87
fc1.weight   :       235200 (6620 | 228580)          2.81
fc2.weight   :        30000 (844 | 29156)            2.81
fcout.weight :          1000 (185 | 815)            18.50
------------------------------------------------------------

Learning start! [Prune_iter : (17/21), Remaining weight : 2.87 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29988) (accu: 0.0974)
[epoch : 1] (l_loss: 0.39847) (t_loss: 0.14878) (accu: 0.9556)
[epoch : 2] (l_loss: 0.12562) (t_loss: 0.11514) (accu: 0.9660)
[epoch : 3] (l_loss: 0.09524) (t_loss: 0.09850) (accu: 0.9720)
[epoch : 4] (l_loss: 0.08168) (t_loss: 0.08614) (accu: 0.9748)
[epoch : 5] (l_loss: 0.07251) (t_loss: 0.08730) (accu: 0.9716)
[epoch : 6] (l_loss: 0.06679) (t_loss: 0.07939) (accu: 0.9772)
[epoch : 7] (l_loss: 0.06223) (t_loss: 0.07860) (accu: 0.9767)
[epoch : 8] (l_loss: 0.05781) (t_loss: 0.07911) (accu: 0.9760)
[epoch : 9] (l_loss: 0.05487) (t_loss: 0.07425) (accu: 0.9773)
[epoch : 10] (l_loss: 0.05219) (t_loss: 0.07434) (accu: 0.9768)
[epoch : 11] (l_loss: 0.04946) (t_loss: 0.07207) (accu: 0.9778)
[epoch : 12] (l_loss: 0.04866) (t_loss: 0.07261) (accu: 0.9769)
[epoch : 13] (l_loss: 0.04739) (t_loss: 0.07390) (accu: 0.9765)
[epoch : 14] (l_loss: 0.04734) (t_loss: 0.07293) (accu: 0.9773)
[epoch : 15] (l_loss: 0.04662) (t_loss: 0.07424) (accu: 0.9784)
[epoch : 16] (l_loss: 0.04594) (t_loss: 0.07433) (accu: 0.9763)
[epoch : 17] (l_loss: 0.04588) (t_loss: 0.07090) (accu: 0.9787)
[epoch : 18] (l_loss: 0.04503) (t_loss: 0.07078) (accu: 0.9782)
[epoch : 19] (l_loss: 0.04506) (t_loss: 0.07250) (accu: 0.9766)
[epoch : 20] (l_loss: 0.04512) (t_loss: 0.07201) (accu: 0.9774)
[epoch : 21] (l_loss: 0.04503) (t_loss: 0.07252) (accu: 0.9787)
[epoch : 22] (l_loss: 0.04490) (t_loss: 0.07346) (accu: 0.9766)
[epoch : 23] (l_loss: 0.04492) (t_loss: 0.07440) (accu: 0.9766)
[epoch : 24] (l_loss: 0.04454) (t_loss: 0.07154) (accu: 0.9776)
[epoch : 25] (l_loss: 0.04409) (t_loss: 0.07619) (accu: 0.9760)
[epoch : 26] (l_loss: 0.04451) (t_loss: 0.07477) (accu: 0.9768)
[epoch : 27] (l_loss: 0.04442) (t_loss: 0.07470) (accu: 0.9758)
[epoch : 28] (l_loss: 0.04438) (t_loss: 0.07138) (accu: 0.9768)
[epoch : 29] (l_loss: 0.04455) (t_loss: 0.07394) (accu: 0.9764)
[epoch : 30] (l_loss: 0.04425) (t_loss: 0.07461) (accu: 0.9765)
[epoch : 31] (l_loss: 0.04494) (t_loss: 0.07188) (accu: 0.9782)
[epoch : 32] (l_loss: 0.04370) (t_loss: 0.07015) (accu: 0.9780)
[epoch : 33] (l_loss: 0.04341) (t_loss: 0.07023) (accu: 0.9780)
[epoch : 34] (l_loss: 0.04340) (t_loss: 0.07365) (accu: 0.9766)
[epoch : 35] (l_loss: 0.04344) (t_loss: 0.07017) (accu: 0.9775)
[epoch : 36] (l_loss: 0.04341) (t_loss: 0.07120) (accu: 0.9789)
[epoch : 37] (l_loss: 0.04353) (t_loss: 0.07116) (accu: 0.9775)
[epoch : 38] (l_loss: 0.04334) (t_loss: 0.06975) (accu: 0.9789)
[epoch : 39] (l_loss: 0.04335) (t_loss: 0.06868) (accu: 0.9786)
[epoch : 40] (l_loss: 0.04332) (t_loss: 0.07691) (accu: 0.9764)
[epoch : 41] (l_loss: 0.04349) (t_loss: 0.06754) (accu: 0.9796)
[epoch : 42] (l_loss: 0.04311) (t_loss: 0.07210) (accu: 0.9770)
[epoch : 43] (l_loss: 0.04345) (t_loss: 0.06930) (accu: 0.9781)
[epoch : 44] (l_loss: 0.04298) (t_loss: 0.07100) (accu: 0.9782)
[epoch : 45] (l_loss: 0.04333) (t_loss: 0.06863) (accu: 0.9799)
[epoch : 46] (l_loss: 0.04347) (t_loss: 0.07075) (accu: 0.9781)
[epoch : 47] (l_loss: 0.04351) (t_loss: 0.07182) (accu: 0.9781)
[epoch : 48] (l_loss: 0.04274) (t_loss: 0.07247) (accu: 0.9774)
[epoch : 49] (l_loss: 0.04373) (t_loss: 0.07137) (accu: 0.9783)
[epoch : 50] (l_loss: 0.04352) (t_loss: 0.06824) (accu: 0.9778)
Finish! (Best accu: 0.9799) (Time taken(sec) : 733.34) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (6139 | 260061)          2.31
fc1.weight   :       235200 (5296 | 229904)          2.25
fc2.weight   :        30000 (676 | 29324)            2.25
fcout.weight :          1000 (167 | 833)            16.70
------------------------------------------------------------

Learning start! [Prune_iter : (18/21), Remaining weight : 2.31 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29874) (accu: 0.0974)
[epoch : 1] (l_loss: 0.40204) (t_loss: 0.15625) (accu: 0.9527)
[epoch : 2] (l_loss: 0.12752) (t_loss: 0.11558) (accu: 0.9645)
[epoch : 3] (l_loss: 0.09631) (t_loss: 0.09533) (accu: 0.9715)
[epoch : 4] (l_loss: 0.08164) (t_loss: 0.08748) (accu: 0.9734)
[epoch : 5] (l_loss: 0.07139) (t_loss: 0.08127) (accu: 0.9763)
[epoch : 6] (l_loss: 0.06527) (t_loss: 0.07909) (accu: 0.9765)
[epoch : 7] (l_loss: 0.06052) (t_loss: 0.07798) (accu: 0.9772)
[epoch : 8] (l_loss: 0.05696) (t_loss: 0.07622) (accu: 0.9776)
[epoch : 9] (l_loss: 0.05388) (t_loss: 0.07850) (accu: 0.9770)
[epoch : 10] (l_loss: 0.05153) (t_loss: 0.07437) (accu: 0.9774)
[epoch : 11] (l_loss: 0.04984) (t_loss: 0.07704) (accu: 0.9761)
[epoch : 12] (l_loss: 0.04833) (t_loss: 0.07500) (accu: 0.9762)
[epoch : 13] (l_loss: 0.04771) (t_loss: 0.07331) (accu: 0.9769)
[epoch : 14] (l_loss: 0.04687) (t_loss: 0.07262) (accu: 0.9759)
[epoch : 15] (l_loss: 0.04646) (t_loss: 0.07439) (accu: 0.9764)
[epoch : 16] (l_loss: 0.04566) (t_loss: 0.07225) (accu: 0.9775)
[epoch : 17] (l_loss: 0.04569) (t_loss: 0.07315) (accu: 0.9770)
[epoch : 18] (l_loss: 0.04583) (t_loss: 0.07551) (accu: 0.9770)
[epoch : 19] (l_loss: 0.04546) (t_loss: 0.07529) (accu: 0.9762)
[epoch : 20] (l_loss: 0.04528) (t_loss: 0.07222) (accu: 0.9776)
[epoch : 21] (l_loss: 0.04514) (t_loss: 0.07184) (accu: 0.9785)
[epoch : 22] (l_loss: 0.04486) (t_loss: 0.07019) (accu: 0.9773)
[epoch : 23] (l_loss: 0.04549) (t_loss: 0.07343) (accu: 0.9759)
[epoch : 24] (l_loss: 0.04509) (t_loss: 0.07072) (accu: 0.9790)
[epoch : 25] (l_loss: 0.04512) (t_loss: 0.07490) (accu: 0.9767)
[epoch : 26] (l_loss: 0.04505) (t_loss: 0.07432) (accu: 0.9763)
[epoch : 27] (l_loss: 0.04458) (t_loss: 0.07266) (accu: 0.9770)
[epoch : 28] (l_loss: 0.04434) (t_loss: 0.07279) (accu: 0.9769)
[epoch : 29] (l_loss: 0.04460) (t_loss: 0.07433) (accu: 0.9777)
[epoch : 30] (l_loss: 0.04360) (t_loss: 0.07250) (accu: 0.9781)
[epoch : 31] (l_loss: 0.04342) (t_loss: 0.07114) (accu: 0.9780)
[epoch : 32] (l_loss: 0.04388) (t_loss: 0.07197) (accu: 0.9775)
[epoch : 33] (l_loss: 0.04356) (t_loss: 0.07331) (accu: 0.9772)
[epoch : 34] (l_loss: 0.04370) (t_loss: 0.07594) (accu: 0.9754)
[epoch : 35] (l_loss: 0.04375) (t_loss: 0.07071) (accu: 0.9774)
[epoch : 36] (l_loss: 0.04386) (t_loss: 0.07184) (accu: 0.9774)
[epoch : 37] (l_loss: 0.04353) (t_loss: 0.07158) (accu: 0.9790)
[epoch : 38] (l_loss: 0.04416) (t_loss: 0.07287) (accu: 0.9770)
[epoch : 39] (l_loss: 0.04408) (t_loss: 0.07239) (accu: 0.9775)
[epoch : 40] (l_loss: 0.04364) (t_loss: 0.07128) (accu: 0.9794)
[epoch : 41] (l_loss: 0.04414) (t_loss: 0.07309) (accu: 0.9760)
[epoch : 42] (l_loss: 0.04363) (t_loss: 0.07214) (accu: 0.9768)
[epoch : 43] (l_loss: 0.04346) (t_loss: 0.07323) (accu: 0.9771)
[epoch : 44] (l_loss: 0.04400) (t_loss: 0.06984) (accu: 0.9790)
[epoch : 45] (l_loss: 0.04370) (t_loss: 0.07207) (accu: 0.9763)
[epoch : 46] (l_loss: 0.04400) (t_loss: 0.07315) (accu: 0.9772)
[epoch : 47] (l_loss: 0.04366) (t_loss: 0.07362) (accu: 0.9771)
[epoch : 48] (l_loss: 0.04355) (t_loss: 0.07279) (accu: 0.9780)
[epoch : 49] (l_loss: 0.04359) (t_loss: 0.07163) (accu: 0.9775)
[epoch : 50] (l_loss: 0.04390) (t_loss: 0.07175) (accu: 0.9775)
Finish! (Best accu: 0.9794) (Time taken(sec) : 713.75) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (4927 | 261273)          1.85
fc1.weight   :       235200 (4237 | 230963)          1.80
fc2.weight   :        30000 (540 | 29460)            1.80
fcout.weight :          1000 (150 | 850)            15.00
------------------------------------------------------------

Learning start! [Prune_iter : (19/21), Remaining weight : 1.85 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29727) (accu: 0.0974)
[epoch : 1] (l_loss: 0.39947) (t_loss: 0.15103) (accu: 0.9566)
[epoch : 2] (l_loss: 0.12679) (t_loss: 0.11128) (accu: 0.9653)
[epoch : 3] (l_loss: 0.09581) (t_loss: 0.09537) (accu: 0.9712)
[epoch : 4] (l_loss: 0.08111) (t_loss: 0.08744) (accu: 0.9734)
[epoch : 5] (l_loss: 0.07223) (t_loss: 0.08311) (accu: 0.9745)
[epoch : 6] (l_loss: 0.06599) (t_loss: 0.08353) (accu: 0.9750)
[epoch : 7] (l_loss: 0.06118) (t_loss: 0.08241) (accu: 0.9746)
[epoch : 8] (l_loss: 0.05757) (t_loss: 0.07744) (accu: 0.9763)
[epoch : 9] (l_loss: 0.05433) (t_loss: 0.07411) (accu: 0.9777)
[epoch : 10] (l_loss: 0.05164) (t_loss: 0.07478) (accu: 0.9766)
[epoch : 11] (l_loss: 0.05008) (t_loss: 0.07302) (accu: 0.9776)
[epoch : 12] (l_loss: 0.04896) (t_loss: 0.07555) (accu: 0.9764)
[epoch : 13] (l_loss: 0.04819) (t_loss: 0.07274) (accu: 0.9777)
[epoch : 14] (l_loss: 0.04783) (t_loss: 0.07353) (accu: 0.9780)
[epoch : 15] (l_loss: 0.04707) (t_loss: 0.07235) (accu: 0.9775)
[epoch : 16] (l_loss: 0.04645) (t_loss: 0.07278) (accu: 0.9775)
[epoch : 17] (l_loss: 0.04674) (t_loss: 0.06952) (accu: 0.9780)
[epoch : 18] (l_loss: 0.04625) (t_loss: 0.07311) (accu: 0.9770)
[epoch : 19] (l_loss: 0.04580) (t_loss: 0.07431) (accu: 0.9765)
[epoch : 20] (l_loss: 0.04596) (t_loss: 0.07519) (accu: 0.9760)
[epoch : 21] (l_loss: 0.04573) (t_loss: 0.07371) (accu: 0.9767)
[epoch : 22] (l_loss: 0.04545) (t_loss: 0.07505) (accu: 0.9772)
[epoch : 23] (l_loss: 0.04539) (t_loss: 0.07353) (accu: 0.9771)
[epoch : 24] (l_loss: 0.04561) (t_loss: 0.07055) (accu: 0.9776)
[epoch : 25] (l_loss: 0.04513) (t_loss: 0.07470) (accu: 0.9768)
[epoch : 26] (l_loss: 0.04584) (t_loss: 0.07114) (accu: 0.9784)
[epoch : 27] (l_loss: 0.04554) (t_loss: 0.07269) (accu: 0.9776)
[epoch : 28] (l_loss: 0.04499) (t_loss: 0.07351) (accu: 0.9773)
[epoch : 29] (l_loss: 0.04513) (t_loss: 0.07407) (accu: 0.9776)
[epoch : 30] (l_loss: 0.04515) (t_loss: 0.07660) (accu: 0.9747)
[epoch : 31] (l_loss: 0.04513) (t_loss: 0.07415) (accu: 0.9774)
[epoch : 32] (l_loss: 0.04540) (t_loss: 0.07276) (accu: 0.9777)
[epoch : 33] (l_loss: 0.04487) (t_loss: 0.07392) (accu: 0.9774)
[epoch : 34] (l_loss: 0.04538) (t_loss: 0.07291) (accu: 0.9768)
[epoch : 35] (l_loss: 0.04516) (t_loss: 0.07488) (accu: 0.9766)
[epoch : 36] (l_loss: 0.04544) (t_loss: 0.07219) (accu: 0.9782)
[epoch : 37] (l_loss: 0.04532) (t_loss: 0.07194) (accu: 0.9773)
[epoch : 38] (l_loss: 0.04500) (t_loss: 0.07391) (accu: 0.9767)
[epoch : 39] (l_loss: 0.04544) (t_loss: 0.07357) (accu: 0.9774)
[epoch : 40] (l_loss: 0.04515) (t_loss: 0.07439) (accu: 0.9767)
[epoch : 41] (l_loss: 0.04528) (t_loss: 0.07244) (accu: 0.9762)
[epoch : 42] (l_loss: 0.04510) (t_loss: 0.07122) (accu: 0.9783)
[epoch : 43] (l_loss: 0.04500) (t_loss: 0.07244) (accu: 0.9773)
[epoch : 44] (l_loss: 0.04448) (t_loss: 0.07135) (accu: 0.9774)
[epoch : 45] (l_loss: 0.04510) (t_loss: 0.07199) (accu: 0.9782)
[epoch : 46] (l_loss: 0.04472) (t_loss: 0.07238) (accu: 0.9780)
[epoch : 47] (l_loss: 0.04570) (t_loss: 0.07257) (accu: 0.9775)
[epoch : 48] (l_loss: 0.04519) (t_loss: 0.07449) (accu: 0.9776)
[epoch : 49] (l_loss: 0.04496) (t_loss: 0.07078) (accu: 0.9779)
[epoch : 50] (l_loss: 0.04418) (t_loss: 0.07148) (accu: 0.9785)
Finish! (Best accu: 0.9785) (Time taken(sec) : 715.65) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (3957 | 262243)          1.49
fc1.weight   :       235200 (3390 | 231810)          1.44
fc2.weight   :        30000 (432 | 29568)            1.44
fcout.weight :          1000 (135 | 865)            13.50
------------------------------------------------------------

Learning start! [Prune_iter : (20/21), Remaining weight : 1.49 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29521) (accu: 0.0974)
[epoch : 1] (l_loss: 0.39773) (t_loss: 0.15100) (accu: 0.9547)
[epoch : 2] (l_loss: 0.12791) (t_loss: 0.11565) (accu: 0.9650)
[epoch : 3] (l_loss: 0.09772) (t_loss: 0.09553) (accu: 0.9714)
[epoch : 4] (l_loss: 0.08290) (t_loss: 0.08752) (accu: 0.9744)
[epoch : 5] (l_loss: 0.07368) (t_loss: 0.08333) (accu: 0.9766)
[epoch : 6] (l_loss: 0.06637) (t_loss: 0.08167) (accu: 0.9746)
[epoch : 7] (l_loss: 0.06112) (t_loss: 0.07872) (accu: 0.9758)
[epoch : 8] (l_loss: 0.05792) (t_loss: 0.07855) (accu: 0.9765)
[epoch : 9] (l_loss: 0.05488) (t_loss: 0.07451) (accu: 0.9782)
[epoch : 10] (l_loss: 0.05261) (t_loss: 0.07719) (accu: 0.9768)
[epoch : 11] (l_loss: 0.05030) (t_loss: 0.07443) (accu: 0.9772)
[epoch : 12] (l_loss: 0.04896) (t_loss: 0.07340) (accu: 0.9764)
[epoch : 13] (l_loss: 0.04804) (t_loss: 0.07241) (accu: 0.9793)
[epoch : 14] (l_loss: 0.04762) (t_loss: 0.07392) (accu: 0.9764)
[epoch : 15] (l_loss: 0.04704) (t_loss: 0.07227) (accu: 0.9775)
[epoch : 16] (l_loss: 0.04632) (t_loss: 0.07492) (accu: 0.9749)
[epoch : 17] (l_loss: 0.04590) (t_loss: 0.07360) (accu: 0.9768)
[epoch : 18] (l_loss: 0.04564) (t_loss: 0.07572) (accu: 0.9756)
[epoch : 19] (l_loss: 0.04559) (t_loss: 0.07192) (accu: 0.9766)
[epoch : 20] (l_loss: 0.04578) (t_loss: 0.07187) (accu: 0.9786)
[epoch : 21] (l_loss: 0.04529) (t_loss: 0.07276) (accu: 0.9769)
[epoch : 22] (l_loss: 0.04503) (t_loss: 0.07198) (accu: 0.9770)
[epoch : 23] (l_loss: 0.04494) (t_loss: 0.07206) (accu: 0.9773)
[epoch : 24] (l_loss: 0.04503) (t_loss: 0.07484) (accu: 0.9749)
[epoch : 25] (l_loss: 0.04505) (t_loss: 0.07413) (accu: 0.9767)
[epoch : 26] (l_loss: 0.04486) (t_loss: 0.07798) (accu: 0.9748)
[epoch : 27] (l_loss: 0.04514) (t_loss: 0.07296) (accu: 0.9777)
[epoch : 28] (l_loss: 0.04460) (t_loss: 0.07071) (accu: 0.9783)
[epoch : 29] (l_loss: 0.04497) (t_loss: 0.07508) (accu: 0.9768)
[epoch : 30] (l_loss: 0.04472) (t_loss: 0.07165) (accu: 0.9786)
[epoch : 31] (l_loss: 0.04461) (t_loss: 0.07196) (accu: 0.9775)
[epoch : 32] (l_loss: 0.04509) (t_loss: 0.07328) (accu: 0.9779)
[epoch : 33] (l_loss: 0.04442) (t_loss: 0.07281) (accu: 0.9778)
[epoch : 34] (l_loss: 0.04450) (t_loss: 0.07368) (accu: 0.9773)
[epoch : 35] (l_loss: 0.04478) (t_loss: 0.07310) (accu: 0.9769)
[epoch : 36] (l_loss: 0.04437) (t_loss: 0.07507) (accu: 0.9770)
[epoch : 37] (l_loss: 0.04469) (t_loss: 0.07132) (accu: 0.9771)
[epoch : 38] (l_loss: 0.04491) (t_loss: 0.07296) (accu: 0.9783)
[epoch : 39] (l_loss: 0.04453) (t_loss: 0.07089) (accu: 0.9770)
[epoch : 40] (l_loss: 0.04496) (t_loss: 0.07412) (accu: 0.9763)
[epoch : 41] (l_loss: 0.04441) (t_loss: 0.07583) (accu: 0.9760)
[epoch : 42] (l_loss: 0.04410) (t_loss: 0.07167) (accu: 0.9770)
[epoch : 43] (l_loss: 0.04444) (t_loss: 0.07050) (accu: 0.9778)
[epoch : 44] (l_loss: 0.04383) (t_loss: 0.07668) (accu: 0.9771)
[epoch : 45] (l_loss: 0.04396) (t_loss: 0.07485) (accu: 0.9762)
[epoch : 46] (l_loss: 0.04349) (t_loss: 0.07285) (accu: 0.9766)
[epoch : 47] (l_loss: 0.04414) (t_loss: 0.07657) (accu: 0.9757)
[epoch : 48] (l_loss: 0.04352) (t_loss: 0.07416) (accu: 0.9772)
[epoch : 49] (l_loss: 0.04411) (t_loss: 0.07048) (accu: 0.9778)
[epoch : 50] (l_loss: 0.04370) (t_loss: 0.07620) (accu: 0.9757)
Finish! (Best accu: 0.9793) (Time taken(sec) : 702.59) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (3180 | 263020)          1.19
fc1.weight   :       235200 (2712 | 232488)          1.15
fc2.weight   :        30000 (346 | 29654)            1.15
fcout.weight :          1000 (122 | 878)            12.20
------------------------------------------------------------

Learning start! [Prune_iter : (21/21), Remaining weight : 1.19 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29794) (accu: 0.0974)
[epoch : 1] (l_loss: 0.40072) (t_loss: 0.15801) (accu: 0.9531)
[epoch : 2] (l_loss: 0.12728) (t_loss: 0.11311) (accu: 0.9664)
[epoch : 3] (l_loss: 0.09662) (t_loss: 0.09542) (accu: 0.9715)
[epoch : 4] (l_loss: 0.08218) (t_loss: 0.08650) (accu: 0.9745)
[epoch : 5] (l_loss: 0.07247) (t_loss: 0.08646) (accu: 0.9733)
[epoch : 6] (l_loss: 0.06677) (t_loss: 0.08243) (accu: 0.9750)
[epoch : 7] (l_loss: 0.06237) (t_loss: 0.08191) (accu: 0.9758)
[epoch : 8] (l_loss: 0.05919) (t_loss: 0.08248) (accu: 0.9735)
[epoch : 9] (l_loss: 0.05638) (t_loss: 0.07485) (accu: 0.9776)
[epoch : 10] (l_loss: 0.05356) (t_loss: 0.07398) (accu: 0.9766)
[epoch : 11] (l_loss: 0.05043) (t_loss: 0.07413) (accu: 0.9779)
[epoch : 12] (l_loss: 0.04977) (t_loss: 0.07320) (accu: 0.9777)
[epoch : 13] (l_loss: 0.04818) (t_loss: 0.07311) (accu: 0.9772)
[epoch : 14] (l_loss: 0.04783) (t_loss: 0.07031) (accu: 0.9782)
[epoch : 15] (l_loss: 0.04729) (t_loss: 0.07238) (accu: 0.9771)
[epoch : 16] (l_loss: 0.04669) (t_loss: 0.07370) (accu: 0.9774)
[epoch : 17] (l_loss: 0.04635) (t_loss: 0.07998) (accu: 0.9746)
[epoch : 18] (l_loss: 0.04647) (t_loss: 0.07637) (accu: 0.9773)
[epoch : 19] (l_loss: 0.04610) (t_loss: 0.07513) (accu: 0.9775)
[epoch : 20] (l_loss: 0.04637) (t_loss: 0.07728) (accu: 0.9756)
[epoch : 21] (l_loss: 0.04587) (t_loss: 0.07223) (accu: 0.9762)
[epoch : 22] (l_loss: 0.04583) (t_loss: 0.07334) (accu: 0.9769)
[epoch : 23] (l_loss: 0.04551) (t_loss: 0.07404) (accu: 0.9758)
[epoch : 24] (l_loss: 0.04572) (t_loss: 0.07393) (accu: 0.9766)
[epoch : 25] (l_loss: 0.04588) (t_loss: 0.07557) (accu: 0.9765)
[epoch : 26] (l_loss: 0.04473) (t_loss: 0.07179) (accu: 0.9782)
[epoch : 27] (l_loss: 0.04556) (t_loss: 0.07418) (accu: 0.9775)
[epoch : 28] (l_loss: 0.04580) (t_loss: 0.07548) (accu: 0.9759)
[epoch : 29] (l_loss: 0.04546) (t_loss: 0.07287) (accu: 0.9769)
[epoch : 30] (l_loss: 0.04547) (t_loss: 0.07339) (accu: 0.9770)
[epoch : 31] (l_loss: 0.04515) (t_loss: 0.07336) (accu: 0.9764)
[epoch : 32] (l_loss: 0.04491) (t_loss: 0.07317) (accu: 0.9773)
[epoch : 33] (l_loss: 0.04452) (t_loss: 0.07139) (accu: 0.9780)
[epoch : 34] (l_loss: 0.04385) (t_loss: 0.07075) (accu: 0.9771)
[epoch : 35] (l_loss: 0.04389) (t_loss: 0.07015) (accu: 0.9789)
[epoch : 36] (l_loss: 0.04425) (t_loss: 0.07192) (accu: 0.9775)
[epoch : 37] (l_loss: 0.04398) (t_loss: 0.07200) (accu: 0.9774)
[epoch : 38] (l_loss: 0.04376) (t_loss: 0.07489) (accu: 0.9770)
[epoch : 39] (l_loss: 0.04400) (t_loss: 0.07290) (accu: 0.9770)
[epoch : 40] (l_loss: 0.04443) (t_loss: 0.07056) (accu: 0.9777)
[epoch : 41] (l_loss: 0.04387) (t_loss: 0.07319) (accu: 0.9770)
[epoch : 42] (l_loss: 0.04340) (t_loss: 0.07161) (accu: 0.9789)
[epoch : 43] (l_loss: 0.04412) (t_loss: 0.07728) (accu: 0.9745)
[epoch : 44] (l_loss: 0.04407) (t_loss: 0.07155) (accu: 0.9787)
[epoch : 45] (l_loss: 0.04368) (t_loss: 0.07433) (accu: 0.9762)
[epoch : 46] (l_loss: 0.04409) (t_loss: 0.07162) (accu: 0.9782)
[epoch : 47] (l_loss: 0.04421) (t_loss: 0.07265) (accu: 0.9779)
[epoch : 48] (l_loss: 0.04372) (t_loss: 0.07228) (accu: 0.9773)
[epoch : 49] (l_loss: 0.04396) (t_loss: 0.07390) (accu: 0.9773)
[epoch : 50] (l_loss: 0.04402) (t_loss: 0.07017) (accu: 0.9783)
Finish! (Best accu: 0.9789) (Time taken(sec) : 739.25) 


Maximum accuracy per weight remaining
Remaining weight 100.0 %  Epoch 29 Accu 0.9787
Remaining weight 80.04 %  Epoch 35 Accu 0.9790
Remaining weight 64.06 %  Epoch 9 Accu 0.9786
Remaining weight 51.28 %  Epoch 32 Accu 0.9784
Remaining weight 41.05 %  Epoch 47 Accu 0.9796
Remaining weight 32.87 %  Epoch 28 Accu 0.9790
Remaining weight 26.32 %  Epoch 19 Accu 0.9799
Remaining weight 21.07 %  Epoch 22 Accu 0.9799
Remaining weight 16.88 %  Epoch 37 Accu 0.9792
Remaining weight 13.52 %  Epoch 14 Accu 0.9793
Remaining weight 10.83 %  Epoch 41 Accu 0.9790
Remaining weight 8.68 %  Epoch 49 Accu 0.9788
Remaining weight 6.95 %  Epoch 43 Accu 0.9793
Remaining weight 5.57 %  Epoch 49 Accu 0.9793
Remaining weight 4.47 %  Epoch 33 Accu 0.9793
Remaining weight 3.58 %  Epoch 48 Accu 0.9798
Remaining weight 2.87 %  Epoch 44 Accu 0.9799
Remaining weight 2.31 %  Epoch 39 Accu 0.9794
Remaining weight 1.85 %  Epoch 49 Accu 0.9785
Remaining weight 1.49 %  Epoch 12 Accu 0.9793
Remaining weight 1.19 %  Epoch 41 Accu 0.9789
===================================================================== 

Test_Iter (4/5)
------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :        266200 (266200 | 0)          100.00
fc1.weight   :        235200 (235200 | 0)          100.00
fc2.weight   :         30000 (30000 | 0)           100.00
fcout.weight :          1000 (1000 | 0)            100.00
------------------------------------------------------------

Learning start! [Prune_iter : (1/21), Remaining weight : 100.0 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.83349) (accu: 0.0857)
[epoch : 1] (l_loss: 0.44351) (t_loss: 0.14581) (accu: 0.9579)
[epoch : 2] (l_loss: 0.12193) (t_loss: 0.10716) (accu: 0.9675)
[epoch : 3] (l_loss: 0.09286) (t_loss: 0.09412) (accu: 0.9710)
[epoch : 4] (l_loss: 0.07991) (t_loss: 0.08996) (accu: 0.9722)
[epoch : 5] (l_loss: 0.07112) (t_loss: 0.08305) (accu: 0.9749)
[epoch : 6] (l_loss: 0.06529) (t_loss: 0.08236) (accu: 0.9737)
[epoch : 7] (l_loss: 0.06028) (t_loss: 0.07384) (accu: 0.9772)
[epoch : 8] (l_loss: 0.05593) (t_loss: 0.07596) (accu: 0.9771)
[epoch : 9] (l_loss: 0.05301) (t_loss: 0.07264) (accu: 0.9771)
[epoch : 10] (l_loss: 0.05079) (t_loss: 0.06895) (accu: 0.9781)
[epoch : 11] (l_loss: 0.04933) (t_loss: 0.07304) (accu: 0.9782)
[epoch : 12] (l_loss: 0.04828) (t_loss: 0.07366) (accu: 0.9763)
[epoch : 13] (l_loss: 0.04735) (t_loss: 0.07171) (accu: 0.9765)
[epoch : 14] (l_loss: 0.04733) (t_loss: 0.07219) (accu: 0.9771)
[epoch : 15] (l_loss: 0.04637) (t_loss: 0.07348) (accu: 0.9768)
[epoch : 16] (l_loss: 0.04600) (t_loss: 0.07528) (accu: 0.9757)
[epoch : 17] (l_loss: 0.04607) (t_loss: 0.07384) (accu: 0.9771)
[epoch : 18] (l_loss: 0.04578) (t_loss: 0.07513) (accu: 0.9762)
[epoch : 19] (l_loss: 0.04583) (t_loss: 0.07249) (accu: 0.9770)
[epoch : 20] (l_loss: 0.04528) (t_loss: 0.07114) (accu: 0.9761)
[epoch : 21] (l_loss: 0.04519) (t_loss: 0.07440) (accu: 0.9751)
[epoch : 22] (l_loss: 0.04480) (t_loss: 0.07247) (accu: 0.9781)
[epoch : 23] (l_loss: 0.04528) (t_loss: 0.07437) (accu: 0.9771)
[epoch : 24] (l_loss: 0.04501) (t_loss: 0.07101) (accu: 0.9769)
[epoch : 25] (l_loss: 0.04513) (t_loss: 0.07169) (accu: 0.9772)
[epoch : 26] (l_loss: 0.04485) (t_loss: 0.07036) (accu: 0.9780)
[epoch : 27] (l_loss: 0.04494) (t_loss: 0.07303) (accu: 0.9768)
[epoch : 28] (l_loss: 0.04484) (t_loss: 0.07269) (accu: 0.9768)
[epoch : 29] (l_loss: 0.04476) (t_loss: 0.07437) (accu: 0.9772)
[epoch : 30] (l_loss: 0.04484) (t_loss: 0.07350) (accu: 0.9764)
[epoch : 31] (l_loss: 0.04464) (t_loss: 0.06984) (accu: 0.9772)
[epoch : 32] (l_loss: 0.04438) (t_loss: 0.06997) (accu: 0.9780)
[epoch : 33] (l_loss: 0.04434) (t_loss: 0.07240) (accu: 0.9768)
[epoch : 34] (l_loss: 0.04467) (t_loss: 0.07246) (accu: 0.9768)
[epoch : 35] (l_loss: 0.04451) (t_loss: 0.07110) (accu: 0.9767)
[epoch : 36] (l_loss: 0.04456) (t_loss: 0.07047) (accu: 0.9770)
[epoch : 37] (l_loss: 0.04451) (t_loss: 0.07323) (accu: 0.9775)
[epoch : 38] (l_loss: 0.04440) (t_loss: 0.07157) (accu: 0.9774)
[epoch : 39] (l_loss: 0.04435) (t_loss: 0.07513) (accu: 0.9751)
[epoch : 40] (l_loss: 0.04401) (t_loss: 0.07052) (accu: 0.9771)
[epoch : 41] (l_loss: 0.04496) (t_loss: 0.07279) (accu: 0.9768)
[epoch : 42] (l_loss: 0.04451) (t_loss: 0.07366) (accu: 0.9779)
[epoch : 43] (l_loss: 0.04437) (t_loss: 0.07309) (accu: 0.9772)
[epoch : 44] (l_loss: 0.04444) (t_loss: 0.07212) (accu: 0.9774)
[epoch : 45] (l_loss: 0.04464) (t_loss: 0.07790) (accu: 0.9753)
[epoch : 46] (l_loss: 0.04431) (t_loss: 0.07220) (accu: 0.9770)
[epoch : 47] (l_loss: 0.04457) (t_loss: 0.07083) (accu: 0.9771)
[epoch : 48] (l_loss: 0.04436) (t_loss: 0.07300) (accu: 0.9780)
[epoch : 49] (l_loss: 0.04432) (t_loss: 0.07372) (accu: 0.9746)
[epoch : 50] (l_loss: 0.04435) (t_loss: 0.06979) (accu: 0.9767)
Finish! (Best accu: 0.9782) (Time taken(sec) : 754.52) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (213060 | 53140)         80.04
fc1.weight   :      235200 (188160 | 47040)         80.00
fc2.weight   :        30000 (24000 | 6000)          80.00
fcout.weight :          1000 (900 | 100)            90.00
------------------------------------------------------------

Learning start! [Prune_iter : (2/21), Remaining weight : 80.04 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.58663) (accu: 0.1093)
[epoch : 1] (l_loss: 0.44074) (t_loss: 0.15238) (accu: 0.9555)
[epoch : 2] (l_loss: 0.12367) (t_loss: 0.10997) (accu: 0.9659)
[epoch : 3] (l_loss: 0.09292) (t_loss: 0.09362) (accu: 0.9709)
[epoch : 4] (l_loss: 0.07835) (t_loss: 0.08321) (accu: 0.9745)
[epoch : 5] (l_loss: 0.06963) (t_loss: 0.08332) (accu: 0.9744)
[epoch : 6] (l_loss: 0.06369) (t_loss: 0.07846) (accu: 0.9771)
[epoch : 7] (l_loss: 0.05934) (t_loss: 0.07820) (accu: 0.9762)
[epoch : 8] (l_loss: 0.05569) (t_loss: 0.07432) (accu: 0.9768)
[epoch : 9] (l_loss: 0.05320) (t_loss: 0.07310) (accu: 0.9787)
[epoch : 10] (l_loss: 0.05074) (t_loss: 0.07186) (accu: 0.9775)
[epoch : 11] (l_loss: 0.04925) (t_loss: 0.07042) (accu: 0.9782)
[epoch : 12] (l_loss: 0.04811) (t_loss: 0.07425) (accu: 0.9760)
[epoch : 13] (l_loss: 0.04711) (t_loss: 0.07239) (accu: 0.9773)
[epoch : 14] (l_loss: 0.04657) (t_loss: 0.07143) (accu: 0.9779)
[epoch : 15] (l_loss: 0.04620) (t_loss: 0.07309) (accu: 0.9783)
[epoch : 16] (l_loss: 0.04563) (t_loss: 0.07200) (accu: 0.9768)
[epoch : 17] (l_loss: 0.04568) (t_loss: 0.07430) (accu: 0.9763)
[epoch : 18] (l_loss: 0.04559) (t_loss: 0.07189) (accu: 0.9768)
[epoch : 19] (l_loss: 0.04533) (t_loss: 0.07338) (accu: 0.9774)
[epoch : 20] (l_loss: 0.04543) (t_loss: 0.07254) (accu: 0.9768)
[epoch : 21] (l_loss: 0.04497) (t_loss: 0.07276) (accu: 0.9775)
[epoch : 22] (l_loss: 0.04473) (t_loss: 0.07061) (accu: 0.9776)
[epoch : 23] (l_loss: 0.04483) (t_loss: 0.07189) (accu: 0.9778)
[epoch : 24] (l_loss: 0.04508) (t_loss: 0.07024) (accu: 0.9787)
[epoch : 25] (l_loss: 0.04491) (t_loss: 0.07177) (accu: 0.9779)
[epoch : 26] (l_loss: 0.04455) (t_loss: 0.07427) (accu: 0.9769)
[epoch : 27] (l_loss: 0.04444) (t_loss: 0.07082) (accu: 0.9772)
[epoch : 28] (l_loss: 0.04506) (t_loss: 0.07079) (accu: 0.9774)
[epoch : 29] (l_loss: 0.04429) (t_loss: 0.07053) (accu: 0.9782)
[epoch : 30] (l_loss: 0.04502) (t_loss: 0.07331) (accu: 0.9767)
[epoch : 31] (l_loss: 0.04446) (t_loss: 0.07178) (accu: 0.9765)
[epoch : 32] (l_loss: 0.04447) (t_loss: 0.07381) (accu: 0.9770)
[epoch : 33] (l_loss: 0.04461) (t_loss: 0.07268) (accu: 0.9768)
[epoch : 34] (l_loss: 0.04435) (t_loss: 0.07325) (accu: 0.9774)
[epoch : 35] (l_loss: 0.04404) (t_loss: 0.07454) (accu: 0.9765)
[epoch : 36] (l_loss: 0.04402) (t_loss: 0.07022) (accu: 0.9767)
[epoch : 37] (l_loss: 0.04367) (t_loss: 0.07226) (accu: 0.9770)
[epoch : 38] (l_loss: 0.04414) (t_loss: 0.07071) (accu: 0.9768)
[epoch : 39] (l_loss: 0.04429) (t_loss: 0.07226) (accu: 0.9773)
[epoch : 40] (l_loss: 0.04377) (t_loss: 0.06984) (accu: 0.9793)
[epoch : 41] (l_loss: 0.04403) (t_loss: 0.07098) (accu: 0.9794)
[epoch : 42] (l_loss: 0.04387) (t_loss: 0.07260) (accu: 0.9783)
[epoch : 43] (l_loss: 0.04428) (t_loss: 0.07144) (accu: 0.9785)
[epoch : 44] (l_loss: 0.04422) (t_loss: 0.07246) (accu: 0.9775)
[epoch : 45] (l_loss: 0.04424) (t_loss: 0.07565) (accu: 0.9754)
[epoch : 46] (l_loss: 0.04451) (t_loss: 0.07536) (accu: 0.9767)
[epoch : 47] (l_loss: 0.04384) (t_loss: 0.07318) (accu: 0.9774)
[epoch : 48] (l_loss: 0.04410) (t_loss: 0.07137) (accu: 0.9793)
[epoch : 49] (l_loss: 0.04401) (t_loss: 0.07372) (accu: 0.9761)
[epoch : 50] (l_loss: 0.04404) (t_loss: 0.07281) (accu: 0.9764)
Finish! (Best accu: 0.9794) (Time taken(sec) : 742.47) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (170538 | 95662)         64.06
fc1.weight   :      235200 (150528 | 84672)         64.00
fc2.weight   :       30000 (19200 | 10800)          64.00
fcout.weight :          1000 (810 | 190)            81.00
------------------------------------------------------------

Learning start! [Prune_iter : (3/21), Remaining weight : 64.06 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.43070) (accu: 0.1090)
[epoch : 1] (l_loss: 0.43721) (t_loss: 0.15658) (accu: 0.9540)
[epoch : 2] (l_loss: 0.12805) (t_loss: 0.11328) (accu: 0.9669)
[epoch : 3] (l_loss: 0.09550) (t_loss: 0.09452) (accu: 0.9714)
[epoch : 4] (l_loss: 0.08025) (t_loss: 0.08728) (accu: 0.9727)
[epoch : 5] (l_loss: 0.07138) (t_loss: 0.08172) (accu: 0.9744)
[epoch : 6] (l_loss: 0.06532) (t_loss: 0.07999) (accu: 0.9746)
[epoch : 7] (l_loss: 0.06072) (t_loss: 0.07941) (accu: 0.9765)
[epoch : 8] (l_loss: 0.05744) (t_loss: 0.07437) (accu: 0.9776)
[epoch : 9] (l_loss: 0.05472) (t_loss: 0.07414) (accu: 0.9760)
[epoch : 10] (l_loss: 0.05276) (t_loss: 0.07274) (accu: 0.9775)
[epoch : 11] (l_loss: 0.05140) (t_loss: 0.07513) (accu: 0.9764)
[epoch : 12] (l_loss: 0.05017) (t_loss: 0.07494) (accu: 0.9771)
[epoch : 13] (l_loss: 0.04931) (t_loss: 0.07074) (accu: 0.9786)
[epoch : 14] (l_loss: 0.04903) (t_loss: 0.07355) (accu: 0.9765)
[epoch : 15] (l_loss: 0.04838) (t_loss: 0.07465) (accu: 0.9765)
[epoch : 16] (l_loss: 0.04791) (t_loss: 0.07203) (accu: 0.9780)
[epoch : 17] (l_loss: 0.04783) (t_loss: 0.07349) (accu: 0.9786)
[epoch : 18] (l_loss: 0.04771) (t_loss: 0.07186) (accu: 0.9777)
[epoch : 19] (l_loss: 0.04747) (t_loss: 0.07182) (accu: 0.9784)
[epoch : 20] (l_loss: 0.04744) (t_loss: 0.07283) (accu: 0.9772)
[epoch : 21] (l_loss: 0.04709) (t_loss: 0.07076) (accu: 0.9780)
[epoch : 22] (l_loss: 0.04683) (t_loss: 0.07568) (accu: 0.9777)
[epoch : 23] (l_loss: 0.04671) (t_loss: 0.07165) (accu: 0.9782)
[epoch : 24] (l_loss: 0.04693) (t_loss: 0.07352) (accu: 0.9775)
[epoch : 25] (l_loss: 0.04725) (t_loss: 0.07495) (accu: 0.9765)
[epoch : 26] (l_loss: 0.04703) (t_loss: 0.07107) (accu: 0.9782)
[epoch : 27] (l_loss: 0.04651) (t_loss: 0.07197) (accu: 0.9778)
[epoch : 28] (l_loss: 0.04637) (t_loss: 0.07487) (accu: 0.9758)
[epoch : 29] (l_loss: 0.04635) (t_loss: 0.07141) (accu: 0.9776)
[epoch : 30] (l_loss: 0.04648) (t_loss: 0.07236) (accu: 0.9772)
[epoch : 31] (l_loss: 0.04611) (t_loss: 0.07207) (accu: 0.9766)
[epoch : 32] (l_loss: 0.04627) (t_loss: 0.07107) (accu: 0.9784)
[epoch : 33] (l_loss: 0.04645) (t_loss: 0.07081) (accu: 0.9778)
[epoch : 34] (l_loss: 0.04595) (t_loss: 0.07199) (accu: 0.9786)
[epoch : 35] (l_loss: 0.04640) (t_loss: 0.07301) (accu: 0.9774)
[epoch : 36] (l_loss: 0.04649) (t_loss: 0.07373) (accu: 0.9756)
[epoch : 37] (l_loss: 0.04622) (t_loss: 0.06916) (accu: 0.9792)
[epoch : 38] (l_loss: 0.04633) (t_loss: 0.07246) (accu: 0.9777)
[epoch : 39] (l_loss: 0.04615) (t_loss: 0.07452) (accu: 0.9771)
[epoch : 40] (l_loss: 0.04625) (t_loss: 0.07083) (accu: 0.9781)
[epoch : 41] (l_loss: 0.04613) (t_loss: 0.07446) (accu: 0.9768)
[epoch : 42] (l_loss: 0.04555) (t_loss: 0.06954) (accu: 0.9778)
[epoch : 43] (l_loss: 0.04681) (t_loss: 0.07406) (accu: 0.9753)
[epoch : 44] (l_loss: 0.04624) (t_loss: 0.07297) (accu: 0.9762)
[epoch : 45] (l_loss: 0.04637) (t_loss: 0.07402) (accu: 0.9768)
[epoch : 46] (l_loss: 0.04634) (t_loss: 0.07260) (accu: 0.9766)
[epoch : 47] (l_loss: 0.04668) (t_loss: 0.07044) (accu: 0.9790)
[epoch : 48] (l_loss: 0.04627) (t_loss: 0.07361) (accu: 0.9762)
[epoch : 49] (l_loss: 0.04648) (t_loss: 0.07157) (accu: 0.9773)
[epoch : 50] (l_loss: 0.04603) (t_loss: 0.07264) (accu: 0.9770)
Finish! (Best accu: 0.9792) (Time taken(sec) : 725.97) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (136511 | 129689)        51.28
fc1.weight   :      235200 (120422 | 114778)        51.20
fc2.weight   :       30000 (15360 | 14640)          51.20
fcout.weight :          1000 (729 | 271)            72.90
------------------------------------------------------------

Learning start! [Prune_iter : (4/21), Remaining weight : 51.28 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.40025) (accu: 0.1245)
[epoch : 1] (l_loss: 0.43272) (t_loss: 0.15099) (accu: 0.9571)
[epoch : 2] (l_loss: 0.12731) (t_loss: 0.11055) (accu: 0.9651)
[epoch : 3] (l_loss: 0.09475) (t_loss: 0.09684) (accu: 0.9703)
[epoch : 4] (l_loss: 0.08035) (t_loss: 0.08607) (accu: 0.9732)
[epoch : 5] (l_loss: 0.07114) (t_loss: 0.08204) (accu: 0.9728)
[epoch : 6] (l_loss: 0.06559) (t_loss: 0.07785) (accu: 0.9754)
[epoch : 7] (l_loss: 0.06178) (t_loss: 0.07846) (accu: 0.9760)
[epoch : 8] (l_loss: 0.05771) (t_loss: 0.07742) (accu: 0.9754)
[epoch : 9] (l_loss: 0.05529) (t_loss: 0.07416) (accu: 0.9776)
[epoch : 10] (l_loss: 0.05272) (t_loss: 0.07588) (accu: 0.9753)
[epoch : 11] (l_loss: 0.05053) (t_loss: 0.07283) (accu: 0.9768)
[epoch : 12] (l_loss: 0.04956) (t_loss: 0.07251) (accu: 0.9779)
[epoch : 13] (l_loss: 0.04880) (t_loss: 0.07133) (accu: 0.9782)
[epoch : 14] (l_loss: 0.04825) (t_loss: 0.07199) (accu: 0.9778)
[epoch : 15] (l_loss: 0.04744) (t_loss: 0.07065) (accu: 0.9772)
[epoch : 16] (l_loss: 0.04765) (t_loss: 0.07193) (accu: 0.9775)
[epoch : 17] (l_loss: 0.04719) (t_loss: 0.07537) (accu: 0.9755)
[epoch : 18] (l_loss: 0.04711) (t_loss: 0.07294) (accu: 0.9758)
[epoch : 19] (l_loss: 0.04704) (t_loss: 0.07478) (accu: 0.9773)
[epoch : 20] (l_loss: 0.04678) (t_loss: 0.07290) (accu: 0.9779)
[epoch : 21] (l_loss: 0.04677) (t_loss: 0.07488) (accu: 0.9767)
[epoch : 22] (l_loss: 0.04630) (t_loss: 0.07172) (accu: 0.9772)
[epoch : 23] (l_loss: 0.04655) (t_loss: 0.07159) (accu: 0.9770)
[epoch : 24] (l_loss: 0.04634) (t_loss: 0.07325) (accu: 0.9774)
[epoch : 25] (l_loss: 0.04605) (t_loss: 0.07319) (accu: 0.9773)
[epoch : 26] (l_loss: 0.04601) (t_loss: 0.07568) (accu: 0.9763)
[epoch : 27] (l_loss: 0.04585) (t_loss: 0.07271) (accu: 0.9761)
[epoch : 28] (l_loss: 0.04588) (t_loss: 0.07266) (accu: 0.9768)
[epoch : 29] (l_loss: 0.04616) (t_loss: 0.07463) (accu: 0.9756)
[epoch : 30] (l_loss: 0.04617) (t_loss: 0.07084) (accu: 0.9781)
[epoch : 31] (l_loss: 0.04600) (t_loss: 0.07175) (accu: 0.9773)
[epoch : 32] (l_loss: 0.04596) (t_loss: 0.07080) (accu: 0.9779)
[epoch : 33] (l_loss: 0.04617) (t_loss: 0.07327) (accu: 0.9777)
[epoch : 34] (l_loss: 0.04584) (t_loss: 0.07143) (accu: 0.9765)
[epoch : 35] (l_loss: 0.04629) (t_loss: 0.07327) (accu: 0.9770)
[epoch : 36] (l_loss: 0.04542) (t_loss: 0.07270) (accu: 0.9768)
[epoch : 37] (l_loss: 0.04579) (t_loss: 0.07544) (accu: 0.9762)
[epoch : 38] (l_loss: 0.04575) (t_loss: 0.07648) (accu: 0.9764)
[epoch : 39] (l_loss: 0.04586) (t_loss: 0.07647) (accu: 0.9753)
[epoch : 40] (l_loss: 0.04580) (t_loss: 0.07525) (accu: 0.9763)
[epoch : 41] (l_loss: 0.04560) (t_loss: 0.07312) (accu: 0.9781)
[epoch : 42] (l_loss: 0.04566) (t_loss: 0.07176) (accu: 0.9770)
[epoch : 43] (l_loss: 0.04568) (t_loss: 0.07052) (accu: 0.9771)
[epoch : 44] (l_loss: 0.04582) (t_loss: 0.07054) (accu: 0.9780)
[epoch : 45] (l_loss: 0.04556) (t_loss: 0.07253) (accu: 0.9767)
[epoch : 46] (l_loss: 0.04565) (t_loss: 0.07471) (accu: 0.9766)
[epoch : 47] (l_loss: 0.04509) (t_loss: 0.07348) (accu: 0.9760)
[epoch : 48] (l_loss: 0.04585) (t_loss: 0.07340) (accu: 0.9765)
[epoch : 49] (l_loss: 0.04575) (t_loss: 0.07395) (accu: 0.9764)
[epoch : 50] (l_loss: 0.04602) (t_loss: 0.07254) (accu: 0.9773)
Finish! (Best accu: 0.9782) (Time taken(sec) : 713.29) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (109282 | 156918)        41.05
fc1.weight   :      235200 (96338 | 138862)         40.96
fc2.weight   :       30000 (12288 | 17712)          40.96
fcout.weight :          1000 (656 | 344)            65.60
------------------------------------------------------------

Learning start! [Prune_iter : (5/21), Remaining weight : 41.05 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.37126) (accu: 0.1561)
[epoch : 1] (l_loss: 0.43177) (t_loss: 0.15856) (accu: 0.9553)
[epoch : 2] (l_loss: 0.13201) (t_loss: 0.11344) (accu: 0.9663)
[epoch : 3] (l_loss: 0.09951) (t_loss: 0.09774) (accu: 0.9702)
[epoch : 4] (l_loss: 0.08454) (t_loss: 0.09149) (accu: 0.9710)
[epoch : 5] (l_loss: 0.07344) (t_loss: 0.08146) (accu: 0.9754)
[epoch : 6] (l_loss: 0.06605) (t_loss: 0.08470) (accu: 0.9748)
[epoch : 7] (l_loss: 0.06151) (t_loss: 0.08092) (accu: 0.9748)
[epoch : 8] (l_loss: 0.05717) (t_loss: 0.07842) (accu: 0.9753)
[epoch : 9] (l_loss: 0.05487) (t_loss: 0.07323) (accu: 0.9782)
[epoch : 10] (l_loss: 0.05231) (t_loss: 0.08007) (accu: 0.9754)
[epoch : 11] (l_loss: 0.05135) (t_loss: 0.07485) (accu: 0.9763)
[epoch : 12] (l_loss: 0.05000) (t_loss: 0.07494) (accu: 0.9775)
[epoch : 13] (l_loss: 0.04993) (t_loss: 0.07813) (accu: 0.9755)
[epoch : 14] (l_loss: 0.04907) (t_loss: 0.07316) (accu: 0.9779)
[epoch : 15] (l_loss: 0.04892) (t_loss: 0.07528) (accu: 0.9756)
[epoch : 16] (l_loss: 0.04807) (t_loss: 0.07426) (accu: 0.9774)
[epoch : 17] (l_loss: 0.04803) (t_loss: 0.07427) (accu: 0.9764)
[epoch : 18] (l_loss: 0.04793) (t_loss: 0.07316) (accu: 0.9782)
[epoch : 19] (l_loss: 0.04766) (t_loss: 0.07783) (accu: 0.9760)
[epoch : 20] (l_loss: 0.04751) (t_loss: 0.07448) (accu: 0.9760)
[epoch : 21] (l_loss: 0.04723) (t_loss: 0.07855) (accu: 0.9759)
[epoch : 22] (l_loss: 0.04701) (t_loss: 0.07334) (accu: 0.9768)
[epoch : 23] (l_loss: 0.04762) (t_loss: 0.07449) (accu: 0.9768)
[epoch : 24] (l_loss: 0.04741) (t_loss: 0.07034) (accu: 0.9784)
[epoch : 25] (l_loss: 0.04725) (t_loss: 0.07701) (accu: 0.9765)
[epoch : 26] (l_loss: 0.04661) (t_loss: 0.07933) (accu: 0.9753)
[epoch : 27] (l_loss: 0.04696) (t_loss: 0.07534) (accu: 0.9772)
[epoch : 28] (l_loss: 0.04724) (t_loss: 0.07027) (accu: 0.9786)
[epoch : 29] (l_loss: 0.04710) (t_loss: 0.07205) (accu: 0.9781)
[epoch : 30] (l_loss: 0.04691) (t_loss: 0.07622) (accu: 0.9770)
[epoch : 31] (l_loss: 0.04686) (t_loss: 0.07362) (accu: 0.9771)
[epoch : 32] (l_loss: 0.04698) (t_loss: 0.07611) (accu: 0.9773)
[epoch : 33] (l_loss: 0.04683) (t_loss: 0.07465) (accu: 0.9770)
[epoch : 34] (l_loss: 0.04702) (t_loss: 0.07406) (accu: 0.9768)
[epoch : 35] (l_loss: 0.04661) (t_loss: 0.07325) (accu: 0.9783)
[epoch : 36] (l_loss: 0.04719) (t_loss: 0.07755) (accu: 0.9770)
[epoch : 37] (l_loss: 0.04664) (t_loss: 0.07543) (accu: 0.9769)
[epoch : 38] (l_loss: 0.04666) (t_loss: 0.07569) (accu: 0.9777)
[epoch : 39] (l_loss: 0.04697) (t_loss: 0.07519) (accu: 0.9772)
[epoch : 40] (l_loss: 0.04635) (t_loss: 0.07582) (accu: 0.9771)
[epoch : 41] (l_loss: 0.04649) (t_loss: 0.07398) (accu: 0.9776)
[epoch : 42] (l_loss: 0.04706) (t_loss: 0.07390) (accu: 0.9775)
[epoch : 43] (l_loss: 0.04664) (t_loss: 0.07399) (accu: 0.9772)
[epoch : 44] (l_loss: 0.04639) (t_loss: 0.07638) (accu: 0.9760)
[epoch : 45] (l_loss: 0.04684) (t_loss: 0.07455) (accu: 0.9769)
[epoch : 46] (l_loss: 0.04671) (t_loss: 0.07884) (accu: 0.9752)
[epoch : 47] (l_loss: 0.04634) (t_loss: 0.07595) (accu: 0.9773)
[epoch : 48] (l_loss: 0.04667) (t_loss: 0.07358) (accu: 0.9779)
[epoch : 49] (l_loss: 0.04655) (t_loss: 0.07503) (accu: 0.9764)
[epoch : 50] (l_loss: 0.04643) (t_loss: 0.07338) (accu: 0.9783)
Finish! (Best accu: 0.9786) (Time taken(sec) : 707.41) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (87490 | 178710)         32.87
fc1.weight   :      235200 (77070 | 158130)         32.77
fc2.weight   :        30000 (9830 | 20170)          32.77
fcout.weight :          1000 (590 | 410)            59.00
------------------------------------------------------------

Learning start! [Prune_iter : (6/21), Remaining weight : 32.87 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.35374) (accu: 0.1104)
[epoch : 1] (l_loss: 0.41622) (t_loss: 0.14541) (accu: 0.9579)
[epoch : 2] (l_loss: 0.12225) (t_loss: 0.10668) (accu: 0.9680)
[epoch : 3] (l_loss: 0.09211) (t_loss: 0.09183) (accu: 0.9722)
[epoch : 4] (l_loss: 0.07848) (t_loss: 0.09261) (accu: 0.9703)
[epoch : 5] (l_loss: 0.07090) (t_loss: 0.08345) (accu: 0.9751)
[epoch : 6] (l_loss: 0.06519) (t_loss: 0.08154) (accu: 0.9742)
[epoch : 7] (l_loss: 0.06159) (t_loss: 0.07660) (accu: 0.9757)
[epoch : 8] (l_loss: 0.05852) (t_loss: 0.07917) (accu: 0.9762)
[epoch : 9] (l_loss: 0.05579) (t_loss: 0.07787) (accu: 0.9767)
[epoch : 10] (l_loss: 0.05383) (t_loss: 0.07833) (accu: 0.9755)
[epoch : 11] (l_loss: 0.05179) (t_loss: 0.07726) (accu: 0.9759)
[epoch : 12] (l_loss: 0.05121) (t_loss: 0.07714) (accu: 0.9775)
[epoch : 13] (l_loss: 0.05083) (t_loss: 0.07516) (accu: 0.9767)
[epoch : 14] (l_loss: 0.05005) (t_loss: 0.07696) (accu: 0.9755)
[epoch : 15] (l_loss: 0.04971) (t_loss: 0.07605) (accu: 0.9768)
[epoch : 16] (l_loss: 0.04942) (t_loss: 0.07239) (accu: 0.9778)
[epoch : 17] (l_loss: 0.04919) (t_loss: 0.07658) (accu: 0.9764)
[epoch : 18] (l_loss: 0.04898) (t_loss: 0.07434) (accu: 0.9783)
[epoch : 19] (l_loss: 0.04877) (t_loss: 0.07457) (accu: 0.9771)
[epoch : 20] (l_loss: 0.04864) (t_loss: 0.07793) (accu: 0.9755)
[epoch : 21] (l_loss: 0.04824) (t_loss: 0.07902) (accu: 0.9754)
[epoch : 22] (l_loss: 0.04836) (t_loss: 0.07584) (accu: 0.9771)
[epoch : 23] (l_loss: 0.04776) (t_loss: 0.07436) (accu: 0.9780)
[epoch : 24] (l_loss: 0.04776) (t_loss: 0.07071) (accu: 0.9782)
[epoch : 25] (l_loss: 0.04749) (t_loss: 0.07384) (accu: 0.9773)
[epoch : 26] (l_loss: 0.04730) (t_loss: 0.07472) (accu: 0.9764)
[epoch : 27] (l_loss: 0.04731) (t_loss: 0.07768) (accu: 0.9748)
[epoch : 28] (l_loss: 0.04659) (t_loss: 0.07708) (accu: 0.9769)
[epoch : 29] (l_loss: 0.04739) (t_loss: 0.07399) (accu: 0.9767)
[epoch : 30] (l_loss: 0.04720) (t_loss: 0.07405) (accu: 0.9771)
[epoch : 31] (l_loss: 0.04650) (t_loss: 0.07090) (accu: 0.9782)
[epoch : 32] (l_loss: 0.04723) (t_loss: 0.07584) (accu: 0.9758)
[epoch : 33] (l_loss: 0.04669) (t_loss: 0.07243) (accu: 0.9770)
[epoch : 34] (l_loss: 0.04705) (t_loss: 0.07154) (accu: 0.9767)
[epoch : 35] (l_loss: 0.04683) (t_loss: 0.07496) (accu: 0.9758)
[epoch : 36] (l_loss: 0.04647) (t_loss: 0.07489) (accu: 0.9764)
[epoch : 37] (l_loss: 0.04673) (t_loss: 0.07550) (accu: 0.9773)
[epoch : 38] (l_loss: 0.04685) (t_loss: 0.07268) (accu: 0.9773)
[epoch : 39] (l_loss: 0.04685) (t_loss: 0.07459) (accu: 0.9769)
[epoch : 40] (l_loss: 0.04697) (t_loss: 0.07355) (accu: 0.9761)
[epoch : 41] (l_loss: 0.04669) (t_loss: 0.07381) (accu: 0.9766)
[epoch : 42] (l_loss: 0.04673) (t_loss: 0.07150) (accu: 0.9780)
[epoch : 43] (l_loss: 0.04632) (t_loss: 0.07480) (accu: 0.9770)
[epoch : 44] (l_loss: 0.04685) (t_loss: 0.07517) (accu: 0.9764)
[epoch : 45] (l_loss: 0.04635) (t_loss: 0.07320) (accu: 0.9777)
[epoch : 46] (l_loss: 0.04659) (t_loss: 0.07623) (accu: 0.9761)
[epoch : 47] (l_loss: 0.04677) (t_loss: 0.07503) (accu: 0.9763)
[epoch : 48] (l_loss: 0.04686) (t_loss: 0.07256) (accu: 0.9773)
[epoch : 49] (l_loss: 0.04636) (t_loss: 0.07218) (accu: 0.9776)
[epoch : 50] (l_loss: 0.04701) (t_loss: 0.07610) (accu: 0.9771)
Finish! (Best accu: 0.9783) (Time taken(sec) : 747.68) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (70051 | 196149)         26.32
fc1.weight   :      235200 (61656 | 173544)         26.21
fc2.weight   :        30000 (7864 | 22136)          26.21
fcout.weight :          1000 (531 | 469)            53.10
------------------------------------------------------------

Learning start! [Prune_iter : (7/21), Remaining weight : 26.32 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.32447) (accu: 0.1203)
[epoch : 1] (l_loss: 0.41872) (t_loss: 0.14987) (accu: 0.9565)
[epoch : 2] (l_loss: 0.12657) (t_loss: 0.11026) (accu: 0.9672)
[epoch : 3] (l_loss: 0.09487) (t_loss: 0.09569) (accu: 0.9710)
[epoch : 4] (l_loss: 0.07993) (t_loss: 0.08498) (accu: 0.9740)
[epoch : 5] (l_loss: 0.07074) (t_loss: 0.08659) (accu: 0.9738)
[epoch : 6] (l_loss: 0.06479) (t_loss: 0.08010) (accu: 0.9760)
[epoch : 7] (l_loss: 0.06004) (t_loss: 0.07551) (accu: 0.9780)
[epoch : 8] (l_loss: 0.05562) (t_loss: 0.07716) (accu: 0.9770)
[epoch : 9] (l_loss: 0.05261) (t_loss: 0.07296) (accu: 0.9762)
[epoch : 10] (l_loss: 0.05081) (t_loss: 0.07366) (accu: 0.9774)
[epoch : 11] (l_loss: 0.04980) (t_loss: 0.07085) (accu: 0.9786)
[epoch : 12] (l_loss: 0.04867) (t_loss: 0.07412) (accu: 0.9772)
[epoch : 13] (l_loss: 0.04802) (t_loss: 0.07315) (accu: 0.9774)
[epoch : 14] (l_loss: 0.04700) (t_loss: 0.07199) (accu: 0.9782)
[epoch : 15] (l_loss: 0.04690) (t_loss: 0.07313) (accu: 0.9781)
[epoch : 16] (l_loss: 0.04652) (t_loss: 0.07200) (accu: 0.9779)
[epoch : 17] (l_loss: 0.04618) (t_loss: 0.07154) (accu: 0.9778)
[epoch : 18] (l_loss: 0.04561) (t_loss: 0.07131) (accu: 0.9778)
[epoch : 19] (l_loss: 0.04543) (t_loss: 0.07174) (accu: 0.9782)
[epoch : 20] (l_loss: 0.04521) (t_loss: 0.07535) (accu: 0.9765)
[epoch : 21] (l_loss: 0.04472) (t_loss: 0.07002) (accu: 0.9786)
[epoch : 22] (l_loss: 0.04512) (t_loss: 0.07532) (accu: 0.9757)
[epoch : 23] (l_loss: 0.04426) (t_loss: 0.07339) (accu: 0.9770)
[epoch : 24] (l_loss: 0.04436) (t_loss: 0.07676) (accu: 0.9746)
[epoch : 25] (l_loss: 0.04405) (t_loss: 0.07098) (accu: 0.9781)
[epoch : 26] (l_loss: 0.04423) (t_loss: 0.07167) (accu: 0.9778)
[epoch : 27] (l_loss: 0.04440) (t_loss: 0.07365) (accu: 0.9784)
[epoch : 28] (l_loss: 0.04432) (t_loss: 0.07046) (accu: 0.9780)
[epoch : 29] (l_loss: 0.04401) (t_loss: 0.07023) (accu: 0.9788)
[epoch : 30] (l_loss: 0.04446) (t_loss: 0.07308) (accu: 0.9769)
[epoch : 31] (l_loss: 0.04423) (t_loss: 0.07173) (accu: 0.9786)
[epoch : 32] (l_loss: 0.04394) (t_loss: 0.06919) (accu: 0.9790)
[epoch : 33] (l_loss: 0.04453) (t_loss: 0.07493) (accu: 0.9771)
[epoch : 34] (l_loss: 0.04456) (t_loss: 0.07387) (accu: 0.9776)
[epoch : 35] (l_loss: 0.04371) (t_loss: 0.07368) (accu: 0.9777)
[epoch : 36] (l_loss: 0.04416) (t_loss: 0.07041) (accu: 0.9792)
[epoch : 37] (l_loss: 0.04419) (t_loss: 0.06923) (accu: 0.9785)
[epoch : 38] (l_loss: 0.04425) (t_loss: 0.07040) (accu: 0.9777)
[epoch : 39] (l_loss: 0.04379) (t_loss: 0.06977) (accu: 0.9775)
[epoch : 40] (l_loss: 0.04437) (t_loss: 0.07079) (accu: 0.9786)
[epoch : 41] (l_loss: 0.04360) (t_loss: 0.06955) (accu: 0.9780)
[epoch : 42] (l_loss: 0.04368) (t_loss: 0.07211) (accu: 0.9782)
[epoch : 43] (l_loss: 0.04419) (t_loss: 0.06860) (accu: 0.9790)
[epoch : 44] (l_loss: 0.04416) (t_loss: 0.07083) (accu: 0.9781)
[epoch : 45] (l_loss: 0.04404) (t_loss: 0.07321) (accu: 0.9775)
[epoch : 46] (l_loss: 0.04386) (t_loss: 0.07204) (accu: 0.9780)
[epoch : 47] (l_loss: 0.04443) (t_loss: 0.06969) (accu: 0.9780)
[epoch : 48] (l_loss: 0.04444) (t_loss: 0.07005) (accu: 0.9792)
[epoch : 49] (l_loss: 0.04398) (t_loss: 0.07134) (accu: 0.9765)
[epoch : 50] (l_loss: 0.04397) (t_loss: 0.07559) (accu: 0.9755)
Finish! (Best accu: 0.9792) (Time taken(sec) : 746.49) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (56094 | 210106)         21.07
fc1.weight   :      235200 (49325 | 185875)         20.97
fc2.weight   :        30000 (6291 | 23709)          20.97
fcout.weight :          1000 (478 | 522)            47.80
------------------------------------------------------------

Learning start! [Prune_iter : (8/21), Remaining weight : 21.07 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.31613) (accu: 0.1199)
[epoch : 1] (l_loss: 0.41545) (t_loss: 0.15975) (accu: 0.9528)
[epoch : 2] (l_loss: 0.12870) (t_loss: 0.11399) (accu: 0.9657)
[epoch : 3] (l_loss: 0.09695) (t_loss: 0.09424) (accu: 0.9708)
[epoch : 4] (l_loss: 0.08160) (t_loss: 0.08707) (accu: 0.9746)
[epoch : 5] (l_loss: 0.07171) (t_loss: 0.08335) (accu: 0.9749)
[epoch : 6] (l_loss: 0.06520) (t_loss: 0.07813) (accu: 0.9764)
[epoch : 7] (l_loss: 0.06018) (t_loss: 0.07694) (accu: 0.9773)
[epoch : 8] (l_loss: 0.05672) (t_loss: 0.07831) (accu: 0.9769)
[epoch : 9] (l_loss: 0.05370) (t_loss: 0.07305) (accu: 0.9772)
[epoch : 10] (l_loss: 0.05144) (t_loss: 0.07290) (accu: 0.9787)
[epoch : 11] (l_loss: 0.05002) (t_loss: 0.07621) (accu: 0.9773)
[epoch : 12] (l_loss: 0.04900) (t_loss: 0.07527) (accu: 0.9782)
[epoch : 13] (l_loss: 0.04824) (t_loss: 0.07618) (accu: 0.9761)
[epoch : 14] (l_loss: 0.04771) (t_loss: 0.07094) (accu: 0.9779)
[epoch : 15] (l_loss: 0.04704) (t_loss: 0.07429) (accu: 0.9773)
[epoch : 16] (l_loss: 0.04690) (t_loss: 0.07524) (accu: 0.9772)
[epoch : 17] (l_loss: 0.04717) (t_loss: 0.07223) (accu: 0.9781)
[epoch : 18] (l_loss: 0.04632) (t_loss: 0.07205) (accu: 0.9795)
[epoch : 19] (l_loss: 0.04660) (t_loss: 0.07413) (accu: 0.9771)
[epoch : 20] (l_loss: 0.04670) (t_loss: 0.07276) (accu: 0.9786)
[epoch : 21] (l_loss: 0.04630) (t_loss: 0.07431) (accu: 0.9780)
[epoch : 22] (l_loss: 0.04558) (t_loss: 0.07333) (accu: 0.9773)
[epoch : 23] (l_loss: 0.04510) (t_loss: 0.07469) (accu: 0.9783)
[epoch : 24] (l_loss: 0.04540) (t_loss: 0.07185) (accu: 0.9775)
[epoch : 25] (l_loss: 0.04468) (t_loss: 0.07303) (accu: 0.9778)
[epoch : 26] (l_loss: 0.04487) (t_loss: 0.07496) (accu: 0.9773)
[epoch : 27] (l_loss: 0.04514) (t_loss: 0.07218) (accu: 0.9777)
[epoch : 28] (l_loss: 0.04454) (t_loss: 0.07746) (accu: 0.9771)
[epoch : 29] (l_loss: 0.04501) (t_loss: 0.06875) (accu: 0.9791)
[epoch : 30] (l_loss: 0.04524) (t_loss: 0.06829) (accu: 0.9791)
[epoch : 31] (l_loss: 0.04457) (t_loss: 0.07132) (accu: 0.9782)
[epoch : 32] (l_loss: 0.04475) (t_loss: 0.07544) (accu: 0.9774)
[epoch : 33] (l_loss: 0.04506) (t_loss: 0.07455) (accu: 0.9776)
[epoch : 34] (l_loss: 0.04482) (t_loss: 0.07135) (accu: 0.9799)
[epoch : 35] (l_loss: 0.04482) (t_loss: 0.07097) (accu: 0.9785)
[epoch : 36] (l_loss: 0.04493) (t_loss: 0.07262) (accu: 0.9779)
[epoch : 37] (l_loss: 0.04462) (t_loss: 0.07093) (accu: 0.9790)
[epoch : 38] (l_loss: 0.04469) (t_loss: 0.07138) (accu: 0.9785)
[epoch : 39] (l_loss: 0.04486) (t_loss: 0.07151) (accu: 0.9795)
[epoch : 40] (l_loss: 0.04455) (t_loss: 0.07158) (accu: 0.9785)
[epoch : 41] (l_loss: 0.04489) (t_loss: 0.07186) (accu: 0.9782)
[epoch : 42] (l_loss: 0.04461) (t_loss: 0.06953) (accu: 0.9784)
[epoch : 43] (l_loss: 0.04476) (t_loss: 0.07335) (accu: 0.9783)
[epoch : 44] (l_loss: 0.04479) (t_loss: 0.07297) (accu: 0.9764)
[epoch : 45] (l_loss: 0.04481) (t_loss: 0.07022) (accu: 0.9783)
[epoch : 46] (l_loss: 0.04509) (t_loss: 0.07344) (accu: 0.9779)
[epoch : 47] (l_loss: 0.04439) (t_loss: 0.07141) (accu: 0.9791)
[epoch : 48] (l_loss: 0.04471) (t_loss: 0.07167) (accu: 0.9784)
[epoch : 49] (l_loss: 0.04494) (t_loss: 0.07153) (accu: 0.9779)
[epoch : 50] (l_loss: 0.04438) (t_loss: 0.07186) (accu: 0.9776)
Finish! (Best accu: 0.9799) (Time taken(sec) : 763.32) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (44923 | 221277)         16.88
fc1.weight   :      235200 (39460 | 195740)         16.78
fc2.weight   :        30000 (5033 | 24967)          16.78
fcout.weight :          1000 (430 | 570)            43.00
------------------------------------------------------------

Learning start! [Prune_iter : (9/21), Remaining weight : 16.88 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30251) (accu: 0.1290)
[epoch : 1] (l_loss: 0.40771) (t_loss: 0.15219) (accu: 0.9554)
[epoch : 2] (l_loss: 0.12615) (t_loss: 0.10918) (accu: 0.9678)
[epoch : 3] (l_loss: 0.09609) (t_loss: 0.09822) (accu: 0.9701)
[epoch : 4] (l_loss: 0.08147) (t_loss: 0.08466) (accu: 0.9753)
[epoch : 5] (l_loss: 0.07232) (t_loss: 0.08127) (accu: 0.9757)
[epoch : 6] (l_loss: 0.06664) (t_loss: 0.07922) (accu: 0.9757)
[epoch : 7] (l_loss: 0.06211) (t_loss: 0.07666) (accu: 0.9759)
[epoch : 8] (l_loss: 0.05819) (t_loss: 0.07773) (accu: 0.9764)
[epoch : 9] (l_loss: 0.05542) (t_loss: 0.07236) (accu: 0.9779)
[epoch : 10] (l_loss: 0.05336) (t_loss: 0.07938) (accu: 0.9760)
[epoch : 11] (l_loss: 0.05201) (t_loss: 0.07461) (accu: 0.9769)
[epoch : 12] (l_loss: 0.05061) (t_loss: 0.07459) (accu: 0.9767)
[epoch : 13] (l_loss: 0.04954) (t_loss: 0.07374) (accu: 0.9770)
[epoch : 14] (l_loss: 0.04886) (t_loss: 0.07208) (accu: 0.9775)
[epoch : 15] (l_loss: 0.04770) (t_loss: 0.07656) (accu: 0.9757)
[epoch : 16] (l_loss: 0.04723) (t_loss: 0.07424) (accu: 0.9769)
[epoch : 17] (l_loss: 0.04619) (t_loss: 0.07424) (accu: 0.9772)
[epoch : 18] (l_loss: 0.04606) (t_loss: 0.07446) (accu: 0.9779)
[epoch : 19] (l_loss: 0.04544) (t_loss: 0.07396) (accu: 0.9758)
[epoch : 20] (l_loss: 0.04535) (t_loss: 0.07034) (accu: 0.9783)
[epoch : 21] (l_loss: 0.04513) (t_loss: 0.07061) (accu: 0.9782)
[epoch : 22] (l_loss: 0.04454) (t_loss: 0.07322) (accu: 0.9781)
[epoch : 23] (l_loss: 0.04467) (t_loss: 0.07129) (accu: 0.9786)
[epoch : 24] (l_loss: 0.04469) (t_loss: 0.07431) (accu: 0.9766)
[epoch : 25] (l_loss: 0.04477) (t_loss: 0.07302) (accu: 0.9779)
[epoch : 26] (l_loss: 0.04454) (t_loss: 0.07335) (accu: 0.9772)
[epoch : 27] (l_loss: 0.04422) (t_loss: 0.07133) (accu: 0.9788)
[epoch : 28] (l_loss: 0.04423) (t_loss: 0.07647) (accu: 0.9762)
[epoch : 29] (l_loss: 0.04447) (t_loss: 0.07418) (accu: 0.9750)
[epoch : 30] (l_loss: 0.04425) (t_loss: 0.06835) (accu: 0.9794)
[epoch : 31] (l_loss: 0.04395) (t_loss: 0.07049) (accu: 0.9791)
[epoch : 32] (l_loss: 0.04425) (t_loss: 0.07142) (accu: 0.9780)
[epoch : 33] (l_loss: 0.04438) (t_loss: 0.07032) (accu: 0.9782)
[epoch : 34] (l_loss: 0.04410) (t_loss: 0.07119) (accu: 0.9783)
[epoch : 35] (l_loss: 0.04430) (t_loss: 0.07014) (accu: 0.9792)
[epoch : 36] (l_loss: 0.04395) (t_loss: 0.07420) (accu: 0.9761)
[epoch : 37] (l_loss: 0.04394) (t_loss: 0.07075) (accu: 0.9783)
[epoch : 38] (l_loss: 0.04398) (t_loss: 0.07178) (accu: 0.9779)
[epoch : 39] (l_loss: 0.04450) (t_loss: 0.07151) (accu: 0.9773)
[epoch : 40] (l_loss: 0.04359) (t_loss: 0.07169) (accu: 0.9767)
[epoch : 41] (l_loss: 0.04356) (t_loss: 0.07052) (accu: 0.9780)
[epoch : 42] (l_loss: 0.04365) (t_loss: 0.06933) (accu: 0.9788)
[epoch : 43] (l_loss: 0.04372) (t_loss: 0.06800) (accu: 0.9793)
[epoch : 44] (l_loss: 0.04348) (t_loss: 0.07288) (accu: 0.9763)
[epoch : 45] (l_loss: 0.04389) (t_loss: 0.07071) (accu: 0.9769)
[epoch : 46] (l_loss: 0.04382) (t_loss: 0.07063) (accu: 0.9776)
[epoch : 47] (l_loss: 0.04357) (t_loss: 0.07559) (accu: 0.9776)
[epoch : 48] (l_loss: 0.04402) (t_loss: 0.07215) (accu: 0.9786)
[epoch : 49] (l_loss: 0.04381) (t_loss: 0.06791) (accu: 0.9790)
[epoch : 50] (l_loss: 0.04386) (t_loss: 0.07220) (accu: 0.9790)
Finish! (Best accu: 0.9794) (Time taken(sec) : 753.61) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (35982 | 230218)         13.52
fc1.weight   :      235200 (31568 | 203632)         13.42
fc2.weight   :        30000 (4027 | 25973)          13.42
fcout.weight :          1000 (387 | 613)            38.70
------------------------------------------------------------

Learning start! [Prune_iter : (10/21), Remaining weight : 13.52 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29876) (accu: 0.0925)
[epoch : 1] (l_loss: 0.40376) (t_loss: 0.15003) (accu: 0.9556)
[epoch : 2] (l_loss: 0.12830) (t_loss: 0.10902) (accu: 0.9665)
[epoch : 3] (l_loss: 0.09700) (t_loss: 0.09861) (accu: 0.9686)
[epoch : 4] (l_loss: 0.08255) (t_loss: 0.08674) (accu: 0.9733)
[epoch : 5] (l_loss: 0.07401) (t_loss: 0.08177) (accu: 0.9747)
[epoch : 6] (l_loss: 0.06749) (t_loss: 0.08143) (accu: 0.9750)
[epoch : 7] (l_loss: 0.06243) (t_loss: 0.07633) (accu: 0.9763)
[epoch : 8] (l_loss: 0.05756) (t_loss: 0.07640) (accu: 0.9767)
[epoch : 9] (l_loss: 0.05431) (t_loss: 0.07665) (accu: 0.9763)
[epoch : 10] (l_loss: 0.05212) (t_loss: 0.07500) (accu: 0.9764)
[epoch : 11] (l_loss: 0.05037) (t_loss: 0.07179) (accu: 0.9768)
[epoch : 12] (l_loss: 0.04936) (t_loss: 0.07336) (accu: 0.9769)
[epoch : 13] (l_loss: 0.04799) (t_loss: 0.07223) (accu: 0.9778)
[epoch : 14] (l_loss: 0.04734) (t_loss: 0.07633) (accu: 0.9767)
[epoch : 15] (l_loss: 0.04697) (t_loss: 0.07624) (accu: 0.9762)
[epoch : 16] (l_loss: 0.04613) (t_loss: 0.07441) (accu: 0.9760)
[epoch : 17] (l_loss: 0.04542) (t_loss: 0.07321) (accu: 0.9777)
[epoch : 18] (l_loss: 0.04545) (t_loss: 0.07452) (accu: 0.9764)
[epoch : 19] (l_loss: 0.04542) (t_loss: 0.07272) (accu: 0.9762)
[epoch : 20] (l_loss: 0.04514) (t_loss: 0.07278) (accu: 0.9765)
[epoch : 21] (l_loss: 0.04523) (t_loss: 0.07374) (accu: 0.9764)
[epoch : 22] (l_loss: 0.04489) (t_loss: 0.07425) (accu: 0.9763)
[epoch : 23] (l_loss: 0.04493) (t_loss: 0.07450) (accu: 0.9763)
[epoch : 24] (l_loss: 0.04492) (t_loss: 0.07312) (accu: 0.9781)
[epoch : 25] (l_loss: 0.04575) (t_loss: 0.07490) (accu: 0.9769)
[epoch : 26] (l_loss: 0.04504) (t_loss: 0.07619) (accu: 0.9765)
[epoch : 27] (l_loss: 0.04496) (t_loss: 0.07254) (accu: 0.9774)
[epoch : 28] (l_loss: 0.04482) (t_loss: 0.07380) (accu: 0.9771)
[epoch : 29] (l_loss: 0.04443) (t_loss: 0.07832) (accu: 0.9733)
[epoch : 30] (l_loss: 0.04474) (t_loss: 0.07526) (accu: 0.9786)
[epoch : 31] (l_loss: 0.04491) (t_loss: 0.07343) (accu: 0.9764)
[epoch : 32] (l_loss: 0.04484) (t_loss: 0.07183) (accu: 0.9785)
[epoch : 33] (l_loss: 0.04464) (t_loss: 0.07085) (accu: 0.9788)
[epoch : 34] (l_loss: 0.04494) (t_loss: 0.06881) (accu: 0.9790)
[epoch : 35] (l_loss: 0.04472) (t_loss: 0.07502) (accu: 0.9760)
[epoch : 36] (l_loss: 0.04497) (t_loss: 0.07380) (accu: 0.9772)
[epoch : 37] (l_loss: 0.04501) (t_loss: 0.07401) (accu: 0.9770)
[epoch : 38] (l_loss: 0.04496) (t_loss: 0.07088) (accu: 0.9783)
[epoch : 39] (l_loss: 0.04468) (t_loss: 0.07653) (accu: 0.9752)
[epoch : 40] (l_loss: 0.04450) (t_loss: 0.07127) (accu: 0.9787)
[epoch : 41] (l_loss: 0.04394) (t_loss: 0.07201) (accu: 0.9787)
[epoch : 42] (l_loss: 0.04427) (t_loss: 0.06843) (accu: 0.9797)
[epoch : 43] (l_loss: 0.04349) (t_loss: 0.07163) (accu: 0.9784)
[epoch : 44] (l_loss: 0.04349) (t_loss: 0.07514) (accu: 0.9761)
[epoch : 45] (l_loss: 0.04394) (t_loss: 0.07683) (accu: 0.9770)
[epoch : 46] (l_loss: 0.04410) (t_loss: 0.07039) (accu: 0.9783)
[epoch : 47] (l_loss: 0.04387) (t_loss: 0.07251) (accu: 0.9776)
[epoch : 48] (l_loss: 0.04377) (t_loss: 0.07020) (accu: 0.9779)
[epoch : 49] (l_loss: 0.04411) (t_loss: 0.07369) (accu: 0.9771)
[epoch : 50] (l_loss: 0.04367) (t_loss: 0.06965) (accu: 0.9793)
Finish! (Best accu: 0.9797) (Time taken(sec) : 728.34) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (28824 | 237376)         10.83
fc1.weight   :      235200 (25254 | 209946)         10.74
fc2.weight   :        30000 (3221 | 26779)          10.74
fcout.weight :          1000 (349 | 651)            34.90
------------------------------------------------------------

Learning start! [Prune_iter : (11/21), Remaining weight : 10.83 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30120) (accu: 0.1075)
[epoch : 1] (l_loss: 0.40117) (t_loss: 0.14950) (accu: 0.9554)
[epoch : 2] (l_loss: 0.12651) (t_loss: 0.11000) (accu: 0.9680)
[epoch : 3] (l_loss: 0.09591) (t_loss: 0.09705) (accu: 0.9688)
[epoch : 4] (l_loss: 0.08126) (t_loss: 0.08701) (accu: 0.9728)
[epoch : 5] (l_loss: 0.07298) (t_loss: 0.08115) (accu: 0.9750)
[epoch : 6] (l_loss: 0.06780) (t_loss: 0.07955) (accu: 0.9752)
[epoch : 7] (l_loss: 0.06361) (t_loss: 0.07755) (accu: 0.9760)
[epoch : 8] (l_loss: 0.06050) (t_loss: 0.07719) (accu: 0.9773)
[epoch : 9] (l_loss: 0.05749) (t_loss: 0.07549) (accu: 0.9766)
[epoch : 10] (l_loss: 0.05492) (t_loss: 0.07316) (accu: 0.9776)
[epoch : 11] (l_loss: 0.05321) (t_loss: 0.07634) (accu: 0.9768)
[epoch : 12] (l_loss: 0.05192) (t_loss: 0.07602) (accu: 0.9764)
[epoch : 13] (l_loss: 0.05058) (t_loss: 0.07081) (accu: 0.9771)
[epoch : 14] (l_loss: 0.05002) (t_loss: 0.07689) (accu: 0.9754)
[epoch : 15] (l_loss: 0.04950) (t_loss: 0.07328) (accu: 0.9771)
[epoch : 16] (l_loss: 0.04876) (t_loss: 0.07552) (accu: 0.9761)
[epoch : 17] (l_loss: 0.04870) (t_loss: 0.07130) (accu: 0.9763)
[epoch : 18] (l_loss: 0.04780) (t_loss: 0.08228) (accu: 0.9746)
[epoch : 19] (l_loss: 0.04771) (t_loss: 0.07322) (accu: 0.9767)
[epoch : 20] (l_loss: 0.04744) (t_loss: 0.06985) (accu: 0.9773)
[epoch : 21] (l_loss: 0.04771) (t_loss: 0.07148) (accu: 0.9788)
[epoch : 22] (l_loss: 0.04736) (t_loss: 0.07232) (accu: 0.9775)
[epoch : 23] (l_loss: 0.04690) (t_loss: 0.07370) (accu: 0.9766)
[epoch : 24] (l_loss: 0.04695) (t_loss: 0.07291) (accu: 0.9766)
[epoch : 25] (l_loss: 0.04701) (t_loss: 0.07298) (accu: 0.9769)
[epoch : 26] (l_loss: 0.04679) (t_loss: 0.07343) (accu: 0.9768)
[epoch : 27] (l_loss: 0.04667) (t_loss: 0.07441) (accu: 0.9761)
[epoch : 28] (l_loss: 0.04663) (t_loss: 0.07465) (accu: 0.9774)
[epoch : 29] (l_loss: 0.04620) (t_loss: 0.07173) (accu: 0.9760)
[epoch : 30] (l_loss: 0.04689) (t_loss: 0.07332) (accu: 0.9779)
[epoch : 31] (l_loss: 0.04670) (t_loss: 0.07694) (accu: 0.9758)
[epoch : 32] (l_loss: 0.04618) (t_loss: 0.07175) (accu: 0.9780)
[epoch : 33] (l_loss: 0.04660) (t_loss: 0.07130) (accu: 0.9782)
[epoch : 34] (l_loss: 0.04636) (t_loss: 0.07416) (accu: 0.9778)
[epoch : 35] (l_loss: 0.04708) (t_loss: 0.07126) (accu: 0.9785)
[epoch : 36] (l_loss: 0.04624) (t_loss: 0.07171) (accu: 0.9775)
[epoch : 37] (l_loss: 0.04651) (t_loss: 0.07264) (accu: 0.9792)
[epoch : 38] (l_loss: 0.04606) (t_loss: 0.06945) (accu: 0.9780)
[epoch : 39] (l_loss: 0.04607) (t_loss: 0.07442) (accu: 0.9776)
[epoch : 40] (l_loss: 0.04615) (t_loss: 0.07204) (accu: 0.9770)
[epoch : 41] (l_loss: 0.04613) (t_loss: 0.07323) (accu: 0.9771)
[epoch : 42] (l_loss: 0.04659) (t_loss: 0.07227) (accu: 0.9784)
[epoch : 43] (l_loss: 0.04610) (t_loss: 0.06908) (accu: 0.9784)
[epoch : 44] (l_loss: 0.04632) (t_loss: 0.07496) (accu: 0.9776)
[epoch : 45] (l_loss: 0.04630) (t_loss: 0.06984) (accu: 0.9794)
[epoch : 46] (l_loss: 0.04660) (t_loss: 0.07240) (accu: 0.9784)
[epoch : 47] (l_loss: 0.04589) (t_loss: 0.07041) (accu: 0.9786)
[epoch : 48] (l_loss: 0.04547) (t_loss: 0.07298) (accu: 0.9785)
[epoch : 49] (l_loss: 0.04547) (t_loss: 0.06955) (accu: 0.9781)
[epoch : 50] (l_loss: 0.04520) (t_loss: 0.07079) (accu: 0.9777)
Finish! (Best accu: 0.9794) (Time taken(sec) : 783.23) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (23095 | 243105)          8.68
fc1.weight   :      235200 (20204 | 214996)          8.59
fc2.weight   :        30000 (2577 | 27423)           8.59
fcout.weight :          1000 (314 | 686)            31.40
------------------------------------------------------------

Learning start! [Prune_iter : (12/21), Remaining weight : 8.68 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30303) (accu: 0.0974)
[epoch : 1] (l_loss: 0.40234) (t_loss: 0.15218) (accu: 0.9576)
[epoch : 2] (l_loss: 0.12487) (t_loss: 0.11518) (accu: 0.9650)
[epoch : 3] (l_loss: 0.09568) (t_loss: 0.09231) (accu: 0.9727)
[epoch : 4] (l_loss: 0.08097) (t_loss: 0.08439) (accu: 0.9743)
[epoch : 5] (l_loss: 0.07155) (t_loss: 0.08062) (accu: 0.9750)
[epoch : 6] (l_loss: 0.06554) (t_loss: 0.07642) (accu: 0.9763)
[epoch : 7] (l_loss: 0.06171) (t_loss: 0.07848) (accu: 0.9755)
[epoch : 8] (l_loss: 0.05839) (t_loss: 0.07568) (accu: 0.9762)
[epoch : 9] (l_loss: 0.05547) (t_loss: 0.07645) (accu: 0.9766)
[epoch : 10] (l_loss: 0.05366) (t_loss: 0.07511) (accu: 0.9772)
[epoch : 11] (l_loss: 0.05182) (t_loss: 0.07421) (accu: 0.9759)
[epoch : 12] (l_loss: 0.05091) (t_loss: 0.07573) (accu: 0.9766)
[epoch : 13] (l_loss: 0.05020) (t_loss: 0.07108) (accu: 0.9786)
[epoch : 14] (l_loss: 0.04940) (t_loss: 0.07513) (accu: 0.9769)
[epoch : 15] (l_loss: 0.04901) (t_loss: 0.07392) (accu: 0.9774)
[epoch : 16] (l_loss: 0.04833) (t_loss: 0.07116) (accu: 0.9768)
[epoch : 17] (l_loss: 0.04793) (t_loss: 0.07213) (accu: 0.9765)
[epoch : 18] (l_loss: 0.04775) (t_loss: 0.07519) (accu: 0.9765)
[epoch : 19] (l_loss: 0.04753) (t_loss: 0.07550) (accu: 0.9761)
[epoch : 20] (l_loss: 0.04750) (t_loss: 0.07341) (accu: 0.9769)
[epoch : 21] (l_loss: 0.04711) (t_loss: 0.07370) (accu: 0.9767)
[epoch : 22] (l_loss: 0.04777) (t_loss: 0.07400) (accu: 0.9771)
[epoch : 23] (l_loss: 0.04646) (t_loss: 0.07059) (accu: 0.9775)
[epoch : 24] (l_loss: 0.04690) (t_loss: 0.07263) (accu: 0.9782)
[epoch : 25] (l_loss: 0.04691) (t_loss: 0.06986) (accu: 0.9773)
[epoch : 26] (l_loss: 0.04678) (t_loss: 0.07200) (accu: 0.9770)
[epoch : 27] (l_loss: 0.04647) (t_loss: 0.07479) (accu: 0.9754)
[epoch : 28] (l_loss: 0.04639) (t_loss: 0.07514) (accu: 0.9768)
[epoch : 29] (l_loss: 0.04688) (t_loss: 0.07027) (accu: 0.9783)
[epoch : 30] (l_loss: 0.04660) (t_loss: 0.07380) (accu: 0.9766)
[epoch : 31] (l_loss: 0.04627) (t_loss: 0.07337) (accu: 0.9779)
[epoch : 32] (l_loss: 0.04627) (t_loss: 0.07144) (accu: 0.9767)
[epoch : 33] (l_loss: 0.04619) (t_loss: 0.07176) (accu: 0.9778)
[epoch : 34] (l_loss: 0.04649) (t_loss: 0.07286) (accu: 0.9773)
[epoch : 35] (l_loss: 0.04637) (t_loss: 0.07095) (accu: 0.9789)
[epoch : 36] (l_loss: 0.04599) (t_loss: 0.07516) (accu: 0.9764)
[epoch : 37] (l_loss: 0.04633) (t_loss: 0.07199) (accu: 0.9787)
[epoch : 38] (l_loss: 0.04613) (t_loss: 0.07276) (accu: 0.9772)
[epoch : 39] (l_loss: 0.04574) (t_loss: 0.07294) (accu: 0.9777)
[epoch : 40] (l_loss: 0.04572) (t_loss: 0.07198) (accu: 0.9782)
[epoch : 41] (l_loss: 0.04537) (t_loss: 0.07181) (accu: 0.9767)
[epoch : 42] (l_loss: 0.04515) (t_loss: 0.06909) (accu: 0.9784)
[epoch : 43] (l_loss: 0.04529) (t_loss: 0.07329) (accu: 0.9767)
[epoch : 44] (l_loss: 0.04541) (t_loss: 0.07262) (accu: 0.9785)
[epoch : 45] (l_loss: 0.04537) (t_loss: 0.07162) (accu: 0.9783)
[epoch : 46] (l_loss: 0.04556) (t_loss: 0.07156) (accu: 0.9782)
[epoch : 47] (l_loss: 0.04494) (t_loss: 0.07077) (accu: 0.9775)
[epoch : 48] (l_loss: 0.04516) (t_loss: 0.06891) (accu: 0.9787)
[epoch : 49] (l_loss: 0.04541) (t_loss: 0.06745) (accu: 0.9791)
[epoch : 50] (l_loss: 0.04457) (t_loss: 0.07078) (accu: 0.9783)
Finish! (Best accu: 0.9791) (Time taken(sec) : 774.35) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (18507 | 247693)          6.95
fc1.weight   :      235200 (16163 | 219037)          6.87
fc2.weight   :        30000 (2062 | 27938)           6.87
fcout.weight :          1000 (282 | 718)            28.20
------------------------------------------------------------

Learning start! [Prune_iter : (13/21), Remaining weight : 6.95 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30410) (accu: 0.0985)
[epoch : 1] (l_loss: 0.40040) (t_loss: 0.14680) (accu: 0.9562)
[epoch : 2] (l_loss: 0.12169) (t_loss: 0.10646) (accu: 0.9680)
[epoch : 3] (l_loss: 0.09177) (t_loss: 0.09622) (accu: 0.9701)
[epoch : 4] (l_loss: 0.07754) (t_loss: 0.08661) (accu: 0.9730)
[epoch : 5] (l_loss: 0.06922) (t_loss: 0.08250) (accu: 0.9748)
[epoch : 6] (l_loss: 0.06288) (t_loss: 0.07810) (accu: 0.9771)
[epoch : 7] (l_loss: 0.05904) (t_loss: 0.07819) (accu: 0.9767)
[epoch : 8] (l_loss: 0.05545) (t_loss: 0.08106) (accu: 0.9740)
[epoch : 9] (l_loss: 0.05262) (t_loss: 0.07574) (accu: 0.9777)
[epoch : 10] (l_loss: 0.05047) (t_loss: 0.07411) (accu: 0.9772)
[epoch : 11] (l_loss: 0.04911) (t_loss: 0.07176) (accu: 0.9788)
[epoch : 12] (l_loss: 0.04813) (t_loss: 0.07514) (accu: 0.9766)
[epoch : 13] (l_loss: 0.04720) (t_loss: 0.07205) (accu: 0.9784)
[epoch : 14] (l_loss: 0.04644) (t_loss: 0.07364) (accu: 0.9769)
[epoch : 15] (l_loss: 0.04666) (t_loss: 0.07537) (accu: 0.9763)
[epoch : 16] (l_loss: 0.04636) (t_loss: 0.07298) (accu: 0.9773)
[epoch : 17] (l_loss: 0.04619) (t_loss: 0.07629) (accu: 0.9764)
[epoch : 18] (l_loss: 0.04496) (t_loss: 0.07786) (accu: 0.9755)
[epoch : 19] (l_loss: 0.04560) (t_loss: 0.07143) (accu: 0.9783)
[epoch : 20] (l_loss: 0.04526) (t_loss: 0.07118) (accu: 0.9784)
[epoch : 21] (l_loss: 0.04476) (t_loss: 0.07317) (accu: 0.9773)
[epoch : 22] (l_loss: 0.04511) (t_loss: 0.07129) (accu: 0.9775)
[epoch : 23] (l_loss: 0.04525) (t_loss: 0.07467) (accu: 0.9775)
[epoch : 24] (l_loss: 0.04455) (t_loss: 0.07546) (accu: 0.9780)
[epoch : 25] (l_loss: 0.04516) (t_loss: 0.07461) (accu: 0.9773)
[epoch : 26] (l_loss: 0.04490) (t_loss: 0.07653) (accu: 0.9761)
[epoch : 27] (l_loss: 0.04512) (t_loss: 0.06906) (accu: 0.9777)
[epoch : 28] (l_loss: 0.04516) (t_loss: 0.07518) (accu: 0.9765)
[epoch : 29] (l_loss: 0.04507) (t_loss: 0.07401) (accu: 0.9773)
[epoch : 30] (l_loss: 0.04467) (t_loss: 0.07167) (accu: 0.9781)
[epoch : 31] (l_loss: 0.04463) (t_loss: 0.07398) (accu: 0.9783)
[epoch : 32] (l_loss: 0.04457) (t_loss: 0.07396) (accu: 0.9767)
[epoch : 33] (l_loss: 0.04490) (t_loss: 0.07305) (accu: 0.9773)
[epoch : 34] (l_loss: 0.04471) (t_loss: 0.07177) (accu: 0.9774)
[epoch : 35] (l_loss: 0.04383) (t_loss: 0.07171) (accu: 0.9774)
[epoch : 36] (l_loss: 0.04396) (t_loss: 0.07266) (accu: 0.9779)
[epoch : 37] (l_loss: 0.04408) (t_loss: 0.07003) (accu: 0.9772)
[epoch : 38] (l_loss: 0.04400) (t_loss: 0.06972) (accu: 0.9791)
[epoch : 39] (l_loss: 0.04364) (t_loss: 0.07099) (accu: 0.9778)
[epoch : 40] (l_loss: 0.04386) (t_loss: 0.07065) (accu: 0.9785)
[epoch : 41] (l_loss: 0.04407) (t_loss: 0.07254) (accu: 0.9771)
[epoch : 42] (l_loss: 0.04360) (t_loss: 0.07060) (accu: 0.9789)
[epoch : 43] (l_loss: 0.04352) (t_loss: 0.07333) (accu: 0.9756)
[epoch : 44] (l_loss: 0.04389) (t_loss: 0.07132) (accu: 0.9779)
[epoch : 45] (l_loss: 0.04392) (t_loss: 0.07353) (accu: 0.9784)
[epoch : 46] (l_loss: 0.04330) (t_loss: 0.07190) (accu: 0.9780)
[epoch : 47] (l_loss: 0.04410) (t_loss: 0.07060) (accu: 0.9772)
[epoch : 48] (l_loss: 0.04400) (t_loss: 0.07412) (accu: 0.9773)
[epoch : 49] (l_loss: 0.04360) (t_loss: 0.07178) (accu: 0.9776)
[epoch : 50] (l_loss: 0.04330) (t_loss: 0.06979) (accu: 0.9789)
Finish! (Best accu: 0.9791) (Time taken(sec) : 769.49) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (14833 | 251367)          5.57
fc1.weight   :      235200 (12930 | 222270)          5.50
fc2.weight   :        30000 (1649 | 28351)           5.50
fcout.weight :          1000 (254 | 746)            25.40
------------------------------------------------------------

Learning start! [Prune_iter : (14/21), Remaining weight : 5.57 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30084) (accu: 0.1003)
[epoch : 1] (l_loss: 0.40383) (t_loss: 0.15038) (accu: 0.9560)
[epoch : 2] (l_loss: 0.12947) (t_loss: 0.11247) (accu: 0.9671)
[epoch : 3] (l_loss: 0.09811) (t_loss: 0.09554) (accu: 0.9707)
[epoch : 4] (l_loss: 0.08393) (t_loss: 0.08935) (accu: 0.9727)
[epoch : 5] (l_loss: 0.07431) (t_loss: 0.08505) (accu: 0.9730)
[epoch : 6] (l_loss: 0.06768) (t_loss: 0.07993) (accu: 0.9770)
[epoch : 7] (l_loss: 0.06369) (t_loss: 0.07982) (accu: 0.9760)
[epoch : 8] (l_loss: 0.05954) (t_loss: 0.07694) (accu: 0.9779)
[epoch : 9] (l_loss: 0.05689) (t_loss: 0.07381) (accu: 0.9773)
[epoch : 10] (l_loss: 0.05382) (t_loss: 0.07536) (accu: 0.9770)
[epoch : 11] (l_loss: 0.05276) (t_loss: 0.07359) (accu: 0.9776)
[epoch : 12] (l_loss: 0.05027) (t_loss: 0.07375) (accu: 0.9773)
[epoch : 13] (l_loss: 0.04934) (t_loss: 0.07546) (accu: 0.9763)
[epoch : 14] (l_loss: 0.04794) (t_loss: 0.07515) (accu: 0.9760)
[epoch : 15] (l_loss: 0.04795) (t_loss: 0.07301) (accu: 0.9780)
[epoch : 16] (l_loss: 0.04671) (t_loss: 0.07402) (accu: 0.9753)
[epoch : 17] (l_loss: 0.04688) (t_loss: 0.07377) (accu: 0.9770)
[epoch : 18] (l_loss: 0.04654) (t_loss: 0.07313) (accu: 0.9756)
[epoch : 19] (l_loss: 0.04643) (t_loss: 0.07288) (accu: 0.9768)
[epoch : 20] (l_loss: 0.04553) (t_loss: 0.07603) (accu: 0.9752)
[epoch : 21] (l_loss: 0.04569) (t_loss: 0.07346) (accu: 0.9773)
[epoch : 22] (l_loss: 0.04596) (t_loss: 0.07228) (accu: 0.9777)
[epoch : 23] (l_loss: 0.04608) (t_loss: 0.07188) (accu: 0.9771)
[epoch : 24] (l_loss: 0.04542) (t_loss: 0.07501) (accu: 0.9757)
[epoch : 25] (l_loss: 0.04562) (t_loss: 0.07738) (accu: 0.9738)
[epoch : 26] (l_loss: 0.04490) (t_loss: 0.07457) (accu: 0.9757)
[epoch : 27] (l_loss: 0.04542) (t_loss: 0.07244) (accu: 0.9773)
[epoch : 28] (l_loss: 0.04524) (t_loss: 0.07248) (accu: 0.9775)
[epoch : 29] (l_loss: 0.04499) (t_loss: 0.07127) (accu: 0.9773)
[epoch : 30] (l_loss: 0.04467) (t_loss: 0.07178) (accu: 0.9778)
[epoch : 31] (l_loss: 0.04405) (t_loss: 0.07345) (accu: 0.9759)
[epoch : 32] (l_loss: 0.04414) (t_loss: 0.07109) (accu: 0.9780)
[epoch : 33] (l_loss: 0.04374) (t_loss: 0.07298) (accu: 0.9765)
[epoch : 34] (l_loss: 0.04421) (t_loss: 0.07160) (accu: 0.9777)
[epoch : 35] (l_loss: 0.04381) (t_loss: 0.07609) (accu: 0.9770)
[epoch : 36] (l_loss: 0.04442) (t_loss: 0.07095) (accu: 0.9774)
[epoch : 37] (l_loss: 0.04421) (t_loss: 0.07397) (accu: 0.9778)
[epoch : 38] (l_loss: 0.04373) (t_loss: 0.07060) (accu: 0.9780)
[epoch : 39] (l_loss: 0.04408) (t_loss: 0.07236) (accu: 0.9770)
[epoch : 40] (l_loss: 0.04383) (t_loss: 0.07324) (accu: 0.9776)
[epoch : 41] (l_loss: 0.04431) (t_loss: 0.07389) (accu: 0.9767)
[epoch : 42] (l_loss: 0.04409) (t_loss: 0.07110) (accu: 0.9783)
[epoch : 43] (l_loss: 0.04400) (t_loss: 0.07361) (accu: 0.9765)
[epoch : 44] (l_loss: 0.04400) (t_loss: 0.07323) (accu: 0.9775)
[epoch : 45] (l_loss: 0.04396) (t_loss: 0.07054) (accu: 0.9779)
[epoch : 46] (l_loss: 0.04392) (t_loss: 0.07251) (accu: 0.9775)
[epoch : 47] (l_loss: 0.04365) (t_loss: 0.07234) (accu: 0.9771)
[epoch : 48] (l_loss: 0.04452) (t_loss: 0.07378) (accu: 0.9775)
[epoch : 49] (l_loss: 0.04399) (t_loss: 0.07646) (accu: 0.9758)
[epoch : 50] (l_loss: 0.04372) (t_loss: 0.07110) (accu: 0.9792)
Finish! (Best accu: 0.9792) (Time taken(sec) : 761.44) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (11892 | 254308)          4.47
fc1.weight   :      235200 (10344 | 224856)          4.40
fc2.weight   :        30000 (1319 | 28681)           4.40
fcout.weight :          1000 (229 | 771)            22.90
------------------------------------------------------------

Learning start! [Prune_iter : (15/21), Remaining weight : 4.47 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29776) (accu: 0.1051)
[epoch : 1] (l_loss: 0.39572) (t_loss: 0.15393) (accu: 0.9564)
[epoch : 2] (l_loss: 0.12498) (t_loss: 0.10675) (accu: 0.9691)
[epoch : 3] (l_loss: 0.09504) (t_loss: 0.09373) (accu: 0.9712)
[epoch : 4] (l_loss: 0.08148) (t_loss: 0.08477) (accu: 0.9747)
[epoch : 5] (l_loss: 0.07273) (t_loss: 0.08251) (accu: 0.9755)
[epoch : 6] (l_loss: 0.06745) (t_loss: 0.07852) (accu: 0.9748)
[epoch : 7] (l_loss: 0.06304) (t_loss: 0.07623) (accu: 0.9770)
[epoch : 8] (l_loss: 0.05927) (t_loss: 0.07627) (accu: 0.9762)
[epoch : 9] (l_loss: 0.05609) (t_loss: 0.07395) (accu: 0.9779)
[epoch : 10] (l_loss: 0.05347) (t_loss: 0.07427) (accu: 0.9765)
[epoch : 11] (l_loss: 0.05120) (t_loss: 0.07470) (accu: 0.9772)
[epoch : 12] (l_loss: 0.04980) (t_loss: 0.07420) (accu: 0.9765)
[epoch : 13] (l_loss: 0.04922) (t_loss: 0.07104) (accu: 0.9771)
[epoch : 14] (l_loss: 0.04816) (t_loss: 0.07565) (accu: 0.9769)
[epoch : 15] (l_loss: 0.04731) (t_loss: 0.07782) (accu: 0.9762)
[epoch : 16] (l_loss: 0.04695) (t_loss: 0.07504) (accu: 0.9783)
[epoch : 17] (l_loss: 0.04651) (t_loss: 0.07316) (accu: 0.9778)
[epoch : 18] (l_loss: 0.04652) (t_loss: 0.07194) (accu: 0.9770)
[epoch : 19] (l_loss: 0.04630) (t_loss: 0.07343) (accu: 0.9778)
[epoch : 20] (l_loss: 0.04624) (t_loss: 0.07255) (accu: 0.9770)
[epoch : 21] (l_loss: 0.04609) (t_loss: 0.07586) (accu: 0.9768)
[epoch : 22] (l_loss: 0.04563) (t_loss: 0.07426) (accu: 0.9775)
[epoch : 23] (l_loss: 0.04574) (t_loss: 0.07358) (accu: 0.9774)
[epoch : 24] (l_loss: 0.04557) (t_loss: 0.07784) (accu: 0.9764)
[epoch : 25] (l_loss: 0.04574) (t_loss: 0.07123) (accu: 0.9777)
[epoch : 26] (l_loss: 0.04589) (t_loss: 0.07775) (accu: 0.9761)
[epoch : 27] (l_loss: 0.04537) (t_loss: 0.07334) (accu: 0.9777)
[epoch : 28] (l_loss: 0.04545) (t_loss: 0.07354) (accu: 0.9784)
[epoch : 29] (l_loss: 0.04478) (t_loss: 0.07090) (accu: 0.9763)
[epoch : 30] (l_loss: 0.04517) (t_loss: 0.07374) (accu: 0.9762)
[epoch : 31] (l_loss: 0.04507) (t_loss: 0.07356) (accu: 0.9762)
[epoch : 32] (l_loss: 0.04538) (t_loss: 0.07628) (accu: 0.9768)
[epoch : 33] (l_loss: 0.04517) (t_loss: 0.07478) (accu: 0.9759)
[epoch : 34] (l_loss: 0.04556) (t_loss: 0.07284) (accu: 0.9768)
[epoch : 35] (l_loss: 0.04472) (t_loss: 0.07503) (accu: 0.9770)
[epoch : 36] (l_loss: 0.04517) (t_loss: 0.07127) (accu: 0.9775)
[epoch : 37] (l_loss: 0.04477) (t_loss: 0.07126) (accu: 0.9775)
[epoch : 38] (l_loss: 0.04499) (t_loss: 0.07422) (accu: 0.9776)
[epoch : 39] (l_loss: 0.04458) (t_loss: 0.07253) (accu: 0.9771)
[epoch : 40] (l_loss: 0.04432) (t_loss: 0.07396) (accu: 0.9761)
[epoch : 41] (l_loss: 0.04425) (t_loss: 0.07395) (accu: 0.9770)
[epoch : 42] (l_loss: 0.04361) (t_loss: 0.07081) (accu: 0.9782)
[epoch : 43] (l_loss: 0.04441) (t_loss: 0.06937) (accu: 0.9788)
[epoch : 44] (l_loss: 0.04313) (t_loss: 0.07437) (accu: 0.9765)
[epoch : 45] (l_loss: 0.04456) (t_loss: 0.07394) (accu: 0.9775)
[epoch : 46] (l_loss: 0.04397) (t_loss: 0.07073) (accu: 0.9782)
[epoch : 47] (l_loss: 0.04383) (t_loss: 0.07291) (accu: 0.9777)
[epoch : 48] (l_loss: 0.04393) (t_loss: 0.06900) (accu: 0.9779)
[epoch : 49] (l_loss: 0.04411) (t_loss: 0.07007) (accu: 0.9783)
[epoch : 50] (l_loss: 0.04413) (t_loss: 0.07141) (accu: 0.9786)
Finish! (Best accu: 0.9788) (Time taken(sec) : 762.01) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (9537 | 256663)          3.58
fc1.weight   :       235200 (8275 | 226925)          3.52
fc2.weight   :        30000 (1056 | 28944)           3.52
fcout.weight :          1000 (206 | 794)            20.60
------------------------------------------------------------

Learning start! [Prune_iter : (16/21), Remaining weight : 3.58 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29544) (accu: 0.0989)
[epoch : 1] (l_loss: 0.40236) (t_loss: 0.15500) (accu: 0.9545)
[epoch : 2] (l_loss: 0.12763) (t_loss: 0.10946) (accu: 0.9670)
[epoch : 3] (l_loss: 0.09539) (t_loss: 0.09473) (accu: 0.9713)
[epoch : 4] (l_loss: 0.08100) (t_loss: 0.08845) (accu: 0.9741)
[epoch : 5] (l_loss: 0.07270) (t_loss: 0.08750) (accu: 0.9726)
[epoch : 6] (l_loss: 0.06668) (t_loss: 0.08720) (accu: 0.9726)
[epoch : 7] (l_loss: 0.06247) (t_loss: 0.08129) (accu: 0.9746)
[epoch : 8] (l_loss: 0.05932) (t_loss: 0.07446) (accu: 0.9766)
[epoch : 9] (l_loss: 0.05593) (t_loss: 0.07706) (accu: 0.9761)
[epoch : 10] (l_loss: 0.05251) (t_loss: 0.07575) (accu: 0.9765)
[epoch : 11] (l_loss: 0.05109) (t_loss: 0.07390) (accu: 0.9764)
[epoch : 12] (l_loss: 0.04955) (t_loss: 0.07274) (accu: 0.9772)
[epoch : 13] (l_loss: 0.04831) (t_loss: 0.07261) (accu: 0.9782)
[epoch : 14] (l_loss: 0.04721) (t_loss: 0.07342) (accu: 0.9767)
[epoch : 15] (l_loss: 0.04725) (t_loss: 0.07252) (accu: 0.9774)
[epoch : 16] (l_loss: 0.04678) (t_loss: 0.07672) (accu: 0.9757)
[epoch : 17] (l_loss: 0.04676) (t_loss: 0.07464) (accu: 0.9762)
[epoch : 18] (l_loss: 0.04657) (t_loss: 0.07266) (accu: 0.9775)
[epoch : 19] (l_loss: 0.04580) (t_loss: 0.07358) (accu: 0.9781)
[epoch : 20] (l_loss: 0.04580) (t_loss: 0.07673) (accu: 0.9757)
[epoch : 21] (l_loss: 0.04636) (t_loss: 0.07492) (accu: 0.9759)
[epoch : 22] (l_loss: 0.04520) (t_loss: 0.07441) (accu: 0.9771)
[epoch : 23] (l_loss: 0.04565) (t_loss: 0.07687) (accu: 0.9758)
[epoch : 24] (l_loss: 0.04556) (t_loss: 0.07345) (accu: 0.9774)
[epoch : 25] (l_loss: 0.04546) (t_loss: 0.07646) (accu: 0.9770)
[epoch : 26] (l_loss: 0.04555) (t_loss: 0.07295) (accu: 0.9768)
[epoch : 27] (l_loss: 0.04534) (t_loss: 0.07087) (accu: 0.9783)
[epoch : 28] (l_loss: 0.04564) (t_loss: 0.07350) (accu: 0.9772)
[epoch : 29] (l_loss: 0.04516) (t_loss: 0.07264) (accu: 0.9776)
[epoch : 30] (l_loss: 0.04527) (t_loss: 0.07271) (accu: 0.9762)
[epoch : 31] (l_loss: 0.04542) (t_loss: 0.07248) (accu: 0.9773)
[epoch : 32] (l_loss: 0.04524) (t_loss: 0.07336) (accu: 0.9770)
[epoch : 33] (l_loss: 0.04491) (t_loss: 0.07367) (accu: 0.9783)
[epoch : 34] (l_loss: 0.04459) (t_loss: 0.07559) (accu: 0.9766)
[epoch : 35] (l_loss: 0.04562) (t_loss: 0.07446) (accu: 0.9777)
[epoch : 36] (l_loss: 0.04537) (t_loss: 0.07489) (accu: 0.9766)
[epoch : 37] (l_loss: 0.04478) (t_loss: 0.07741) (accu: 0.9755)
[epoch : 38] (l_loss: 0.04500) (t_loss: 0.07752) (accu: 0.9753)
[epoch : 39] (l_loss: 0.04497) (t_loss: 0.07506) (accu: 0.9771)
[epoch : 40] (l_loss: 0.04453) (t_loss: 0.07077) (accu: 0.9795)
[epoch : 41] (l_loss: 0.04448) (t_loss: 0.07119) (accu: 0.9770)
[epoch : 42] (l_loss: 0.04393) (t_loss: 0.07082) (accu: 0.9766)
[epoch : 43] (l_loss: 0.04453) (t_loss: 0.07118) (accu: 0.9790)
[epoch : 44] (l_loss: 0.04399) (t_loss: 0.07136) (accu: 0.9780)
[epoch : 45] (l_loss: 0.04429) (t_loss: 0.07185) (accu: 0.9772)
[epoch : 46] (l_loss: 0.04384) (t_loss: 0.07008) (accu: 0.9777)
[epoch : 47] (l_loss: 0.04417) (t_loss: 0.07282) (accu: 0.9777)
[epoch : 48] (l_loss: 0.04391) (t_loss: 0.07150) (accu: 0.9771)
[epoch : 49] (l_loss: 0.04405) (t_loss: 0.07232) (accu: 0.9769)
[epoch : 50] (l_loss: 0.04359) (t_loss: 0.07306) (accu: 0.9780)
Finish! (Best accu: 0.9795) (Time taken(sec) : 746.81) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (7649 | 258551)          2.87
fc1.weight   :       235200 (6620 | 228580)          2.81
fc2.weight   :        30000 (844 | 29156)            2.81
fcout.weight :          1000 (185 | 815)            18.50
------------------------------------------------------------

Learning start! [Prune_iter : (17/21), Remaining weight : 2.87 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29833) (accu: 0.0974)
[epoch : 1] (l_loss: 0.40188) (t_loss: 0.15010) (accu: 0.9566)
[epoch : 2] (l_loss: 0.12619) (t_loss: 0.10727) (accu: 0.9678)
[epoch : 3] (l_loss: 0.09508) (t_loss: 0.09222) (accu: 0.9722)
[epoch : 4] (l_loss: 0.08136) (t_loss: 0.08851) (accu: 0.9721)
[epoch : 5] (l_loss: 0.07274) (t_loss: 0.08481) (accu: 0.9740)
[epoch : 6] (l_loss: 0.06539) (t_loss: 0.08014) (accu: 0.9752)
[epoch : 7] (l_loss: 0.06163) (t_loss: 0.08057) (accu: 0.9753)
[epoch : 8] (l_loss: 0.05750) (t_loss: 0.07781) (accu: 0.9759)
[epoch : 9] (l_loss: 0.05439) (t_loss: 0.07389) (accu: 0.9773)
[epoch : 10] (l_loss: 0.05164) (t_loss: 0.07396) (accu: 0.9763)
[epoch : 11] (l_loss: 0.04998) (t_loss: 0.07388) (accu: 0.9765)
[epoch : 12] (l_loss: 0.04880) (t_loss: 0.07620) (accu: 0.9764)
[epoch : 13] (l_loss: 0.04744) (t_loss: 0.07507) (accu: 0.9770)
[epoch : 14] (l_loss: 0.04791) (t_loss: 0.07255) (accu: 0.9784)
[epoch : 15] (l_loss: 0.04674) (t_loss: 0.07740) (accu: 0.9757)
[epoch : 16] (l_loss: 0.04664) (t_loss: 0.07643) (accu: 0.9762)
[epoch : 17] (l_loss: 0.04602) (t_loss: 0.07479) (accu: 0.9758)
[epoch : 18] (l_loss: 0.04639) (t_loss: 0.07192) (accu: 0.9762)
[epoch : 19] (l_loss: 0.04640) (t_loss: 0.07469) (accu: 0.9760)
[epoch : 20] (l_loss: 0.04611) (t_loss: 0.07574) (accu: 0.9773)
[epoch : 21] (l_loss: 0.04588) (t_loss: 0.07148) (accu: 0.9780)
[epoch : 22] (l_loss: 0.04557) (t_loss: 0.07298) (accu: 0.9786)
[epoch : 23] (l_loss: 0.04603) (t_loss: 0.07189) (accu: 0.9780)
[epoch : 24] (l_loss: 0.04578) (t_loss: 0.07592) (accu: 0.9756)
[epoch : 25] (l_loss: 0.04540) (t_loss: 0.07349) (accu: 0.9774)
[epoch : 26] (l_loss: 0.04545) (t_loss: 0.07257) (accu: 0.9775)
[epoch : 27] (l_loss: 0.04526) (t_loss: 0.07452) (accu: 0.9770)
[epoch : 28] (l_loss: 0.04533) (t_loss: 0.07451) (accu: 0.9762)
[epoch : 29] (l_loss: 0.04527) (t_loss: 0.07333) (accu: 0.9754)
[epoch : 30] (l_loss: 0.04554) (t_loss: 0.07341) (accu: 0.9763)
[epoch : 31] (l_loss: 0.04403) (t_loss: 0.07405) (accu: 0.9758)
[epoch : 32] (l_loss: 0.04436) (t_loss: 0.07341) (accu: 0.9775)
[epoch : 33] (l_loss: 0.04427) (t_loss: 0.07129) (accu: 0.9772)
[epoch : 34] (l_loss: 0.04358) (t_loss: 0.07457) (accu: 0.9774)
[epoch : 35] (l_loss: 0.04411) (t_loss: 0.07368) (accu: 0.9764)
[epoch : 36] (l_loss: 0.04411) (t_loss: 0.07369) (accu: 0.9776)
[epoch : 37] (l_loss: 0.04435) (t_loss: 0.07255) (accu: 0.9771)
[epoch : 38] (l_loss: 0.04412) (t_loss: 0.07334) (accu: 0.9777)
[epoch : 39] (l_loss: 0.04384) (t_loss: 0.07163) (accu: 0.9777)
[epoch : 40] (l_loss: 0.04390) (t_loss: 0.07237) (accu: 0.9785)
[epoch : 41] (l_loss: 0.04392) (t_loss: 0.07140) (accu: 0.9778)
[epoch : 42] (l_loss: 0.04357) (t_loss: 0.07186) (accu: 0.9779)
[epoch : 43] (l_loss: 0.04416) (t_loss: 0.06995) (accu: 0.9773)
[epoch : 44] (l_loss: 0.04403) (t_loss: 0.07344) (accu: 0.9762)
[epoch : 45] (l_loss: 0.04358) (t_loss: 0.07440) (accu: 0.9769)
[epoch : 46] (l_loss: 0.04374) (t_loss: 0.06912) (accu: 0.9777)
[epoch : 47] (l_loss: 0.04362) (t_loss: 0.07530) (accu: 0.9760)
[epoch : 48] (l_loss: 0.04394) (t_loss: 0.07069) (accu: 0.9788)
[epoch : 49] (l_loss: 0.04414) (t_loss: 0.07136) (accu: 0.9785)
[epoch : 50] (l_loss: 0.04383) (t_loss: 0.07232) (accu: 0.9768)
Finish! (Best accu: 0.9788) (Time taken(sec) : 771.49) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (6139 | 260061)          2.31
fc1.weight   :       235200 (5296 | 229904)          2.25
fc2.weight   :        30000 (676 | 29324)            2.25
fcout.weight :          1000 (167 | 833)            16.70
------------------------------------------------------------

Learning start! [Prune_iter : (18/21), Remaining weight : 2.31 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29827) (accu: 0.0974)
[epoch : 1] (l_loss: 0.40105) (t_loss: 0.15274) (accu: 0.9566)
[epoch : 2] (l_loss: 0.12551) (t_loss: 0.10859) (accu: 0.9671)
[epoch : 3] (l_loss: 0.09425) (t_loss: 0.09171) (accu: 0.9722)
[epoch : 4] (l_loss: 0.08022) (t_loss: 0.08386) (accu: 0.9752)
[epoch : 5] (l_loss: 0.07227) (t_loss: 0.08262) (accu: 0.9749)
[epoch : 6] (l_loss: 0.06565) (t_loss: 0.07846) (accu: 0.9758)
[epoch : 7] (l_loss: 0.06112) (t_loss: 0.07680) (accu: 0.9764)
[epoch : 8] (l_loss: 0.05766) (t_loss: 0.07658) (accu: 0.9773)
[epoch : 9] (l_loss: 0.05385) (t_loss: 0.07832) (accu: 0.9761)
[epoch : 10] (l_loss: 0.05155) (t_loss: 0.07672) (accu: 0.9770)
[epoch : 11] (l_loss: 0.04982) (t_loss: 0.07354) (accu: 0.9765)
[epoch : 12] (l_loss: 0.04908) (t_loss: 0.07406) (accu: 0.9775)
[epoch : 13] (l_loss: 0.04821) (t_loss: 0.07268) (accu: 0.9778)
[epoch : 14] (l_loss: 0.04748) (t_loss: 0.07511) (accu: 0.9758)
[epoch : 15] (l_loss: 0.04711) (t_loss: 0.07449) (accu: 0.9765)
[epoch : 16] (l_loss: 0.04687) (t_loss: 0.07671) (accu: 0.9761)
[epoch : 17] (l_loss: 0.04685) (t_loss: 0.07249) (accu: 0.9773)
[epoch : 18] (l_loss: 0.04621) (t_loss: 0.07499) (accu: 0.9773)
[epoch : 19] (l_loss: 0.04629) (t_loss: 0.07322) (accu: 0.9768)
[epoch : 20] (l_loss: 0.04595) (t_loss: 0.07303) (accu: 0.9774)
[epoch : 21] (l_loss: 0.04558) (t_loss: 0.07271) (accu: 0.9779)
[epoch : 22] (l_loss: 0.04607) (t_loss: 0.07228) (accu: 0.9784)
[epoch : 23] (l_loss: 0.04547) (t_loss: 0.07330) (accu: 0.9767)
[epoch : 24] (l_loss: 0.04571) (t_loss: 0.07268) (accu: 0.9761)
[epoch : 25] (l_loss: 0.04555) (t_loss: 0.07707) (accu: 0.9765)
[epoch : 26] (l_loss: 0.04558) (t_loss: 0.07283) (accu: 0.9768)
[epoch : 27] (l_loss: 0.04546) (t_loss: 0.07464) (accu: 0.9773)
[epoch : 28] (l_loss: 0.04523) (t_loss: 0.07234) (accu: 0.9771)
[epoch : 29] (l_loss: 0.04554) (t_loss: 0.07186) (accu: 0.9778)
[epoch : 30] (l_loss: 0.04517) (t_loss: 0.07541) (accu: 0.9765)
[epoch : 31] (l_loss: 0.04518) (t_loss: 0.07373) (accu: 0.9747)
[epoch : 32] (l_loss: 0.04508) (t_loss: 0.07776) (accu: 0.9755)
[epoch : 33] (l_loss: 0.04536) (t_loss: 0.07447) (accu: 0.9774)
[epoch : 34] (l_loss: 0.04486) (t_loss: 0.07227) (accu: 0.9785)
[epoch : 35] (l_loss: 0.04525) (t_loss: 0.07058) (accu: 0.9784)
[epoch : 36] (l_loss: 0.04524) (t_loss: 0.07336) (accu: 0.9778)
[epoch : 37] (l_loss: 0.04489) (t_loss: 0.07230) (accu: 0.9775)
[epoch : 38] (l_loss: 0.04428) (t_loss: 0.07207) (accu: 0.9768)
[epoch : 39] (l_loss: 0.04418) (t_loss: 0.06975) (accu: 0.9782)
[epoch : 40] (l_loss: 0.04406) (t_loss: 0.07034) (accu: 0.9790)
[epoch : 41] (l_loss: 0.04380) (t_loss: 0.06958) (accu: 0.9781)
[epoch : 42] (l_loss: 0.04393) (t_loss: 0.07567) (accu: 0.9764)
[epoch : 43] (l_loss: 0.04373) (t_loss: 0.07337) (accu: 0.9771)
[epoch : 44] (l_loss: 0.04396) (t_loss: 0.07071) (accu: 0.9786)
[epoch : 45] (l_loss: 0.04420) (t_loss: 0.07075) (accu: 0.9775)
[epoch : 46] (l_loss: 0.04408) (t_loss: 0.06982) (accu: 0.9788)
[epoch : 47] (l_loss: 0.04394) (t_loss: 0.07153) (accu: 0.9774)
[epoch : 48] (l_loss: 0.04390) (t_loss: 0.06996) (accu: 0.9779)
[epoch : 49] (l_loss: 0.04425) (t_loss: 0.07316) (accu: 0.9763)
[epoch : 50] (l_loss: 0.04396) (t_loss: 0.07072) (accu: 0.9777)
Finish! (Best accu: 0.9790) (Time taken(sec) : 763.43) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (4927 | 261273)          1.85
fc1.weight   :       235200 (4237 | 230963)          1.80
fc2.weight   :        30000 (540 | 29460)            1.80
fcout.weight :          1000 (150 | 850)            15.00
------------------------------------------------------------

Learning start! [Prune_iter : (19/21), Remaining weight : 1.85 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29736) (accu: 0.0974)
[epoch : 1] (l_loss: 0.40173) (t_loss: 0.15312) (accu: 0.9547)
[epoch : 2] (l_loss: 0.12631) (t_loss: 0.11180) (accu: 0.9666)
[epoch : 3] (l_loss: 0.09466) (t_loss: 0.09592) (accu: 0.9702)
[epoch : 4] (l_loss: 0.07938) (t_loss: 0.08621) (accu: 0.9746)
[epoch : 5] (l_loss: 0.07046) (t_loss: 0.08389) (accu: 0.9743)
[epoch : 6] (l_loss: 0.06422) (t_loss: 0.08159) (accu: 0.9745)
[epoch : 7] (l_loss: 0.06001) (t_loss: 0.07715) (accu: 0.9768)
[epoch : 8] (l_loss: 0.05651) (t_loss: 0.07558) (accu: 0.9766)
[epoch : 9] (l_loss: 0.05348) (t_loss: 0.07401) (accu: 0.9764)
[epoch : 10] (l_loss: 0.05100) (t_loss: 0.07546) (accu: 0.9758)
[epoch : 11] (l_loss: 0.04925) (t_loss: 0.07608) (accu: 0.9768)
[epoch : 12] (l_loss: 0.04777) (t_loss: 0.07273) (accu: 0.9777)
[epoch : 13] (l_loss: 0.04726) (t_loss: 0.07556) (accu: 0.9759)
[epoch : 14] (l_loss: 0.04660) (t_loss: 0.07124) (accu: 0.9786)
[epoch : 15] (l_loss: 0.04628) (t_loss: 0.07452) (accu: 0.9769)
[epoch : 16] (l_loss: 0.04609) (t_loss: 0.07372) (accu: 0.9775)
[epoch : 17] (l_loss: 0.04593) (t_loss: 0.07503) (accu: 0.9759)
[epoch : 18] (l_loss: 0.04584) (t_loss: 0.07296) (accu: 0.9760)
[epoch : 19] (l_loss: 0.04561) (t_loss: 0.07081) (accu: 0.9770)
[epoch : 20] (l_loss: 0.04515) (t_loss: 0.07371) (accu: 0.9775)
[epoch : 21] (l_loss: 0.04500) (t_loss: 0.07300) (accu: 0.9760)
[epoch : 22] (l_loss: 0.04498) (t_loss: 0.07103) (accu: 0.9785)
[epoch : 23] (l_loss: 0.04455) (t_loss: 0.07277) (accu: 0.9783)
[epoch : 24] (l_loss: 0.04469) (t_loss: 0.07352) (accu: 0.9762)
[epoch : 25] (l_loss: 0.04457) (t_loss: 0.07708) (accu: 0.9765)
[epoch : 26] (l_loss: 0.04537) (t_loss: 0.07011) (accu: 0.9775)
[epoch : 27] (l_loss: 0.04510) (t_loss: 0.06949) (accu: 0.9786)
[epoch : 28] (l_loss: 0.04514) (t_loss: 0.07094) (accu: 0.9775)
[epoch : 29] (l_loss: 0.04515) (t_loss: 0.07108) (accu: 0.9767)
[epoch : 30] (l_loss: 0.04458) (t_loss: 0.07411) (accu: 0.9772)
[epoch : 31] (l_loss: 0.04466) (t_loss: 0.07459) (accu: 0.9754)
[epoch : 32] (l_loss: 0.04450) (t_loss: 0.07388) (accu: 0.9784)
[epoch : 33] (l_loss: 0.04423) (t_loss: 0.07319) (accu: 0.9770)
[epoch : 34] (l_loss: 0.04458) (t_loss: 0.07093) (accu: 0.9777)
[epoch : 35] (l_loss: 0.04385) (t_loss: 0.06945) (accu: 0.9783)
[epoch : 36] (l_loss: 0.04394) (t_loss: 0.07091) (accu: 0.9778)
[epoch : 37] (l_loss: 0.04402) (t_loss: 0.07249) (accu: 0.9780)
[epoch : 38] (l_loss: 0.04361) (t_loss: 0.07066) (accu: 0.9779)
[epoch : 39] (l_loss: 0.04391) (t_loss: 0.07365) (accu: 0.9765)
[epoch : 40] (l_loss: 0.04391) (t_loss: 0.07448) (accu: 0.9778)
[epoch : 41] (l_loss: 0.04413) (t_loss: 0.07454) (accu: 0.9778)
[epoch : 42] (l_loss: 0.04358) (t_loss: 0.07051) (accu: 0.9784)
[epoch : 43] (l_loss: 0.04353) (t_loss: 0.07617) (accu: 0.9761)
[epoch : 44] (l_loss: 0.04390) (t_loss: 0.07198) (accu: 0.9770)
[epoch : 45] (l_loss: 0.04392) (t_loss: 0.07173) (accu: 0.9785)
[epoch : 46] (l_loss: 0.04330) (t_loss: 0.07127) (accu: 0.9784)
[epoch : 47] (l_loss: 0.04380) (t_loss: 0.07121) (accu: 0.9785)
[epoch : 48] (l_loss: 0.04307) (t_loss: 0.07211) (accu: 0.9781)
[epoch : 49] (l_loss: 0.04388) (t_loss: 0.06924) (accu: 0.9777)
[epoch : 50] (l_loss: 0.04409) (t_loss: 0.07099) (accu: 0.9782)
Finish! (Best accu: 0.9786) (Time taken(sec) : 769.06) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (3957 | 262243)          1.49
fc1.weight   :       235200 (3390 | 231810)          1.44
fc2.weight   :        30000 (432 | 29568)            1.44
fcout.weight :          1000 (135 | 865)            13.50
------------------------------------------------------------

Learning start! [Prune_iter : (20/21), Remaining weight : 1.49 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29722) (accu: 0.0974)
[epoch : 1] (l_loss: 0.39978) (t_loss: 0.15213) (accu: 0.9550)
[epoch : 2] (l_loss: 0.12776) (t_loss: 0.11044) (accu: 0.9680)
[epoch : 3] (l_loss: 0.09734) (t_loss: 0.09471) (accu: 0.9722)
[epoch : 4] (l_loss: 0.08230) (t_loss: 0.08968) (accu: 0.9732)
[epoch : 5] (l_loss: 0.07436) (t_loss: 0.08333) (accu: 0.9749)
[epoch : 6] (l_loss: 0.06808) (t_loss: 0.08139) (accu: 0.9751)
[epoch : 7] (l_loss: 0.06334) (t_loss: 0.07883) (accu: 0.9759)
[epoch : 8] (l_loss: 0.05945) (t_loss: 0.07841) (accu: 0.9771)
[epoch : 9] (l_loss: 0.05583) (t_loss: 0.07955) (accu: 0.9762)
[epoch : 10] (l_loss: 0.05285) (t_loss: 0.07194) (accu: 0.9780)
[epoch : 11] (l_loss: 0.05036) (t_loss: 0.07482) (accu: 0.9777)
[epoch : 12] (l_loss: 0.04865) (t_loss: 0.07304) (accu: 0.9759)
[epoch : 13] (l_loss: 0.04736) (t_loss: 0.07389) (accu: 0.9779)
[epoch : 14] (l_loss: 0.04620) (t_loss: 0.07215) (accu: 0.9769)
[epoch : 15] (l_loss: 0.04623) (t_loss: 0.07049) (accu: 0.9787)
[epoch : 16] (l_loss: 0.04570) (t_loss: 0.07274) (accu: 0.9794)
[epoch : 17] (l_loss: 0.04512) (t_loss: 0.07160) (accu: 0.9791)
[epoch : 18] (l_loss: 0.04504) (t_loss: 0.07138) (accu: 0.9778)
[epoch : 19] (l_loss: 0.04437) (t_loss: 0.07298) (accu: 0.9778)
[epoch : 20] (l_loss: 0.04454) (t_loss: 0.07009) (accu: 0.9776)
[epoch : 21] (l_loss: 0.04442) (t_loss: 0.07513) (accu: 0.9765)
[epoch : 22] (l_loss: 0.04437) (t_loss: 0.06992) (accu: 0.9781)
[epoch : 23] (l_loss: 0.04373) (t_loss: 0.07138) (accu: 0.9779)
[epoch : 24] (l_loss: 0.04383) (t_loss: 0.07193) (accu: 0.9769)
[epoch : 25] (l_loss: 0.04420) (t_loss: 0.07368) (accu: 0.9762)
[epoch : 26] (l_loss: 0.04399) (t_loss: 0.06820) (accu: 0.9796)
[epoch : 27] (l_loss: 0.04393) (t_loss: 0.06978) (accu: 0.9789)
[epoch : 28] (l_loss: 0.04381) (t_loss: 0.07216) (accu: 0.9785)
[epoch : 29] (l_loss: 0.04376) (t_loss: 0.07027) (accu: 0.9792)
[epoch : 30] (l_loss: 0.04393) (t_loss: 0.07160) (accu: 0.9789)
[epoch : 31] (l_loss: 0.04371) (t_loss: 0.07712) (accu: 0.9759)
[epoch : 32] (l_loss: 0.04375) (t_loss: 0.07070) (accu: 0.9796)
[epoch : 33] (l_loss: 0.04364) (t_loss: 0.07142) (accu: 0.9785)
[epoch : 34] (l_loss: 0.04345) (t_loss: 0.07348) (accu: 0.9772)
[epoch : 35] (l_loss: 0.04390) (t_loss: 0.07222) (accu: 0.9779)
[epoch : 36] (l_loss: 0.04363) (t_loss: 0.07172) (accu: 0.9772)
[epoch : 37] (l_loss: 0.04398) (t_loss: 0.07223) (accu: 0.9766)
[epoch : 38] (l_loss: 0.04382) (t_loss: 0.07102) (accu: 0.9781)
[epoch : 39] (l_loss: 0.04382) (t_loss: 0.07034) (accu: 0.9780)
[epoch : 40] (l_loss: 0.04397) (t_loss: 0.07161) (accu: 0.9780)
[epoch : 41] (l_loss: 0.04388) (t_loss: 0.07139) (accu: 0.9778)
[epoch : 42] (l_loss: 0.04405) (t_loss: 0.07328) (accu: 0.9783)
[epoch : 43] (l_loss: 0.04367) (t_loss: 0.07308) (accu: 0.9770)
[epoch : 44] (l_loss: 0.04345) (t_loss: 0.07405) (accu: 0.9772)
[epoch : 45] (l_loss: 0.04379) (t_loss: 0.07373) (accu: 0.9766)
[epoch : 46] (l_loss: 0.04352) (t_loss: 0.07459) (accu: 0.9771)
[epoch : 47] (l_loss: 0.04394) (t_loss: 0.07264) (accu: 0.9776)
[epoch : 48] (l_loss: 0.04379) (t_loss: 0.07384) (accu: 0.9772)
[epoch : 49] (l_loss: 0.04392) (t_loss: 0.06974) (accu: 0.9781)
[epoch : 50] (l_loss: 0.04383) (t_loss: 0.06967) (accu: 0.9770)
Finish! (Best accu: 0.9796) (Time taken(sec) : 770.28) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (3180 | 263020)          1.19
fc1.weight   :       235200 (2712 | 232488)          1.15
fc2.weight   :        30000 (346 | 29654)            1.15
fcout.weight :          1000 (122 | 878)            12.20
------------------------------------------------------------

Learning start! [Prune_iter : (21/21), Remaining weight : 1.19 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29794) (accu: 0.0974)
[epoch : 1] (l_loss: 0.39949) (t_loss: 0.15433) (accu: 0.9556)
[epoch : 2] (l_loss: 0.12736) (t_loss: 0.11206) (accu: 0.9681)
[epoch : 3] (l_loss: 0.09565) (t_loss: 0.09556) (accu: 0.9714)
[epoch : 4] (l_loss: 0.08053) (t_loss: 0.09472) (accu: 0.9712)
[epoch : 5] (l_loss: 0.07241) (t_loss: 0.08639) (accu: 0.9744)
[epoch : 6] (l_loss: 0.06619) (t_loss: 0.08374) (accu: 0.9750)
[epoch : 7] (l_loss: 0.06161) (t_loss: 0.07857) (accu: 0.9774)
[epoch : 8] (l_loss: 0.05862) (t_loss: 0.08099) (accu: 0.9749)
[epoch : 9] (l_loss: 0.05567) (t_loss: 0.07359) (accu: 0.9792)
[epoch : 10] (l_loss: 0.05263) (t_loss: 0.07432) (accu: 0.9765)
[epoch : 11] (l_loss: 0.05109) (t_loss: 0.07289) (accu: 0.9763)
[epoch : 12] (l_loss: 0.04959) (t_loss: 0.07715) (accu: 0.9763)
[epoch : 13] (l_loss: 0.04866) (t_loss: 0.07564) (accu: 0.9762)
[epoch : 14] (l_loss: 0.04870) (t_loss: 0.07200) (accu: 0.9774)
[epoch : 15] (l_loss: 0.04780) (t_loss: 0.07510) (accu: 0.9776)
[epoch : 16] (l_loss: 0.04732) (t_loss: 0.07507) (accu: 0.9760)
[epoch : 17] (l_loss: 0.04721) (t_loss: 0.07278) (accu: 0.9769)
[epoch : 18] (l_loss: 0.04667) (t_loss: 0.07522) (accu: 0.9758)
[epoch : 19] (l_loss: 0.04683) (t_loss: 0.07696) (accu: 0.9761)
[epoch : 20] (l_loss: 0.04673) (t_loss: 0.07311) (accu: 0.9776)
[epoch : 21] (l_loss: 0.04601) (t_loss: 0.07562) (accu: 0.9759)
[epoch : 22] (l_loss: 0.04648) (t_loss: 0.07191) (accu: 0.9774)
[epoch : 23] (l_loss: 0.04599) (t_loss: 0.07446) (accu: 0.9766)
[epoch : 24] (l_loss: 0.04577) (t_loss: 0.07247) (accu: 0.9775)
[epoch : 25] (l_loss: 0.04583) (t_loss: 0.07109) (accu: 0.9791)
[epoch : 26] (l_loss: 0.04563) (t_loss: 0.07245) (accu: 0.9781)
[epoch : 27] (l_loss: 0.04474) (t_loss: 0.07424) (accu: 0.9778)
[epoch : 28] (l_loss: 0.04540) (t_loss: 0.07759) (accu: 0.9755)
[epoch : 29] (l_loss: 0.04525) (t_loss: 0.07321) (accu: 0.9778)
[epoch : 30] (l_loss: 0.04483) (t_loss: 0.07110) (accu: 0.9782)
[epoch : 31] (l_loss: 0.04512) (t_loss: 0.07522) (accu: 0.9764)
[epoch : 32] (l_loss: 0.04526) (t_loss: 0.07285) (accu: 0.9774)
[epoch : 33] (l_loss: 0.04509) (t_loss: 0.07599) (accu: 0.9762)
[epoch : 34] (l_loss: 0.04494) (t_loss: 0.07516) (accu: 0.9766)
[epoch : 35] (l_loss: 0.04500) (t_loss: 0.07424) (accu: 0.9775)
[epoch : 36] (l_loss: 0.04501) (t_loss: 0.07428) (accu: 0.9767)
[epoch : 37] (l_loss: 0.04442) (t_loss: 0.07293) (accu: 0.9774)
[epoch : 38] (l_loss: 0.04467) (t_loss: 0.07420) (accu: 0.9777)
[epoch : 39] (l_loss: 0.04383) (t_loss: 0.07014) (accu: 0.9780)
[epoch : 40] (l_loss: 0.04429) (t_loss: 0.07294) (accu: 0.9770)
[epoch : 41] (l_loss: 0.04415) (t_loss: 0.07475) (accu: 0.9783)
[epoch : 42] (l_loss: 0.04427) (t_loss: 0.07116) (accu: 0.9772)
[epoch : 43] (l_loss: 0.04444) (t_loss: 0.07143) (accu: 0.9779)
[epoch : 44] (l_loss: 0.04420) (t_loss: 0.07302) (accu: 0.9769)
[epoch : 45] (l_loss: 0.04376) (t_loss: 0.06996) (accu: 0.9781)
[epoch : 46] (l_loss: 0.04419) (t_loss: 0.07133) (accu: 0.9771)
[epoch : 47] (l_loss: 0.04429) (t_loss: 0.07193) (accu: 0.9776)
[epoch : 48] (l_loss: 0.04418) (t_loss: 0.07425) (accu: 0.9762)
[epoch : 49] (l_loss: 0.04408) (t_loss: 0.07146) (accu: 0.9771)
[epoch : 50] (l_loss: 0.04396) (t_loss: 0.06959) (accu: 0.9790)
Finish! (Best accu: 0.9792) (Time taken(sec) : 760.50) 


Maximum accuracy per weight remaining
Remaining weight 100.0 %  Epoch 10 Accu 0.9782
Remaining weight 80.04 %  Epoch 40 Accu 0.9794
Remaining weight 64.06 %  Epoch 36 Accu 0.9792
Remaining weight 51.28 %  Epoch 12 Accu 0.9782
Remaining weight 41.05 %  Epoch 27 Accu 0.9786
Remaining weight 32.87 %  Epoch 17 Accu 0.9783
Remaining weight 26.32 %  Epoch 47 Accu 0.9792
Remaining weight 21.07 %  Epoch 33 Accu 0.9799
Remaining weight 16.88 %  Epoch 29 Accu 0.9794
Remaining weight 13.52 %  Epoch 41 Accu 0.9797
Remaining weight 10.83 %  Epoch 44 Accu 0.9794
Remaining weight 8.68 %  Epoch 48 Accu 0.9791
Remaining weight 6.95 %  Epoch 37 Accu 0.9791
Remaining weight 5.57 %  Epoch 49 Accu 0.9792
Remaining weight 4.47 %  Epoch 42 Accu 0.9788
Remaining weight 3.58 %  Epoch 39 Accu 0.9795
Remaining weight 2.87 %  Epoch 47 Accu 0.9788
Remaining weight 2.31 %  Epoch 39 Accu 0.9790
Remaining weight 1.85 %  Epoch 26 Accu 0.9786
Remaining weight 1.49 %  Epoch 31 Accu 0.9796
Remaining weight 1.19 %  Epoch 8 Accu 0.9792
===================================================================== 

Test_Iter (5/5)
------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :        266200 (266200 | 0)          100.00
fc1.weight   :        235200 (235200 | 0)          100.00
fc2.weight   :         30000 (30000 | 0)           100.00
fcout.weight :          1000 (1000 | 0)            100.00
------------------------------------------------------------

Learning start! [Prune_iter : (1/21), Remaining weight : 100.0 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.83489) (accu: 0.0861)
[epoch : 1] (l_loss: 0.45102) (t_loss: 0.14520) (accu: 0.9574)
[epoch : 2] (l_loss: 0.12137) (t_loss: 0.10700) (accu: 0.9675)
[epoch : 3] (l_loss: 0.09220) (t_loss: 0.09563) (accu: 0.9711)
[epoch : 4] (l_loss: 0.07920) (t_loss: 0.09024) (accu: 0.9720)
[epoch : 5] (l_loss: 0.07090) (t_loss: 0.08329) (accu: 0.9738)
[epoch : 6] (l_loss: 0.06458) (t_loss: 0.07937) (accu: 0.9744)
[epoch : 7] (l_loss: 0.05991) (t_loss: 0.07644) (accu: 0.9760)
[epoch : 8] (l_loss: 0.05547) (t_loss: 0.07379) (accu: 0.9757)
[epoch : 9] (l_loss: 0.05188) (t_loss: 0.07466) (accu: 0.9769)
[epoch : 10] (l_loss: 0.05050) (t_loss: 0.07581) (accu: 0.9772)
[epoch : 11] (l_loss: 0.04920) (t_loss: 0.07030) (accu: 0.9788)
[epoch : 12] (l_loss: 0.04793) (t_loss: 0.07209) (accu: 0.9768)
[epoch : 13] (l_loss: 0.04740) (t_loss: 0.07141) (accu: 0.9775)
[epoch : 14] (l_loss: 0.04679) (t_loss: 0.07222) (accu: 0.9766)
[epoch : 15] (l_loss: 0.04662) (t_loss: 0.07277) (accu: 0.9767)
[epoch : 16] (l_loss: 0.04617) (t_loss: 0.07184) (accu: 0.9775)
[epoch : 17] (l_loss: 0.04566) (t_loss: 0.07037) (accu: 0.9774)
[epoch : 18] (l_loss: 0.04542) (t_loss: 0.07061) (accu: 0.9786)
[epoch : 19] (l_loss: 0.04548) (t_loss: 0.07241) (accu: 0.9777)
[epoch : 20] (l_loss: 0.04517) (t_loss: 0.07208) (accu: 0.9772)
[epoch : 21] (l_loss: 0.04505) (t_loss: 0.07242) (accu: 0.9766)
[epoch : 22] (l_loss: 0.04481) (t_loss: 0.07326) (accu: 0.9761)
[epoch : 23] (l_loss: 0.04518) (t_loss: 0.07385) (accu: 0.9764)
[epoch : 24] (l_loss: 0.04521) (t_loss: 0.07231) (accu: 0.9766)
[epoch : 25] (l_loss: 0.04489) (t_loss: 0.07070) (accu: 0.9774)
[epoch : 26] (l_loss: 0.04489) (t_loss: 0.07261) (accu: 0.9784)
[epoch : 27] (l_loss: 0.04518) (t_loss: 0.07106) (accu: 0.9775)
[epoch : 28] (l_loss: 0.04511) (t_loss: 0.07305) (accu: 0.9767)
[epoch : 29] (l_loss: 0.04492) (t_loss: 0.07387) (accu: 0.9766)
[epoch : 30] (l_loss: 0.04429) (t_loss: 0.07033) (accu: 0.9766)
[epoch : 31] (l_loss: 0.04464) (t_loss: 0.07223) (accu: 0.9776)
[epoch : 32] (l_loss: 0.04466) (t_loss: 0.07146) (accu: 0.9783)
[epoch : 33] (l_loss: 0.04462) (t_loss: 0.07259) (accu: 0.9769)
[epoch : 34] (l_loss: 0.04506) (t_loss: 0.07096) (accu: 0.9767)
[epoch : 35] (l_loss: 0.04402) (t_loss: 0.07323) (accu: 0.9772)
[epoch : 36] (l_loss: 0.04458) (t_loss: 0.07257) (accu: 0.9768)
[epoch : 37] (l_loss: 0.04480) (t_loss: 0.07416) (accu: 0.9769)
[epoch : 38] (l_loss: 0.04448) (t_loss: 0.07618) (accu: 0.9774)
[epoch : 39] (l_loss: 0.04463) (t_loss: 0.07480) (accu: 0.9776)
[epoch : 40] (l_loss: 0.04462) (t_loss: 0.07072) (accu: 0.9766)
[epoch : 41] (l_loss: 0.04459) (t_loss: 0.07320) (accu: 0.9769)
[epoch : 42] (l_loss: 0.04454) (t_loss: 0.07200) (accu: 0.9779)
[epoch : 43] (l_loss: 0.04473) (t_loss: 0.07269) (accu: 0.9774)
[epoch : 44] (l_loss: 0.04444) (t_loss: 0.07210) (accu: 0.9761)
[epoch : 45] (l_loss: 0.04435) (t_loss: 0.07532) (accu: 0.9762)
[epoch : 46] (l_loss: 0.04440) (t_loss: 0.07701) (accu: 0.9757)
[epoch : 47] (l_loss: 0.04512) (t_loss: 0.07300) (accu: 0.9765)
[epoch : 48] (l_loss: 0.04448) (t_loss: 0.07283) (accu: 0.9767)
[epoch : 49] (l_loss: 0.04452) (t_loss: 0.07144) (accu: 0.9771)
[epoch : 50] (l_loss: 0.04471) (t_loss: 0.07339) (accu: 0.9766)
Finish! (Best accu: 0.9788) (Time taken(sec) : 753.99) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (213060 | 53140)         80.04
fc1.weight   :      235200 (188160 | 47040)         80.00
fc2.weight   :        30000 (24000 | 6000)          80.00
fcout.weight :          1000 (900 | 100)            90.00
------------------------------------------------------------

Learning start! [Prune_iter : (2/21), Remaining weight : 80.04 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.58429) (accu: 0.1110)
[epoch : 1] (l_loss: 0.43796) (t_loss: 0.14929) (accu: 0.9577)
[epoch : 2] (l_loss: 0.12451) (t_loss: 0.10595) (accu: 0.9679)
[epoch : 3] (l_loss: 0.09308) (t_loss: 0.09566) (accu: 0.9708)
[epoch : 4] (l_loss: 0.07951) (t_loss: 0.08594) (accu: 0.9736)
[epoch : 5] (l_loss: 0.07184) (t_loss: 0.08177) (accu: 0.9752)
[epoch : 6] (l_loss: 0.06599) (t_loss: 0.07925) (accu: 0.9752)
[epoch : 7] (l_loss: 0.06216) (t_loss: 0.07578) (accu: 0.9767)
[epoch : 8] (l_loss: 0.05877) (t_loss: 0.07532) (accu: 0.9771)
[epoch : 9] (l_loss: 0.05544) (t_loss: 0.07843) (accu: 0.9760)
[epoch : 10] (l_loss: 0.05374) (t_loss: 0.07538) (accu: 0.9767)
[epoch : 11] (l_loss: 0.05141) (t_loss: 0.07688) (accu: 0.9760)
[epoch : 12] (l_loss: 0.04981) (t_loss: 0.07670) (accu: 0.9766)
[epoch : 13] (l_loss: 0.04954) (t_loss: 0.07238) (accu: 0.9767)
[epoch : 14] (l_loss: 0.04867) (t_loss: 0.07304) (accu: 0.9770)
[epoch : 15] (l_loss: 0.04823) (t_loss: 0.07174) (accu: 0.9773)
[epoch : 16] (l_loss: 0.04733) (t_loss: 0.07389) (accu: 0.9766)
[epoch : 17] (l_loss: 0.04730) (t_loss: 0.07331) (accu: 0.9773)
[epoch : 18] (l_loss: 0.04648) (t_loss: 0.07096) (accu: 0.9779)
[epoch : 19] (l_loss: 0.04656) (t_loss: 0.07208) (accu: 0.9773)
[epoch : 20] (l_loss: 0.04629) (t_loss: 0.07229) (accu: 0.9771)
[epoch : 21] (l_loss: 0.04578) (t_loss: 0.07369) (accu: 0.9773)
[epoch : 22] (l_loss: 0.04589) (t_loss: 0.06993) (accu: 0.9781)
[epoch : 23] (l_loss: 0.04594) (t_loss: 0.07208) (accu: 0.9778)
[epoch : 24] (l_loss: 0.04547) (t_loss: 0.07283) (accu: 0.9772)
[epoch : 25] (l_loss: 0.04557) (t_loss: 0.07356) (accu: 0.9781)
[epoch : 26] (l_loss: 0.04624) (t_loss: 0.07023) (accu: 0.9791)
[epoch : 27] (l_loss: 0.04524) (t_loss: 0.06969) (accu: 0.9795)
[epoch : 28] (l_loss: 0.04533) (t_loss: 0.07060) (accu: 0.9789)
[epoch : 29] (l_loss: 0.04531) (t_loss: 0.07155) (accu: 0.9787)
[epoch : 30] (l_loss: 0.04521) (t_loss: 0.07142) (accu: 0.9774)
[epoch : 31] (l_loss: 0.04538) (t_loss: 0.07430) (accu: 0.9777)
[epoch : 32] (l_loss: 0.04568) (t_loss: 0.07257) (accu: 0.9788)
[epoch : 33] (l_loss: 0.04542) (t_loss: 0.07218) (accu: 0.9783)
[epoch : 34] (l_loss: 0.04552) (t_loss: 0.07191) (accu: 0.9779)
[epoch : 35] (l_loss: 0.04525) (t_loss: 0.07240) (accu: 0.9790)
[epoch : 36] (l_loss: 0.04574) (t_loss: 0.07279) (accu: 0.9765)
[epoch : 37] (l_loss: 0.04544) (t_loss: 0.07388) (accu: 0.9772)
[epoch : 38] (l_loss: 0.04474) (t_loss: 0.06926) (accu: 0.9779)
[epoch : 39] (l_loss: 0.04522) (t_loss: 0.07186) (accu: 0.9776)
[epoch : 40] (l_loss: 0.04519) (t_loss: 0.07252) (accu: 0.9756)
[epoch : 41] (l_loss: 0.04510) (t_loss: 0.07080) (accu: 0.9775)
[epoch : 42] (l_loss: 0.04510) (t_loss: 0.07147) (accu: 0.9777)
[epoch : 43] (l_loss: 0.04496) (t_loss: 0.07172) (accu: 0.9774)
[epoch : 44] (l_loss: 0.04486) (t_loss: 0.07405) (accu: 0.9773)
[epoch : 45] (l_loss: 0.04469) (t_loss: 0.07519) (accu: 0.9775)
[epoch : 46] (l_loss: 0.04437) (t_loss: 0.07521) (accu: 0.9767)
[epoch : 47] (l_loss: 0.04450) (t_loss: 0.07310) (accu: 0.9781)
[epoch : 48] (l_loss: 0.04450) (t_loss: 0.07225) (accu: 0.9779)
[epoch : 49] (l_loss: 0.04424) (t_loss: 0.07474) (accu: 0.9771)
[epoch : 50] (l_loss: 0.04411) (t_loss: 0.07271) (accu: 0.9766)
Finish! (Best accu: 0.9795) (Time taken(sec) : 783.06) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (170538 | 95662)         64.06
fc1.weight   :      235200 (150528 | 84672)         64.00
fc2.weight   :       30000 (19200 | 10800)          64.00
fcout.weight :          1000 (810 | 190)            81.00
------------------------------------------------------------

Learning start! [Prune_iter : (3/21), Remaining weight : 64.06 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.43365) (accu: 0.1085)
[epoch : 1] (l_loss: 0.43415) (t_loss: 0.15805) (accu: 0.9534)
[epoch : 2] (l_loss: 0.12832) (t_loss: 0.11084) (accu: 0.9667)
[epoch : 3] (l_loss: 0.09578) (t_loss: 0.09708) (accu: 0.9711)
[epoch : 4] (l_loss: 0.08021) (t_loss: 0.08682) (accu: 0.9735)
[epoch : 5] (l_loss: 0.07153) (t_loss: 0.08233) (accu: 0.9744)
[epoch : 6] (l_loss: 0.06613) (t_loss: 0.08263) (accu: 0.9756)
[epoch : 7] (l_loss: 0.06195) (t_loss: 0.08028) (accu: 0.9748)
[epoch : 8] (l_loss: 0.05925) (t_loss: 0.07888) (accu: 0.9751)
[epoch : 9] (l_loss: 0.05593) (t_loss: 0.07700) (accu: 0.9761)
[epoch : 10] (l_loss: 0.05396) (t_loss: 0.07457) (accu: 0.9776)
[epoch : 11] (l_loss: 0.05277) (t_loss: 0.07404) (accu: 0.9780)
[epoch : 12] (l_loss: 0.05164) (t_loss: 0.07370) (accu: 0.9776)
[epoch : 13] (l_loss: 0.05087) (t_loss: 0.07758) (accu: 0.9765)
[epoch : 14] (l_loss: 0.05060) (t_loss: 0.07587) (accu: 0.9776)
[epoch : 15] (l_loss: 0.04976) (t_loss: 0.07204) (accu: 0.9787)
[epoch : 16] (l_loss: 0.04932) (t_loss: 0.07506) (accu: 0.9781)
[epoch : 17] (l_loss: 0.04911) (t_loss: 0.07770) (accu: 0.9779)
[epoch : 18] (l_loss: 0.04876) (t_loss: 0.07307) (accu: 0.9773)
[epoch : 19] (l_loss: 0.04845) (t_loss: 0.07343) (accu: 0.9781)
[epoch : 20] (l_loss: 0.04838) (t_loss: 0.07123) (accu: 0.9792)
[epoch : 21] (l_loss: 0.04801) (t_loss: 0.07057) (accu: 0.9794)
[epoch : 22] (l_loss: 0.04787) (t_loss: 0.07227) (accu: 0.9783)
[epoch : 23] (l_loss: 0.04775) (t_loss: 0.07730) (accu: 0.9768)
[epoch : 24] (l_loss: 0.04731) (t_loss: 0.07712) (accu: 0.9780)
[epoch : 25] (l_loss: 0.04738) (t_loss: 0.07508) (accu: 0.9763)
[epoch : 26] (l_loss: 0.04679) (t_loss: 0.07328) (accu: 0.9775)
[epoch : 27] (l_loss: 0.04736) (t_loss: 0.07288) (accu: 0.9785)
[epoch : 28] (l_loss: 0.04664) (t_loss: 0.07413) (accu: 0.9772)
[epoch : 29] (l_loss: 0.04694) (t_loss: 0.07290) (accu: 0.9777)
[epoch : 30] (l_loss: 0.04686) (t_loss: 0.07277) (accu: 0.9777)
[epoch : 31] (l_loss: 0.04690) (t_loss: 0.07712) (accu: 0.9764)
[epoch : 32] (l_loss: 0.04656) (t_loss: 0.07529) (accu: 0.9766)
[epoch : 33] (l_loss: 0.04692) (t_loss: 0.07330) (accu: 0.9779)
[epoch : 34] (l_loss: 0.04697) (t_loss: 0.07315) (accu: 0.9780)
[epoch : 35] (l_loss: 0.04717) (t_loss: 0.07175) (accu: 0.9787)
[epoch : 36] (l_loss: 0.04673) (t_loss: 0.07371) (accu: 0.9776)
[epoch : 37] (l_loss: 0.04685) (t_loss: 0.07380) (accu: 0.9776)
[epoch : 38] (l_loss: 0.04657) (t_loss: 0.07307) (accu: 0.9777)
[epoch : 39] (l_loss: 0.04679) (t_loss: 0.07384) (accu: 0.9772)
[epoch : 40] (l_loss: 0.04712) (t_loss: 0.07479) (accu: 0.9781)
[epoch : 41] (l_loss: 0.04667) (t_loss: 0.07238) (accu: 0.9777)
[epoch : 42] (l_loss: 0.04697) (t_loss: 0.07522) (accu: 0.9768)
[epoch : 43] (l_loss: 0.04659) (t_loss: 0.07478) (accu: 0.9775)
[epoch : 44] (l_loss: 0.04645) (t_loss: 0.07552) (accu: 0.9762)
[epoch : 45] (l_loss: 0.04693) (t_loss: 0.07295) (accu: 0.9793)
[epoch : 46] (l_loss: 0.04704) (t_loss: 0.07542) (accu: 0.9766)
[epoch : 47] (l_loss: 0.04684) (t_loss: 0.07443) (accu: 0.9783)
[epoch : 48] (l_loss: 0.04679) (t_loss: 0.07193) (accu: 0.9782)
[epoch : 49] (l_loss: 0.04664) (t_loss: 0.07467) (accu: 0.9770)
[epoch : 50] (l_loss: 0.04643) (t_loss: 0.07676) (accu: 0.9780)
Finish! (Best accu: 0.9794) (Time taken(sec) : 783.00) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (136511 | 129689)        51.28
fc1.weight   :      235200 (120422 | 114778)        51.20
fc2.weight   :       30000 (15360 | 14640)          51.20
fcout.weight :          1000 (729 | 271)            72.90
------------------------------------------------------------

Learning start! [Prune_iter : (4/21), Remaining weight : 51.28 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.39699) (accu: 0.1184)
[epoch : 1] (l_loss: 0.44154) (t_loss: 0.16072) (accu: 0.9520)
[epoch : 2] (l_loss: 0.13490) (t_loss: 0.11760) (accu: 0.9647)
[epoch : 3] (l_loss: 0.10050) (t_loss: 0.10029) (accu: 0.9693)
[epoch : 4] (l_loss: 0.08470) (t_loss: 0.09104) (accu: 0.9718)
[epoch : 5] (l_loss: 0.07554) (t_loss: 0.08526) (accu: 0.9733)
[epoch : 6] (l_loss: 0.06904) (t_loss: 0.08592) (accu: 0.9733)
[epoch : 7] (l_loss: 0.06459) (t_loss: 0.08272) (accu: 0.9738)
[epoch : 8] (l_loss: 0.06055) (t_loss: 0.07990) (accu: 0.9756)
[epoch : 9] (l_loss: 0.05701) (t_loss: 0.07463) (accu: 0.9758)
[epoch : 10] (l_loss: 0.05524) (t_loss: 0.07506) (accu: 0.9759)
[epoch : 11] (l_loss: 0.05350) (t_loss: 0.07606) (accu: 0.9762)
[epoch : 12] (l_loss: 0.05278) (t_loss: 0.07452) (accu: 0.9769)
[epoch : 13] (l_loss: 0.05205) (t_loss: 0.07373) (accu: 0.9761)
[epoch : 14] (l_loss: 0.05136) (t_loss: 0.07493) (accu: 0.9775)
[epoch : 15] (l_loss: 0.05039) (t_loss: 0.07288) (accu: 0.9776)
[epoch : 16] (l_loss: 0.05023) (t_loss: 0.07525) (accu: 0.9769)
[epoch : 17] (l_loss: 0.05075) (t_loss: 0.07341) (accu: 0.9770)
[epoch : 18] (l_loss: 0.04968) (t_loss: 0.07304) (accu: 0.9769)
[epoch : 19] (l_loss: 0.04910) (t_loss: 0.07316) (accu: 0.9766)
[epoch : 20] (l_loss: 0.04933) (t_loss: 0.07215) (accu: 0.9777)
[epoch : 21] (l_loss: 0.04917) (t_loss: 0.07252) (accu: 0.9763)
[epoch : 22] (l_loss: 0.04938) (t_loss: 0.07345) (accu: 0.9774)
[epoch : 23] (l_loss: 0.04864) (t_loss: 0.07569) (accu: 0.9760)
[epoch : 24] (l_loss: 0.04871) (t_loss: 0.07487) (accu: 0.9777)
[epoch : 25] (l_loss: 0.04902) (t_loss: 0.07518) (accu: 0.9764)
[epoch : 26] (l_loss: 0.04890) (t_loss: 0.07303) (accu: 0.9789)
[epoch : 27] (l_loss: 0.04863) (t_loss: 0.07364) (accu: 0.9778)
[epoch : 28] (l_loss: 0.04872) (t_loss: 0.07450) (accu: 0.9774)
[epoch : 29] (l_loss: 0.04810) (t_loss: 0.07410) (accu: 0.9768)
[epoch : 30] (l_loss: 0.04826) (t_loss: 0.07082) (accu: 0.9777)
[epoch : 31] (l_loss: 0.04835) (t_loss: 0.07135) (accu: 0.9773)
[epoch : 32] (l_loss: 0.04835) (t_loss: 0.07473) (accu: 0.9770)
[epoch : 33] (l_loss: 0.04782) (t_loss: 0.07366) (accu: 0.9778)
[epoch : 34] (l_loss: 0.04804) (t_loss: 0.07424) (accu: 0.9774)
[epoch : 35] (l_loss: 0.04791) (t_loss: 0.07685) (accu: 0.9755)
[epoch : 36] (l_loss: 0.04792) (t_loss: 0.07659) (accu: 0.9759)
[epoch : 37] (l_loss: 0.04777) (t_loss: 0.07748) (accu: 0.9751)
[epoch : 38] (l_loss: 0.04808) (t_loss: 0.07392) (accu: 0.9768)
[epoch : 39] (l_loss: 0.04799) (t_loss: 0.07186) (accu: 0.9778)
[epoch : 40] (l_loss: 0.04804) (t_loss: 0.07127) (accu: 0.9785)
[epoch : 41] (l_loss: 0.04827) (t_loss: 0.07237) (accu: 0.9789)
[epoch : 42] (l_loss: 0.04805) (t_loss: 0.07692) (accu: 0.9760)
[epoch : 43] (l_loss: 0.04809) (t_loss: 0.07759) (accu: 0.9761)
[epoch : 44] (l_loss: 0.04808) (t_loss: 0.07403) (accu: 0.9777)
[epoch : 45] (l_loss: 0.04824) (t_loss: 0.07359) (accu: 0.9766)
[epoch : 46] (l_loss: 0.04809) (t_loss: 0.07252) (accu: 0.9770)
[epoch : 47] (l_loss: 0.04821) (t_loss: 0.07422) (accu: 0.9766)
[epoch : 48] (l_loss: 0.04805) (t_loss: 0.07365) (accu: 0.9774)
[epoch : 49] (l_loss: 0.04794) (t_loss: 0.07514) (accu: 0.9770)
[epoch : 50] (l_loss: 0.04799) (t_loss: 0.07616) (accu: 0.9757)
Finish! (Best accu: 0.9789) (Time taken(sec) : 783.59) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (109282 | 156918)        41.05
fc1.weight   :      235200 (96338 | 138862)         40.96
fc2.weight   :       30000 (12288 | 17712)          40.96
fcout.weight :          1000 (656 | 344)            65.60
------------------------------------------------------------

Learning start! [Prune_iter : (5/21), Remaining weight : 41.05 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.37304) (accu: 0.1559)
[epoch : 1] (l_loss: 0.43710) (t_loss: 0.16022) (accu: 0.9536)
[epoch : 2] (l_loss: 0.13454) (t_loss: 0.11441) (accu: 0.9658)
[epoch : 3] (l_loss: 0.09988) (t_loss: 0.09920) (accu: 0.9701)
[epoch : 4] (l_loss: 0.08416) (t_loss: 0.09371) (accu: 0.9713)
[epoch : 5] (l_loss: 0.07490) (t_loss: 0.08532) (accu: 0.9738)
[epoch : 6] (l_loss: 0.06801) (t_loss: 0.08230) (accu: 0.9749)
[epoch : 7] (l_loss: 0.06311) (t_loss: 0.07699) (accu: 0.9768)
[epoch : 8] (l_loss: 0.05829) (t_loss: 0.08090) (accu: 0.9753)
[epoch : 9] (l_loss: 0.05528) (t_loss: 0.07662) (accu: 0.9758)
[epoch : 10] (l_loss: 0.05343) (t_loss: 0.07331) (accu: 0.9787)
[epoch : 11] (l_loss: 0.05114) (t_loss: 0.07647) (accu: 0.9765)
[epoch : 12] (l_loss: 0.05069) (t_loss: 0.07557) (accu: 0.9774)
[epoch : 13] (l_loss: 0.04995) (t_loss: 0.07727) (accu: 0.9761)
[epoch : 14] (l_loss: 0.04928) (t_loss: 0.07343) (accu: 0.9786)
[epoch : 15] (l_loss: 0.04895) (t_loss: 0.07527) (accu: 0.9771)
[epoch : 16] (l_loss: 0.04842) (t_loss: 0.07385) (accu: 0.9764)
[epoch : 17] (l_loss: 0.04859) (t_loss: 0.07375) (accu: 0.9773)
[epoch : 18] (l_loss: 0.04879) (t_loss: 0.07275) (accu: 0.9771)
[epoch : 19] (l_loss: 0.04816) (t_loss: 0.07591) (accu: 0.9769)
[epoch : 20] (l_loss: 0.04775) (t_loss: 0.07319) (accu: 0.9771)
[epoch : 21] (l_loss: 0.04781) (t_loss: 0.07325) (accu: 0.9761)
[epoch : 22] (l_loss: 0.04780) (t_loss: 0.07147) (accu: 0.9776)
[epoch : 23] (l_loss: 0.04723) (t_loss: 0.07346) (accu: 0.9778)
[epoch : 24] (l_loss: 0.04716) (t_loss: 0.07421) (accu: 0.9762)
[epoch : 25] (l_loss: 0.04720) (t_loss: 0.07259) (accu: 0.9765)
[epoch : 26] (l_loss: 0.04726) (t_loss: 0.07431) (accu: 0.9771)
[epoch : 27] (l_loss: 0.04690) (t_loss: 0.07442) (accu: 0.9766)
[epoch : 28] (l_loss: 0.04715) (t_loss: 0.07083) (accu: 0.9773)
[epoch : 29] (l_loss: 0.04723) (t_loss: 0.07113) (accu: 0.9791)
[epoch : 30] (l_loss: 0.04673) (t_loss: 0.07347) (accu: 0.9772)
[epoch : 31] (l_loss: 0.04665) (t_loss: 0.07197) (accu: 0.9783)
[epoch : 32] (l_loss: 0.04708) (t_loss: 0.07472) (accu: 0.9759)
[epoch : 33] (l_loss: 0.04663) (t_loss: 0.07495) (accu: 0.9767)
[epoch : 34] (l_loss: 0.04692) (t_loss: 0.07668) (accu: 0.9752)
[epoch : 35] (l_loss: 0.04664) (t_loss: 0.07479) (accu: 0.9768)
[epoch : 36] (l_loss: 0.04710) (t_loss: 0.07369) (accu: 0.9776)
[epoch : 37] (l_loss: 0.04678) (t_loss: 0.07455) (accu: 0.9769)
[epoch : 38] (l_loss: 0.04667) (t_loss: 0.07227) (accu: 0.9773)
[epoch : 39] (l_loss: 0.04668) (t_loss: 0.07300) (accu: 0.9775)
[epoch : 40] (l_loss: 0.04684) (t_loss: 0.07230) (accu: 0.9773)
[epoch : 41] (l_loss: 0.04628) (t_loss: 0.07138) (accu: 0.9773)
[epoch : 42] (l_loss: 0.04687) (t_loss: 0.07541) (accu: 0.9769)
[epoch : 43] (l_loss: 0.04670) (t_loss: 0.07362) (accu: 0.9779)
[epoch : 44] (l_loss: 0.04688) (t_loss: 0.07203) (accu: 0.9781)
[epoch : 45] (l_loss: 0.04676) (t_loss: 0.07693) (accu: 0.9768)
[epoch : 46] (l_loss: 0.04647) (t_loss: 0.07664) (accu: 0.9772)
[epoch : 47] (l_loss: 0.04632) (t_loss: 0.07672) (accu: 0.9770)
[epoch : 48] (l_loss: 0.04687) (t_loss: 0.07310) (accu: 0.9768)
[epoch : 49] (l_loss: 0.04681) (t_loss: 0.07481) (accu: 0.9773)
[epoch : 50] (l_loss: 0.04680) (t_loss: 0.07351) (accu: 0.9777)
Finish! (Best accu: 0.9791) (Time taken(sec) : 774.63) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (87490 | 178710)         32.87
fc1.weight   :      235200 (77070 | 158130)         32.77
fc2.weight   :        30000 (9830 | 20170)          32.77
fcout.weight :          1000 (590 | 410)            59.00
------------------------------------------------------------

Learning start! [Prune_iter : (6/21), Remaining weight : 32.87 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.35419) (accu: 0.1073)
[epoch : 1] (l_loss: 0.42486) (t_loss: 0.16004) (accu: 0.9535)
[epoch : 2] (l_loss: 0.12960) (t_loss: 0.10953) (accu: 0.9678)
[epoch : 3] (l_loss: 0.09766) (t_loss: 0.09915) (accu: 0.9698)
[epoch : 4] (l_loss: 0.08294) (t_loss: 0.08703) (accu: 0.9746)
[epoch : 5] (l_loss: 0.07423) (t_loss: 0.08228) (accu: 0.9757)
[epoch : 6] (l_loss: 0.06795) (t_loss: 0.08038) (accu: 0.9738)
[epoch : 7] (l_loss: 0.06410) (t_loss: 0.07948) (accu: 0.9744)
[epoch : 8] (l_loss: 0.06109) (t_loss: 0.07841) (accu: 0.9753)
[epoch : 9] (l_loss: 0.05867) (t_loss: 0.07606) (accu: 0.9769)
[epoch : 10] (l_loss: 0.05646) (t_loss: 0.07738) (accu: 0.9762)
[epoch : 11] (l_loss: 0.05462) (t_loss: 0.07953) (accu: 0.9751)
[epoch : 12] (l_loss: 0.05342) (t_loss: 0.07804) (accu: 0.9755)
[epoch : 13] (l_loss: 0.05265) (t_loss: 0.07519) (accu: 0.9778)
[epoch : 14] (l_loss: 0.05211) (t_loss: 0.07508) (accu: 0.9776)
[epoch : 15] (l_loss: 0.05156) (t_loss: 0.07892) (accu: 0.9751)
[epoch : 16] (l_loss: 0.05087) (t_loss: 0.07868) (accu: 0.9757)
[epoch : 17] (l_loss: 0.05061) (t_loss: 0.07512) (accu: 0.9773)
[epoch : 18] (l_loss: 0.05000) (t_loss: 0.07569) (accu: 0.9776)
[epoch : 19] (l_loss: 0.05052) (t_loss: 0.07560) (accu: 0.9778)
[epoch : 20] (l_loss: 0.04939) (t_loss: 0.07866) (accu: 0.9763)
[epoch : 21] (l_loss: 0.04980) (t_loss: 0.07635) (accu: 0.9763)
[epoch : 22] (l_loss: 0.04918) (t_loss: 0.07917) (accu: 0.9757)
[epoch : 23] (l_loss: 0.04892) (t_loss: 0.07477) (accu: 0.9770)
[epoch : 24] (l_loss: 0.04932) (t_loss: 0.07620) (accu: 0.9772)
[epoch : 25] (l_loss: 0.04867) (t_loss: 0.07546) (accu: 0.9761)
[epoch : 26] (l_loss: 0.04918) (t_loss: 0.07822) (accu: 0.9761)
[epoch : 27] (l_loss: 0.04941) (t_loss: 0.07807) (accu: 0.9758)
[epoch : 28] (l_loss: 0.04869) (t_loss: 0.07582) (accu: 0.9768)
[epoch : 29] (l_loss: 0.04855) (t_loss: 0.07685) (accu: 0.9771)
[epoch : 30] (l_loss: 0.04911) (t_loss: 0.07401) (accu: 0.9795)
[epoch : 31] (l_loss: 0.04923) (t_loss: 0.07518) (accu: 0.9771)
[epoch : 32] (l_loss: 0.04896) (t_loss: 0.07621) (accu: 0.9768)
[epoch : 33] (l_loss: 0.04911) (t_loss: 0.07835) (accu: 0.9754)
[epoch : 34] (l_loss: 0.04885) (t_loss: 0.07848) (accu: 0.9760)
[epoch : 35] (l_loss: 0.04808) (t_loss: 0.07758) (accu: 0.9762)
[epoch : 36] (l_loss: 0.04792) (t_loss: 0.07560) (accu: 0.9777)
[epoch : 37] (l_loss: 0.04789) (t_loss: 0.07271) (accu: 0.9773)
[epoch : 38] (l_loss: 0.04812) (t_loss: 0.07368) (accu: 0.9766)
[epoch : 39] (l_loss: 0.04763) (t_loss: 0.07310) (accu: 0.9780)
[epoch : 40] (l_loss: 0.04812) (t_loss: 0.07038) (accu: 0.9795)
[epoch : 41] (l_loss: 0.04789) (t_loss: 0.07415) (accu: 0.9774)
[epoch : 42] (l_loss: 0.04796) (t_loss: 0.07470) (accu: 0.9766)
[epoch : 43] (l_loss: 0.04785) (t_loss: 0.07428) (accu: 0.9762)
[epoch : 44] (l_loss: 0.04774) (t_loss: 0.07565) (accu: 0.9770)
[epoch : 45] (l_loss: 0.04739) (t_loss: 0.07462) (accu: 0.9776)
[epoch : 46] (l_loss: 0.04739) (t_loss: 0.07397) (accu: 0.9762)
[epoch : 47] (l_loss: 0.04776) (t_loss: 0.07329) (accu: 0.9770)
[epoch : 48] (l_loss: 0.04827) (t_loss: 0.07665) (accu: 0.9759)
[epoch : 49] (l_loss: 0.04729) (t_loss: 0.07726) (accu: 0.9769)
[epoch : 50] (l_loss: 0.04816) (t_loss: 0.07553) (accu: 0.9781)
Finish! (Best accu: 0.9795) (Time taken(sec) : 761.20) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (70051 | 196149)         26.32
fc1.weight   :      235200 (61656 | 173544)         26.21
fc2.weight   :        30000 (7864 | 22136)          26.21
fcout.weight :          1000 (531 | 469)            53.10
------------------------------------------------------------

Learning start! [Prune_iter : (7/21), Remaining weight : 26.32 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.32614) (accu: 0.1133)
[epoch : 1] (l_loss: 0.41823) (t_loss: 0.15806) (accu: 0.9565)
[epoch : 2] (l_loss: 0.12684) (t_loss: 0.11129) (accu: 0.9665)
[epoch : 3] (l_loss: 0.09465) (t_loss: 0.09411) (accu: 0.9711)
[epoch : 4] (l_loss: 0.07930) (t_loss: 0.08461) (accu: 0.9746)
[epoch : 5] (l_loss: 0.06963) (t_loss: 0.07686) (accu: 0.9764)
[epoch : 6] (l_loss: 0.06377) (t_loss: 0.08045) (accu: 0.9758)
[epoch : 7] (l_loss: 0.05905) (t_loss: 0.07865) (accu: 0.9759)
[epoch : 8] (l_loss: 0.05518) (t_loss: 0.07643) (accu: 0.9760)
[epoch : 9] (l_loss: 0.05167) (t_loss: 0.07177) (accu: 0.9785)
[epoch : 10] (l_loss: 0.04979) (t_loss: 0.07374) (accu: 0.9786)
[epoch : 11] (l_loss: 0.04825) (t_loss: 0.07390) (accu: 0.9775)
[epoch : 12] (l_loss: 0.04748) (t_loss: 0.07133) (accu: 0.9783)
[epoch : 13] (l_loss: 0.04624) (t_loss: 0.07116) (accu: 0.9782)
[epoch : 14] (l_loss: 0.04645) (t_loss: 0.07284) (accu: 0.9780)
[epoch : 15] (l_loss: 0.04637) (t_loss: 0.07172) (accu: 0.9787)
[epoch : 16] (l_loss: 0.04540) (t_loss: 0.07460) (accu: 0.9770)
[epoch : 17] (l_loss: 0.04543) (t_loss: 0.07158) (accu: 0.9770)
[epoch : 18] (l_loss: 0.04507) (t_loss: 0.07109) (accu: 0.9781)
[epoch : 19] (l_loss: 0.04473) (t_loss: 0.07197) (accu: 0.9771)
[epoch : 20] (l_loss: 0.04447) (t_loss: 0.07239) (accu: 0.9783)
[epoch : 21] (l_loss: 0.04459) (t_loss: 0.07280) (accu: 0.9778)
[epoch : 22] (l_loss: 0.04478) (t_loss: 0.07161) (accu: 0.9781)
[epoch : 23] (l_loss: 0.04441) (t_loss: 0.07179) (accu: 0.9765)
[epoch : 24] (l_loss: 0.04454) (t_loss: 0.06930) (accu: 0.9797)
[epoch : 25] (l_loss: 0.04405) (t_loss: 0.07235) (accu: 0.9778)
[epoch : 26] (l_loss: 0.04399) (t_loss: 0.06965) (accu: 0.9796)
[epoch : 27] (l_loss: 0.04377) (t_loss: 0.07289) (accu: 0.9766)
[epoch : 28] (l_loss: 0.04457) (t_loss: 0.07222) (accu: 0.9779)
[epoch : 29] (l_loss: 0.04400) (t_loss: 0.07513) (accu: 0.9766)
[epoch : 30] (l_loss: 0.04393) (t_loss: 0.06934) (accu: 0.9780)
[epoch : 31] (l_loss: 0.04451) (t_loss: 0.07129) (accu: 0.9779)
[epoch : 32] (l_loss: 0.04436) (t_loss: 0.07084) (accu: 0.9788)
[epoch : 33] (l_loss: 0.04387) (t_loss: 0.07106) (accu: 0.9786)
[epoch : 34] (l_loss: 0.04431) (t_loss: 0.06939) (accu: 0.9792)
[epoch : 35] (l_loss: 0.04414) (t_loss: 0.07164) (accu: 0.9780)
[epoch : 36] (l_loss: 0.04444) (t_loss: 0.07128) (accu: 0.9786)
[epoch : 37] (l_loss: 0.04409) (t_loss: 0.07293) (accu: 0.9778)
[epoch : 38] (l_loss: 0.04411) (t_loss: 0.07192) (accu: 0.9775)
[epoch : 39] (l_loss: 0.04388) (t_loss: 0.07221) (accu: 0.9784)
[epoch : 40] (l_loss: 0.04410) (t_loss: 0.07101) (accu: 0.9785)
[epoch : 41] (l_loss: 0.04397) (t_loss: 0.07231) (accu: 0.9770)
[epoch : 42] (l_loss: 0.04399) (t_loss: 0.06969) (accu: 0.9792)
[epoch : 43] (l_loss: 0.04408) (t_loss: 0.07019) (accu: 0.9782)
[epoch : 44] (l_loss: 0.04408) (t_loss: 0.07177) (accu: 0.9783)
[epoch : 45] (l_loss: 0.04432) (t_loss: 0.07337) (accu: 0.9781)
[epoch : 46] (l_loss: 0.04415) (t_loss: 0.07330) (accu: 0.9779)
[epoch : 47] (l_loss: 0.04411) (t_loss: 0.07480) (accu: 0.9784)
[epoch : 48] (l_loss: 0.04396) (t_loss: 0.07218) (accu: 0.9786)
[epoch : 49] (l_loss: 0.04402) (t_loss: 0.07150) (accu: 0.9784)
[epoch : 50] (l_loss: 0.04435) (t_loss: 0.07321) (accu: 0.9791)
Finish! (Best accu: 0.9797) (Time taken(sec) : 781.16) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (56094 | 210106)         21.07
fc1.weight   :      235200 (49325 | 185875)         20.97
fc2.weight   :        30000 (6291 | 23709)          20.97
fcout.weight :          1000 (478 | 522)            47.80
------------------------------------------------------------

Learning start! [Prune_iter : (8/21), Remaining weight : 21.07 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.31518) (accu: 0.1170)
[epoch : 1] (l_loss: 0.41908) (t_loss: 0.15876) (accu: 0.9539)
[epoch : 2] (l_loss: 0.12904) (t_loss: 0.11381) (accu: 0.9644)
[epoch : 3] (l_loss: 0.09687) (t_loss: 0.09458) (accu: 0.9704)
[epoch : 4] (l_loss: 0.08209) (t_loss: 0.09097) (accu: 0.9728)
[epoch : 5] (l_loss: 0.07199) (t_loss: 0.08042) (accu: 0.9768)
[epoch : 6] (l_loss: 0.06575) (t_loss: 0.08281) (accu: 0.9746)
[epoch : 7] (l_loss: 0.06126) (t_loss: 0.07751) (accu: 0.9767)
[epoch : 8] (l_loss: 0.05741) (t_loss: 0.07496) (accu: 0.9773)
[epoch : 9] (l_loss: 0.05420) (t_loss: 0.07346) (accu: 0.9774)
[epoch : 10] (l_loss: 0.05242) (t_loss: 0.07667) (accu: 0.9758)
[epoch : 11] (l_loss: 0.05085) (t_loss: 0.07276) (accu: 0.9770)
[epoch : 12] (l_loss: 0.04979) (t_loss: 0.07670) (accu: 0.9772)
[epoch : 13] (l_loss: 0.04827) (t_loss: 0.07406) (accu: 0.9773)
[epoch : 14] (l_loss: 0.04764) (t_loss: 0.07887) (accu: 0.9747)
[epoch : 15] (l_loss: 0.04714) (t_loss: 0.07079) (accu: 0.9778)
[epoch : 16] (l_loss: 0.04676) (t_loss: 0.07098) (accu: 0.9786)
[epoch : 17] (l_loss: 0.04652) (t_loss: 0.07486) (accu: 0.9789)
[epoch : 18] (l_loss: 0.04607) (t_loss: 0.07180) (accu: 0.9790)
[epoch : 19] (l_loss: 0.04533) (t_loss: 0.07531) (accu: 0.9775)
[epoch : 20] (l_loss: 0.04497) (t_loss: 0.07209) (accu: 0.9781)
[epoch : 21] (l_loss: 0.04472) (t_loss: 0.07569) (accu: 0.9762)
[epoch : 22] (l_loss: 0.04451) (t_loss: 0.07407) (accu: 0.9766)
[epoch : 23] (l_loss: 0.04413) (t_loss: 0.07074) (accu: 0.9780)
[epoch : 24] (l_loss: 0.04399) (t_loss: 0.07142) (accu: 0.9779)
[epoch : 25] (l_loss: 0.04389) (t_loss: 0.07139) (accu: 0.9785)
[epoch : 26] (l_loss: 0.04379) (t_loss: 0.07013) (accu: 0.9785)
[epoch : 27] (l_loss: 0.04400) (t_loss: 0.07038) (accu: 0.9790)
[epoch : 28] (l_loss: 0.04385) (t_loss: 0.07684) (accu: 0.9751)
[epoch : 29] (l_loss: 0.04366) (t_loss: 0.07037) (accu: 0.9782)
[epoch : 30] (l_loss: 0.04383) (t_loss: 0.07358) (accu: 0.9759)
[epoch : 31] (l_loss: 0.04383) (t_loss: 0.07110) (accu: 0.9773)
[epoch : 32] (l_loss: 0.04383) (t_loss: 0.07238) (accu: 0.9774)
[epoch : 33] (l_loss: 0.04353) (t_loss: 0.06836) (accu: 0.9781)
[epoch : 34] (l_loss: 0.04337) (t_loss: 0.07561) (accu: 0.9758)
[epoch : 35] (l_loss: 0.04381) (t_loss: 0.07048) (accu: 0.9787)
[epoch : 36] (l_loss: 0.04379) (t_loss: 0.07404) (accu: 0.9762)
[epoch : 37] (l_loss: 0.04358) (t_loss: 0.07186) (accu: 0.9768)
[epoch : 38] (l_loss: 0.04368) (t_loss: 0.07170) (accu: 0.9771)
[epoch : 39] (l_loss: 0.04378) (t_loss: 0.06957) (accu: 0.9791)
[epoch : 40] (l_loss: 0.04385) (t_loss: 0.06954) (accu: 0.9780)
[epoch : 41] (l_loss: 0.04367) (t_loss: 0.07427) (accu: 0.9773)
[epoch : 42] (l_loss: 0.04322) (t_loss: 0.07232) (accu: 0.9782)
[epoch : 43] (l_loss: 0.04367) (t_loss: 0.07384) (accu: 0.9768)
[epoch : 44] (l_loss: 0.04385) (t_loss: 0.07181) (accu: 0.9786)
[epoch : 45] (l_loss: 0.04319) (t_loss: 0.07070) (accu: 0.9789)
[epoch : 46] (l_loss: 0.04419) (t_loss: 0.06898) (accu: 0.9783)
[epoch : 47] (l_loss: 0.04358) (t_loss: 0.07009) (accu: 0.9782)
[epoch : 48] (l_loss: 0.04351) (t_loss: 0.06992) (accu: 0.9782)
[epoch : 49] (l_loss: 0.04341) (t_loss: 0.07150) (accu: 0.9789)
[epoch : 50] (l_loss: 0.04391) (t_loss: 0.07400) (accu: 0.9768)
Finish! (Best accu: 0.9791) (Time taken(sec) : 772.11) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (44923 | 221277)         16.88
fc1.weight   :      235200 (39460 | 195740)         16.78
fc2.weight   :        30000 (5033 | 24967)          16.78
fcout.weight :          1000 (430 | 570)            43.00
------------------------------------------------------------

Learning start! [Prune_iter : (9/21), Remaining weight : 16.88 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30151) (accu: 0.1318)
[epoch : 1] (l_loss: 0.40818) (t_loss: 0.15182) (accu: 0.9553)
[epoch : 2] (l_loss: 0.12548) (t_loss: 0.11212) (accu: 0.9648)
[epoch : 3] (l_loss: 0.09526) (t_loss: 0.09542) (accu: 0.9707)
[epoch : 4] (l_loss: 0.08013) (t_loss: 0.08767) (accu: 0.9738)
[epoch : 5] (l_loss: 0.07114) (t_loss: 0.08176) (accu: 0.9751)
[epoch : 6] (l_loss: 0.06540) (t_loss: 0.07915) (accu: 0.9761)
[epoch : 7] (l_loss: 0.06087) (t_loss: 0.07767) (accu: 0.9757)
[epoch : 8] (l_loss: 0.05741) (t_loss: 0.07337) (accu: 0.9769)
[epoch : 9] (l_loss: 0.05476) (t_loss: 0.07494) (accu: 0.9790)
[epoch : 10] (l_loss: 0.05247) (t_loss: 0.07513) (accu: 0.9761)
[epoch : 11] (l_loss: 0.05100) (t_loss: 0.07425) (accu: 0.9784)
[epoch : 12] (l_loss: 0.04914) (t_loss: 0.07284) (accu: 0.9788)
[epoch : 13] (l_loss: 0.04853) (t_loss: 0.07402) (accu: 0.9775)
[epoch : 14] (l_loss: 0.04827) (t_loss: 0.07165) (accu: 0.9781)
[epoch : 15] (l_loss: 0.04740) (t_loss: 0.07258) (accu: 0.9773)
[epoch : 16] (l_loss: 0.04733) (t_loss: 0.06986) (accu: 0.9786)
[epoch : 17] (l_loss: 0.04704) (t_loss: 0.07252) (accu: 0.9780)
[epoch : 18] (l_loss: 0.04678) (t_loss: 0.07251) (accu: 0.9785)
[epoch : 19] (l_loss: 0.04630) (t_loss: 0.07282) (accu: 0.9779)
[epoch : 20] (l_loss: 0.04609) (t_loss: 0.07180) (accu: 0.9774)
[epoch : 21] (l_loss: 0.04587) (t_loss: 0.07136) (accu: 0.9789)
[epoch : 22] (l_loss: 0.04590) (t_loss: 0.06885) (accu: 0.9787)
[epoch : 23] (l_loss: 0.04575) (t_loss: 0.07218) (accu: 0.9776)
[epoch : 24] (l_loss: 0.04571) (t_loss: 0.07000) (accu: 0.9782)
[epoch : 25] (l_loss: 0.04546) (t_loss: 0.07150) (accu: 0.9785)
[epoch : 26] (l_loss: 0.04563) (t_loss: 0.06949) (accu: 0.9779)
[epoch : 27] (l_loss: 0.04566) (t_loss: 0.07209) (accu: 0.9771)
[epoch : 28] (l_loss: 0.04576) (t_loss: 0.07386) (accu: 0.9775)
[epoch : 29] (l_loss: 0.04553) (t_loss: 0.06967) (accu: 0.9792)
[epoch : 30] (l_loss: 0.04545) (t_loss: 0.07270) (accu: 0.9777)
[epoch : 31] (l_loss: 0.04590) (t_loss: 0.06986) (accu: 0.9787)
[epoch : 32] (l_loss: 0.04570) (t_loss: 0.07048) (accu: 0.9787)
[epoch : 33] (l_loss: 0.04598) (t_loss: 0.07104) (accu: 0.9786)
[epoch : 34] (l_loss: 0.04507) (t_loss: 0.06978) (accu: 0.9780)
[epoch : 35] (l_loss: 0.04558) (t_loss: 0.07005) (accu: 0.9790)
[epoch : 36] (l_loss: 0.04566) (t_loss: 0.06937) (accu: 0.9791)
[epoch : 37] (l_loss: 0.04536) (t_loss: 0.07123) (accu: 0.9797)
[epoch : 38] (l_loss: 0.04547) (t_loss: 0.07316) (accu: 0.9775)
[epoch : 39] (l_loss: 0.04598) (t_loss: 0.07290) (accu: 0.9766)
[epoch : 40] (l_loss: 0.04543) (t_loss: 0.07120) (accu: 0.9801)
[epoch : 41] (l_loss: 0.04556) (t_loss: 0.07016) (accu: 0.9789)
[epoch : 42] (l_loss: 0.04547) (t_loss: 0.07163) (accu: 0.9792)
[epoch : 43] (l_loss: 0.04580) (t_loss: 0.07005) (accu: 0.9780)
[epoch : 44] (l_loss: 0.04556) (t_loss: 0.07593) (accu: 0.9752)
[epoch : 45] (l_loss: 0.04563) (t_loss: 0.07209) (accu: 0.9782)
[epoch : 46] (l_loss: 0.04600) (t_loss: 0.07161) (accu: 0.9787)
[epoch : 47] (l_loss: 0.04537) (t_loss: 0.06994) (accu: 0.9787)
[epoch : 48] (l_loss: 0.04552) (t_loss: 0.07013) (accu: 0.9786)
[epoch : 49] (l_loss: 0.04553) (t_loss: 0.07061) (accu: 0.9782)
[epoch : 50] (l_loss: 0.04548) (t_loss: 0.07356) (accu: 0.9767)
Finish! (Best accu: 0.9801) (Time taken(sec) : 755.42) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (35982 | 230218)         13.52
fc1.weight   :      235200 (31568 | 203632)         13.42
fc2.weight   :        30000 (4027 | 25973)          13.42
fcout.weight :          1000 (387 | 613)            38.70
------------------------------------------------------------

Learning start! [Prune_iter : (10/21), Remaining weight : 13.52 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30127) (accu: 0.0958)
[epoch : 1] (l_loss: 0.40334) (t_loss: 0.15161) (accu: 0.9563)
[epoch : 2] (l_loss: 0.12387) (t_loss: 0.11194) (accu: 0.9652)
[epoch : 3] (l_loss: 0.09432) (t_loss: 0.09263) (accu: 0.9720)
[epoch : 4] (l_loss: 0.07965) (t_loss: 0.08702) (accu: 0.9737)
[epoch : 5] (l_loss: 0.07011) (t_loss: 0.08078) (accu: 0.9757)
[epoch : 6] (l_loss: 0.06508) (t_loss: 0.07583) (accu: 0.9768)
[epoch : 7] (l_loss: 0.06016) (t_loss: 0.07541) (accu: 0.9772)
[epoch : 8] (l_loss: 0.05732) (t_loss: 0.07608) (accu: 0.9765)
[epoch : 9] (l_loss: 0.05460) (t_loss: 0.07523) (accu: 0.9776)
[epoch : 10] (l_loss: 0.05274) (t_loss: 0.07710) (accu: 0.9760)
[epoch : 11] (l_loss: 0.05114) (t_loss: 0.07241) (accu: 0.9771)
[epoch : 12] (l_loss: 0.04997) (t_loss: 0.07609) (accu: 0.9768)
[epoch : 13] (l_loss: 0.04950) (t_loss: 0.07561) (accu: 0.9758)
[epoch : 14] (l_loss: 0.04814) (t_loss: 0.07047) (accu: 0.9777)
[epoch : 15] (l_loss: 0.04794) (t_loss: 0.07226) (accu: 0.9786)
[epoch : 16] (l_loss: 0.04681) (t_loss: 0.07216) (accu: 0.9781)
[epoch : 17] (l_loss: 0.04713) (t_loss: 0.07126) (accu: 0.9788)
[epoch : 18] (l_loss: 0.04667) (t_loss: 0.07028) (accu: 0.9779)
[epoch : 19] (l_loss: 0.04672) (t_loss: 0.07097) (accu: 0.9799)
[epoch : 20] (l_loss: 0.04643) (t_loss: 0.07124) (accu: 0.9772)
[epoch : 21] (l_loss: 0.04602) (t_loss: 0.07020) (accu: 0.9796)
[epoch : 22] (l_loss: 0.04607) (t_loss: 0.07169) (accu: 0.9781)
[epoch : 23] (l_loss: 0.04616) (t_loss: 0.07148) (accu: 0.9797)
[epoch : 24] (l_loss: 0.04556) (t_loss: 0.07193) (accu: 0.9771)
[epoch : 25] (l_loss: 0.04562) (t_loss: 0.06959) (accu: 0.9793)
[epoch : 26] (l_loss: 0.04602) (t_loss: 0.07197) (accu: 0.9765)
[epoch : 27] (l_loss: 0.04576) (t_loss: 0.07198) (accu: 0.9778)
[epoch : 28] (l_loss: 0.04482) (t_loss: 0.07170) (accu: 0.9781)
[epoch : 29] (l_loss: 0.04471) (t_loss: 0.07001) (accu: 0.9792)
[epoch : 30] (l_loss: 0.04456) (t_loss: 0.07619) (accu: 0.9765)
[epoch : 31] (l_loss: 0.04493) (t_loss: 0.07010) (accu: 0.9791)
[epoch : 32] (l_loss: 0.04511) (t_loss: 0.07289) (accu: 0.9788)
[epoch : 33] (l_loss: 0.04529) (t_loss: 0.07055) (accu: 0.9791)
[epoch : 34] (l_loss: 0.04462) (t_loss: 0.07096) (accu: 0.9783)
[epoch : 35] (l_loss: 0.04475) (t_loss: 0.07011) (accu: 0.9786)
[epoch : 36] (l_loss: 0.04470) (t_loss: 0.06890) (accu: 0.9796)
[epoch : 37] (l_loss: 0.04451) (t_loss: 0.07031) (accu: 0.9794)
[epoch : 38] (l_loss: 0.04480) (t_loss: 0.07073) (accu: 0.9774)
[epoch : 39] (l_loss: 0.04410) (t_loss: 0.06942) (accu: 0.9791)
[epoch : 40] (l_loss: 0.04454) (t_loss: 0.06877) (accu: 0.9791)
[epoch : 41] (l_loss: 0.04462) (t_loss: 0.07132) (accu: 0.9770)
[epoch : 42] (l_loss: 0.04490) (t_loss: 0.07072) (accu: 0.9784)
[epoch : 43] (l_loss: 0.04450) (t_loss: 0.07179) (accu: 0.9767)
[epoch : 44] (l_loss: 0.04431) (t_loss: 0.07265) (accu: 0.9777)
[epoch : 45] (l_loss: 0.04452) (t_loss: 0.07183) (accu: 0.9774)
[epoch : 46] (l_loss: 0.04463) (t_loss: 0.07157) (accu: 0.9781)
[epoch : 47] (l_loss: 0.04445) (t_loss: 0.07009) (accu: 0.9792)
[epoch : 48] (l_loss: 0.04476) (t_loss: 0.06814) (accu: 0.9786)
[epoch : 49] (l_loss: 0.04464) (t_loss: 0.07143) (accu: 0.9782)
[epoch : 50] (l_loss: 0.04463) (t_loss: 0.07015) (accu: 0.9779)
Finish! (Best accu: 0.9799) (Time taken(sec) : 737.90) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (28824 | 237376)         10.83
fc1.weight   :      235200 (25254 | 209946)         10.74
fc2.weight   :        30000 (3221 | 26779)          10.74
fcout.weight :          1000 (349 | 651)            34.90
------------------------------------------------------------

Learning start! [Prune_iter : (11/21), Remaining weight : 10.83 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30640) (accu: 0.1044)
[epoch : 1] (l_loss: 0.40162) (t_loss: 0.15223) (accu: 0.9559)
[epoch : 2] (l_loss: 0.12303) (t_loss: 0.10993) (accu: 0.9665)
[epoch : 3] (l_loss: 0.09457) (t_loss: 0.09613) (accu: 0.9701)
[epoch : 4] (l_loss: 0.08103) (t_loss: 0.08600) (accu: 0.9747)
[epoch : 5] (l_loss: 0.07182) (t_loss: 0.08609) (accu: 0.9727)
[epoch : 6] (l_loss: 0.06582) (t_loss: 0.07716) (accu: 0.9764)
[epoch : 7] (l_loss: 0.06215) (t_loss: 0.08002) (accu: 0.9760)
[epoch : 8] (l_loss: 0.05793) (t_loss: 0.07608) (accu: 0.9768)
[epoch : 9] (l_loss: 0.05550) (t_loss: 0.07441) (accu: 0.9781)
[epoch : 10] (l_loss: 0.05311) (t_loss: 0.07370) (accu: 0.9774)
[epoch : 11] (l_loss: 0.05144) (t_loss: 0.07534) (accu: 0.9771)
[epoch : 12] (l_loss: 0.04992) (t_loss: 0.07486) (accu: 0.9770)
[epoch : 13] (l_loss: 0.04872) (t_loss: 0.07955) (accu: 0.9751)
[epoch : 14] (l_loss: 0.04855) (t_loss: 0.07342) (accu: 0.9767)
[epoch : 15] (l_loss: 0.04837) (t_loss: 0.07540) (accu: 0.9761)
[epoch : 16] (l_loss: 0.04723) (t_loss: 0.07351) (accu: 0.9762)
[epoch : 17] (l_loss: 0.04691) (t_loss: 0.07473) (accu: 0.9771)
[epoch : 18] (l_loss: 0.04715) (t_loss: 0.07753) (accu: 0.9752)
[epoch : 19] (l_loss: 0.04679) (t_loss: 0.07433) (accu: 0.9775)
[epoch : 20] (l_loss: 0.04681) (t_loss: 0.07095) (accu: 0.9770)
[epoch : 21] (l_loss: 0.04632) (t_loss: 0.07289) (accu: 0.9767)
[epoch : 22] (l_loss: 0.04614) (t_loss: 0.07379) (accu: 0.9764)
[epoch : 23] (l_loss: 0.04634) (t_loss: 0.07593) (accu: 0.9770)
[epoch : 24] (l_loss: 0.04628) (t_loss: 0.07384) (accu: 0.9766)
[epoch : 25] (l_loss: 0.04585) (t_loss: 0.07422) (accu: 0.9759)
[epoch : 26] (l_loss: 0.04619) (t_loss: 0.07277) (accu: 0.9763)
[epoch : 27] (l_loss: 0.04567) (t_loss: 0.07586) (accu: 0.9767)
[epoch : 28] (l_loss: 0.04576) (t_loss: 0.07506) (accu: 0.9762)
[epoch : 29] (l_loss: 0.04592) (t_loss: 0.07390) (accu: 0.9759)
[epoch : 30] (l_loss: 0.04536) (t_loss: 0.07259) (accu: 0.9772)
[epoch : 31] (l_loss: 0.04569) (t_loss: 0.07254) (accu: 0.9773)
[epoch : 32] (l_loss: 0.04529) (t_loss: 0.07387) (accu: 0.9762)
[epoch : 33] (l_loss: 0.04521) (t_loss: 0.07274) (accu: 0.9764)
[epoch : 34] (l_loss: 0.04519) (t_loss: 0.07341) (accu: 0.9773)
[epoch : 35] (l_loss: 0.04545) (t_loss: 0.07446) (accu: 0.9762)
[epoch : 36] (l_loss: 0.04472) (t_loss: 0.07688) (accu: 0.9759)
[epoch : 37] (l_loss: 0.04467) (t_loss: 0.07100) (accu: 0.9776)
[epoch : 38] (l_loss: 0.04419) (t_loss: 0.07047) (accu: 0.9773)
[epoch : 39] (l_loss: 0.04439) (t_loss: 0.06822) (accu: 0.9790)
[epoch : 40] (l_loss: 0.04402) (t_loss: 0.07151) (accu: 0.9789)
[epoch : 41] (l_loss: 0.04439) (t_loss: 0.07236) (accu: 0.9776)
[epoch : 42] (l_loss: 0.04385) (t_loss: 0.07111) (accu: 0.9776)
[epoch : 43] (l_loss: 0.04435) (t_loss: 0.07140) (accu: 0.9784)
[epoch : 44] (l_loss: 0.04401) (t_loss: 0.07148) (accu: 0.9769)
[epoch : 45] (l_loss: 0.04367) (t_loss: 0.07201) (accu: 0.9763)
[epoch : 46] (l_loss: 0.04370) (t_loss: 0.07517) (accu: 0.9764)
[epoch : 47] (l_loss: 0.04411) (t_loss: 0.07071) (accu: 0.9787)
[epoch : 48] (l_loss: 0.04416) (t_loss: 0.07132) (accu: 0.9766)
[epoch : 49] (l_loss: 0.04375) (t_loss: 0.07215) (accu: 0.9776)
[epoch : 50] (l_loss: 0.04412) (t_loss: 0.07314) (accu: 0.9771)
Finish! (Best accu: 0.9790) (Time taken(sec) : 743.66) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (23095 | 243105)          8.68
fc1.weight   :      235200 (20204 | 214996)          8.59
fc2.weight   :        30000 (2577 | 27423)           8.59
fcout.weight :          1000 (314 | 686)            31.40
------------------------------------------------------------

Learning start! [Prune_iter : (12/21), Remaining weight : 8.68 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29974) (accu: 0.0995)
[epoch : 1] (l_loss: 0.40318) (t_loss: 0.15074) (accu: 0.9545)
[epoch : 2] (l_loss: 0.12597) (t_loss: 0.10980) (accu: 0.9663)
[epoch : 3] (l_loss: 0.09595) (t_loss: 0.09597) (accu: 0.9718)
[epoch : 4] (l_loss: 0.08114) (t_loss: 0.08920) (accu: 0.9732)
[epoch : 5] (l_loss: 0.07230) (t_loss: 0.08170) (accu: 0.9750)
[epoch : 6] (l_loss: 0.06665) (t_loss: 0.07782) (accu: 0.9768)
[epoch : 7] (l_loss: 0.06192) (t_loss: 0.07522) (accu: 0.9773)
[epoch : 8] (l_loss: 0.05903) (t_loss: 0.07868) (accu: 0.9754)
[epoch : 9] (l_loss: 0.05551) (t_loss: 0.07396) (accu: 0.9770)
[epoch : 10] (l_loss: 0.05405) (t_loss: 0.07385) (accu: 0.9774)
[epoch : 11] (l_loss: 0.05209) (t_loss: 0.07714) (accu: 0.9757)
[epoch : 12] (l_loss: 0.05174) (t_loss: 0.07383) (accu: 0.9764)
[epoch : 13] (l_loss: 0.05064) (t_loss: 0.07683) (accu: 0.9755)
[epoch : 14] (l_loss: 0.04952) (t_loss: 0.07714) (accu: 0.9767)
[epoch : 15] (l_loss: 0.04885) (t_loss: 0.07637) (accu: 0.9765)
[epoch : 16] (l_loss: 0.04857) (t_loss: 0.07477) (accu: 0.9762)
[epoch : 17] (l_loss: 0.04816) (t_loss: 0.07227) (accu: 0.9774)
[epoch : 18] (l_loss: 0.04791) (t_loss: 0.07064) (accu: 0.9775)
[epoch : 19] (l_loss: 0.04753) (t_loss: 0.07370) (accu: 0.9767)
[epoch : 20] (l_loss: 0.04737) (t_loss: 0.07668) (accu: 0.9766)
[epoch : 21] (l_loss: 0.04748) (t_loss: 0.07469) (accu: 0.9769)
[epoch : 22] (l_loss: 0.04737) (t_loss: 0.07447) (accu: 0.9767)
[epoch : 23] (l_loss: 0.04678) (t_loss: 0.07051) (accu: 0.9791)
[epoch : 24] (l_loss: 0.04712) (t_loss: 0.07385) (accu: 0.9767)
[epoch : 25] (l_loss: 0.04697) (t_loss: 0.07264) (accu: 0.9782)
[epoch : 26] (l_loss: 0.04645) (t_loss: 0.07226) (accu: 0.9775)
[epoch : 27] (l_loss: 0.04637) (t_loss: 0.07215) (accu: 0.9779)
[epoch : 28] (l_loss: 0.04626) (t_loss: 0.07209) (accu: 0.9771)
[epoch : 29] (l_loss: 0.04636) (t_loss: 0.07343) (accu: 0.9771)
[epoch : 30] (l_loss: 0.04612) (t_loss: 0.07487) (accu: 0.9763)
[epoch : 31] (l_loss: 0.04660) (t_loss: 0.07224) (accu: 0.9768)
[epoch : 32] (l_loss: 0.04629) (t_loss: 0.07238) (accu: 0.9773)
[epoch : 33] (l_loss: 0.04629) (t_loss: 0.07530) (accu: 0.9754)
[epoch : 34] (l_loss: 0.04616) (t_loss: 0.07257) (accu: 0.9772)
[epoch : 35] (l_loss: 0.04606) (t_loss: 0.07551) (accu: 0.9771)
[epoch : 36] (l_loss: 0.04618) (t_loss: 0.07193) (accu: 0.9775)
[epoch : 37] (l_loss: 0.04658) (t_loss: 0.07280) (accu: 0.9785)
[epoch : 38] (l_loss: 0.04590) (t_loss: 0.07450) (accu: 0.9772)
[epoch : 39] (l_loss: 0.04598) (t_loss: 0.07692) (accu: 0.9773)
[epoch : 40] (l_loss: 0.04674) (t_loss: 0.07299) (accu: 0.9764)
[epoch : 41] (l_loss: 0.04603) (t_loss: 0.07201) (accu: 0.9778)
[epoch : 42] (l_loss: 0.04647) (t_loss: 0.07169) (accu: 0.9767)
[epoch : 43] (l_loss: 0.04630) (t_loss: 0.07386) (accu: 0.9759)
[epoch : 44] (l_loss: 0.04585) (t_loss: 0.07551) (accu: 0.9763)
[epoch : 45] (l_loss: 0.04644) (t_loss: 0.07543) (accu: 0.9757)
[epoch : 46] (l_loss: 0.04656) (t_loss: 0.07214) (accu: 0.9783)
[epoch : 47] (l_loss: 0.04603) (t_loss: 0.07166) (accu: 0.9773)
[epoch : 48] (l_loss: 0.04591) (t_loss: 0.07177) (accu: 0.9778)
[epoch : 49] (l_loss: 0.04586) (t_loss: 0.07441) (accu: 0.9762)
[epoch : 50] (l_loss: 0.04562) (t_loss: 0.07058) (accu: 0.9778)
Finish! (Best accu: 0.9791) (Time taken(sec) : 763.02) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (18507 | 247693)          6.95
fc1.weight   :      235200 (16163 | 219037)          6.87
fc2.weight   :        30000 (2062 | 27938)           6.87
fcout.weight :          1000 (282 | 718)            28.20
------------------------------------------------------------

Learning start! [Prune_iter : (13/21), Remaining weight : 6.95 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30031) (accu: 0.0981)
[epoch : 1] (l_loss: 0.39686) (t_loss: 0.14564) (accu: 0.9581)
[epoch : 2] (l_loss: 0.12263) (t_loss: 0.11061) (accu: 0.9666)
[epoch : 3] (l_loss: 0.09296) (t_loss: 0.09289) (accu: 0.9718)
[epoch : 4] (l_loss: 0.07856) (t_loss: 0.08876) (accu: 0.9743)
[epoch : 5] (l_loss: 0.06996) (t_loss: 0.08328) (accu: 0.9744)
[epoch : 6] (l_loss: 0.06427) (t_loss: 0.07623) (accu: 0.9767)
[epoch : 7] (l_loss: 0.05984) (t_loss: 0.07672) (accu: 0.9764)
[epoch : 8] (l_loss: 0.05683) (t_loss: 0.07707) (accu: 0.9756)
[epoch : 9] (l_loss: 0.05396) (t_loss: 0.07531) (accu: 0.9768)
[epoch : 10] (l_loss: 0.05216) (t_loss: 0.07484) (accu: 0.9769)
[epoch : 11] (l_loss: 0.05014) (t_loss: 0.07476) (accu: 0.9772)
[epoch : 12] (l_loss: 0.04895) (t_loss: 0.07539) (accu: 0.9772)
[epoch : 13] (l_loss: 0.04763) (t_loss: 0.07501) (accu: 0.9763)
[epoch : 14] (l_loss: 0.04779) (t_loss: 0.07225) (accu: 0.9789)
[epoch : 15] (l_loss: 0.04655) (t_loss: 0.07510) (accu: 0.9768)
[epoch : 16] (l_loss: 0.04655) (t_loss: 0.07343) (accu: 0.9773)
[epoch : 17] (l_loss: 0.04591) (t_loss: 0.07499) (accu: 0.9769)
[epoch : 18] (l_loss: 0.04604) (t_loss: 0.07406) (accu: 0.9770)
[epoch : 19] (l_loss: 0.04582) (t_loss: 0.07517) (accu: 0.9772)
[epoch : 20] (l_loss: 0.04579) (t_loss: 0.06971) (accu: 0.9791)
[epoch : 21] (l_loss: 0.04577) (t_loss: 0.07149) (accu: 0.9773)
[epoch : 22] (l_loss: 0.04542) (t_loss: 0.07513) (accu: 0.9771)
[epoch : 23] (l_loss: 0.04555) (t_loss: 0.07321) (accu: 0.9774)
[epoch : 24] (l_loss: 0.04537) (t_loss: 0.07382) (accu: 0.9767)
[epoch : 25] (l_loss: 0.04514) (t_loss: 0.07287) (accu: 0.9777)
[epoch : 26] (l_loss: 0.04534) (t_loss: 0.07209) (accu: 0.9780)
[epoch : 27] (l_loss: 0.04538) (t_loss: 0.07663) (accu: 0.9753)
[epoch : 28] (l_loss: 0.04495) (t_loss: 0.07284) (accu: 0.9778)
[epoch : 29] (l_loss: 0.04506) (t_loss: 0.07230) (accu: 0.9772)
[epoch : 30] (l_loss: 0.04517) (t_loss: 0.07406) (accu: 0.9766)
[epoch : 31] (l_loss: 0.04480) (t_loss: 0.07159) (accu: 0.9769)
[epoch : 32] (l_loss: 0.04534) (t_loss: 0.07286) (accu: 0.9780)
[epoch : 33] (l_loss: 0.04544) (t_loss: 0.07171) (accu: 0.9772)
[epoch : 34] (l_loss: 0.04538) (t_loss: 0.07875) (accu: 0.9744)
[epoch : 35] (l_loss: 0.04500) (t_loss: 0.07071) (accu: 0.9773)
[epoch : 36] (l_loss: 0.04524) (t_loss: 0.07459) (accu: 0.9781)
[epoch : 37] (l_loss: 0.04528) (t_loss: 0.07584) (accu: 0.9753)
[epoch : 38] (l_loss: 0.04494) (t_loss: 0.07277) (accu: 0.9776)
[epoch : 39] (l_loss: 0.04473) (t_loss: 0.07462) (accu: 0.9771)
[epoch : 40] (l_loss: 0.04528) (t_loss: 0.07540) (accu: 0.9764)
[epoch : 41] (l_loss: 0.04513) (t_loss: 0.07346) (accu: 0.9764)
[epoch : 42] (l_loss: 0.04535) (t_loss: 0.07490) (accu: 0.9761)
[epoch : 43] (l_loss: 0.04470) (t_loss: 0.07010) (accu: 0.9779)
[epoch : 44] (l_loss: 0.04506) (t_loss: 0.07204) (accu: 0.9770)
[epoch : 45] (l_loss: 0.04521) (t_loss: 0.07427) (accu: 0.9767)
[epoch : 46] (l_loss: 0.04532) (t_loss: 0.07065) (accu: 0.9780)
[epoch : 47] (l_loss: 0.04401) (t_loss: 0.06983) (accu: 0.9778)
[epoch : 48] (l_loss: 0.04446) (t_loss: 0.07101) (accu: 0.9776)
[epoch : 49] (l_loss: 0.04403) (t_loss: 0.07394) (accu: 0.9768)
[epoch : 50] (l_loss: 0.04441) (t_loss: 0.07372) (accu: 0.9780)
Finish! (Best accu: 0.9791) (Time taken(sec) : 782.56) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (14833 | 251367)          5.57
fc1.weight   :      235200 (12930 | 222270)          5.50
fc2.weight   :        30000 (1649 | 28351)           5.50
fcout.weight :          1000 (254 | 746)            25.40
------------------------------------------------------------

Learning start! [Prune_iter : (14/21), Remaining weight : 5.57 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.30071) (accu: 0.0990)
[epoch : 1] (l_loss: 0.39926) (t_loss: 0.15259) (accu: 0.9565)
[epoch : 2] (l_loss: 0.12697) (t_loss: 0.11085) (accu: 0.9660)
[epoch : 3] (l_loss: 0.09653) (t_loss: 0.09437) (accu: 0.9715)
[epoch : 4] (l_loss: 0.08223) (t_loss: 0.09253) (accu: 0.9725)
[epoch : 5] (l_loss: 0.07364) (t_loss: 0.08435) (accu: 0.9745)
[epoch : 6] (l_loss: 0.06718) (t_loss: 0.08271) (accu: 0.9754)
[epoch : 7] (l_loss: 0.06233) (t_loss: 0.08195) (accu: 0.9752)
[epoch : 8] (l_loss: 0.05877) (t_loss: 0.07844) (accu: 0.9758)
[epoch : 9] (l_loss: 0.05537) (t_loss: 0.07953) (accu: 0.9755)
[epoch : 10] (l_loss: 0.05277) (t_loss: 0.07752) (accu: 0.9773)
[epoch : 11] (l_loss: 0.05104) (t_loss: 0.07478) (accu: 0.9776)
[epoch : 12] (l_loss: 0.05006) (t_loss: 0.07380) (accu: 0.9774)
[epoch : 13] (l_loss: 0.04864) (t_loss: 0.07690) (accu: 0.9765)
[epoch : 14] (l_loss: 0.04800) (t_loss: 0.07749) (accu: 0.9756)
[epoch : 15] (l_loss: 0.04781) (t_loss: 0.07811) (accu: 0.9761)
[epoch : 16] (l_loss: 0.04752) (t_loss: 0.07515) (accu: 0.9758)
[epoch : 17] (l_loss: 0.04681) (t_loss: 0.07323) (accu: 0.9770)
[epoch : 18] (l_loss: 0.04672) (t_loss: 0.07425) (accu: 0.9775)
[epoch : 19] (l_loss: 0.04643) (t_loss: 0.07674) (accu: 0.9758)
[epoch : 20] (l_loss: 0.04641) (t_loss: 0.07577) (accu: 0.9767)
[epoch : 21] (l_loss: 0.04577) (t_loss: 0.07072) (accu: 0.9780)
[epoch : 22] (l_loss: 0.04590) (t_loss: 0.07756) (accu: 0.9749)
[epoch : 23] (l_loss: 0.04541) (t_loss: 0.07494) (accu: 0.9763)
[epoch : 24] (l_loss: 0.04561) (t_loss: 0.07380) (accu: 0.9775)
[epoch : 25] (l_loss: 0.04576) (t_loss: 0.07284) (accu: 0.9759)
[epoch : 26] (l_loss: 0.04556) (t_loss: 0.07308) (accu: 0.9758)
[epoch : 27] (l_loss: 0.04524) (t_loss: 0.07176) (accu: 0.9761)
[epoch : 28] (l_loss: 0.04582) (t_loss: 0.07262) (accu: 0.9779)
[epoch : 29] (l_loss: 0.04504) (t_loss: 0.07372) (accu: 0.9766)
[epoch : 30] (l_loss: 0.04516) (t_loss: 0.07259) (accu: 0.9763)
[epoch : 31] (l_loss: 0.04539) (t_loss: 0.07340) (accu: 0.9777)
[epoch : 32] (l_loss: 0.04581) (t_loss: 0.07301) (accu: 0.9764)
[epoch : 33] (l_loss: 0.04516) (t_loss: 0.07581) (accu: 0.9758)
[epoch : 34] (l_loss: 0.04527) (t_loss: 0.07465) (accu: 0.9787)
[epoch : 35] (l_loss: 0.04505) (t_loss: 0.07436) (accu: 0.9754)
[epoch : 36] (l_loss: 0.04444) (t_loss: 0.07623) (accu: 0.9759)
[epoch : 37] (l_loss: 0.04478) (t_loss: 0.07367) (accu: 0.9766)
[epoch : 38] (l_loss: 0.04419) (t_loss: 0.07339) (accu: 0.9765)
[epoch : 39] (l_loss: 0.04443) (t_loss: 0.06943) (accu: 0.9792)
[epoch : 40] (l_loss: 0.04447) (t_loss: 0.06872) (accu: 0.9799)
[epoch : 41] (l_loss: 0.04380) (t_loss: 0.07049) (accu: 0.9782)
[epoch : 42] (l_loss: 0.04413) (t_loss: 0.07347) (accu: 0.9757)
[epoch : 43] (l_loss: 0.04407) (t_loss: 0.07169) (accu: 0.9783)
[epoch : 44] (l_loss: 0.04394) (t_loss: 0.07234) (accu: 0.9780)
[epoch : 45] (l_loss: 0.04455) (t_loss: 0.07459) (accu: 0.9766)
[epoch : 46] (l_loss: 0.04430) (t_loss: 0.06993) (accu: 0.9781)
[epoch : 47] (l_loss: 0.04439) (t_loss: 0.07255) (accu: 0.9775)
[epoch : 48] (l_loss: 0.04410) (t_loss: 0.07417) (accu: 0.9765)
[epoch : 49] (l_loss: 0.04421) (t_loss: 0.06938) (accu: 0.9775)
[epoch : 50] (l_loss: 0.04394) (t_loss: 0.07310) (accu: 0.9770)
Finish! (Best accu: 0.9799) (Time taken(sec) : 791.34) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (11892 | 254308)          4.47
fc1.weight   :      235200 (10344 | 224856)          4.40
fc2.weight   :        30000 (1319 | 28681)           4.40
fcout.weight :          1000 (229 | 771)            22.90
------------------------------------------------------------

Learning start! [Prune_iter : (15/21), Remaining weight : 4.47 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29811) (accu: 0.1105)
[epoch : 1] (l_loss: 0.39810) (t_loss: 0.14955) (accu: 0.9578)
[epoch : 2] (l_loss: 0.12644) (t_loss: 0.10731) (accu: 0.9684)
[epoch : 3] (l_loss: 0.09645) (t_loss: 0.09623) (accu: 0.9707)
[epoch : 4] (l_loss: 0.08251) (t_loss: 0.09596) (accu: 0.9702)
[epoch : 5] (l_loss: 0.07475) (t_loss: 0.08781) (accu: 0.9736)
[epoch : 6] (l_loss: 0.06897) (t_loss: 0.08021) (accu: 0.9766)
[epoch : 7] (l_loss: 0.06432) (t_loss: 0.07802) (accu: 0.9759)
[epoch : 8] (l_loss: 0.06125) (t_loss: 0.08362) (accu: 0.9723)
[epoch : 9] (l_loss: 0.05807) (t_loss: 0.07572) (accu: 0.9767)
[epoch : 10] (l_loss: 0.05505) (t_loss: 0.07399) (accu: 0.9777)
[epoch : 11] (l_loss: 0.05309) (t_loss: 0.07446) (accu: 0.9770)
[epoch : 12] (l_loss: 0.05163) (t_loss: 0.07349) (accu: 0.9774)
[epoch : 13] (l_loss: 0.05087) (t_loss: 0.07431) (accu: 0.9763)
[epoch : 14] (l_loss: 0.05022) (t_loss: 0.07501) (accu: 0.9766)
[epoch : 15] (l_loss: 0.04892) (t_loss: 0.07347) (accu: 0.9771)
[epoch : 16] (l_loss: 0.04844) (t_loss: 0.07724) (accu: 0.9745)
[epoch : 17] (l_loss: 0.04761) (t_loss: 0.07521) (accu: 0.9769)
[epoch : 18] (l_loss: 0.04765) (t_loss: 0.07305) (accu: 0.9759)
[epoch : 19] (l_loss: 0.04725) (t_loss: 0.07734) (accu: 0.9762)
[epoch : 20] (l_loss: 0.04751) (t_loss: 0.07676) (accu: 0.9750)
[epoch : 21] (l_loss: 0.04689) (t_loss: 0.07588) (accu: 0.9777)
[epoch : 22] (l_loss: 0.04670) (t_loss: 0.07182) (accu: 0.9769)
[epoch : 23] (l_loss: 0.04668) (t_loss: 0.07653) (accu: 0.9740)
[epoch : 24] (l_loss: 0.04674) (t_loss: 0.07541) (accu: 0.9762)
[epoch : 25] (l_loss: 0.04660) (t_loss: 0.07242) (accu: 0.9781)
[epoch : 26] (l_loss: 0.04661) (t_loss: 0.07409) (accu: 0.9771)
[epoch : 27] (l_loss: 0.04642) (t_loss: 0.07692) (accu: 0.9758)
[epoch : 28] (l_loss: 0.04573) (t_loss: 0.07281) (accu: 0.9763)
[epoch : 29] (l_loss: 0.04563) (t_loss: 0.07341) (accu: 0.9776)
[epoch : 30] (l_loss: 0.04642) (t_loss: 0.07247) (accu: 0.9772)
[epoch : 31] (l_loss: 0.04554) (t_loss: 0.07160) (accu: 0.9782)
[epoch : 32] (l_loss: 0.04613) (t_loss: 0.07686) (accu: 0.9765)
[epoch : 33] (l_loss: 0.04552) (t_loss: 0.07414) (accu: 0.9776)
[epoch : 34] (l_loss: 0.04564) (t_loss: 0.07315) (accu: 0.9770)
[epoch : 35] (l_loss: 0.04545) (t_loss: 0.07314) (accu: 0.9776)
[epoch : 36] (l_loss: 0.04492) (t_loss: 0.06941) (accu: 0.9782)
[epoch : 37] (l_loss: 0.04451) (t_loss: 0.07454) (accu: 0.9772)
[epoch : 38] (l_loss: 0.04498) (t_loss: 0.07489) (accu: 0.9756)
[epoch : 39] (l_loss: 0.04494) (t_loss: 0.07364) (accu: 0.9772)
[epoch : 40] (l_loss: 0.04483) (t_loss: 0.07189) (accu: 0.9775)
[epoch : 41] (l_loss: 0.04483) (t_loss: 0.07213) (accu: 0.9776)
[epoch : 42] (l_loss: 0.04495) (t_loss: 0.07443) (accu: 0.9774)
[epoch : 43] (l_loss: 0.04506) (t_loss: 0.07187) (accu: 0.9769)
[epoch : 44] (l_loss: 0.04503) (t_loss: 0.06975) (accu: 0.9778)
[epoch : 45] (l_loss: 0.04497) (t_loss: 0.07225) (accu: 0.9770)
[epoch : 46] (l_loss: 0.04427) (t_loss: 0.07149) (accu: 0.9780)
[epoch : 47] (l_loss: 0.04453) (t_loss: 0.07304) (accu: 0.9776)
[epoch : 48] (l_loss: 0.04476) (t_loss: 0.07549) (accu: 0.9771)
[epoch : 49] (l_loss: 0.04496) (t_loss: 0.07173) (accu: 0.9767)
[epoch : 50] (l_loss: 0.04426) (t_loss: 0.07518) (accu: 0.9768)
Finish! (Best accu: 0.9782) (Time taken(sec) : 783.89) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (9537 | 256663)          3.58
fc1.weight   :       235200 (8275 | 226925)          3.52
fc2.weight   :        30000 (1056 | 28944)           3.52
fcout.weight :          1000 (206 | 794)            20.60
------------------------------------------------------------

Learning start! [Prune_iter : (16/21), Remaining weight : 3.58 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29411) (accu: 0.0979)
[epoch : 1] (l_loss: 0.39808) (t_loss: 0.14822) (accu: 0.9564)
[epoch : 2] (l_loss: 0.12596) (t_loss: 0.10808) (accu: 0.9687)
[epoch : 3] (l_loss: 0.09534) (t_loss: 0.09413) (accu: 0.9715)
[epoch : 4] (l_loss: 0.08188) (t_loss: 0.08901) (accu: 0.9734)
[epoch : 5] (l_loss: 0.07266) (t_loss: 0.08213) (accu: 0.9758)
[epoch : 6] (l_loss: 0.06716) (t_loss: 0.08097) (accu: 0.9749)
[epoch : 7] (l_loss: 0.06187) (t_loss: 0.07686) (accu: 0.9763)
[epoch : 8] (l_loss: 0.05837) (t_loss: 0.07281) (accu: 0.9788)
[epoch : 9] (l_loss: 0.05579) (t_loss: 0.07447) (accu: 0.9779)
[epoch : 10] (l_loss: 0.05384) (t_loss: 0.07587) (accu: 0.9782)
[epoch : 11] (l_loss: 0.05103) (t_loss: 0.07233) (accu: 0.9779)
[epoch : 12] (l_loss: 0.04962) (t_loss: 0.07579) (accu: 0.9754)
[epoch : 13] (l_loss: 0.04823) (t_loss: 0.07480) (accu: 0.9771)
[epoch : 14] (l_loss: 0.04768) (t_loss: 0.07402) (accu: 0.9768)
[epoch : 15] (l_loss: 0.04727) (t_loss: 0.07633) (accu: 0.9770)
[epoch : 16] (l_loss: 0.04658) (t_loss: 0.07367) (accu: 0.9770)
[epoch : 17] (l_loss: 0.04655) (t_loss: 0.07221) (accu: 0.9786)
[epoch : 18] (l_loss: 0.04609) (t_loss: 0.07230) (accu: 0.9782)
[epoch : 19] (l_loss: 0.04645) (t_loss: 0.07161) (accu: 0.9780)
[epoch : 20] (l_loss: 0.04608) (t_loss: 0.07399) (accu: 0.9767)
[epoch : 21] (l_loss: 0.04540) (t_loss: 0.07303) (accu: 0.9779)
[epoch : 22] (l_loss: 0.04531) (t_loss: 0.07273) (accu: 0.9773)
[epoch : 23] (l_loss: 0.04552) (t_loss: 0.07375) (accu: 0.9772)
[epoch : 24] (l_loss: 0.04584) (t_loss: 0.07056) (accu: 0.9777)
[epoch : 25] (l_loss: 0.04553) (t_loss: 0.07493) (accu: 0.9760)
[epoch : 26] (l_loss: 0.04510) (t_loss: 0.07671) (accu: 0.9751)
[epoch : 27] (l_loss: 0.04561) (t_loss: 0.07125) (accu: 0.9778)
[epoch : 28] (l_loss: 0.04510) (t_loss: 0.07293) (accu: 0.9769)
[epoch : 29] (l_loss: 0.04538) (t_loss: 0.07640) (accu: 0.9750)
[epoch : 30] (l_loss: 0.04508) (t_loss: 0.07129) (accu: 0.9782)
[epoch : 31] (l_loss: 0.04550) (t_loss: 0.07610) (accu: 0.9768)
[epoch : 32] (l_loss: 0.04518) (t_loss: 0.07286) (accu: 0.9780)
[epoch : 33] (l_loss: 0.04488) (t_loss: 0.07114) (accu: 0.9779)
[epoch : 34] (l_loss: 0.04461) (t_loss: 0.07224) (accu: 0.9779)
[epoch : 35] (l_loss: 0.04442) (t_loss: 0.07059) (accu: 0.9782)
[epoch : 36] (l_loss: 0.04397) (t_loss: 0.07032) (accu: 0.9774)
[epoch : 37] (l_loss: 0.04381) (t_loss: 0.07184) (accu: 0.9781)
[epoch : 38] (l_loss: 0.04444) (t_loss: 0.07361) (accu: 0.9769)
[epoch : 39] (l_loss: 0.04433) (t_loss: 0.07376) (accu: 0.9763)
[epoch : 40] (l_loss: 0.04371) (t_loss: 0.07290) (accu: 0.9789)
[epoch : 41] (l_loss: 0.04402) (t_loss: 0.07115) (accu: 0.9780)
[epoch : 42] (l_loss: 0.04411) (t_loss: 0.07086) (accu: 0.9791)
[epoch : 43] (l_loss: 0.04373) (t_loss: 0.06972) (accu: 0.9789)
[epoch : 44] (l_loss: 0.04398) (t_loss: 0.07179) (accu: 0.9773)
[epoch : 45] (l_loss: 0.04367) (t_loss: 0.07700) (accu: 0.9752)
[epoch : 46] (l_loss: 0.04414) (t_loss: 0.07029) (accu: 0.9789)
[epoch : 47] (l_loss: 0.04383) (t_loss: 0.07092) (accu: 0.9785)
[epoch : 48] (l_loss: 0.04407) (t_loss: 0.07100) (accu: 0.9770)
[epoch : 49] (l_loss: 0.04415) (t_loss: 0.07119) (accu: 0.9784)
[epoch : 50] (l_loss: 0.04403) (t_loss: 0.07193) (accu: 0.9783)
Finish! (Best accu: 0.9791) (Time taken(sec) : 762.03) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (7649 | 258551)          2.87
fc1.weight   :       235200 (6620 | 228580)          2.81
fc2.weight   :        30000 (844 | 29156)            2.81
fcout.weight :          1000 (185 | 815)            18.50
------------------------------------------------------------

Learning start! [Prune_iter : (17/21), Remaining weight : 2.87 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29836) (accu: 0.0974)
[epoch : 1] (l_loss: 0.40014) (t_loss: 0.15232) (accu: 0.9554)
[epoch : 2] (l_loss: 0.12652) (t_loss: 0.10902) (accu: 0.9667)
[epoch : 3] (l_loss: 0.09512) (t_loss: 0.09314) (accu: 0.9737)
[epoch : 4] (l_loss: 0.08069) (t_loss: 0.08990) (accu: 0.9725)
[epoch : 5] (l_loss: 0.07205) (t_loss: 0.08116) (accu: 0.9741)
[epoch : 6] (l_loss: 0.06645) (t_loss: 0.07833) (accu: 0.9760)
[epoch : 7] (l_loss: 0.06134) (t_loss: 0.07696) (accu: 0.9766)
[epoch : 8] (l_loss: 0.05768) (t_loss: 0.07590) (accu: 0.9771)
[epoch : 9] (l_loss: 0.05503) (t_loss: 0.07431) (accu: 0.9776)
[epoch : 10] (l_loss: 0.05268) (t_loss: 0.07653) (accu: 0.9753)
[epoch : 11] (l_loss: 0.05010) (t_loss: 0.07233) (accu: 0.9769)
[epoch : 12] (l_loss: 0.04899) (t_loss: 0.07663) (accu: 0.9767)
[epoch : 13] (l_loss: 0.04875) (t_loss: 0.07424) (accu: 0.9766)
[epoch : 14] (l_loss: 0.04768) (t_loss: 0.07266) (accu: 0.9770)
[epoch : 15] (l_loss: 0.04707) (t_loss: 0.07153) (accu: 0.9789)
[epoch : 16] (l_loss: 0.04608) (t_loss: 0.07057) (accu: 0.9775)
[epoch : 17] (l_loss: 0.04558) (t_loss: 0.07323) (accu: 0.9753)
[epoch : 18] (l_loss: 0.04536) (t_loss: 0.07131) (accu: 0.9781)
[epoch : 19] (l_loss: 0.04473) (t_loss: 0.07243) (accu: 0.9776)
[epoch : 20] (l_loss: 0.04486) (t_loss: 0.07246) (accu: 0.9779)
[epoch : 21] (l_loss: 0.04469) (t_loss: 0.07228) (accu: 0.9765)
[epoch : 22] (l_loss: 0.04424) (t_loss: 0.07069) (accu: 0.9789)
[epoch : 23] (l_loss: 0.04424) (t_loss: 0.07178) (accu: 0.9771)
[epoch : 24] (l_loss: 0.04423) (t_loss: 0.06871) (accu: 0.9792)
[epoch : 25] (l_loss: 0.04380) (t_loss: 0.07259) (accu: 0.9779)
[epoch : 26] (l_loss: 0.04396) (t_loss: 0.07470) (accu: 0.9762)
[epoch : 27] (l_loss: 0.04376) (t_loss: 0.07099) (accu: 0.9782)
[epoch : 28] (l_loss: 0.04362) (t_loss: 0.07162) (accu: 0.9778)
[epoch : 29] (l_loss: 0.04375) (t_loss: 0.07111) (accu: 0.9787)
[epoch : 30] (l_loss: 0.04405) (t_loss: 0.07234) (accu: 0.9771)
[epoch : 31] (l_loss: 0.04381) (t_loss: 0.07185) (accu: 0.9776)
[epoch : 32] (l_loss: 0.04348) (t_loss: 0.07099) (accu: 0.9785)
[epoch : 33] (l_loss: 0.04424) (t_loss: 0.07639) (accu: 0.9756)
[epoch : 34] (l_loss: 0.04322) (t_loss: 0.07192) (accu: 0.9780)
[epoch : 35] (l_loss: 0.04384) (t_loss: 0.07046) (accu: 0.9784)
[epoch : 36] (l_loss: 0.04400) (t_loss: 0.06946) (accu: 0.9776)
[epoch : 37] (l_loss: 0.04359) (t_loss: 0.07241) (accu: 0.9762)
[epoch : 38] (l_loss: 0.04382) (t_loss: 0.07579) (accu: 0.9771)
[epoch : 39] (l_loss: 0.04346) (t_loss: 0.07102) (accu: 0.9785)
[epoch : 40] (l_loss: 0.04370) (t_loss: 0.07440) (accu: 0.9765)
[epoch : 41] (l_loss: 0.04335) (t_loss: 0.07384) (accu: 0.9768)
[epoch : 42] (l_loss: 0.04369) (t_loss: 0.07174) (accu: 0.9775)
[epoch : 43] (l_loss: 0.04373) (t_loss: 0.07269) (accu: 0.9775)
[epoch : 44] (l_loss: 0.04348) (t_loss: 0.07115) (accu: 0.9778)
[epoch : 45] (l_loss: 0.04370) (t_loss: 0.07083) (accu: 0.9773)
[epoch : 46] (l_loss: 0.04373) (t_loss: 0.06862) (accu: 0.9795)
[epoch : 47] (l_loss: 0.04377) (t_loss: 0.07341) (accu: 0.9787)
[epoch : 48] (l_loss: 0.04340) (t_loss: 0.07008) (accu: 0.9772)
[epoch : 49] (l_loss: 0.04375) (t_loss: 0.07244) (accu: 0.9770)
[epoch : 50] (l_loss: 0.04327) (t_loss: 0.07000) (accu: 0.9782)
Finish! (Best accu: 0.9795) (Time taken(sec) : 792.43) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (6139 | 260061)          2.31
fc1.weight   :       235200 (5296 | 229904)          2.25
fc2.weight   :        30000 (676 | 29324)            2.25
fcout.weight :          1000 (167 | 833)            16.70
------------------------------------------------------------

Learning start! [Prune_iter : (18/21), Remaining weight : 2.31 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29749) (accu: 0.0974)
[epoch : 1] (l_loss: 0.39879) (t_loss: 0.14856) (accu: 0.9563)
[epoch : 2] (l_loss: 0.12482) (t_loss: 0.11146) (accu: 0.9662)
[epoch : 3] (l_loss: 0.09456) (t_loss: 0.09368) (accu: 0.9720)
[epoch : 4] (l_loss: 0.07956) (t_loss: 0.08541) (accu: 0.9745)
[epoch : 5] (l_loss: 0.07084) (t_loss: 0.08371) (accu: 0.9746)
[epoch : 6] (l_loss: 0.06402) (t_loss: 0.07913) (accu: 0.9752)
[epoch : 7] (l_loss: 0.05972) (t_loss: 0.07555) (accu: 0.9773)
[epoch : 8] (l_loss: 0.05640) (t_loss: 0.07646) (accu: 0.9767)
[epoch : 9] (l_loss: 0.05326) (t_loss: 0.07420) (accu: 0.9771)
[epoch : 10] (l_loss: 0.05025) (t_loss: 0.07328) (accu: 0.9772)
[epoch : 11] (l_loss: 0.04958) (t_loss: 0.07306) (accu: 0.9774)
[epoch : 12] (l_loss: 0.04798) (t_loss: 0.07222) (accu: 0.9778)
[epoch : 13] (l_loss: 0.04716) (t_loss: 0.07783) (accu: 0.9759)
[epoch : 14] (l_loss: 0.04665) (t_loss: 0.07046) (accu: 0.9787)
[epoch : 15] (l_loss: 0.04649) (t_loss: 0.07735) (accu: 0.9757)
[epoch : 16] (l_loss: 0.04573) (t_loss: 0.07397) (accu: 0.9774)
[epoch : 17] (l_loss: 0.04582) (t_loss: 0.07187) (accu: 0.9769)
[epoch : 18] (l_loss: 0.04530) (t_loss: 0.07583) (accu: 0.9782)
[epoch : 19] (l_loss: 0.04532) (t_loss: 0.07061) (accu: 0.9783)
[epoch : 20] (l_loss: 0.04506) (t_loss: 0.07297) (accu: 0.9774)
[epoch : 21] (l_loss: 0.04510) (t_loss: 0.07549) (accu: 0.9767)
[epoch : 22] (l_loss: 0.04489) (t_loss: 0.07284) (accu: 0.9775)
[epoch : 23] (l_loss: 0.04440) (t_loss: 0.07398) (accu: 0.9766)
[epoch : 24] (l_loss: 0.04479) (t_loss: 0.07195) (accu: 0.9775)
[epoch : 25] (l_loss: 0.04499) (t_loss: 0.07361) (accu: 0.9768)
[epoch : 26] (l_loss: 0.04458) (t_loss: 0.07337) (accu: 0.9773)
[epoch : 27] (l_loss: 0.04438) (t_loss: 0.07530) (accu: 0.9753)
[epoch : 28] (l_loss: 0.04442) (t_loss: 0.07147) (accu: 0.9779)
[epoch : 29] (l_loss: 0.04457) (t_loss: 0.07145) (accu: 0.9778)
[epoch : 30] (l_loss: 0.04407) (t_loss: 0.07182) (accu: 0.9780)
[epoch : 31] (l_loss: 0.04451) (t_loss: 0.07059) (accu: 0.9774)
[epoch : 32] (l_loss: 0.04440) (t_loss: 0.07399) (accu: 0.9762)
[epoch : 33] (l_loss: 0.04431) (t_loss: 0.07112) (accu: 0.9775)
[epoch : 34] (l_loss: 0.04449) (t_loss: 0.07156) (accu: 0.9783)
[epoch : 35] (l_loss: 0.04453) (t_loss: 0.07070) (accu: 0.9782)
[epoch : 36] (l_loss: 0.04415) (t_loss: 0.07769) (accu: 0.9758)
[epoch : 37] (l_loss: 0.04432) (t_loss: 0.07319) (accu: 0.9773)
[epoch : 38] (l_loss: 0.04405) (t_loss: 0.07663) (accu: 0.9757)
[epoch : 39] (l_loss: 0.04398) (t_loss: 0.07212) (accu: 0.9765)
[epoch : 40] (l_loss: 0.04421) (t_loss: 0.07310) (accu: 0.9769)
[epoch : 41] (l_loss: 0.04478) (t_loss: 0.07279) (accu: 0.9781)
[epoch : 42] (l_loss: 0.04411) (t_loss: 0.07107) (accu: 0.9774)
[epoch : 43] (l_loss: 0.04443) (t_loss: 0.07405) (accu: 0.9765)
[epoch : 44] (l_loss: 0.04438) (t_loss: 0.07625) (accu: 0.9770)
[epoch : 45] (l_loss: 0.04409) (t_loss: 0.07415) (accu: 0.9770)
[epoch : 46] (l_loss: 0.04424) (t_loss: 0.07051) (accu: 0.9782)
[epoch : 47] (l_loss: 0.04447) (t_loss: 0.07075) (accu: 0.9776)
[epoch : 48] (l_loss: 0.04438) (t_loss: 0.07239) (accu: 0.9787)
[epoch : 49] (l_loss: 0.04469) (t_loss: 0.07233) (accu: 0.9773)
[epoch : 50] (l_loss: 0.04440) (t_loss: 0.07212) (accu: 0.9772)
Finish! (Best accu: 0.9787) (Time taken(sec) : 816.32) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (4927 | 261273)          1.85
fc1.weight   :       235200 (4237 | 230963)          1.80
fc2.weight   :        30000 (540 | 29460)            1.80
fcout.weight :          1000 (150 | 850)            15.00
------------------------------------------------------------

Learning start! [Prune_iter : (19/21), Remaining weight : 1.85 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29507) (accu: 0.0973)
[epoch : 1] (l_loss: 0.39866) (t_loss: 0.14786) (accu: 0.9576)
[epoch : 2] (l_loss: 0.12537) (t_loss: 0.11613) (accu: 0.9661)
[epoch : 3] (l_loss: 0.09527) (t_loss: 0.09393) (accu: 0.9723)
[epoch : 4] (l_loss: 0.08114) (t_loss: 0.08655) (accu: 0.9752)
[epoch : 5] (l_loss: 0.07208) (t_loss: 0.08257) (accu: 0.9752)
[epoch : 6] (l_loss: 0.06584) (t_loss: 0.07935) (accu: 0.9764)
[epoch : 7] (l_loss: 0.06093) (t_loss: 0.07567) (accu: 0.9775)
[epoch : 8] (l_loss: 0.05684) (t_loss: 0.07662) (accu: 0.9767)
[epoch : 9] (l_loss: 0.05384) (t_loss: 0.07496) (accu: 0.9770)
[epoch : 10] (l_loss: 0.05155) (t_loss: 0.07305) (accu: 0.9768)
[epoch : 11] (l_loss: 0.04995) (t_loss: 0.07513) (accu: 0.9763)
[epoch : 12] (l_loss: 0.04864) (t_loss: 0.07543) (accu: 0.9759)
[epoch : 13] (l_loss: 0.04797) (t_loss: 0.07322) (accu: 0.9767)
[epoch : 14] (l_loss: 0.04717) (t_loss: 0.07443) (accu: 0.9773)
[epoch : 15] (l_loss: 0.04653) (t_loss: 0.07410) (accu: 0.9777)
[epoch : 16] (l_loss: 0.04670) (t_loss: 0.07265) (accu: 0.9776)
[epoch : 17] (l_loss: 0.04610) (t_loss: 0.07555) (accu: 0.9762)
[epoch : 18] (l_loss: 0.04650) (t_loss: 0.07407) (accu: 0.9763)
[epoch : 19] (l_loss: 0.04585) (t_loss: 0.07365) (accu: 0.9779)
[epoch : 20] (l_loss: 0.04549) (t_loss: 0.07718) (accu: 0.9751)
[epoch : 21] (l_loss: 0.04549) (t_loss: 0.07557) (accu: 0.9766)
[epoch : 22] (l_loss: 0.04494) (t_loss: 0.07153) (accu: 0.9773)
[epoch : 23] (l_loss: 0.04496) (t_loss: 0.07950) (accu: 0.9757)
[epoch : 24] (l_loss: 0.04528) (t_loss: 0.07439) (accu: 0.9758)
[epoch : 25] (l_loss: 0.04492) (t_loss: 0.07369) (accu: 0.9770)
[epoch : 26] (l_loss: 0.04485) (t_loss: 0.07721) (accu: 0.9751)
[epoch : 27] (l_loss: 0.04525) (t_loss: 0.07231) (accu: 0.9781)
[epoch : 28] (l_loss: 0.04446) (t_loss: 0.07105) (accu: 0.9771)
[epoch : 29] (l_loss: 0.04477) (t_loss: 0.07238) (accu: 0.9779)
[epoch : 30] (l_loss: 0.04502) (t_loss: 0.07183) (accu: 0.9780)
[epoch : 31] (l_loss: 0.04476) (t_loss: 0.07105) (accu: 0.9778)
[epoch : 32] (l_loss: 0.04454) (t_loss: 0.07254) (accu: 0.9777)
[epoch : 33] (l_loss: 0.04473) (t_loss: 0.07018) (accu: 0.9792)
[epoch : 34] (l_loss: 0.04458) (t_loss: 0.07304) (accu: 0.9775)
[epoch : 35] (l_loss: 0.04463) (t_loss: 0.07120) (accu: 0.9780)
[epoch : 36] (l_loss: 0.04496) (t_loss: 0.07244) (accu: 0.9783)
[epoch : 37] (l_loss: 0.04476) (t_loss: 0.07205) (accu: 0.9771)
[epoch : 38] (l_loss: 0.04463) (t_loss: 0.07814) (accu: 0.9749)
[epoch : 39] (l_loss: 0.04474) (t_loss: 0.07106) (accu: 0.9776)
[epoch : 40] (l_loss: 0.04487) (t_loss: 0.07345) (accu: 0.9772)
[epoch : 41] (l_loss: 0.04489) (t_loss: 0.07277) (accu: 0.9762)
[epoch : 42] (l_loss: 0.04503) (t_loss: 0.07327) (accu: 0.9760)
[epoch : 43] (l_loss: 0.04462) (t_loss: 0.07098) (accu: 0.9782)
[epoch : 44] (l_loss: 0.04458) (t_loss: 0.07305) (accu: 0.9769)
[epoch : 45] (l_loss: 0.04479) (t_loss: 0.07411) (accu: 0.9761)
[epoch : 46] (l_loss: 0.04506) (t_loss: 0.07556) (accu: 0.9769)
[epoch : 47] (l_loss: 0.04483) (t_loss: 0.07421) (accu: 0.9782)
[epoch : 48] (l_loss: 0.04463) (t_loss: 0.07468) (accu: 0.9756)
[epoch : 49] (l_loss: 0.04476) (t_loss: 0.07478) (accu: 0.9773)
[epoch : 50] (l_loss: 0.04397) (t_loss: 0.07265) (accu: 0.9782)
Finish! (Best accu: 0.9792) (Time taken(sec) : 793.19) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (3957 | 262243)          1.49
fc1.weight   :       235200 (3390 | 231810)          1.44
fc2.weight   :        30000 (432 | 29568)            1.44
fcout.weight :          1000 (135 | 865)            13.50
------------------------------------------------------------

Learning start! [Prune_iter : (20/21), Remaining weight : 1.49 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29534) (accu: 0.0974)
[epoch : 1] (l_loss: 0.39881) (t_loss: 0.14979) (accu: 0.9562)
[epoch : 2] (l_loss: 0.12611) (t_loss: 0.11010) (accu: 0.9666)
[epoch : 3] (l_loss: 0.09564) (t_loss: 0.10277) (accu: 0.9688)
[epoch : 4] (l_loss: 0.08213) (t_loss: 0.09135) (accu: 0.9730)
[epoch : 5] (l_loss: 0.07334) (t_loss: 0.08403) (accu: 0.9750)
[epoch : 6] (l_loss: 0.06720) (t_loss: 0.08325) (accu: 0.9745)
[epoch : 7] (l_loss: 0.06252) (t_loss: 0.08036) (accu: 0.9765)
[epoch : 8] (l_loss: 0.05905) (t_loss: 0.07640) (accu: 0.9765)
[epoch : 9] (l_loss: 0.05539) (t_loss: 0.08223) (accu: 0.9745)
[epoch : 10] (l_loss: 0.05293) (t_loss: 0.07603) (accu: 0.9769)
[epoch : 11] (l_loss: 0.05134) (t_loss: 0.07368) (accu: 0.9769)
[epoch : 12] (l_loss: 0.04968) (t_loss: 0.07759) (accu: 0.9753)
[epoch : 13] (l_loss: 0.04921) (t_loss: 0.07457) (accu: 0.9766)
[epoch : 14] (l_loss: 0.04854) (t_loss: 0.07052) (accu: 0.9788)
[epoch : 15] (l_loss: 0.04750) (t_loss: 0.07427) (accu: 0.9769)
[epoch : 16] (l_loss: 0.04731) (t_loss: 0.07504) (accu: 0.9768)
[epoch : 17] (l_loss: 0.04705) (t_loss: 0.07617) (accu: 0.9754)
[epoch : 18] (l_loss: 0.04688) (t_loss: 0.07408) (accu: 0.9759)
[epoch : 19] (l_loss: 0.04731) (t_loss: 0.07347) (accu: 0.9777)
[epoch : 20] (l_loss: 0.04705) (t_loss: 0.07373) (accu: 0.9773)
[epoch : 21] (l_loss: 0.04717) (t_loss: 0.07629) (accu: 0.9769)
[epoch : 22] (l_loss: 0.04637) (t_loss: 0.07535) (accu: 0.9756)
[epoch : 23] (l_loss: 0.04690) (t_loss: 0.07626) (accu: 0.9756)
[epoch : 24] (l_loss: 0.04631) (t_loss: 0.07531) (accu: 0.9770)
[epoch : 25] (l_loss: 0.04591) (t_loss: 0.07157) (accu: 0.9775)
[epoch : 26] (l_loss: 0.04558) (t_loss: 0.07584) (accu: 0.9762)
[epoch : 27] (l_loss: 0.04532) (t_loss: 0.07669) (accu: 0.9765)
[epoch : 28] (l_loss: 0.04558) (t_loss: 0.07214) (accu: 0.9785)
[epoch : 29] (l_loss: 0.04453) (t_loss: 0.07225) (accu: 0.9772)
[epoch : 30] (l_loss: 0.04468) (t_loss: 0.07494) (accu: 0.9762)
[epoch : 31] (l_loss: 0.04449) (t_loss: 0.07242) (accu: 0.9774)
[epoch : 32] (l_loss: 0.04411) (t_loss: 0.06878) (accu: 0.9793)
[epoch : 33] (l_loss: 0.04442) (t_loss: 0.07749) (accu: 0.9755)
[epoch : 34] (l_loss: 0.04461) (t_loss: 0.07272) (accu: 0.9779)
[epoch : 35] (l_loss: 0.04406) (t_loss: 0.07378) (accu: 0.9779)
[epoch : 36] (l_loss: 0.04410) (t_loss: 0.07450) (accu: 0.9771)
[epoch : 37] (l_loss: 0.04441) (t_loss: 0.06878) (accu: 0.9784)
[epoch : 38] (l_loss: 0.04405) (t_loss: 0.06962) (accu: 0.9777)
[epoch : 39] (l_loss: 0.04411) (t_loss: 0.07559) (accu: 0.9772)
[epoch : 40] (l_loss: 0.04430) (t_loss: 0.07383) (accu: 0.9770)
[epoch : 41] (l_loss: 0.04372) (t_loss: 0.07091) (accu: 0.9780)
[epoch : 42] (l_loss: 0.04385) (t_loss: 0.06885) (accu: 0.9783)
[epoch : 43] (l_loss: 0.04378) (t_loss: 0.07105) (accu: 0.9787)
[epoch : 44] (l_loss: 0.04410) (t_loss: 0.07091) (accu: 0.9785)
[epoch : 45] (l_loss: 0.04433) (t_loss: 0.07110) (accu: 0.9785)
[epoch : 46] (l_loss: 0.04444) (t_loss: 0.06978) (accu: 0.9788)
[epoch : 47] (l_loss: 0.04421) (t_loss: 0.07229) (accu: 0.9779)
[epoch : 48] (l_loss: 0.04422) (t_loss: 0.07160) (accu: 0.9784)
[epoch : 49] (l_loss: 0.04391) (t_loss: 0.07497) (accu: 0.9769)
[epoch : 50] (l_loss: 0.04404) (t_loss: 0.07314) (accu: 0.9771)
Finish! (Best accu: 0.9793) (Time taken(sec) : 792.34) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (3180 | 263020)          1.19
fc1.weight   :       235200 (2712 | 232488)          1.15
fc2.weight   :        30000 (346 | 29654)            1.15
fcout.weight :          1000 (122 | 878)            12.20
------------------------------------------------------------

Learning start! [Prune_iter : (21/21), Remaining weight : 1.19 %] 

[epoch : 0] (l_loss: x.xxxxx) (t_loss: 2.29718) (accu: 0.0974)
[epoch : 1] (l_loss: 0.39895) (t_loss: 0.15391) (accu: 0.9540)
[epoch : 2] (l_loss: 0.12946) (t_loss: 0.11549) (accu: 0.9658)
[epoch : 3] (l_loss: 0.09933) (t_loss: 0.10258) (accu: 0.9675)
[epoch : 4] (l_loss: 0.08457) (t_loss: 0.09189) (accu: 0.9727)
[epoch : 5] (l_loss: 0.07460) (t_loss: 0.08693) (accu: 0.9749)
[epoch : 6] (l_loss: 0.06858) (t_loss: 0.08060) (accu: 0.9766)
[epoch : 7] (l_loss: 0.06378) (t_loss: 0.07982) (accu: 0.9769)
[epoch : 8] (l_loss: 0.05990) (t_loss: 0.07679) (accu: 0.9757)
[epoch : 9] (l_loss: 0.05607) (t_loss: 0.07381) (accu: 0.9776)
[epoch : 10] (l_loss: 0.05262) (t_loss: 0.07445) (accu: 0.9773)
[epoch : 11] (l_loss: 0.05042) (t_loss: 0.07701) (accu: 0.9770)
[epoch : 12] (l_loss: 0.04902) (t_loss: 0.07317) (accu: 0.9780)
[epoch : 13] (l_loss: 0.04808) (t_loss: 0.07622) (accu: 0.9766)
[epoch : 14] (l_loss: 0.04767) (t_loss: 0.07514) (accu: 0.9770)
[epoch : 15] (l_loss: 0.04716) (t_loss: 0.07312) (accu: 0.9778)
[epoch : 16] (l_loss: 0.04700) (t_loss: 0.07286) (accu: 0.9775)
[epoch : 17] (l_loss: 0.04620) (t_loss: 0.07331) (accu: 0.9767)
[epoch : 18] (l_loss: 0.04629) (t_loss: 0.07185) (accu: 0.9779)
[epoch : 19] (l_loss: 0.04592) (t_loss: 0.07343) (accu: 0.9771)
[epoch : 20] (l_loss: 0.04532) (t_loss: 0.07317) (accu: 0.9774)
[epoch : 21] (l_loss: 0.04582) (t_loss: 0.07124) (accu: 0.9773)
[epoch : 22] (l_loss: 0.04507) (t_loss: 0.07249) (accu: 0.9770)
[epoch : 23] (l_loss: 0.04525) (t_loss: 0.07430) (accu: 0.9768)
[epoch : 24] (l_loss: 0.04539) (t_loss: 0.07504) (accu: 0.9768)
[epoch : 25] (l_loss: 0.04523) (t_loss: 0.07557) (accu: 0.9755)
[epoch : 26] (l_loss: 0.04530) (t_loss: 0.07071) (accu: 0.9787)
[epoch : 27] (l_loss: 0.04511) (t_loss: 0.07299) (accu: 0.9770)
[epoch : 28] (l_loss: 0.04505) (t_loss: 0.07248) (accu: 0.9777)
[epoch : 29] (l_loss: 0.04520) (t_loss: 0.07293) (accu: 0.9778)
[epoch : 30] (l_loss: 0.04535) (t_loss: 0.07343) (accu: 0.9788)
[epoch : 31] (l_loss: 0.04505) (t_loss: 0.07331) (accu: 0.9773)
[epoch : 32] (l_loss: 0.04543) (t_loss: 0.07319) (accu: 0.9774)
[epoch : 33] (l_loss: 0.04499) (t_loss: 0.07224) (accu: 0.9770)
[epoch : 34] (l_loss: 0.04451) (t_loss: 0.07155) (accu: 0.9782)
[epoch : 35] (l_loss: 0.04457) (t_loss: 0.07467) (accu: 0.9766)
[epoch : 36] (l_loss: 0.04426) (t_loss: 0.07605) (accu: 0.9752)
[epoch : 37] (l_loss: 0.04416) (t_loss: 0.07273) (accu: 0.9780)
[epoch : 38] (l_loss: 0.04438) (t_loss: 0.07595) (accu: 0.9751)
[epoch : 39] (l_loss: 0.04420) (t_loss: 0.07232) (accu: 0.9783)
[epoch : 40] (l_loss: 0.04440) (t_loss: 0.07442) (accu: 0.9761)
[epoch : 41] (l_loss: 0.04418) (t_loss: 0.07323) (accu: 0.9784)
[epoch : 42] (l_loss: 0.04454) (t_loss: 0.07189) (accu: 0.9777)
[epoch : 43] (l_loss: 0.04412) (t_loss: 0.07155) (accu: 0.9785)
[epoch : 44] (l_loss: 0.04409) (t_loss: 0.07222) (accu: 0.9783)
[epoch : 45] (l_loss: 0.04462) (t_loss: 0.07274) (accu: 0.9775)
[epoch : 46] (l_loss: 0.04394) (t_loss: 0.07473) (accu: 0.9775)
[epoch : 47] (l_loss: 0.04397) (t_loss: 0.07501) (accu: 0.9778)
[epoch : 48] (l_loss: 0.04442) (t_loss: 0.07265) (accu: 0.9763)
[epoch : 49] (l_loss: 0.04463) (t_loss: 0.07094) (accu: 0.9782)
[epoch : 50] (l_loss: 0.04368) (t_loss: 0.07245) (accu: 0.9782)
Finish! (Best accu: 0.9788) (Time taken(sec) : 794.86) 


Maximum accuracy per weight remaining
Remaining weight 100.0 %  Epoch 10 Accu 0.9788
Remaining weight 80.04 %  Epoch 26 Accu 0.9795
Remaining weight 64.06 %  Epoch 20 Accu 0.9794
Remaining weight 51.28 %  Epoch 40 Accu 0.9789
Remaining weight 41.05 %  Epoch 28 Accu 0.9791
Remaining weight 32.87 %  Epoch 39 Accu 0.9795
Remaining weight 26.32 %  Epoch 23 Accu 0.9797
Remaining weight 21.07 %  Epoch 38 Accu 0.9791
Remaining weight 16.88 %  Epoch 39 Accu 0.9801
Remaining weight 13.52 %  Epoch 18 Accu 0.9799
Remaining weight 10.83 %  Epoch 38 Accu 0.9790
Remaining weight 8.68 %  Epoch 22 Accu 0.9791
Remaining weight 6.95 %  Epoch 19 Accu 0.9791
Remaining weight 5.57 %  Epoch 39 Accu 0.9799
Remaining weight 4.47 %  Epoch 35 Accu 0.9782
Remaining weight 3.58 %  Epoch 41 Accu 0.9791
Remaining weight 2.87 %  Epoch 45 Accu 0.9795
Remaining weight 2.31 %  Epoch 47 Accu 0.9787
Remaining weight 1.85 %  Epoch 32 Accu 0.9792
Remaining weight 1.49 %  Epoch 31 Accu 0.9793
Remaining weight 1.19 %  Epoch 29 Accu 0.9788
Average test data
Remaining weight 100.00 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.836617   0.0854
1     0.399328    0.141625   0.9582
2     0.119215    0.108885   0.9665
3     0.092259    0.098978   0.9697
4     0.080549    0.090085   0.9717
5     0.072700    0.084606   0.9739
6     0.066839    0.079849   0.9746
7     0.062146    0.077216   0.9760
8     0.058278    0.077402   0.9752
9     0.055781    0.075243   0.9765
10     0.053780    0.074398   0.9767
11     0.052143    0.074504   0.9768
12     0.051178    0.075150   0.9760
13     0.050767    0.077367   0.9753
14     0.050059    0.073914   0.9764
15     0.049412    0.073447   0.9769
16     0.049092    0.075313   0.9768
17     0.048885    0.075397   0.9765
18     0.048612    0.074424   0.9769
19     0.048631    0.072895   0.9774
20     0.048475    0.073039   0.9768
21     0.047949    0.076308   0.9755
22     0.047474    0.073562   0.9768
23     0.047939    0.072200   0.9773
24     0.048111    0.072019   0.9766
25     0.047935    0.072767   0.9773
26     0.047619    0.072360   0.9776
27     0.047363    0.072900   0.9771
28     0.047038    0.073949   0.9766
29     0.047349    0.072635   0.9771
30     0.046989    0.075147   0.9766
31     0.047259    0.078366   0.9752
32     0.046806    0.073234   0.9769
33     0.047105    0.074255   0.9767
34     0.047108    0.074570   0.9758
35     0.046855    0.072895   0.9763
36     0.046644    0.075597   0.9757
37     0.047015    0.074607   0.9765
38     0.046676    0.073366   0.9770
39     0.046720    0.075049   0.9763
40     0.046404    0.072465   0.9769
41     0.046861    0.075043   0.9762
42     0.046472    0.073119   0.9775
43     0.047018    0.073704   0.9772
44     0.046228    0.072606   0.9772
45     0.046997    0.075945   0.9763
46     0.046400    0.073733   0.9765
47     0.046257    0.072058   0.9770
48     0.046690    0.072825   0.9767
49     0.046367    0.072233   0.9769
50     0.046942    0.073222   0.9769
Remaining weight 80.04 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.578482   0.1123
1     0.388750    0.139740   0.9598
2     0.118830    0.104285   0.9683
3     0.091212    0.091547   0.9715
4     0.078737    0.085362   0.9734
5     0.071089    0.083752   0.9738
6     0.065796    0.078565   0.9759
7     0.061713    0.082582   0.9745
8     0.058642    0.076337   0.9761
9     0.056120    0.076372   0.9769
10     0.053813    0.075229   0.9765
11     0.052525    0.077057   0.9760
12     0.050895    0.075305   0.9761
13     0.050300    0.073261   0.9769
14     0.049551    0.072567   0.9775
15     0.048800    0.074593   0.9768
16     0.048558    0.072600   0.9771
17     0.048605    0.074652   0.9767
18     0.048182    0.072672   0.9771
19     0.047250    0.071211   0.9778
20     0.047585    0.074733   0.9763
21     0.047473    0.072126   0.9778
22     0.046739    0.073224   0.9765
23     0.047045    0.072852   0.9773
24     0.046857    0.072131   0.9774
25     0.046674    0.073042   0.9776
26     0.046711    0.072776   0.9774
27     0.046585    0.073040   0.9775
28     0.046537    0.070177   0.9782
29     0.046350    0.072534   0.9777
30     0.046334    0.072628   0.9771
31     0.046627    0.074364   0.9769
32     0.046296    0.075282   0.9767
33     0.046491    0.073745   0.9773
34     0.046103    0.072952   0.9770
35     0.046385    0.074571   0.9773
36     0.046019    0.071300   0.9776
37     0.046058    0.073773   0.9771
38     0.046117    0.070521   0.9776
39     0.045940    0.073175   0.9769
40     0.045855    0.070805   0.9772
41     0.045908    0.073662   0.9773
42     0.046098    0.071788   0.9774
43     0.045849    0.072134   0.9772
44     0.045638    0.075029   0.9767
45     0.045906    0.074259   0.9768
46     0.045964    0.074714   0.9769
47     0.045692    0.075378   0.9767
48     0.045320    0.072892   0.9776
49     0.045530    0.072974   0.9771
50     0.045680    0.072127   0.9774
Remaining weight 64.06 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.432841   0.1012
1     0.387837    0.144467   0.9574
2     0.122457    0.110345   0.9669
3     0.093810    0.097419   0.9702
4     0.079981    0.088970   0.9725
5     0.072162    0.083050   0.9749
6     0.066293    0.082422   0.9747
7     0.062747    0.079352   0.9756
8     0.059404    0.078540   0.9757
9     0.056694    0.078915   0.9751
10     0.055097    0.075522   0.9769
11     0.053014    0.073684   0.9773
12     0.052214    0.074886   0.9770
13     0.051160    0.074465   0.9771
14     0.050626    0.076374   0.9763
15     0.050014    0.073444   0.9770
16     0.049200    0.072585   0.9780
17     0.049497    0.075496   0.9766
18     0.048757    0.071598   0.9782
19     0.048628    0.079574   0.9757
20     0.048807    0.072170   0.9782
21     0.048282    0.071688   0.9777
22     0.047845    0.074285   0.9771
23     0.047804    0.072407   0.9777
24     0.047485    0.075067   0.9769
25     0.047943    0.075881   0.9764
26     0.047614    0.071645   0.9784
27     0.047587    0.072083   0.9779
28     0.047388    0.072959   0.9775
29     0.047413    0.072225   0.9779
30     0.047222    0.073408   0.9775
31     0.047107    0.074049   0.9768
32     0.047080    0.074451   0.9770
33     0.046981    0.071574   0.9777
34     0.047085    0.072758   0.9777
35     0.047304    0.071951   0.9778
36     0.046732    0.073478   0.9770
37     0.046809    0.071765   0.9780
38     0.046527    0.074259   0.9772
39     0.046741    0.076033   0.9771
40     0.046980    0.073419   0.9776
41     0.046680    0.073124   0.9777
42     0.046704    0.072316   0.9773
43     0.046785    0.074741   0.9761
44     0.046562    0.072388   0.9767
45     0.046922    0.070615   0.9783
46     0.046428    0.072291   0.9775
47     0.046977    0.073371   0.9781
48     0.046657    0.075588   0.9768
49     0.046464    0.072382   0.9774
50     0.046447    0.072362   0.9781
Remaining weight 51.28 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.385971   0.1247
1     0.388265    0.146313   0.9569
2     0.124267    0.109119   0.9667
3     0.094359    0.094273   0.9708
4     0.080684    0.085895   0.9737
5     0.072685    0.083892   0.9735
6     0.066306    0.080732   0.9747
7     0.063122    0.080240   0.9747
8     0.059542    0.082075   0.9751
9     0.056870    0.074884   0.9764
10     0.055097    0.075448   0.9762
11     0.053209    0.077262   0.9752
12     0.052435    0.073955   0.9770
13     0.051688    0.074387   0.9766
14     0.051012    0.073964   0.9772
15     0.050123    0.072591   0.9772
16     0.049905    0.074631   0.9768
17     0.050357    0.074942   0.9760
18     0.049397    0.076205   0.9758
19     0.049314    0.074729   0.9771
20     0.049053    0.073130   0.9771
21     0.049201    0.073245   0.9765
22     0.048646    0.071490   0.9776
23     0.048710    0.073271   0.9771
24     0.048208    0.074861   0.9767
25     0.048614    0.073759   0.9771
26     0.048777    0.073446   0.9774
27     0.048132    0.071845   0.9772
28     0.048408    0.072083   0.9777
29     0.047881    0.077686   0.9755
30     0.048180    0.073966   0.9770
31     0.048144    0.073746   0.9769
32     0.048126    0.076220   0.9761
33     0.047924    0.074325   0.9769
34     0.048046    0.072358   0.9772
35     0.047572    0.074331   0.9769
36     0.048035    0.075936   0.9763
37     0.048006    0.075782   0.9760
38     0.047492    0.072898   0.9774
39     0.047682    0.074717   0.9763
40     0.047442    0.075941   0.9760
41     0.047508    0.074562   0.9767
42     0.047567    0.072860   0.9770
43     0.047275    0.073483   0.9773
44     0.047769    0.072671   0.9775
45     0.047662    0.073307   0.9770
46     0.047694    0.073827   0.9769
47     0.047501    0.076700   0.9758
48     0.047593    0.075639   0.9758
49     0.047242    0.073752   0.9767
50     0.047352    0.075579   0.9761
Remaining weight 41.05 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.361940   0.1500
1     0.390126    0.149526   0.9569
2     0.126785    0.113111   0.9654
3     0.096585    0.096384   0.9698
4     0.082406    0.089615   0.9726
5     0.073138    0.084252   0.9743
6     0.066915    0.083486   0.9739
7     0.062802    0.080877   0.9740
8     0.059245    0.079418   0.9755
9     0.055881    0.076086   0.9769
10     0.054218    0.078113   0.9754
11     0.052423    0.075783   0.9763
12     0.051680    0.074721   0.9773
13     0.050612    0.076740   0.9764
14     0.050131    0.074037   0.9774
15     0.050268    0.076006   0.9762
16     0.049429    0.074168   0.9774
17     0.048959    0.076421   0.9767
18     0.048907    0.075634   0.9772
19     0.048806    0.075324   0.9768
20     0.048493    0.075882   0.9764
21     0.048325    0.075297   0.9764
22     0.048096    0.075095   0.9765
23     0.048166    0.073978   0.9772
24     0.047861    0.072228   0.9780
25     0.047703    0.074673   0.9767
26     0.047631    0.075176   0.9769
27     0.047646    0.074667   0.9768
28     0.047356    0.071224   0.9780
29     0.047346    0.072136   0.9782
30     0.047183    0.073796   0.9773
31     0.047318    0.074839   0.9773
32     0.047474    0.075637   0.9769
33     0.047326    0.073446   0.9773
34     0.047087    0.076063   0.9764
35     0.046937    0.077534   0.9765
36     0.047328    0.074470   0.9770
37     0.047037    0.076290   0.9765
38     0.047035    0.073841   0.9771
39     0.047086    0.073403   0.9774
40     0.046565    0.073816   0.9776
41     0.047070    0.072107   0.9782
42     0.047029    0.073967   0.9772
43     0.047053    0.075471   0.9767
44     0.047209    0.073763   0.9777
45     0.046969    0.077913   0.9757
46     0.046432    0.076311   0.9767
47     0.047062    0.074039   0.9774
48     0.046904    0.072351   0.9780
49     0.046831    0.073198   0.9774
50     0.046472    0.072554   0.9778
Remaining weight 32.87 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.352247   0.1080
1     0.379406    0.145469   0.9567
2     0.119635    0.104511   0.9687
3     0.091655    0.091171   0.9723
4     0.078793    0.085721   0.9735
5     0.071254    0.082684   0.9745
6     0.066204    0.080684   0.9745
7     0.062261    0.078797   0.9752
8     0.059415    0.077994   0.9758
9     0.057213    0.077791   0.9758
10     0.055199    0.076267   0.9762
11     0.053840    0.076865   0.9760
12     0.052392    0.077436   0.9764
13     0.052051    0.074797   0.9768
14     0.051237    0.072750   0.9774
15     0.050831    0.078437   0.9751
16     0.050412    0.075297   0.9768
17     0.050435    0.074685   0.9769
18     0.049676    0.075532   0.9774
19     0.050178    0.072885   0.9779
20     0.049073    0.075915   0.9768
21     0.049257    0.076007   0.9762
22     0.049004    0.077596   0.9763
23     0.048935    0.078412   0.9755
24     0.048782    0.074135   0.9770
25     0.048651    0.080585   0.9750
26     0.048692    0.079847   0.9750
27     0.048836    0.075855   0.9763
28     0.048134    0.074542   0.9768
29     0.048523    0.074772   0.9771
30     0.048106    0.073675   0.9776
31     0.048521    0.073157   0.9775
32     0.048081    0.075560   0.9762
33     0.047729    0.072502   0.9775
34     0.047832    0.073042   0.9772
35     0.047849    0.073934   0.9771
36     0.047464    0.075484   0.9768
37     0.047657    0.073726   0.9772
38     0.047706    0.076099   0.9765
39     0.047897    0.075738   0.9761
40     0.047658    0.073403   0.9775
41     0.047504    0.074760   0.9770
42     0.047314    0.072015   0.9777
43     0.047348    0.074367   0.9766
44     0.047226    0.074749   0.9764
45     0.047296    0.077728   0.9761
46     0.047556    0.073285   0.9774
47     0.047376    0.074714   0.9767
48     0.047607    0.073533   0.9770
49     0.047325    0.074441   0.9765
50     0.047770    0.073892   0.9775
Remaining weight 26.32 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.334216   0.1100
1     0.380323    0.146946   0.9583
2     0.121845    0.108175   0.9671
3     0.092584    0.093726   0.9706
4     0.078805    0.085796   0.9737
5     0.070155    0.079998   0.9755
6     0.064763    0.081281   0.9755
7     0.060035    0.077628   0.9765
8     0.056693    0.077533   0.9765
9     0.054072    0.073853   0.9774
10     0.051863    0.075486   0.9765
11     0.050599    0.072999   0.9775
12     0.049633    0.074012   0.9767
13     0.048743    0.073148   0.9773
14     0.048210    0.073468   0.9779
15     0.047956    0.073653   0.9776
16     0.047241    0.073582   0.9778
17     0.047299    0.074575   0.9761
18     0.046644    0.072184   0.9773
19     0.046688    0.072975   0.9775
20     0.046515    0.073163   0.9775
21     0.046174    0.072016   0.9782
22     0.046084    0.073595   0.9769
23     0.045757    0.071032   0.9775
24     0.045525    0.073667   0.9768
25     0.045172    0.071675   0.9780
26     0.045280    0.074363   0.9775
27     0.045442    0.074394   0.9769
28     0.045381    0.073597   0.9770
29     0.045092    0.075245   0.9766
30     0.045306    0.072658   0.9773
31     0.045133    0.073597   0.9779
32     0.045153    0.071529   0.9784
33     0.045016    0.071574   0.9782
34     0.045261    0.071608   0.9782
35     0.044761    0.073139   0.9776
36     0.045114    0.071019   0.9781
37     0.045149    0.070199   0.9782
38     0.044777    0.074801   0.9767
39     0.044815    0.071693   0.9778
40     0.045136    0.072197   0.9778
41     0.044739    0.072344   0.9771
42     0.044538    0.071854   0.9779
43     0.044947    0.069856   0.9779
44     0.044763    0.073931   0.9781
45     0.044556    0.071382   0.9780
46     0.044950    0.073329   0.9774
47     0.044691    0.072080   0.9781
48     0.044690    0.072092   0.9779
49     0.044822    0.072287   0.9775
50     0.044688    0.072031   0.9784
Remaining weight 21.07 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.326567   0.1103
1     0.379182    0.151156   0.9552
2     0.122794    0.108979   0.9670
3     0.093523    0.093072   0.9716
4     0.080064    0.087516   0.9732
5     0.070717    0.081492   0.9757
6     0.065190    0.079308   0.9754
7     0.060788    0.078705   0.9758
8     0.057556    0.076390   0.9765
9     0.054786    0.074793   0.9769
10     0.052594    0.072973   0.9775
11     0.051245    0.076921   0.9762
12     0.050579    0.075498   0.9771
13     0.049644    0.073759   0.9775
14     0.048986    0.073997   0.9769
15     0.048900    0.073191   0.9777
16     0.048238    0.073135   0.9777
17     0.047936    0.072884   0.9776
18     0.047467    0.072983   0.9781
19     0.046979    0.074536   0.9773
20     0.046562    0.074655   0.9771
21     0.046516    0.074653   0.9771
22     0.046411    0.073871   0.9769
23     0.045844    0.074228   0.9775
24     0.045943    0.074567   0.9765
25     0.045906    0.070581   0.9785
26     0.045231    0.071308   0.9782
27     0.045765    0.072679   0.9776
28     0.045344    0.072178   0.9776
29     0.045388    0.071363   0.9779
30     0.045373    0.071985   0.9774
31     0.045371    0.073140   0.9772
32     0.045488    0.073235   0.9771
33     0.045279    0.074372   0.9772
34     0.045294    0.071183   0.9784
35     0.044927    0.079174   0.9760
36     0.045367    0.072042   0.9779
37     0.044834    0.073708   0.9773
38     0.045210    0.073502   0.9779
39     0.045027    0.070480   0.9789
40     0.045117    0.071544   0.9780
41     0.044887    0.072823   0.9778
42     0.044868    0.073562   0.9773
43     0.045152    0.072842   0.9777
44     0.044987    0.073096   0.9774
45     0.044956    0.071366   0.9785
46     0.045005    0.070541   0.9779
47     0.044538    0.070445   0.9782
48     0.045097    0.070357   0.9786
49     0.044797    0.071327   0.9785
50     0.044915    0.074236   0.9768
Remaining weight 16.88 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.310001   0.1308
1     0.375364    0.145460   0.9576
2     0.121729    0.107297   0.9669
3     0.092579    0.094521   0.9707
4     0.079091    0.085300   0.9738
5     0.070736    0.080068   0.9759
6     0.065447    0.080590   0.9755
7     0.061262    0.078960   0.9754
8     0.058031    0.077108   0.9763
9     0.055150    0.075240   0.9769
10     0.053027    0.075336   0.9767
11     0.051864    0.076610   0.9769
12     0.050330    0.072762   0.9774
13     0.049526    0.076809   0.9762
14     0.049125    0.072371   0.9774
15     0.048206    0.073900   0.9766
16     0.048106    0.072910   0.9772
17     0.047077    0.074425   0.9774
18     0.047146    0.073960   0.9780
19     0.046757    0.072993   0.9767
20     0.046463    0.072435   0.9773
21     0.046551    0.072796   0.9777
22     0.046462    0.072331   0.9776
23     0.046095    0.071964   0.9777
24     0.046019    0.074187   0.9769
25     0.045771    0.073597   0.9774
26     0.045657    0.072121   0.9774
27     0.045952    0.073062   0.9771
28     0.045381    0.073575   0.9769
29     0.045875    0.071725   0.9771
30     0.045645    0.071244   0.9777
31     0.045420    0.072742   0.9775
32     0.045504    0.073084   0.9770
33     0.045471    0.072586   0.9776
34     0.045027    0.072191   0.9776
35     0.045316    0.073795   0.9778
36     0.045424    0.074139   0.9771
37     0.045192    0.072747   0.9775
38     0.044998    0.071160   0.9778
39     0.045269    0.073461   0.9770
40     0.045287    0.075339   0.9767
41     0.044889    0.071534   0.9778
42     0.045251    0.072057   0.9780
43     0.045114    0.071580   0.9780
44     0.044874    0.072949   0.9769
45     0.045297    0.073309   0.9772
46     0.045250    0.074005   0.9770
47     0.045228    0.073828   0.9777
48     0.045170    0.072054   0.9782
49     0.044872    0.072290   0.9778
50     0.044770    0.070676   0.9779
Remaining weight 13.52 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.303613   0.0969
1     0.372561    0.147333   0.9566
2     0.123008    0.106928   0.9671
3     0.093886    0.094078   0.9713
4     0.079800    0.086147   0.9741
5     0.071276    0.081053   0.9749
6     0.065992    0.078501   0.9759
7     0.061093    0.076898   0.9769
8     0.057719    0.075699   0.9768
9     0.055172    0.075686   0.9766
10     0.053046    0.076211   0.9764
11     0.051415    0.073303   0.9771
12     0.050422    0.074138   0.9766
13     0.049458    0.073667   0.9770
14     0.048554    0.073715   0.9770
15     0.048343    0.073075   0.9776
16     0.047435    0.072458   0.9775
17     0.046731    0.073123   0.9776
18     0.046855    0.073377   0.9767
19     0.046562    0.072181   0.9776
20     0.046538    0.072880   0.9773
21     0.046104    0.072805   0.9777
22     0.045871    0.072991   0.9768
23     0.046051    0.072503   0.9777
24     0.045984    0.072113   0.9775
25     0.045793    0.072210   0.9773
26     0.045842    0.073615   0.9766
27     0.045676    0.070918   0.9777
28     0.045548    0.071936   0.9771
29     0.045366    0.073592   0.9764
30     0.045382    0.074066   0.9774
31     0.045414    0.073985   0.9769
32     0.045501    0.072463   0.9781
33     0.045548    0.072476   0.9773
34     0.045153    0.072951   0.9775
35     0.045053    0.072249   0.9774
36     0.045402    0.074349   0.9768
37     0.045232    0.072291   0.9775
38     0.045454    0.073328   0.9768
39     0.044964    0.073327   0.9769
40     0.045151    0.074383   0.9771
41     0.044970    0.074271   0.9769
42     0.044988    0.072876   0.9774
43     0.044737    0.072832   0.9770
44     0.044376    0.072227   0.9774
45     0.045008    0.073765   0.9774
46     0.044513    0.071494   0.9779
47     0.044919    0.074921   0.9771
48     0.044616    0.071671   0.9772
49     0.044801    0.071232   0.9776
50     0.044378    0.071222   0.9780
Remaining weight 10.83 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.302926   0.1064
1     0.373017    0.146915   0.9567
2     0.122689    0.107242   0.9674
3     0.093663    0.094635   0.9705
4     0.080231    0.087276   0.9733
5     0.072528    0.084648   0.9737
6     0.066726    0.080249   0.9752
7     0.062490    0.082175   0.9744
8     0.059485    0.076958   0.9767
9     0.056230    0.075070   0.9766
10     0.054266    0.075851   0.9769
11     0.052655    0.076943   0.9766
12     0.051416    0.075831   0.9761
13     0.050270    0.076805   0.9758
14     0.049913    0.077284   0.9756
15     0.049374    0.075623   0.9759
16     0.048819    0.072751   0.9771
17     0.048427    0.074001   0.9767
18     0.047969    0.077878   0.9752
19     0.047718    0.074600   0.9767
20     0.047759    0.075185   0.9764
21     0.047238    0.075575   0.9761
22     0.047324    0.075350   0.9758
23     0.047368    0.075144   0.9763
24     0.046720    0.074070   0.9766
25     0.046850    0.072942   0.9768
26     0.046938    0.073578   0.9769
27     0.046418    0.074186   0.9764
28     0.046575    0.076301   0.9757
29     0.046379    0.074330   0.9762
30     0.046253    0.073034   0.9774
31     0.046112    0.073295   0.9770
32     0.046177    0.073602   0.9762
33     0.046388    0.072987   0.9769
34     0.046048    0.073266   0.9767
35     0.046588    0.076319   0.9763
36     0.045669    0.074242   0.9765
37     0.045984    0.072157   0.9776
38     0.045867    0.072911   0.9768
39     0.045709    0.072708   0.9773
40     0.045614    0.073747   0.9771
41     0.045831    0.072608   0.9772
42     0.045775    0.071794   0.9774
43     0.045281    0.071955   0.9780
44     0.045319    0.073107   0.9769
45     0.045323    0.073995   0.9766
46     0.045547    0.073953   0.9768
47     0.045458    0.072690   0.9774
48     0.045184    0.071539   0.9778
49     0.044752    0.071608   0.9773
50     0.045097    0.072426   0.9770
Remaining weight 8.68 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.300578   0.1060
1     0.374612    0.147185   0.9563
2     0.123207    0.110419   0.9665
3     0.094497    0.093873   0.9712
4     0.080439    0.086101   0.9736
5     0.071552    0.081484   0.9751
6     0.066052    0.078842   0.9762
7     0.061846    0.076744   0.9765
8     0.058457    0.075931   0.9762
9     0.055538    0.075349   0.9768
10     0.053524    0.076470   0.9763
11     0.051786    0.077658   0.9758
12     0.051067    0.076775   0.9762
13     0.050230    0.075580   0.9765
14     0.049296    0.075112   0.9773
15     0.048946    0.074117   0.9770
16     0.048468    0.075381   0.9763
17     0.047927    0.075765   0.9763
18     0.047891    0.074710   0.9767
19     0.047601    0.075359   0.9761
20     0.047238    0.075529   0.9761
21     0.047024    0.073905   0.9770
22     0.047200    0.073991   0.9771
23     0.046634    0.073814   0.9779
24     0.046874    0.074770   0.9768
25     0.046647    0.072441   0.9774
26     0.046211    0.072221   0.9773
27     0.046384    0.072755   0.9769
28     0.045948    0.074295   0.9768
29     0.046120    0.073307   0.9770
30     0.046014    0.076875   0.9755
31     0.046242    0.073694   0.9770
32     0.045871    0.073688   0.9771
33     0.046006    0.072357   0.9771
34     0.045864    0.073797   0.9774
35     0.046006    0.073738   0.9774
36     0.045670    0.073090   0.9769
37     0.046260    0.075760   0.9769
38     0.045607    0.073130   0.9773
39     0.045433    0.074644   0.9772
40     0.045945    0.072628   0.9771
41     0.045783    0.073045   0.9771
42     0.045522    0.073673   0.9768
43     0.045679    0.072981   0.9771
44     0.045324    0.074872   0.9773
45     0.045313    0.074319   0.9767
46     0.045506    0.071952   0.9781
47     0.045471    0.073745   0.9769
48     0.045238    0.071018   0.9778
49     0.045185    0.071302   0.9769
50     0.045016    0.071674   0.9777
Remaining weight 6.95 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.302421   0.1077
1     0.375506    0.146750   0.9569
2     0.122374    0.106783   0.9675
3     0.093470    0.098386   0.9703
4     0.079292    0.087041   0.9739
5     0.070897    0.085015   0.9739
6     0.065351    0.078805   0.9758
7     0.060949    0.076937   0.9764
8     0.057797    0.078182   0.9755
9     0.054754    0.075684   0.9768
10     0.052690    0.075311   0.9765
11     0.050984    0.075914   0.9773
12     0.050008    0.075829   0.9764
13     0.048521    0.075151   0.9770
14     0.048110    0.076039   0.9765
15     0.047966    0.075242   0.9760
16     0.047230    0.077113   0.9758
17     0.046846    0.075762   0.9763
18     0.046482    0.074228   0.9767
19     0.046542    0.072824   0.9770
20     0.046130    0.072973   0.9779
21     0.045880    0.074000   0.9767
22     0.045858    0.074533   0.9768
23     0.046028    0.075540   0.9769
24     0.045760    0.074214   0.9775
25     0.045495    0.076904   0.9764
26     0.045666    0.074086   0.9771
27     0.045414    0.073911   0.9770
28     0.045532    0.073535   0.9773
29     0.045352    0.073435   0.9771
30     0.045496    0.072624   0.9770
31     0.045042    0.072565   0.9774
32     0.045214    0.073460   0.9770
33     0.045292    0.073613   0.9773
34     0.045234    0.073665   0.9769
35     0.044942    0.071824   0.9778
36     0.045111    0.074663   0.9774
37     0.044887    0.074419   0.9766
38     0.044800    0.072914   0.9773
39     0.044659    0.072394   0.9773
40     0.044754    0.074256   0.9765
41     0.044854    0.073297   0.9764
42     0.044605    0.074895   0.9770
43     0.044272    0.071949   0.9771
44     0.044609    0.073018   0.9772
45     0.044835    0.072652   0.9775
46     0.044740    0.072208   0.9774
47     0.044455    0.072466   0.9773
48     0.044385    0.072382   0.9776
49     0.044200    0.072734   0.9774
50     0.044401    0.072889   0.9774
Remaining weight 5.57 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.300139   0.1114
1     0.378931    0.148061   0.9571
2     0.126282    0.108531   0.9677
3     0.096596    0.095357   0.9714
4     0.082112    0.089117   0.9733
5     0.073197    0.085816   0.9738
6     0.067219    0.080577   0.9760
7     0.062810    0.079432   0.9757
8     0.059569    0.078747   0.9760
9     0.056627    0.077863   0.9758
10     0.053977    0.074518   0.9773
11     0.052310    0.077193   0.9760
12     0.050793    0.075628   0.9766
13     0.049905    0.073804   0.9773
14     0.048707    0.076125   0.9760
15     0.048443    0.074658   0.9771
16     0.047791    0.073870   0.9768
17     0.047768    0.073991   0.9768
18     0.047039    0.074980   0.9761
19     0.047051    0.074691   0.9766
20     0.046908    0.074772   0.9765
21     0.046182    0.073668   0.9773
22     0.046333    0.074747   0.9763
23     0.046318    0.075036   0.9764
24     0.045725    0.074130   0.9766
25     0.046145    0.074236   0.9760
26     0.045678    0.073935   0.9763
27     0.045736    0.072365   0.9770
28     0.045492    0.072021   0.9776
29     0.045392    0.072861   0.9773
30     0.045080    0.073404   0.9763
31     0.045131    0.071563   0.9774
32     0.045246    0.072407   0.9771
33     0.044879    0.074291   0.9764
34     0.044919    0.074031   0.9771
35     0.044914    0.074192   0.9770
36     0.044761    0.072858   0.9772
37     0.044916    0.072905   0.9768
38     0.044709    0.071960   0.9777
39     0.044784    0.072702   0.9777
40     0.044494    0.071693   0.9778
41     0.044678    0.073252   0.9772
42     0.044649    0.071580   0.9768
43     0.044375    0.072132   0.9776
44     0.044284    0.073531   0.9772
45     0.044506    0.073091   0.9770
46     0.044471    0.071177   0.9777
47     0.044372    0.073523   0.9767
48     0.044373    0.074788   0.9767
49     0.044381    0.073119   0.9773
50     0.044169    0.070758   0.9782
Remaining weight 4.47 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.292776   0.1193
1     0.377103    0.149595   0.9571
2     0.124902    0.109344   0.9680
3     0.095314    0.093263   0.9713
4     0.081341    0.088947   0.9730
5     0.073390    0.083314   0.9748
6     0.067399    0.081427   0.9757
7     0.063217    0.078431   0.9761
8     0.059785    0.079061   0.9751
9     0.056991    0.075872   0.9771
10     0.054419    0.075930   0.9766
11     0.052429    0.073900   0.9775
12     0.051010    0.074419   0.9772
13     0.050177    0.072584   0.9769
14     0.049092    0.074567   0.9777
15     0.047896    0.074514   0.9765
16     0.047637    0.076820   0.9763
17     0.046815    0.074497   0.9765
18     0.046720    0.073447   0.9771
19     0.046351    0.075385   0.9767
20     0.046302    0.074533   0.9761
21     0.045994    0.074611   0.9769
22     0.045777    0.073544   0.9772
23     0.045702    0.074583   0.9765
24     0.045424    0.073399   0.9774
25     0.045493    0.071284   0.9778
26     0.045574    0.073963   0.9769
27     0.045286    0.076258   0.9765
28     0.045001    0.072771   0.9773
29     0.044786    0.072283   0.9774
30     0.044981    0.072037   0.9772
31     0.044901    0.073866   0.9768
32     0.044614    0.074782   0.9766
33     0.044659    0.073218   0.9776
34     0.044847    0.072321   0.9775
35     0.044541    0.075159   0.9766
36     0.044369    0.071359   0.9778
37     0.044168    0.072217   0.9778
38     0.044451    0.073137   0.9767
39     0.044249    0.074323   0.9768
40     0.044277    0.073082   0.9769
41     0.043984    0.072609   0.9772
42     0.043890    0.071220   0.9780
43     0.043869    0.073069   0.9772
44     0.043711    0.073320   0.9771
45     0.043991    0.073230   0.9769
46     0.043807    0.072000   0.9777
47     0.043845    0.072542   0.9774
48     0.043560    0.073129   0.9772
49     0.043787    0.069799   0.9783
50     0.043951    0.073208   0.9777
Remaining weight 3.58 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.291392   0.1106
1     0.381276    0.149284   0.9563
2     0.124472    0.108223   0.9676
3     0.093981    0.092374   0.9720
4     0.080195    0.087889   0.9738
5     0.071502    0.083746   0.9743
6     0.065812    0.081359   0.9748
7     0.060682    0.077902   0.9757
8     0.057277    0.076528   0.9767
9     0.054370    0.077469   0.9754
10     0.051922    0.074823   0.9774
11     0.049863    0.073987   0.9769
12     0.048412    0.073929   0.9772
13     0.047128    0.074218   0.9768
14     0.046658    0.072006   0.9778
15     0.046456    0.073718   0.9770
16     0.046008    0.074780   0.9770
17     0.045741    0.073025   0.9776
18     0.045268    0.070946   0.9779
19     0.045033    0.074567   0.9768
20     0.044934    0.072902   0.9771
21     0.044727    0.072277   0.9776
22     0.044574    0.073584   0.9771
23     0.044330    0.074030   0.9767
24     0.044655    0.072515   0.9773
25     0.044436    0.074769   0.9769
26     0.044344    0.074823   0.9766
27     0.044230    0.071768   0.9779
28     0.044113    0.072686   0.9778
29     0.044277    0.072418   0.9772
30     0.044104    0.073324   0.9767
31     0.044209    0.073626   0.9770
32     0.044242    0.072934   0.9771
33     0.043930    0.072196   0.9780
34     0.043787    0.074785   0.9767
35     0.043850    0.073117   0.9774
36     0.043717    0.073795   0.9767
37     0.043791    0.074681   0.9768
38     0.043837    0.073955   0.9770
39     0.043810    0.074335   0.9771
40     0.043500    0.073405   0.9776
41     0.043660    0.071321   0.9773
42     0.043487    0.072442   0.9776
43     0.043247    0.072141   0.9779
44     0.043220    0.071459   0.9777
45     0.043166    0.072214   0.9775
46     0.042985    0.070653   0.9783
47     0.043226    0.073811   0.9776
48     0.043167    0.072283   0.9763
49     0.043202    0.070913   0.9784
50     0.042878    0.072681   0.9776
Remaining weight 2.87 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.293889   0.1115
1     0.382041    0.146770   0.9571
2     0.122763    0.107132   0.9677
3     0.092605    0.092095   0.9730
4     0.078757    0.087094   0.9730
5     0.070426    0.083595   0.9737
6     0.064651    0.079127   0.9761
7     0.060151    0.077806   0.9760
8     0.056535    0.077061   0.9763
9     0.053789    0.073854   0.9774
10     0.051412    0.075776   0.9761
11     0.049441    0.072891   0.9774
12     0.048398    0.075304   0.9768
13     0.047475    0.073043   0.9774
14     0.046872    0.072892   0.9773
15     0.046111    0.074529   0.9775
16     0.045651    0.073344   0.9766
17     0.045373    0.071802   0.9770
18     0.045049    0.072224   0.9772
19     0.044886    0.072733   0.9770
20     0.044628    0.072957   0.9775
21     0.044508    0.072387   0.9773
22     0.044292    0.071701   0.9783
23     0.044201    0.071729   0.9775
24     0.043964    0.071029   0.9782
25     0.043593    0.072764   0.9773
26     0.043779    0.073158   0.9773
27     0.043779    0.071807   0.9775
28     0.043355    0.071408   0.9773
29     0.043750    0.072621   0.9773
30     0.043609    0.072431   0.9774
31     0.043592    0.071312   0.9773
32     0.043216    0.072127   0.9777
33     0.043289    0.072160   0.9770
34     0.042826    0.072616   0.9775
35     0.043104    0.071503   0.9776
36     0.043212    0.071986   0.9778
37     0.043134    0.071722   0.9772
38     0.043111    0.071512   0.9783
39     0.043103    0.070512   0.9780
40     0.043128    0.073268   0.9771
41     0.042933    0.069824   0.9785
42     0.043026    0.072325   0.9776
43     0.043061    0.070161   0.9777
44     0.042995    0.070763   0.9780
45     0.042844    0.069687   0.9785
46     0.042953    0.069694   0.9782
47     0.043050    0.072297   0.9777
48     0.042859    0.071539   0.9777
49     0.042944    0.071870   0.9775
50     0.043144    0.070389   0.9775
Remaining weight 2.31 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.294200   0.1041
1     0.385185    0.148535   0.9568
2     0.123282    0.110138   0.9667
3     0.092991    0.092242   0.9723
4     0.078458    0.084610   0.9746
5     0.069503    0.080298   0.9760
6     0.063059    0.078313   0.9762
7     0.058597    0.074976   0.9774
8     0.054781    0.076112   0.9769
9     0.051941    0.075231   0.9768
10     0.049425    0.073203   0.9774
11     0.048275    0.074373   0.9767
12     0.047207    0.072902   0.9774
13     0.046339    0.072701   0.9775
14     0.045656    0.074246   0.9767
15     0.045254    0.073500   0.9769
16     0.044946    0.073880   0.9770
17     0.044989    0.072031   0.9775
18     0.044520    0.073195   0.9779
19     0.044497    0.072589   0.9772
20     0.044311    0.072147   0.9775
21     0.044151    0.072171   0.9780
22     0.044058    0.070889   0.9781
23     0.043906    0.072979   0.9768
24     0.043988    0.072982   0.9773
25     0.044024    0.073409   0.9774
26     0.043904    0.073473   0.9769
27     0.043626    0.072633   0.9771
28     0.043506    0.071815   0.9773
29     0.043688    0.071767   0.9778
30     0.043381    0.074548   0.9767
31     0.043203    0.070836   0.9774
32     0.043433    0.074179   0.9768
33     0.043154    0.072568   0.9775
34     0.043171    0.072808   0.9774
35     0.043120    0.070979   0.9778
36     0.043309    0.072626   0.9776
37     0.043022    0.071802   0.9780
38     0.043011    0.072742   0.9771
39     0.042890    0.071557   0.9775
40     0.042958    0.071927   0.9781
41     0.043197    0.070956   0.9778
42     0.042842    0.072439   0.9771
43     0.042871    0.072856   0.9772
44     0.042878    0.071633   0.9781
45     0.042940    0.071401   0.9773
46     0.042954    0.071136   0.9779
47     0.042827    0.071286   0.9775
48     0.042672    0.070409   0.9783
49     0.043058    0.071808   0.9773
50     0.042858    0.072061   0.9774
Remaining weight 1.85 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.295194   0.1022
1     0.390241    0.150112   0.9572
2     0.125519    0.111364   0.9670
3     0.094833    0.094230   0.9716
4     0.079627    0.087214   0.9741
5     0.070519    0.083530   0.9745
6     0.064145    0.080971   0.9756
7     0.059374    0.078112   0.9760
8     0.055774    0.076038   0.9764
9     0.053006    0.075303   0.9768
10     0.050230    0.073563   0.9767
11     0.048802    0.073961   0.9770
12     0.047506    0.074134   0.9769
13     0.046741    0.072885   0.9771
14     0.046133    0.072514   0.9777
15     0.045666    0.072846   0.9775
16     0.045251    0.072761   0.9773
17     0.045046    0.072136   0.9771
18     0.044889    0.072508   0.9774
19     0.044385    0.072305   0.9774
20     0.044306    0.072499   0.9772
21     0.044072    0.072654   0.9772
22     0.043819    0.072173   0.9776
23     0.043725    0.074042   0.9771
24     0.044085    0.071707   0.9768
25     0.043646    0.073901   0.9770
26     0.043937    0.072299   0.9773
27     0.043760    0.072587   0.9776
28     0.043678    0.071447   0.9779
29     0.043679    0.071831   0.9776
30     0.043578    0.072170   0.9777
31     0.043430    0.072834   0.9771
32     0.043417    0.072972   0.9779
33     0.043501    0.072561   0.9775
34     0.043503    0.072392   0.9775
35     0.043456    0.071189   0.9777
36     0.043505    0.071931   0.9778
37     0.043369    0.072553   0.9773
38     0.043456    0.072425   0.9774
39     0.043450    0.072911   0.9770
40     0.043394    0.073515   0.9777
41     0.043452    0.072206   0.9770
42     0.043218    0.071584   0.9775
43     0.042962    0.072674   0.9772
44     0.042986    0.072102   0.9772
45     0.043219    0.071877   0.9779
46     0.043123    0.072265   0.9778
47     0.043205    0.071167   0.9778
48     0.043045    0.072656   0.9777
49     0.043215    0.071915   0.9777
50     0.042870    0.071532   0.9777
Remaining weight 1.49 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.294535   0.0983
1     0.392895    0.150469   0.9559
2     0.125809    0.110514   0.9672
3     0.095908    0.095601   0.9715
4     0.081675    0.088822   0.9737
5     0.072801    0.082801   0.9753
6     0.066722    0.080849   0.9748
7     0.061732    0.079274   0.9762
8     0.058225    0.076546   0.9768
9     0.055061    0.076950   0.9764
10     0.052385    0.074488   0.9776
11     0.050330    0.073512   0.9772
12     0.048928    0.074192   0.9765
13     0.047915    0.072883   0.9782
14     0.047177    0.071823   0.9772
15     0.046733    0.072312   0.9778
16     0.046413    0.073513   0.9769
17     0.045932    0.073720   0.9770
18     0.045625    0.073526   0.9763
19     0.045534    0.072602   0.9771
20     0.045473    0.071151   0.9781
21     0.045212    0.073357   0.9773
22     0.045044    0.071886   0.9773
23     0.044907    0.072310   0.9771
24     0.044744    0.073032   0.9768
25     0.044737    0.072589   0.9773
26     0.044491    0.072594   0.9770
27     0.044333    0.072231   0.9779
28     0.044220    0.071440   0.9783
29     0.044140    0.072917   0.9774
30     0.044010    0.071917   0.9780
31     0.043946    0.073338   0.9767
32     0.043971    0.071093   0.9787
33     0.043912    0.073049   0.9776
34     0.043838    0.073293   0.9775
35     0.043866    0.071809   0.9775
36     0.043868    0.072773   0.9772
37     0.043843    0.071277   0.9774
38     0.043686    0.070882   0.9779
39     0.043589    0.071535   0.9780
40     0.043798    0.071873   0.9775
41     0.043510    0.072120   0.9774
42     0.043546    0.071345   0.9778
43     0.043431    0.072117   0.9775
44     0.043404    0.073621   0.9772
45     0.043617    0.072302   0.9773
46     0.043370    0.072266   0.9776
47     0.043616    0.072172   0.9777
48     0.043377    0.072345   0.9777
49     0.043451    0.071007   0.9781
50     0.043328    0.071863   0.9773
Remaining weight 1.19 %
Epoch Train_loss  Test_loss  Accuracy
0     0.000000    2.297197   0.0974
1     0.399970    0.155193   0.9543
2     0.128346    0.113610   0.9664
3     0.097146    0.097610   0.9700
4     0.082009    0.089721   0.9731
5     0.072823    0.084908   0.9747
6     0.066845    0.080695   0.9756
7     0.062017    0.078904   0.9768
8     0.058439    0.078459   0.9757
9     0.055307    0.075386   0.9773
10     0.052595    0.073902   0.9771
11     0.050528    0.074849   0.9767
12     0.049258    0.074089   0.9774
13     0.048132    0.074245   0.9766
14     0.047735    0.073377   0.9770
15     0.047212    0.074085   0.9771
16     0.046692    0.074598   0.9768
17     0.046331    0.075065   0.9765
18     0.046168    0.074190   0.9769
19     0.045891    0.074979   0.9763
20     0.045887    0.074319   0.9769
21     0.045649    0.072976   0.9767
22     0.045479    0.072459   0.9774
23     0.045344    0.073511   0.9767
24     0.045287    0.073065   0.9771
25     0.045259    0.074295   0.9770
26     0.045117    0.071574   0.9779
27     0.044915    0.074287   0.9766
28     0.045403    0.074329   0.9765
29     0.045197    0.073280   0.9771
30     0.044868    0.072151   0.9778
31     0.044860    0.072894   0.9771
32     0.044814    0.073214   0.9772
33     0.044585    0.072652   0.9772
34     0.044330    0.072131   0.9772
35     0.044295    0.072355   0.9779
36     0.044439    0.073758   0.9767
37     0.044096    0.072923   0.9774
38     0.044096    0.074029   0.9768
39     0.044130    0.072180   0.9777
40     0.044359    0.072623   0.9769
41     0.043977    0.073076   0.9776
42     0.044106    0.072531   0.9773
43     0.044192    0.073376   0.9769
44     0.044003    0.072372   0.9773
45     0.044066    0.072739   0.9773
46     0.043981    0.073420   0.9769
47     0.044008    0.073548   0.9776
48     0.043876    0.072997   0.9767
49     0.043968    0.071435   0.9776
50     0.043755    0.071843   0.9778
