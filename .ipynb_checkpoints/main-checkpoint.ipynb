{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import visdom\n",
    "import copy\n",
    "import torch.nn.utils.prune as prune\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import timeit\n",
    "import sys\n",
    "import os\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import argparse\n",
    "\n",
    "# custom model\n",
    "from model_archs import Lenet300_100, Lenet250_75, Lenet200_50, TESTMODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# random seed for test\\ntorch.manual_seed(55)\\ntorch.cuda.manual_seed_all(55)\\ntorch.backends.cudnn.enabled = False\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# random seed for test\n",
    "torch.manual_seed(55)\n",
    "torch.cuda.manual_seed_all(55)\n",
    "torch.backends.cudnn.enabled = False\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices : 2\n",
      "Current cuda device : 1 (GeForce RTX 2080 Ti))\n",
      "cpu와 cuda 중 다음 기기로 학습함: cuda:1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cuda setting\n",
    "GPU_NUM = 1\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "print ('Available GPU devices :', torch.cuda.device_count())\n",
    "print ('Current cuda device : %d (%s))' % (torch.cuda.current_device(), torch.cuda.get_device_name(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--testname TESTNAME] [--epochs EPOCHS]\n",
      "                             [--lr LR] [--batch_size BATCH_SIZE]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--test_iters TEST_ITERS]\n",
      "                             [--prune_iters PRUNE_ITERS]\n",
      "                             [--prune_per_conv PRUNE_PER_CONV]\n",
      "                             [--prune_per_linear PRUNE_PER_LINEAR]\n",
      "                             [--prune_per_out PRUNE_PER_OUT]\n",
      "                             [--dataset DATASET]\n",
      "                             [--validation_ratio VALIDATION_RATIO]\n",
      "                             [--model_arch MODEL_ARCH] [--test_type TEST_TYPE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/jinhyuk/.local/share/jupyter/runtime/kernel-12757b54-a9e4-4110-8442-de2015f021f7.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinhyuk/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    # Make instance\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Register parameter value\n",
    "    parser.add_argument(\"--testname\", default = \"Lenet300\", type=str, help=\"Check your test file name\")\n",
    "    parser.add_argument(\"--epochs\",default=50, type=int)\n",
    "    parser.add_argument(\"--lr\",default=1.2e-3, type=float)\n",
    "    parser.add_argument(\"--batch_size\", default=60, type=int)\n",
    "    parser.add_argument(\"--weight_decay\", default=0, type=float)\n",
    "    parser.add_argument(\"--test_iters\", default=5, type=int)\n",
    "    parser.add_argument(\"--prune_iters\", default=22, type=int)\n",
    "    parser.add_argument(\"--prune_per_conv\", default=1, type=float, help=\"Prune percentage of convoultion layer\")\n",
    "    parser.add_argument(\"--prune_per_linear\", default=0.2, type=float, help=\"Prune percentage of linear layer\")\n",
    "    parser.add_argument(\"--prune_per_out\", default=0.1, type=float, help=\"Prune percentage of out layer\")\n",
    "    parser.add_argument(\"--dataset\", default=\"mnist\", type=str, help=\"mnist | cifar10\")\n",
    "    parser.add_argument(\"--validation_ratio\", default = (1/12), type=float, help=\"Validation ratio\")\n",
    "    parser.add_argument(\"--model_arch\", default=\"Lenet300_100\", type=str, help=\"Lenet300_100 | Lenet250_75 | Lenet200_50\")\n",
    "    parser.add_argument(\"--test_type\", default=\"test_accu\", type=str, help=\"test_accu | val_accu\")\n",
    "\n",
    "    # Save to args\n",
    "    args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model function. return model\n",
    "def import_model():\n",
    "    if args.model_arch == \"Lenet300_100\":\n",
    "        model = Lenet300_100.Lenet().to(device)\n",
    "    elif args.model_arch == \"Lenet250_75\":\n",
    "        model = Lenet300_100.Lenet().to(device)\n",
    "    elif args.model_arch == \"Lenet200_50\":\n",
    "        model = Lenet300_100.Lenet().to(device)\n",
    "    elif args.model_arch == \"TESTMODEL\":\n",
    "        model = TESTMODEL.TESTMODEL().to(device)\n",
    "    return model\n",
    "\n",
    "# Data loader function. return dataloader(train, validation, test)\n",
    "def data_loader():\n",
    "    if args.dataset == \"mnist\":\n",
    "        # Load mnist dataset\n",
    "        transform = transforms.Compose([\n",
    "                        transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])\n",
    "\n",
    "        trainset = dsets.MNIST(root='../MNIST_data/',\n",
    "                                 train=True,\n",
    "                                 transform = transform,\n",
    "                                 download=True)\n",
    "        testset = dsets.MNIST(root='../MNIST_data/',\n",
    "                                train=False,\n",
    "                                transform = transform,\n",
    "                                download=True)\n",
    "        valset = dsets.MNIST('../MNIST_data/',\n",
    "                                 train=True,\n",
    "                                 transform = transform,\n",
    "                                 download=True)\n",
    "\n",
    "        # Validation set classification\n",
    "        num_train = len(trainset)\n",
    "        indices = list(range(num_train))\n",
    "        split = int(np.floor(args.validation_ratio * num_train))\n",
    "        np.random.shuffle(indices)\n",
    "        train_idx, val_idx = indices[split:], indices[:split]\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        val_sampler = SubsetRandomSampler(val_idx)\n",
    "        # Make data lodaer\n",
    "        train_loader = torch.utils.data.DataLoader(dataset = trainset,\n",
    "                                                  batch_size = args.batch_size,\n",
    "                                                  sampler = train_sampler,\n",
    "                                                  drop_last = True)\n",
    "\n",
    "        val_loader = torch.utils.data.DataLoader(dataset = valset,\n",
    "                                                  batch_size = args.batch_size,\n",
    "                                                  sampler = val_sampler,\n",
    "                                                  drop_last = True)\n",
    "\n",
    "        test_loader = torch.utils.data.DataLoader(dataset = testset,\n",
    "                                                  shuffle = False,\n",
    "                                                  drop_last = True)\n",
    "    # Add aditional dataloader    \n",
    "    #elif: ...\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "    \n",
    "# Model training function    \n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, label) in enumerate(dataloader):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss / len(dataloader)\n",
    "    return running_loss\n",
    "\n",
    "# Model test function return accuracy n loss\n",
    "def test(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in dataloader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            test_loss += loss / len(dataloader)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "        # loader -> # of batch loader.dataset -> # of data \n",
    "    return (correct/total), test_loss\n",
    "\n",
    "# Find initial accuracy\n",
    "def zero_accu(model, dataloader, criterion, remaining_weight):\n",
    "    accuracy, test_loss = test(model, dataloader, criterion)\n",
    "    running_loss = 0\n",
    "    print('[epoch : 0] (l_loss: 0.00000) (t_loss: %.5f) (accu: %.4f)' % (test_loss, accuracy))\n",
    "    test_result[test_iter][prune_iter][0][\"Running_loss\"] = running_loss\n",
    "    test_result[test_iter][prune_iter][0][\"Test_loss\"] = test_loss\n",
    "    test_result[test_iter][prune_iter][0][\"Accuracy\"] = accuracy\n",
    "    \n",
    "    return accuracy, test_loss, running_loss\n",
    "\n",
    "# Prune function. weight pruning n copied mask\n",
    "def weight_prune(prune_iter):\n",
    "    conv_rate = (1 - ((1-args.prune_per_conv) ** prune_iter))\n",
    "    fc_rate = (1 - ((1-args.prune_per_linear) ** prune_iter))\n",
    "    out_rate = (1 - ((1-args.prune_per_out) ** prune_iter))\n",
    "    # Make prune mask\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.l1_unstructured(module, name = 'weight', amount = conv_rate)\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if 'out' in name:\n",
    "                prune.l1_unstructured(module, name = 'weight', amount = out_rate)\n",
    "            else:\n",
    "                prune.l1_unstructured(module, name = 'weight', amount = fc_rate)\n",
    "            \n",
    "    # Copy a mask   \n",
    "    cpd_mask = {}\n",
    "    for name, mask in model.named_buffers():\n",
    "        cpd_mask[name] = mask\n",
    "    \n",
    "    # Apply prune function\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.remove(module, name = 'weight')\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            prune.remove(module, name = 'weight')                \n",
    "\n",
    "    return cpd_mask\n",
    "\n",
    "# Weight initialize and apply mask function. \n",
    "def weight_init_apply():\n",
    "    # Weight initialize to first model\n",
    "    for name_model, param_model in model.named_parameters():\n",
    "        for name_init, param_init in model_init.named_parameters():\n",
    "            if name_model in name_init:\n",
    "                param_model.data = copy.deepcopy(param_init.data)\n",
    "                break\n",
    "                \n",
    "    # Apply prune mask\n",
    "    for name_model, param_model in model.named_parameters():\n",
    "        for name_mask in cpd_mask:\n",
    "            if name_model in name_mask:\n",
    "                param_model.data = param_model.data.mul_(cpd_mask[name_mask])\n",
    "                break\n",
    "                \n",
    "    # Gradient hook (freeze zero-weight)\n",
    "    for name_model, module in model.named_modules():\n",
    "        for name_mask in cpd_mask:\n",
    "            if name_model != \"\" and name_model in name_mask:\n",
    "                hook = module.weight.register_hook(lambda grad, name_mask=name_mask : grad.mul_(cpd_mask[name_mask]))\n",
    "        \n",
    "    optimizer = optim.Adam(model.parameters(), lr = args.lr, weight_decay = args.weight_decay)\n",
    "    \n",
    "    return optimizer, hook\n",
    "\n",
    "# Weight count function\n",
    "# dict type ['Layer name' : [all, non_zero, zero, ratio]]\n",
    "def weight_counter(model):\n",
    "    layer_weight = {'all.weight':[0, 0, 0, 0]}\n",
    "    for name, p in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            remain, pruned = (p != 0).sum().item(), (p == 0).sum().item()\n",
    "            layer_weight[name] = [remain+pruned, remain, pruned, round((remain/(remain+pruned))*100, 2)]           \n",
    "    for i in layer_weight.keys():\n",
    "        for j in range(0, 3):\n",
    "            layer_weight['all.weight'][j] += layer_weight[i][j]\n",
    "    layer_weight['all.weight'][3] = round(layer_weight['all.weight'][1]/layer_weight['all.weight'][0]*100, 2)\n",
    "    print(\"------------------------------------------------------------\\n\",\n",
    "          \"Layer\".center(12), \"Weight\".center(39), \"Ratio(%)\".rjust(7), sep='')\n",
    "    for i in layer_weight.keys():        \n",
    "        print(\"%s\" % i.ljust(13), \":\",\n",
    "              (\"%s (%s | %s)\" % (layer_weight[i][0], layer_weight[i][1], layer_weight[i][2])).center(36),\n",
    "              (\"%.2f\" % layer_weight[i][3]).rjust(7),\n",
    "              sep=''\n",
    "             )\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    return layer_weight\n",
    "\n",
    "# Print best accuracy in each iteration\n",
    "def best_accuracy(best_accu):\n",
    "    print(\"Maximum accuracy per weight remaining\")\n",
    "    for name in best_accu:\n",
    "        print(\"Remaining weight %s %% \" % name,\n",
    "             \"Epoch %d\" % best_accu[name][0],\n",
    "             \"Accu %.4f\" % best_accu[name][1])\n",
    "\n",
    "# Make result dictionary\n",
    "def mk_result_dict():\n",
    "    test_result = {}\n",
    "    for test_iter in range(args.test_iters):\n",
    "        test_result[test_iter+1] = {}\n",
    "        for prune_iter in range(args.prune_iters):\n",
    "            test_result[test_iter+1][prune_iter] = {}\n",
    "            for epoch in range((args.epochs)+1):\n",
    "                test_result[test_iter+1][prune_iter][epoch] = {}            \n",
    "    return test_result\n",
    "\n",
    "# Save result to result dict\n",
    "def save_result():\n",
    "    test_result[test_iter][prune_iter][epoch+1][\"Running_loss\"] = running_loss\n",
    "    test_result[test_iter][prune_iter][epoch+1][\"Test_loss\"] = test_loss\n",
    "    test_result[test_iter][prune_iter][epoch+1][\"Accuracy\"] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_result/1111_result.txt test_result/1111_AccuData\n"
     ]
    }
   ],
   "source": [
    "# Filename n location\n",
    "FolderLocation = \"test_result\"\n",
    "FName_result, FName_accu = args.testname.split(), args.testname.split()\n",
    "FName_result.append('result.txt'), FName_accu.append('AccuData')\n",
    "FName_result, FName_accu = os.path.join(FolderLocation, \"_\".join(FName_result)), os.path.join(FolderLocation, \"_\".join(FName_accu))\n",
    "\n",
    "model = import_model()\n",
    "model_init = copy.deepcopy(model)\n",
    "train_loader, val_loader, test_loader = data_loader()\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "test_result = mk_result_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print('\\n'.join(\"%s: %s\" % item for item in __dict__.items()),'\\n\\n')\n",
    "print(\"Learning start!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning start!\n",
      "Test_Iter (1/3) Start!\n",
      "Learning start! [Prune_iter : (1/20), Remaining weight : 100.0 %]\n",
      "Learning start! [Prune_iter : (2/20), Remaining weight : 80.04 %]\n",
      "Learning start! [Prune_iter : (3/20), Remaining weight : 64.06 %]\n"
     ]
    }
   ],
   "source": [
    "temp = sys.stdout\n",
    "sys.stdout = open(FName_result,'w')\n",
    "\n",
    "print(args)\n",
    "print(\"Learning start!\")\n",
    "\n",
    "sys.stdout.close()\n",
    "sys.stdout = temp\n",
    "\n",
    "for test_iter in range(1, args.test_iters+1):\n",
    "    print(\"Test_Iter (%d/%d)\" % (test_iter, args.test_iters))\n",
    "    \n",
    "    best_accu = {}\n",
    "    \n",
    "    for prune_iter in range(args.prune_iters):\n",
    "        \n",
    "        temp = sys.stdout\n",
    "        sys.stdout = open(FName_result,'a')\n",
    "        \n",
    "        if prune_iter != 0:\n",
    "            cpd_mask = weight_prune(prune_iter)\n",
    "            optimizer, hook = weight_init_apply()\n",
    "        else:\n",
    "            model = copy.deepcopy(model_init)\n",
    "            optimizer = optim.Adam(model.parameters(), lr = args.lr, weight_decay = args.weight_decay)\n",
    "        \n",
    "        # Count remaining weight\n",
    "        weight_counts = weight_counter(model)\n",
    "        remaining_weight = weight_counts['all.weight'][3]\n",
    "             \n",
    "        print(\"Learning start! [Test_Iter : (%d/%d), Prune_iter : (%d/%d), Remaining weight : %s %%]\" %\n",
    "              (test_iter, args.test_iters, prune_iter+1 , args.prune_iters, remaining_weight))\n",
    "\n",
    "        # Find initial accuracy\n",
    "        accuracy, test_loss, running_loss = zero_accu(model, test_loader, criterion, remaining_weight)            \n",
    "        best_accu[remaining_weight] = [0, 0]\n",
    "        start_t = timeit.default_timer()\n",
    "        \n",
    "        for epoch in range(args.epochs):\n",
    "            running_loss = train(model, train_loader, optimizer, criterion)\n",
    "            \n",
    "            if args.test_type == 'test_accu':\n",
    "                accuracy, test_loss = test(model, test_loader, criterion)\n",
    "            else:\n",
    "                accuracy, test_loss = test(model, val_loader, criterion)\n",
    "            \n",
    "            # Appending best accuracy in list (weight_remain, epoch, accuracy)\n",
    "            if best_accu[remaining_weight][1] <= accuracy:\n",
    "                best_accu[remaining_weight] = [epoch, accuracy]\n",
    "\n",
    "            print('[epoch : %d] (l_loss: %.5f) (t_loss: %.5f) (accu: %.4f)' %\n",
    "                  ((epoch+1), (running_loss), (test_loss), (accuracy)))\n",
    "            \n",
    "            save_result()\n",
    "        test_result[test_iter][remaining_weight] = test_result[test_iter].pop(prune_iter)    \n",
    "            \n",
    "        if prune_iter != 0:\n",
    "            hook.remove()\n",
    "        stop_t = timeit.default_timer()\n",
    "        \n",
    "        print(\"Finish! (Best accu: %.4f) (Time taken(sec) : %.2f) \\n\\n\" %\n",
    "              ((best_accu[remaining_weight][1]), (stop_t - start_t)))\n",
    "        \n",
    "        sys.stdout.close()\n",
    "        sys.stdout = temp\n",
    "    # Find best accuracy in each iteration\n",
    "    best_accuracy(best_accu)\n",
    "    print(\"Test_Iter (%d/%d) Finish!\" % (test_iter, args.test_iters))\n",
    "\n",
    "print(\"Training End\")\n",
    "torch.save(test_result, FName_accu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
