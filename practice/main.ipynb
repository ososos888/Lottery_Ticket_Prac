{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import visdom\n",
    "import copy\n",
    "import torch.nn.utils.prune as prune\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import timeit\n",
    "import sys\n",
    "\n",
    "# custom librarys (model, parameters...) Lottery_Ticket_Prac/custom/utils.py\n",
    "import custom.utils as cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# random seed for test\\ntorch.manual_seed(55)\\ntorch.cuda.manual_seed_all(55)\\ntorch.backends.cudnn.enabled = False\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# random seed for test\n",
    "torch.manual_seed(55)\n",
    "torch.cuda.manual_seed_all(55)\n",
    "torch.backends.cudnn.enabled = False\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test, prune, util function\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, label) in enumerate(dataloader):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss / len(dataloader)\n",
    "    return running_loss\n",
    "\n",
    "def test(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in dataloader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            test_loss += loss / len(dataloader)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "        # 로더 -> 배치 개수 로더.dataset -> 전체 길이, \n",
    "    return (correct/total), test_loss\n",
    "\n",
    "# prune function\n",
    "# pruning mask 생성 -> mask 복사 -> weight initialize -> prune 진행\n",
    "def weight_init(model, model_init, c_rate, f_rate, o_rate):\n",
    "    # make prune mask\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.l1_unstructured(module, name = 'weight', amount = c_rate)\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if 'out' in name:\n",
    "                prune.l1_unstructured(module, name = 'weight', amount = o_rate)\n",
    "            else:\n",
    "                prune.l1_unstructured(module, name = 'weight', amount = f_rate)\n",
    "            \n",
    "    # mask copy   \n",
    "    cp_mask = {}\n",
    "    for name, mask in model.named_buffers():\n",
    "        cp_mask[name[:(len(name)-12)]] = mask\n",
    "    # weight initialize\n",
    "    # copy weight model_init.layer.weight -> model.layer.weight_orig. Bias is similar.\n",
    "    for name, p in model.named_parameters():\n",
    "        if 'weight_orig' in name:\n",
    "            for name2, p2 in model_init.named_parameters():\n",
    "                if name[0:len(name) - 5] in name2:\n",
    "                    p.data = copy.deepcopy(p2.data)\n",
    "        if 'bias' in name:\n",
    "            for name2, p2 in model_init.named_parameters():\n",
    "                if name in name2:\n",
    "                    p.data = copy.deepcopy(p2.data)\n",
    "    \n",
    "    # go prune\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.remove(module, name = 'weight')\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            prune.remove(module, name = 'weight')            \n",
    "    \n",
    "    # gradient hook (freeze zero-weight)\n",
    "    for name, module in model.named_modules():\n",
    "        if 'fc' in name:\n",
    "            hook = module.weight.register_hook(lambda grad, name=name : grad.mul_(cp_mask[name]))\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr = param.lr, weight_decay = param.weight_decay)\n",
    "    \n",
    "    # return copied mask\n",
    "    return cp_mask, optimizer, hook\n",
    "\n",
    "# visdom append plot\n",
    "def visdom_plot(loss_plot, num, loss_value, name):\n",
    "    vis.line(X = num,\n",
    "            Y = loss_value,\n",
    "            win = loss_plot,\n",
    "            name = str(name),\n",
    "            update = 'append'\n",
    "            )\n",
    "    \n",
    "def result_plot():\n",
    "    x = []\n",
    "    for i in range(param.epochs+1):\n",
    "        x.append(i*1000)\n",
    "\n",
    "    for name in test_result['Average of trials']:\n",
    "        visdom_plot(vis_plt, torch.Tensor(x), torch.Tensor(test_result['Average of trials'][name][2]),\n",
    "                            name)\n",
    "\n",
    "# weight count function\n",
    "# dict type ['Layer name' : [all, non_zero, zero, ratio]]\n",
    "def weight_counter(model):\n",
    "    layer_weight = {'all.weight':[0, 0, 0, 0]}\n",
    "    \n",
    "    for name, p in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            remain, pruned = (p != 0).sum().item(), (p == 0).sum().item()\n",
    "            layer_weight[name] = [remain+pruned, remain, pruned, round((remain/(remain+pruned))*100, 2)]\n",
    "            \n",
    "    for i in layer_weight.keys():\n",
    "        for j in range(0, 3):\n",
    "            layer_weight['all.weight'][j] += layer_weight[i][j]\n",
    "    layer_weight['all.weight'][3] = round(layer_weight['all.weight'][1]/layer_weight['all.weight'][0]*100, 2)\n",
    "    print(\"------------------------------------------------------------\\n\",\n",
    "          \"Layer\".center(12), \"Weight\".center(39), \"Ratio(%)\".rjust(7), sep='')\n",
    "    for i in layer_weight.keys():\n",
    "        \n",
    "        print(\"%s\" % i.ljust(13), \":\",\n",
    "              (\"%s (%s | %s)\" % (layer_weight[i][0], layer_weight[i][1], layer_weight[i][2])).center(36),\n",
    "              (\"%.2f\" % layer_weight[i][3]).rjust(7),\n",
    "              sep=''\n",
    "             )\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    return layer_weight\n",
    "\n",
    "# print best accuracy in each iteration\n",
    "def best_accuracy(best_accu):\n",
    "    print(\"Maximum accuracy per weight remaining\")\n",
    "    \"\"\"\n",
    "    for i in range(len(best_accu)):\n",
    "        print(\"Remaining weight %.1f %% \" % (best_accu[i][0] * 100),\n",
    "             \"Epoch %d\" % best_accu[i][1],\n",
    "             \"Accu %.4f\" % best_accu[i][2])\n",
    "    \"\"\"\n",
    "    for name in best_accu:\n",
    "        print(\"Remaining weight %s %% \" % name,\n",
    "             \"Epoch %d\" % best_accu[name][0],\n",
    "             \"Accu %.4f\" % best_accu[name][1])\n",
    "        \n",
    "# initial accuracy\n",
    "def zero_accu(model, dataloader, criterion, remaining_weight, vis_plt):\n",
    "    accuracy, test_loss = test(model, dataloader, criterion)\n",
    "    #visdom_plot(vis_plt,torch.Tensor([accuracy]), torch.Tensor([0]), str(remaining_weight))\n",
    "    print('[epoch : 0] (l_loss: x.xxxxx) (t_loss: %.5f) (accu: %.4f)' % (test_loss, accuracy))\n",
    "    return accuracy, test_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def append_result_data(running_loss, test_loss, accuracy):\n",
    "    result_data[0].append(running_loss)\n",
    "    result_data[1].append(test_loss)\n",
    "    result_data[2].append(accuracy)\n",
    "    \n",
    "def wcount():\n",
    "    # [전체, 남은, 비율]\n",
    "    fulllist = []\n",
    "    for i in range (param.prune_iter):\n",
    "        weight = [0, 0, 0]\n",
    "        for name, p in model.named_parameters():\n",
    "            if 'conv' in name:\n",
    "                if 'weight' in name:\n",
    "                    a = (p != 0).sum().item() + (p == 0).sum().item()\n",
    "                    weight[0] += a\n",
    "                    weight[1] += int(a * (((1-param.prune_per_c) ** i)))\n",
    "            elif 'fc' in name:\n",
    "                if 'weight' in name:\n",
    "                    if 'out' in name:\n",
    "                        a = (p != 0).sum().item() + (p == 0).sum().item()\n",
    "                        weight[0] += a\n",
    "                        weight[1] += int(a * (((1-param.prune_per_o) ** i)))\n",
    "                    else:\n",
    "                        a = (p != 0).sum().item() + (p == 0).sum().item()\n",
    "                        weight[0] += a\n",
    "                        weight[1] += int(a * (((1-param.prune_per_f) ** i)))\n",
    "        fulllist.append(round(weight[1]/weight[0] * 100, 2))\n",
    "    return fulllist\n",
    "\n",
    "def result_dict():\n",
    "    result = {}\n",
    "    weightper = wcount()\n",
    "    for i in range(param.test_iter):\n",
    "        result[(i+1)] = {}\n",
    "        #for j in range(len(weightper)):\n",
    "            #result[(i+1)][weightper[j]] = {}\n",
    "            #for z in range(param.epochs):\n",
    "                #result[(i+1)][weightper[j]][z] = {}\n",
    "    return result\n",
    "\n",
    "def average_calc():\n",
    "    test_result['Average of trials'] = {}\n",
    "    for weight_per in test_result[1]:\n",
    "        test_result['Average of trials'][weight_per] = [[],[],[]]\n",
    "        for i in range(3):\n",
    "            for j in range(param.epochs+1):\n",
    "                test_result['Average of trials'][weight_per][i].append(0)\n",
    "        for i in range(3):\n",
    "            for j in range(1, param.test_iter+1):\n",
    "                for k in range(param.epochs+1):\n",
    "                    test_result['Average of trials'][weight_per][i][k] += test_result[j][weight_per][i][k]\n",
    "            for z in range(param.epochs+1):\n",
    "                test_result['Average of trials'][weight_per][i][z] /= (param.test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices : 2\n",
      "Current cuda device : 1 (GeForce RTX 2080 Ti))\n",
      "cpu와 cuda 중 다음 기기로 학습함: cuda:1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cuda setting. GPU_NUM = 사용할 GPU의 번호\n",
    "GPU_NUM = 1\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "print ('Available devices :', torch.cuda.device_count())\n",
    "print ('Current cuda device : %d (%s))' % (torch.cuda.current_device(), torch.cuda.get_device_name(device)))\n",
    "print(\"cpu와 cuda 중 다음 기기로 학습함:\", device, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selcet model's number\n",
      " (1 : Lenet_300_100)\n",
      " (2 : Lenet_250_75)\n",
      " (3 : Lenet_200_50)\n",
      " (4 : Conv6)\n",
      "\n",
      "1\n",
      "Selected model : Lenet_300_100\n",
      "Enter a file name \n",
      "Lenet300test20\n",
      "File name is [Lenet300test20]. Are you sure? [1 : Yes, 2: No]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# set model type\n",
    "print(\"Selcet model's number\\n\",\n",
    "      \"(1 : Lenet_300_100)\\n\",\n",
    "      \"(2 : Lenet_250_75)\\n\",\n",
    "      \"(3 : Lenet_200_50)\\n\",\n",
    "      \"(4 : Conv6)\\n\"\n",
    "     )\n",
    "while True:\n",
    "    x = int(input())\n",
    "    if x == 1:\n",
    "        print(\"Selected model : Lenet_300_100\")\n",
    "        model_type = \"Lenet_300_100\"\n",
    "        break\n",
    "    elif x == 2:\n",
    "        print(\"Selected model : Lenet_250_75\")\n",
    "        model_type = \"Lenet_250_75\"\n",
    "        break\n",
    "    elif x == 3:\n",
    "        print(\"Selected model : Lenet_200_50\")\n",
    "        model_type = \"Lenet_200_50\"\n",
    "        break\n",
    "    elif x == 4:\n",
    "        print(\"Selected model : Conv6\")\n",
    "        model_type = \"Conv6\"\n",
    "        break\n",
    "    elif x == 100:\n",
    "        print(\"Selected model : TestModel\")\n",
    "        model_type = \"TestModel\"\n",
    "        break\n",
    "    # elif x (Adding model...)\n",
    "    print(\"Wrong value entered!\")\n",
    "while True:\n",
    "    fname = input(\"Enter a file name \\n\")\n",
    "    x = int(input(\"File name is [%s]. Are you sure? [1 : Yes, 2: No]\\n\" % fname))\n",
    "    if x == 1:\n",
    "        break\n",
    "        \n",
    "FileName = 'test_result/' + fname + '_result.txt'\n",
    "temp = sys.stdout\n",
    "sys.stdout = open(FileName,'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param = cu.parameters()\n",
    "if model_type == 'Lenet_300_100':\n",
    "    model = cu.Lenet_300_100().to(device)\n",
    "elif model_type == 'Lenet_250_75':\n",
    "    model = cu.Lenet_250_75().to(device)\n",
    "elif model_type == 'Lenet_200_50':\n",
    "    model = cu.Lenet_200_50().to(device)\n",
    "elif model_type == 'Conv6':\n",
    "    model = cu.Conv6().to(device)\n",
    "elif model_type == 'TestModel':\n",
    "    model = cu.TestModel().to(device)\n",
    "\n",
    "param.type(model_type)    \n",
    "model_init = copy.deepcopy(model)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# change parameter (원할 경우 class에 접근하여 직접 변경)\n",
    "param.epochs = 20\n",
    "param.test_iter = 3\n",
    "param.prune_iter = 21\n",
    "param.weight_decay = 0\n",
    "param.valset = 'empty'\n",
    "# model.fc1 = nn.Linear(784, 200)\n",
    "\n",
    "trained_weights = {}\n",
    "test_result = result_dict()\n",
    "# parameter check\n",
    "print('\\n'.join(\"%s: %s\" % item for item in param.__dict__.items()),'\\n\\n')\n",
    "print('Model structure\\n',model)\n",
    "\n",
    "sys.stdout.close()\n",
    "sys.stdout = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "# visdom setting\n",
    "vis = visdom.Visdom()\n",
    "vis.close(env=\"main\")\n",
    "\n",
    "Tracker_type = \"Accuracy_Tracker\"\n",
    "title = fname + \"_\" + Tracker_type\n",
    "\n",
    "# make plot\n",
    "vis_plt = vis.line(X=torch.Tensor(1).zero_(), Y=torch.Tensor(1).zero_(), \n",
    "                    opts=dict(title = title,\n",
    "                              legend=['100.0'],\n",
    "                              showlegend=True,\n",
    "                              xtickmin = 0,\n",
    "                              xtickmax = 20000,\n",
    "                              ytickmin = 0.94,\n",
    "                              ytickmax = 0.99\n",
    "                             )\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning start!\n",
      "Test_Iter (1/3) Start!\n",
      "Learning start! [Prune_iter : (1/21), Remaining weight : 100.0 %]\n",
      "Learning start! [Prune_iter : (2/21), Remaining weight : 80.04 %]\n",
      "Learning start! [Prune_iter : (3/21), Remaining weight : 64.06 %]\n",
      "Learning start! [Prune_iter : (4/21), Remaining weight : 51.28 %]\n",
      "Learning start! [Prune_iter : (5/21), Remaining weight : 41.05 %]\n",
      "Learning start! [Prune_iter : (6/21), Remaining weight : 32.87 %]\n",
      "Learning start! [Prune_iter : (7/21), Remaining weight : 26.32 %]\n",
      "Learning start! [Prune_iter : (8/21), Remaining weight : 21.07 %]\n",
      "Learning start! [Prune_iter : (9/21), Remaining weight : 16.88 %]\n",
      "Learning start! [Prune_iter : (10/21), Remaining weight : 13.52 %]\n",
      "Learning start! [Prune_iter : (11/21), Remaining weight : 10.83 %]\n",
      "Learning start! [Prune_iter : (12/21), Remaining weight : 8.68 %]\n",
      "Learning start! [Prune_iter : (13/21), Remaining weight : 6.95 %]\n",
      "Learning start! [Prune_iter : (14/21), Remaining weight : 5.57 %]\n",
      "Learning start! [Prune_iter : (15/21), Remaining weight : 4.47 %]\n",
      "Learning start! [Prune_iter : (16/21), Remaining weight : 3.58 %]\n",
      "Learning start! [Prune_iter : (17/21), Remaining weight : 2.87 %]\n",
      "Learning start! [Prune_iter : (18/21), Remaining weight : 2.31 %]\n",
      "Learning start! [Prune_iter : (19/21), Remaining weight : 1.85 %]\n",
      "Learning start! [Prune_iter : (20/21), Remaining weight : 1.49 %]\n",
      "Learning start! [Prune_iter : (21/21), Remaining weight : 1.19 %]\n",
      "Test_Iter (1/3) Finish!\n",
      "Test_Iter (2/3) Start!\n",
      "Learning start! [Prune_iter : (1/21), Remaining weight : 100.0 %]\n",
      "Learning start! [Prune_iter : (2/21), Remaining weight : 80.04 %]\n",
      "Learning start! [Prune_iter : (3/21), Remaining weight : 64.06 %]\n",
      "Learning start! [Prune_iter : (4/21), Remaining weight : 51.28 %]\n",
      "Learning start! [Prune_iter : (5/21), Remaining weight : 41.05 %]\n",
      "Learning start! [Prune_iter : (6/21), Remaining weight : 32.87 %]\n",
      "Learning start! [Prune_iter : (7/21), Remaining weight : 26.32 %]\n",
      "Learning start! [Prune_iter : (8/21), Remaining weight : 21.07 %]\n",
      "Learning start! [Prune_iter : (9/21), Remaining weight : 16.88 %]\n",
      "Learning start! [Prune_iter : (10/21), Remaining weight : 13.52 %]\n",
      "Learning start! [Prune_iter : (11/21), Remaining weight : 10.83 %]\n",
      "Learning start! [Prune_iter : (12/21), Remaining weight : 8.68 %]\n",
      "Learning start! [Prune_iter : (13/21), Remaining weight : 6.95 %]\n",
      "Learning start! [Prune_iter : (14/21), Remaining weight : 5.57 %]\n",
      "Learning start! [Prune_iter : (15/21), Remaining weight : 4.47 %]\n",
      "Learning start! [Prune_iter : (16/21), Remaining weight : 3.58 %]\n",
      "Learning start! [Prune_iter : (17/21), Remaining weight : 2.87 %]\n",
      "Learning start! [Prune_iter : (18/21), Remaining weight : 2.31 %]\n",
      "Learning start! [Prune_iter : (19/21), Remaining weight : 1.85 %]\n",
      "Learning start! [Prune_iter : (20/21), Remaining weight : 1.49 %]\n",
      "Learning start! [Prune_iter : (21/21), Remaining weight : 1.19 %]\n",
      "Test_Iter (2/3) Finish!\n",
      "Test_Iter (3/3) Start!\n",
      "Learning start! [Prune_iter : (1/21), Remaining weight : 100.0 %]\n",
      "Learning start! [Prune_iter : (2/21), Remaining weight : 80.04 %]\n",
      "Learning start! [Prune_iter : (3/21), Remaining weight : 64.06 %]\n",
      "Learning start! [Prune_iter : (4/21), Remaining weight : 51.28 %]\n",
      "Learning start! [Prune_iter : (5/21), Remaining weight : 41.05 %]\n",
      "Learning start! [Prune_iter : (6/21), Remaining weight : 32.87 %]\n",
      "Learning start! [Prune_iter : (7/21), Remaining weight : 26.32 %]\n",
      "Learning start! [Prune_iter : (8/21), Remaining weight : 21.07 %]\n",
      "Learning start! [Prune_iter : (9/21), Remaining weight : 16.88 %]\n",
      "Learning start! [Prune_iter : (10/21), Remaining weight : 13.52 %]\n",
      "Learning start! [Prune_iter : (11/21), Remaining weight : 10.83 %]\n",
      "Learning start! [Prune_iter : (12/21), Remaining weight : 8.68 %]\n",
      "Learning start! [Prune_iter : (13/21), Remaining weight : 6.95 %]\n",
      "Learning start! [Prune_iter : (14/21), Remaining weight : 5.57 %]\n",
      "Learning start! [Prune_iter : (15/21), Remaining weight : 4.47 %]\n",
      "Learning start! [Prune_iter : (16/21), Remaining weight : 3.58 %]\n",
      "Learning start! [Prune_iter : (17/21), Remaining weight : 2.87 %]\n",
      "Learning start! [Prune_iter : (18/21), Remaining weight : 2.31 %]\n",
      "Learning start! [Prune_iter : (19/21), Remaining weight : 1.85 %]\n",
      "Learning start! [Prune_iter : (20/21), Remaining weight : 1.49 %]\n",
      "Learning start! [Prune_iter : (21/21), Remaining weight : 1.19 %]\n",
      "Test_Iter (3/3) Finish!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nwith open(dic_FileName, 'wb') as f:\\n    pickle.dump(test_result, f)\\nwith open(dic_FileName2, 'wb') as f2:\\n    pickle.dump(trained_weights, f2)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Learning start!\")\n",
    "for i in range(1, (param.test_iter+1)):\n",
    "    print(\"Test_Iter (%d/%d) Start!\" % (i, param.test_iter))\n",
    "    temp = sys.stdout\n",
    "    sys.stdout = open(FileName,'a')\n",
    "\n",
    "    print(\"=====================================================================\",\n",
    "          \"\\n\\nTest_Iter (%d/%d)\" % (i, param.test_iter))\n",
    "    \n",
    "    ########################\n",
    "\n",
    "    ########################\n",
    "    #model, model_init, param = set_model(model_type)\n",
    "    \n",
    "    #param.epochs = epochs\n",
    "    #param.test_iter = test_iter\n",
    "    #param.prune_iter = prune_iter\n",
    "    \n",
    "    best_accu = {}\n",
    "    #result_data = [[],[],[]]\n",
    "    for j in range(param.prune_iter):\n",
    "        result_data = [[],[],[]]\n",
    "        #cp_mask = {}\n",
    "        # pruning weight, mask 복사, optimizer 재설정\n",
    "        # layer별 prune rate를 입력\n",
    "        \n",
    "        # 이 함수 prune per 지워도 작동하나 확인해보기\n",
    "        if j != 0:\n",
    "            cp_mask, optimizer, hook = weight_init(model, model_init, \n",
    "                                   (1 - ((1-param.prune_per_c) ** j)),\n",
    "                                   (1 - ((1-param.prune_per_f) ** j)),\n",
    "                                   (1 - ((1-param.prune_per_o) ** j))\n",
    "                                  )\n",
    "        else:\n",
    "            model = copy.deepcopy(model_init)\n",
    "            optimizer = optim.Adam(model.parameters(), lr = param.lr, weight_decay = param.weight_decay)\n",
    "        #print(model.fcout.weight)\n",
    "        \n",
    "        # prune 진행 후 남은 weight 수 확인\n",
    "        weight_counts = weight_counter(model)\n",
    "        # 총 weight 중 남은 weight의 수 저장 (visdom plot시 사용하기 위함)\n",
    "        remaining_weight = weight_counts['all.weight'][3]\n",
    "        \n",
    "        sys.stdout.close()\n",
    "        sys.stdout = temp\n",
    "        print(\"Learning start! [Prune_iter : (%d/%d), Remaining weight : %s %%]\" % (j+1 , param.prune_iter, remaining_weight))\n",
    "        temp = sys.stdout\n",
    "        sys.stdout = open(FileName,'a')\n",
    "        \n",
    "        print(\"Learning start! [Prune_iter : (%d/%d), Remaining weight : %s %%]\" % (j+1 , param.prune_iter, remaining_weight))\n",
    "        # 시작 시간 check\n",
    "        #print(model.fcout.weight[0])\n",
    "        # initial accuracy 확인 및 plot\n",
    "        accuracy, test_loss = zero_accu(model, param.test_loader, criterion, remaining_weight, vis_plt)\n",
    "        \n",
    "        append_result_data(0, test_loss.item(), accuracy)\n",
    "        \n",
    "        best_accu[remaining_weight] = [0, 0]\n",
    "        \n",
    "        start_t = timeit.default_timer()\n",
    "        \n",
    "        for epoch in range(param.epochs):\n",
    "            # model training, return training loss    \n",
    "            running_loss = train(model, param.train_loader, optimizer, criterion)\n",
    "            # val_set이 있을 경우 val_set을 통해 loss, accu를 구한다.\n",
    "            if param.valset == 'empty':\n",
    "                accuracy, test_loss = test(model, param.test_loader, criterion)\n",
    "            else:\n",
    "                accuracy, test_loss = test(model, param.val_loader, criterion)\n",
    "            \n",
    "            # visdom plot (plot window, x-axis, y-axis, label name)\n",
    "            #visdom_plot(vis_plt, torch.Tensor([(epoch+1) * 1000]), torch.Tensor([accuracy]),\n",
    "             #           remaining_weight)\n",
    "            \n",
    "            append_result_data(running_loss.item(), test_loss.item(), accuracy)\n",
    "            \n",
    "            # Appending best accuracy in list (weight_remain, epoch, accuracy)\n",
    "            if best_accu[remaining_weight][1] <= accuracy:\n",
    "                best_accu[remaining_weight] = [epoch, accuracy]\n",
    "\n",
    "            print('[epoch : %d] (l_loss: %.5f) (t_loss: %.5f) (accu: %.4f)' %\n",
    "                  ((epoch+1), (running_loss), (test_loss), (accuracy)))\n",
    "        if j != 0:\n",
    "            hook.remove()\n",
    "        stop_t = timeit.default_timer()\n",
    "\n",
    "        #print(model.fcout.weight[0])\n",
    "        \n",
    "        print(\"Finish! (Best accu: %.4f) (Time taken(sec) : %.2f) \\n\\n\" %\n",
    "              ((best_accu[remaining_weight][1]), (stop_t - start_t)))\n",
    "        test_result[i][remaining_weight] = result_data\n",
    "    #test_result[i+1][remaining_weight] = result_data    \n",
    "    # iteration별 최고 정확도 확인\n",
    "    best_accuracy(best_accu)\n",
    "    \n",
    "    sys.stdout.close()\n",
    "    sys.stdout = temp\n",
    "    \n",
    "    \n",
    "    print(\"Test_Iter (%d/%d) Finish!\" % (i, param.test_iter))\n",
    "    # 훈련된 model의 weight 저장\n",
    "    trained_weights[i] = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            a = copy.deepcopy(p)\n",
    "            trained_weights[i].append(a)\n",
    "    #print(trained_weights[i][2])\n",
    "average_calc()\n",
    "result_plot()\n",
    "\n",
    "# Save test_result, weights dictionary\n",
    "dic_FileName = \"test_result/\" + fname + \"_result_data\"\n",
    "dic_FileName2 = \"test_result/\" + fname + \"_trained_weights\"\n",
    "torch.save(test_result, dic_FileName)\n",
    "torch.save(trained_weights, dic_FileName2)\n",
    "\"\"\"\n",
    "with open(dic_FileName, 'wb') as f:\n",
    "    pickle.dump(test_result, f)\n",
    "with open(dic_FileName2, 'wb') as f2:\n",
    "    pickle.dump(trained_weights, f2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average_calc()\n",
    "result_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.fcout.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = sys.stdout\n",
    "sys.stdout = open(FileName,'a')\n",
    "\n",
    "print(\"Average test data\")\n",
    "for name in test_result['Average of trials']:\n",
    "    print(\"Remaining weight %.2f %%\" % name)\n",
    "    print(\"Epoch Train_loss  Test_loss  Accuracy\")\n",
    "    for i in range(param.epochs+1):\n",
    "        print('%d     %.6f    %.6f   %.4f' % (\n",
    "            i,\n",
    "            test_result['Average of trials'][name][0][i],\n",
    "            test_result['Average of trials'][name][1][i],\n",
    "            test_result['Average of trials'][name][2][i]))\n",
    "\n",
    "sys.stdout.close()\n",
    "sys.stdout = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntest_result_ = torch.load(dic_FileName)\\ntrained_weights_ = torch.load(dic_FileName2)\\nprint(test_result_)\\nprint(trained_weights_)\\n\\nwith open(dic_fileName, 'rb') as fin:\\n    test_result_ = pickle.load(fin)\\nwith open(dic_fileName2, 'rb') as fin2:\\n    trained_weights_ = pickle.load(fin2)\\n\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "test_result_ = torch.load(dic_FileName)\n",
    "trained_weights_ = torch.load(dic_FileName2)\n",
    "print(test_result_)\n",
    "print(trained_weights_)\n",
    "\n",
    "with open(dic_fileName, 'rb') as fin:\n",
    "    test_result_ = pickle.load(fin)\n",
    "with open(dic_fileName2, 'rb') as fin2:\n",
    "    trained_weights_ = pickle.load(fin2)\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
