Namespace(batch_size=60, dataset='mnist', epochs=50, lr=0.0012, model_arch='Lenet300_100', prune_iters=22, prune_per_conv=1, prune_per_linear=0.2, prune_per_out=0.1, test_iters=5, test_type='test_accu', testname='Lenet300_test1', validation_ratio=0.08333333333333333, weight_decay=0)
Learning start!
------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :        266200 (266200 | 0)          100.00
fc1.weight   :        235200 (235200 | 0)          100.00
fc2.weight   :         30000 (30000 | 0)           100.00
fcout.weight :          1000 (1000 | 0)            100.00
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (1/22), Remaining weight : 100.0 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.84877) (accu: 0.1005)
[epoch : 1] (l_loss: 0.20983) (t_loss: 0.11463) (accu: 0.9630)
[epoch : 2] (l_loss: 0.09454) (t_loss: 0.09596) (accu: 0.9703)
[epoch : 3] (l_loss: 0.06565) (t_loss: 0.08255) (accu: 0.9743)
[epoch : 4] (l_loss: 0.04961) (t_loss: 0.08999) (accu: 0.9742)
[epoch : 5] (l_loss: 0.04135) (t_loss: 0.09083) (accu: 0.9736)
[epoch : 6] (l_loss: 0.03826) (t_loss: 0.08228) (accu: 0.9778)
[epoch : 7] (l_loss: 0.02899) (t_loss: 0.08833) (accu: 0.9776)
[epoch : 8] (l_loss: 0.02928) (t_loss: 0.10562) (accu: 0.9721)
[epoch : 9] (l_loss: 0.02501) (t_loss: 0.10628) (accu: 0.9756)
[epoch : 10] (l_loss: 0.02407) (t_loss: 0.10129) (accu: 0.9778)
[epoch : 11] (l_loss: 0.02653) (t_loss: 0.11471) (accu: 0.9757)
[epoch : 12] (l_loss: 0.01789) (t_loss: 0.11420) (accu: 0.9766)
[epoch : 13] (l_loss: 0.01543) (t_loss: 0.11570) (accu: 0.9771)
[epoch : 14] (l_loss: 0.02415) (t_loss: 0.11587) (accu: 0.9780)
[epoch : 15] (l_loss: 0.01563) (t_loss: 0.11500) (accu: 0.9763)
[epoch : 16] (l_loss: 0.01652) (t_loss: 0.12059) (accu: 0.9778)
[epoch : 17] (l_loss: 0.01453) (t_loss: 0.10928) (accu: 0.9818)
[epoch : 18] (l_loss: 0.01785) (t_loss: 0.11987) (accu: 0.9786)
[epoch : 19] (l_loss: 0.01420) (t_loss: 0.12282) (accu: 0.9777)
[epoch : 20] (l_loss: 0.01591) (t_loss: 0.14068) (accu: 0.9763)
[epoch : 21] (l_loss: 0.00967) (t_loss: 0.15478) (accu: 0.9774)
[epoch : 22] (l_loss: 0.01852) (t_loss: 0.13383) (accu: 0.9807)
[epoch : 23] (l_loss: 0.01361) (t_loss: 0.13463) (accu: 0.9796)
[epoch : 24] (l_loss: 0.01527) (t_loss: 0.11847) (accu: 0.9806)
[epoch : 25] (l_loss: 0.01411) (t_loss: 0.14399) (accu: 0.9770)
[epoch : 26] (l_loss: 0.01049) (t_loss: 0.13751) (accu: 0.9783)
[epoch : 27] (l_loss: 0.01160) (t_loss: 0.13715) (accu: 0.9807)
[epoch : 28] (l_loss: 0.01408) (t_loss: 0.13792) (accu: 0.9804)
[epoch : 29] (l_loss: 0.01439) (t_loss: 0.13401) (accu: 0.9807)
[epoch : 30] (l_loss: 0.00717) (t_loss: 0.19223) (accu: 0.9760)
[epoch : 31] (l_loss: 0.01319) (t_loss: 0.16566) (accu: 0.9788)
[epoch : 32] (l_loss: 0.01207) (t_loss: 0.15231) (accu: 0.9800)
[epoch : 33] (l_loss: 0.01029) (t_loss: 0.14178) (accu: 0.9824)
[epoch : 34] (l_loss: 0.00975) (t_loss: 0.20414) (accu: 0.9766)
[epoch : 35] (l_loss: 0.01367) (t_loss: 0.18188) (accu: 0.9803)
[epoch : 36] (l_loss: 0.01081) (t_loss: 0.15151) (accu: 0.9818)
[epoch : 37] (l_loss: 0.01131) (t_loss: 0.15824) (accu: 0.9805)
[epoch : 38] (l_loss: 0.00793) (t_loss: 0.17910) (accu: 0.9799)
[epoch : 39] (l_loss: 0.01058) (t_loss: 0.23874) (accu: 0.9777)
[epoch : 40] (l_loss: 0.01533) (t_loss: 0.17864) (accu: 0.9802)
[epoch : 41] (l_loss: 0.00963) (t_loss: 0.21288) (accu: 0.9784)
[epoch : 42] (l_loss: 0.00737) (t_loss: 0.18793) (accu: 0.9819)
[epoch : 43] (l_loss: 0.01060) (t_loss: 0.19994) (accu: 0.9807)
[epoch : 44] (l_loss: 0.00825) (t_loss: 0.21479) (accu: 0.9789)
[epoch : 45] (l_loss: 0.00955) (t_loss: 0.21510) (accu: 0.9788)
[epoch : 46] (l_loss: 0.01271) (t_loss: 0.21561) (accu: 0.9793)
[epoch : 47] (l_loss: 0.01268) (t_loss: 0.22435) (accu: 0.9796)
[epoch : 48] (l_loss: 0.00984) (t_loss: 0.20774) (accu: 0.9805)
[epoch : 49] (l_loss: 0.00839) (t_loss: 0.21878) (accu: 0.9799)
[epoch : 50] (l_loss: 0.00769) (t_loss: 0.24885) (accu: 0.9789)
Finish! (Best accu: 0.9824) (Time taken(sec) : 528.06) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (213060 | 53140)         80.04
fc1.weight   :      235200 (188160 | 47040)         80.00
fc2.weight   :        30000 (24000 | 6000)          80.00
fcout.weight :          1000 (900 | 100)            90.00
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (2/22), Remaining weight : 80.04 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.39930) (accu: 0.1377)
[epoch : 1] (l_loss: 0.19391) (t_loss: 0.10583) (accu: 0.9672)
[epoch : 2] (l_loss: 0.08161) (t_loss: 0.09185) (accu: 0.9710)
[epoch : 3] (l_loss: 0.05730) (t_loss: 0.09220) (accu: 0.9709)
[epoch : 4] (l_loss: 0.04670) (t_loss: 0.08373) (accu: 0.9751)
[epoch : 5] (l_loss: 0.03851) (t_loss: 0.08293) (accu: 0.9766)
[epoch : 6] (l_loss: 0.03017) (t_loss: 0.10302) (accu: 0.9745)
[epoch : 7] (l_loss: 0.02745) (t_loss: 0.07801) (accu: 0.9804)
[epoch : 8] (l_loss: 0.02470) (t_loss: 0.09251) (accu: 0.9759)
[epoch : 9] (l_loss: 0.02685) (t_loss: 0.08702) (accu: 0.9778)
[epoch : 10] (l_loss: 0.01655) (t_loss: 0.09628) (accu: 0.9789)
[epoch : 11] (l_loss: 0.01907) (t_loss: 0.12212) (accu: 0.9727)
[epoch : 12] (l_loss: 0.01940) (t_loss: 0.11372) (accu: 0.9768)
[epoch : 13] (l_loss: 0.01632) (t_loss: 0.11774) (accu: 0.9757)
[epoch : 14] (l_loss: 0.01805) (t_loss: 0.10336) (accu: 0.9779)
[epoch : 15] (l_loss: 0.01317) (t_loss: 0.09780) (accu: 0.9813)
[epoch : 16] (l_loss: 0.01283) (t_loss: 0.11399) (accu: 0.9780)
[epoch : 17] (l_loss: 0.01653) (t_loss: 0.11327) (accu: 0.9778)
[epoch : 18] (l_loss: 0.00984) (t_loss: 0.12832) (accu: 0.9767)
[epoch : 19] (l_loss: 0.01589) (t_loss: 0.11246) (accu: 0.9792)
[epoch : 20] (l_loss: 0.01022) (t_loss: 0.13724) (accu: 0.9783)
[epoch : 21] (l_loss: 0.01223) (t_loss: 0.12988) (accu: 0.9779)
[epoch : 22] (l_loss: 0.01326) (t_loss: 0.13983) (accu: 0.9771)
[epoch : 23] (l_loss: 0.01109) (t_loss: 0.12890) (accu: 0.9814)
[epoch : 24] (l_loss: 0.01266) (t_loss: 0.14469) (accu: 0.9797)
[epoch : 25] (l_loss: 0.01040) (t_loss: 0.15883) (accu: 0.9764)
[epoch : 26] (l_loss: 0.00901) (t_loss: 0.13779) (accu: 0.9792)
[epoch : 27] (l_loss: 0.01049) (t_loss: 0.13629) (accu: 0.9784)
[epoch : 28] (l_loss: 0.01241) (t_loss: 0.16105) (accu: 0.9792)
[epoch : 29] (l_loss: 0.01386) (t_loss: 0.16015) (accu: 0.9807)
[epoch : 30] (l_loss: 0.00767) (t_loss: 0.15025) (accu: 0.9792)
[epoch : 31] (l_loss: 0.00987) (t_loss: 0.17035) (accu: 0.9795)
[epoch : 32] (l_loss: 0.01191) (t_loss: 0.17133) (accu: 0.9791)
[epoch : 33] (l_loss: 0.00986) (t_loss: 0.15743) (accu: 0.9812)
[epoch : 34] (l_loss: 0.01122) (t_loss: 0.17358) (accu: 0.9804)
[epoch : 35] (l_loss: 0.01044) (t_loss: 0.19120) (accu: 0.9758)
[epoch : 36] (l_loss: 0.00981) (t_loss: 0.17967) (accu: 0.9790)
[epoch : 37] (l_loss: 0.00824) (t_loss: 0.17752) (accu: 0.9801)
[epoch : 38] (l_loss: 0.00842) (t_loss: 0.19973) (accu: 0.9779)
[epoch : 39] (l_loss: 0.01176) (t_loss: 0.15385) (accu: 0.9809)
[epoch : 40] (l_loss: 0.00783) (t_loss: 0.16459) (accu: 0.9815)
[epoch : 41] (l_loss: 0.01097) (t_loss: 0.21078) (accu: 0.9773)
[epoch : 42] (l_loss: 0.00866) (t_loss: 0.18688) (accu: 0.9777)
[epoch : 43] (l_loss: 0.00651) (t_loss: 0.17170) (accu: 0.9820)
[epoch : 44] (l_loss: 0.00755) (t_loss: 0.21615) (accu: 0.9776)
[epoch : 45] (l_loss: 0.01045) (t_loss: 0.19655) (accu: 0.9793)
[epoch : 46] (l_loss: 0.00867) (t_loss: 0.19631) (accu: 0.9805)
[epoch : 47] (l_loss: 0.00899) (t_loss: 0.20878) (accu: 0.9801)
[epoch : 48] (l_loss: 0.00847) (t_loss: 0.18831) (accu: 0.9815)
[epoch : 49] (l_loss: 0.00731) (t_loss: 0.22244) (accu: 0.9791)
[epoch : 50] (l_loss: 0.00589) (t_loss: 0.21221) (accu: 0.9817)
Finish! (Best accu: 0.9820) (Time taken(sec) : 542.53) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (170538 | 95662)         64.06
fc1.weight   :      235200 (150528 | 84672)         64.00
fc2.weight   :       30000 (19200 | 10800)          64.00
fcout.weight :          1000 (810 | 190)            81.00
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (3/22), Remaining weight : 64.06 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.40698) (accu: 0.1514)
[epoch : 1] (l_loss: 0.19035) (t_loss: 0.10028) (accu: 0.9697)
[epoch : 2] (l_loss: 0.07654) (t_loss: 0.08425) (accu: 0.9745)
[epoch : 3] (l_loss: 0.05136) (t_loss: 0.08070) (accu: 0.9736)
[epoch : 4] (l_loss: 0.03959) (t_loss: 0.08390) (accu: 0.9744)
[epoch : 5] (l_loss: 0.03248) (t_loss: 0.10133) (accu: 0.9720)
[epoch : 6] (l_loss: 0.02524) (t_loss: 0.09211) (accu: 0.9751)
[epoch : 7] (l_loss: 0.02713) (t_loss: 0.09596) (accu: 0.9769)
[epoch : 8] (l_loss: 0.01953) (t_loss: 0.09282) (accu: 0.9775)
[epoch : 9] (l_loss: 0.01796) (t_loss: 0.09332) (accu: 0.9795)
[epoch : 10] (l_loss: 0.01733) (t_loss: 0.11000) (accu: 0.9764)
[epoch : 11] (l_loss: 0.01640) (t_loss: 0.10050) (accu: 0.9787)
[epoch : 12] (l_loss: 0.01631) (t_loss: 0.09652) (accu: 0.9791)
[epoch : 13] (l_loss: 0.01223) (t_loss: 0.11637) (accu: 0.9774)
[epoch : 14] (l_loss: 0.01653) (t_loss: 0.12987) (accu: 0.9766)
[epoch : 15] (l_loss: 0.01173) (t_loss: 0.12962) (accu: 0.9797)
[epoch : 16] (l_loss: 0.01050) (t_loss: 0.11306) (accu: 0.9803)
[epoch : 17] (l_loss: 0.01273) (t_loss: 0.12329) (accu: 0.9791)
[epoch : 18] (l_loss: 0.01173) (t_loss: 0.14093) (accu: 0.9769)
[epoch : 19] (l_loss: 0.01014) (t_loss: 0.15122) (accu: 0.9773)
[epoch : 20] (l_loss: 0.01183) (t_loss: 0.13392) (accu: 0.9787)
[epoch : 21] (l_loss: 0.00507) (t_loss: 0.12331) (accu: 0.9805)
[epoch : 22] (l_loss: 0.01450) (t_loss: 0.15048) (accu: 0.9781)
[epoch : 23] (l_loss: 0.00949) (t_loss: 0.16911) (accu: 0.9755)
[epoch : 24] (l_loss: 0.01035) (t_loss: 0.15961) (accu: 0.9785)
[epoch : 25] (l_loss: 0.01023) (t_loss: 0.16079) (accu: 0.9781)
[epoch : 26] (l_loss: 0.00480) (t_loss: 0.15891) (accu: 0.9786)
[epoch : 27] (l_loss: 0.01144) (t_loss: 0.17591) (accu: 0.9779)
[epoch : 28] (l_loss: 0.01208) (t_loss: 0.14962) (accu: 0.9790)
[epoch : 29] (l_loss: 0.00790) (t_loss: 0.17152) (accu: 0.9784)
[epoch : 30] (l_loss: 0.00949) (t_loss: 0.15721) (accu: 0.9788)
[epoch : 31] (l_loss: 0.00543) (t_loss: 0.16353) (accu: 0.9799)
[epoch : 32] (l_loss: 0.00735) (t_loss: 0.18813) (accu: 0.9769)
[epoch : 33] (l_loss: 0.01161) (t_loss: 0.18033) (accu: 0.9794)
[epoch : 34] (l_loss: 0.00599) (t_loss: 0.15915) (accu: 0.9808)
[epoch : 35] (l_loss: 0.00798) (t_loss: 0.19433) (accu: 0.9791)
[epoch : 36] (l_loss: 0.01116) (t_loss: 0.19024) (accu: 0.9785)
[epoch : 37] (l_loss: 0.00695) (t_loss: 0.16144) (accu: 0.9821)
[epoch : 38] (l_loss: 0.00626) (t_loss: 0.19064) (accu: 0.9798)
[epoch : 39] (l_loss: 0.01124) (t_loss: 0.20185) (accu: 0.9773)
[epoch : 40] (l_loss: 0.00753) (t_loss: 0.17836) (accu: 0.9784)
[epoch : 41] (l_loss: 0.00653) (t_loss: 0.20049) (accu: 0.9777)
[epoch : 42] (l_loss: 0.00964) (t_loss: 0.18255) (accu: 0.9784)
[epoch : 43] (l_loss: 0.00543) (t_loss: 0.18225) (accu: 0.9800)
[epoch : 44] (l_loss: 0.00786) (t_loss: 0.21179) (accu: 0.9780)
[epoch : 45] (l_loss: 0.00678) (t_loss: 0.20129) (accu: 0.9775)
[epoch : 46] (l_loss: 0.00459) (t_loss: 0.19974) (accu: 0.9798)
[epoch : 47] (l_loss: 0.00873) (t_loss: 0.20391) (accu: 0.9800)
[epoch : 48] (l_loss: 0.00708) (t_loss: 0.20831) (accu: 0.9794)
[epoch : 49] (l_loss: 0.00595) (t_loss: 0.20315) (accu: 0.9804)
[epoch : 50] (l_loss: 0.00886) (t_loss: 0.23274) (accu: 0.9785)
Finish! (Best accu: 0.9821) (Time taken(sec) : 552.71) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (136511 | 129689)        51.28
fc1.weight   :      235200 (120422 | 114778)        51.20
fc2.weight   :       30000 (15360 | 14640)          51.20
fcout.weight :          1000 (729 | 271)            72.90
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (4/22), Remaining weight : 51.28 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.34847) (accu: 0.1174)
[epoch : 1] (l_loss: 0.18310) (t_loss: 0.08504) (accu: 0.9747)
[epoch : 2] (l_loss: 0.06799) (t_loss: 0.08524) (accu: 0.9716)
[epoch : 3] (l_loss: 0.04584) (t_loss: 0.06905) (accu: 0.9778)
[epoch : 4] (l_loss: 0.03366) (t_loss: 0.08337) (accu: 0.9742)
[epoch : 5] (l_loss: 0.02640) (t_loss: 0.07927) (accu: 0.9783)
[epoch : 6] (l_loss: 0.02236) (t_loss: 0.08951) (accu: 0.9767)
[epoch : 7] (l_loss: 0.01773) (t_loss: 0.10083) (accu: 0.9763)
[epoch : 8] (l_loss: 0.01977) (t_loss: 0.09958) (accu: 0.9772)
[epoch : 9] (l_loss: 0.01383) (t_loss: 0.10027) (accu: 0.9778)
[epoch : 10] (l_loss: 0.01370) (t_loss: 0.11862) (accu: 0.9759)
[epoch : 11] (l_loss: 0.01245) (t_loss: 0.09735) (accu: 0.9790)
[epoch : 12] (l_loss: 0.01235) (t_loss: 0.10566) (accu: 0.9774)
[epoch : 13] (l_loss: 0.01311) (t_loss: 0.12030) (accu: 0.9755)
[epoch : 14] (l_loss: 0.00865) (t_loss: 0.12026) (accu: 0.9783)
[epoch : 15] (l_loss: 0.01077) (t_loss: 0.12990) (accu: 0.9772)
[epoch : 16] (l_loss: 0.01164) (t_loss: 0.11773) (accu: 0.9784)
[epoch : 17] (l_loss: 0.01193) (t_loss: 0.12617) (accu: 0.9790)
[epoch : 18] (l_loss: 0.00499) (t_loss: 0.12413) (accu: 0.9810)
[epoch : 19] (l_loss: 0.01098) (t_loss: 0.13214) (accu: 0.9785)
[epoch : 20] (l_loss: 0.01243) (t_loss: 0.13506) (accu: 0.9795)
[epoch : 21] (l_loss: 0.00886) (t_loss: 0.12894) (accu: 0.9792)
[epoch : 22] (l_loss: 0.00797) (t_loss: 0.15188) (accu: 0.9780)
[epoch : 23] (l_loss: 0.00669) (t_loss: 0.15925) (accu: 0.9789)
[epoch : 24] (l_loss: 0.00828) (t_loss: 0.14804) (accu: 0.9794)
[epoch : 25] (l_loss: 0.00620) (t_loss: 0.15597) (accu: 0.9787)
[epoch : 26] (l_loss: 0.01009) (t_loss: 0.14485) (accu: 0.9783)
[epoch : 27] (l_loss: 0.00749) (t_loss: 0.14994) (accu: 0.9795)
[epoch : 28] (l_loss: 0.00787) (t_loss: 0.15827) (accu: 0.9780)
[epoch : 29] (l_loss: 0.01058) (t_loss: 0.16039) (accu: 0.9780)
[epoch : 30] (l_loss: 0.00619) (t_loss: 0.15946) (accu: 0.9790)
[epoch : 31] (l_loss: 0.00409) (t_loss: 0.16009) (accu: 0.9798)
[epoch : 32] (l_loss: 0.00811) (t_loss: 0.16653) (accu: 0.9798)
[epoch : 33] (l_loss: 0.01038) (t_loss: 0.16401) (accu: 0.9801)
[epoch : 34] (l_loss: 0.00627) (t_loss: 0.15731) (accu: 0.9812)
[epoch : 35] (l_loss: 0.00442) (t_loss: 0.18496) (accu: 0.9796)
[epoch : 36] (l_loss: 0.00804) (t_loss: 0.20324) (accu: 0.9779)
[epoch : 37] (l_loss: 0.00597) (t_loss: 0.18236) (accu: 0.9815)
[epoch : 38] (l_loss: 0.00838) (t_loss: 0.17602) (accu: 0.9808)
[epoch : 39] (l_loss: 0.00587) (t_loss: 0.19173) (accu: 0.9777)
[epoch : 40] (l_loss: 0.00601) (t_loss: 0.18005) (accu: 0.9802)
[epoch : 41] (l_loss: 0.00589) (t_loss: 0.16840) (accu: 0.9815)
[epoch : 42] (l_loss: 0.00689) (t_loss: 0.17266) (accu: 0.9823)
[epoch : 43] (l_loss: 0.00520) (t_loss: 0.19390) (accu: 0.9797)
[epoch : 44] (l_loss: 0.00650) (t_loss: 0.17566) (accu: 0.9813)
[epoch : 45] (l_loss: 0.00906) (t_loss: 0.17642) (accu: 0.9805)
[epoch : 46] (l_loss: 0.00367) (t_loss: 0.17702) (accu: 0.9819)
[epoch : 47] (l_loss: 0.00234) (t_loss: 0.19445) (accu: 0.9805)
[epoch : 48] (l_loss: 0.00869) (t_loss: 0.18039) (accu: 0.9810)
[epoch : 49] (l_loss: 0.00652) (t_loss: 0.20006) (accu: 0.9805)
[epoch : 50] (l_loss: 0.00804) (t_loss: 0.17772) (accu: 0.9811)
Finish! (Best accu: 0.9823) (Time taken(sec) : 556.82) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (109282 | 156918)        41.05
fc1.weight   :      235200 (96338 | 138862)         40.96
fc2.weight   :       30000 (12288 | 17712)          40.96
fcout.weight :          1000 (656 | 344)            65.60
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (5/22), Remaining weight : 41.05 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.31962) (accu: 0.1043)
[epoch : 1] (l_loss: 0.17545) (t_loss: 0.08918) (accu: 0.9710)
[epoch : 2] (l_loss: 0.06146) (t_loss: 0.07913) (accu: 0.9767)
[epoch : 3] (l_loss: 0.03687) (t_loss: 0.07446) (accu: 0.9772)
[epoch : 4] (l_loss: 0.02850) (t_loss: 0.07074) (accu: 0.9799)
[epoch : 5] (l_loss: 0.02061) (t_loss: 0.08124) (accu: 0.9790)
[epoch : 6] (l_loss: 0.01835) (t_loss: 0.09192) (accu: 0.9760)
[epoch : 7] (l_loss: 0.01494) (t_loss: 0.09106) (accu: 0.9789)
[epoch : 8] (l_loss: 0.01349) (t_loss: 0.09714) (accu: 0.9767)
[epoch : 9] (l_loss: 0.01508) (t_loss: 0.09537) (accu: 0.9797)
[epoch : 10] (l_loss: 0.01178) (t_loss: 0.11671) (accu: 0.9751)
[epoch : 11] (l_loss: 0.00830) (t_loss: 0.10656) (accu: 0.9797)
[epoch : 12] (l_loss: 0.01134) (t_loss: 0.10297) (accu: 0.9814)
[epoch : 13] (l_loss: 0.00889) (t_loss: 0.12738) (accu: 0.9761)
[epoch : 14] (l_loss: 0.00919) (t_loss: 0.11534) (accu: 0.9795)
[epoch : 15] (l_loss: 0.00879) (t_loss: 0.11347) (accu: 0.9807)
[epoch : 16] (l_loss: 0.00905) (t_loss: 0.14187) (accu: 0.9764)
[epoch : 17] (l_loss: 0.00755) (t_loss: 0.11641) (accu: 0.9812)
[epoch : 18] (l_loss: 0.00491) (t_loss: 0.11742) (accu: 0.9802)
[epoch : 19] (l_loss: 0.00970) (t_loss: 0.15210) (accu: 0.9773)
[epoch : 20] (l_loss: 0.00506) (t_loss: 0.14609) (accu: 0.9775)
[epoch : 21] (l_loss: 0.00829) (t_loss: 0.14630) (accu: 0.9772)
[epoch : 22] (l_loss: 0.00662) (t_loss: 0.13863) (accu: 0.9792)
[epoch : 23] (l_loss: 0.00860) (t_loss: 0.14952) (accu: 0.9758)
[epoch : 24] (l_loss: 0.00435) (t_loss: 0.15403) (accu: 0.9790)
[epoch : 25] (l_loss: 0.00718) (t_loss: 0.15253) (accu: 0.9793)
[epoch : 26] (l_loss: 0.00837) (t_loss: 0.14737) (accu: 0.9806)
[epoch : 27] (l_loss: 0.00399) (t_loss: 0.16163) (accu: 0.9801)
[epoch : 28] (l_loss: 0.00710) (t_loss: 0.16189) (accu: 0.9790)
[epoch : 29] (l_loss: 0.01025) (t_loss: 0.17849) (accu: 0.9788)
[epoch : 30] (l_loss: 0.00328) (t_loss: 0.14859) (accu: 0.9806)
[epoch : 31] (l_loss: 0.00324) (t_loss: 0.19492) (accu: 0.9777)
[epoch : 32] (l_loss: 0.00839) (t_loss: 0.18840) (accu: 0.9813)
[epoch : 33] (l_loss: 0.00646) (t_loss: 0.16071) (accu: 0.9800)
[epoch : 34] (l_loss: 0.00699) (t_loss: 0.16117) (accu: 0.9799)
[epoch : 35] (l_loss: 0.00402) (t_loss: 0.19575) (accu: 0.9780)
[epoch : 36] (l_loss: 0.00676) (t_loss: 0.18106) (accu: 0.9800)
[epoch : 37] (l_loss: 0.00436) (t_loss: 0.17676) (accu: 0.9809)
[epoch : 38] (l_loss: 0.00618) (t_loss: 0.22329) (accu: 0.9762)
[epoch : 39] (l_loss: 0.00569) (t_loss: 0.18633) (accu: 0.9803)
[epoch : 40] (l_loss: 0.00502) (t_loss: 0.17426) (accu: 0.9805)
[epoch : 41] (l_loss: 0.00574) (t_loss: 0.16798) (accu: 0.9815)
[epoch : 42] (l_loss: 0.00509) (t_loss: 0.20816) (accu: 0.9796)
[epoch : 43] (l_loss: 0.00366) (t_loss: 0.19914) (accu: 0.9790)
[epoch : 44] (l_loss: 0.00401) (t_loss: 0.21663) (accu: 0.9798)
[epoch : 45] (l_loss: 0.00702) (t_loss: 0.19619) (accu: 0.9800)
[epoch : 46] (l_loss: 0.00512) (t_loss: 0.20832) (accu: 0.9800)
[epoch : 47] (l_loss: 0.00407) (t_loss: 0.23202) (accu: 0.9775)
[epoch : 48] (l_loss: 0.00570) (t_loss: 0.22091) (accu: 0.9807)
[epoch : 49] (l_loss: 0.00660) (t_loss: 0.25250) (accu: 0.9770)
[epoch : 50] (l_loss: 0.00641) (t_loss: 0.21236) (accu: 0.9792)
Finish! (Best accu: 0.9815) (Time taken(sec) : 556.01) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (87490 | 178710)         32.87
fc1.weight   :      235200 (77070 | 158130)         32.77
fc2.weight   :        30000 (9830 | 20170)          32.77
fcout.weight :          1000 (590 | 410)            59.00
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (6/22), Remaining weight : 32.87 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.29288) (accu: 0.0974)
[epoch : 1] (l_loss: 0.17196) (t_loss: 0.09322) (accu: 0.9712)
[epoch : 2] (l_loss: 0.05471) (t_loss: 0.06967) (accu: 0.9791)
[epoch : 3] (l_loss: 0.03267) (t_loss: 0.07658) (accu: 0.9778)
[epoch : 4] (l_loss: 0.02297) (t_loss: 0.07020) (accu: 0.9802)
[epoch : 5] (l_loss: 0.01772) (t_loss: 0.08870) (accu: 0.9772)
[epoch : 6] (l_loss: 0.01604) (t_loss: 0.09615) (accu: 0.9775)
[epoch : 7] (l_loss: 0.01100) (t_loss: 0.09479) (accu: 0.9794)
[epoch : 8] (l_loss: 0.01141) (t_loss: 0.09266) (accu: 0.9780)
[epoch : 9] (l_loss: 0.00913) (t_loss: 0.09511) (accu: 0.9811)
[epoch : 10] (l_loss: 0.00946) (t_loss: 0.10612) (accu: 0.9811)
[epoch : 11] (l_loss: 0.00809) (t_loss: 0.11949) (accu: 0.9766)
[epoch : 12] (l_loss: 0.00741) (t_loss: 0.10183) (accu: 0.9803)
[epoch : 13] (l_loss: 0.00801) (t_loss: 0.13486) (accu: 0.9781)
[epoch : 14] (l_loss: 0.00893) (t_loss: 0.11064) (accu: 0.9815)
[epoch : 15] (l_loss: 0.00709) (t_loss: 0.11539) (accu: 0.9797)
[epoch : 16] (l_loss: 0.00849) (t_loss: 0.12268) (accu: 0.9799)
[epoch : 17] (l_loss: 0.00517) (t_loss: 0.12527) (accu: 0.9801)
[epoch : 18] (l_loss: 0.00388) (t_loss: 0.11359) (accu: 0.9819)
[epoch : 19] (l_loss: 0.00692) (t_loss: 0.13512) (accu: 0.9803)
[epoch : 20] (l_loss: 0.00692) (t_loss: 0.13008) (accu: 0.9798)
[epoch : 21] (l_loss: 0.00487) (t_loss: 0.13911) (accu: 0.9798)
[epoch : 22] (l_loss: 0.00543) (t_loss: 0.13669) (accu: 0.9817)
[epoch : 23] (l_loss: 0.00742) (t_loss: 0.15047) (accu: 0.9771)
[epoch : 24] (l_loss: 0.00708) (t_loss: 0.15133) (accu: 0.9787)
[epoch : 25] (l_loss: 0.00376) (t_loss: 0.14362) (accu: 0.9804)
[epoch : 26] (l_loss: 0.00466) (t_loss: 0.14495) (accu: 0.9818)
[epoch : 27] (l_loss: 0.00594) (t_loss: 0.14989) (accu: 0.9810)
[epoch : 28] (l_loss: 0.00611) (t_loss: 0.15547) (accu: 0.9798)
[epoch : 29] (l_loss: 0.00459) (t_loss: 0.15284) (accu: 0.9810)
[epoch : 30] (l_loss: 0.00349) (t_loss: 0.14258) (accu: 0.9823)
[epoch : 31] (l_loss: 0.00326) (t_loss: 0.17664) (accu: 0.9810)
[epoch : 32] (l_loss: 0.00700) (t_loss: 0.17738) (accu: 0.9789)
[epoch : 33] (l_loss: 0.00553) (t_loss: 0.16265) (accu: 0.9804)
[epoch : 34] (l_loss: 0.00287) (t_loss: 0.19425) (accu: 0.9762)
[epoch : 35] (l_loss: 0.00449) (t_loss: 0.17019) (accu: 0.9813)
[epoch : 36] (l_loss: 0.00304) (t_loss: 0.21662) (accu: 0.9780)
[epoch : 37] (l_loss: 0.00663) (t_loss: 0.18884) (accu: 0.9783)
[epoch : 38] (l_loss: 0.00371) (t_loss: 0.16444) (accu: 0.9817)
[epoch : 39] (l_loss: 0.00402) (t_loss: 0.17672) (accu: 0.9808)
[epoch : 40] (l_loss: 0.00423) (t_loss: 0.19973) (accu: 0.9805)
[epoch : 41] (l_loss: 0.00414) (t_loss: 0.19503) (accu: 0.9806)
[epoch : 42] (l_loss: 0.00393) (t_loss: 0.20426) (accu: 0.9797)
[epoch : 43] (l_loss: 0.00560) (t_loss: 0.19904) (accu: 0.9787)
[epoch : 44] (l_loss: 0.00255) (t_loss: 0.19575) (accu: 0.9796)
[epoch : 45] (l_loss: 0.00243) (t_loss: 0.20498) (accu: 0.9802)
[epoch : 46] (l_loss: 0.00417) (t_loss: 0.18806) (accu: 0.9799)
[epoch : 47] (l_loss: 0.00526) (t_loss: 0.22586) (accu: 0.9777)
[epoch : 48] (l_loss: 0.00482) (t_loss: 0.20424) (accu: 0.9801)
[epoch : 49] (l_loss: 0.00443) (t_loss: 0.22211) (accu: 0.9802)
[epoch : 50] (l_loss: 0.00188) (t_loss: 0.20499) (accu: 0.9810)
Finish! (Best accu: 0.9823) (Time taken(sec) : 582.74) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (70051 | 196149)         26.32
fc1.weight   :      235200 (61656 | 173544)         26.21
fc2.weight   :        30000 (7864 | 22136)          26.21
fcout.weight :          1000 (531 | 469)            53.10
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (7/22), Remaining weight : 26.32 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.31776) (accu: 0.0961)
[epoch : 1] (l_loss: 0.16820) (t_loss: 0.08390) (accu: 0.9744)
[epoch : 2] (l_loss: 0.04749) (t_loss: 0.06753) (accu: 0.9776)
[epoch : 3] (l_loss: 0.02701) (t_loss: 0.06621) (accu: 0.9795)
[epoch : 4] (l_loss: 0.01873) (t_loss: 0.07597) (accu: 0.9783)
[epoch : 5] (l_loss: 0.01528) (t_loss: 0.07845) (accu: 0.9782)
[epoch : 6] (l_loss: 0.01040) (t_loss: 0.07446) (accu: 0.9805)
[epoch : 7] (l_loss: 0.01007) (t_loss: 0.08178) (accu: 0.9801)
[epoch : 8] (l_loss: 0.00698) (t_loss: 0.10462) (accu: 0.9759)
[epoch : 9] (l_loss: 0.00906) (t_loss: 0.08184) (accu: 0.9807)
[epoch : 10] (l_loss: 0.00562) (t_loss: 0.09555) (accu: 0.9810)
[epoch : 11] (l_loss: 0.00779) (t_loss: 0.10098) (accu: 0.9797)
[epoch : 12] (l_loss: 0.00633) (t_loss: 0.09369) (accu: 0.9817)
[epoch : 13] (l_loss: 0.00504) (t_loss: 0.10814) (accu: 0.9794)
[epoch : 14] (l_loss: 0.00743) (t_loss: 0.11057) (accu: 0.9788)
[epoch : 15] (l_loss: 0.00336) (t_loss: 0.12746) (accu: 0.9770)
[epoch : 16] (l_loss: 0.00305) (t_loss: 0.11832) (accu: 0.9811)
[epoch : 17] (l_loss: 0.00763) (t_loss: 0.11000) (accu: 0.9812)
[epoch : 18] (l_loss: 0.00324) (t_loss: 0.11002) (accu: 0.9806)
[epoch : 19] (l_loss: 0.00592) (t_loss: 0.14358) (accu: 0.9777)
[epoch : 20] (l_loss: 0.00304) (t_loss: 0.13363) (accu: 0.9799)
[epoch : 21] (l_loss: 0.00556) (t_loss: 0.12197) (accu: 0.9815)
[epoch : 22] (l_loss: 0.00233) (t_loss: 0.12072) (accu: 0.9824)
[epoch : 23] (l_loss: 0.00562) (t_loss: 0.13024) (accu: 0.9833)
[epoch : 24] (l_loss: 0.00356) (t_loss: 0.13592) (accu: 0.9812)
[epoch : 25] (l_loss: 0.00509) (t_loss: 0.15442) (accu: 0.9805)
[epoch : 26] (l_loss: 0.00561) (t_loss: 0.15597) (accu: 0.9799)
[epoch : 27] (l_loss: 0.00396) (t_loss: 0.14552) (accu: 0.9800)
[epoch : 28] (l_loss: 0.00342) (t_loss: 0.13169) (accu: 0.9813)
[epoch : 29] (l_loss: 0.00386) (t_loss: 0.13989) (accu: 0.9796)
[epoch : 30] (l_loss: 0.00416) (t_loss: 0.16102) (accu: 0.9809)
[epoch : 31] (l_loss: 0.00213) (t_loss: 0.15216) (accu: 0.9805)
[epoch : 32] (l_loss: 0.00712) (t_loss: 0.13690) (accu: 0.9827)
[epoch : 33] (l_loss: 0.00273) (t_loss: 0.16972) (accu: 0.9797)
[epoch : 34] (l_loss: 0.00438) (t_loss: 0.16038) (accu: 0.9795)
[epoch : 35] (l_loss: 0.00149) (t_loss: 0.14305) (accu: 0.9825)
[epoch : 36] (l_loss: 0.00186) (t_loss: 0.17815) (accu: 0.9810)
[epoch : 37] (l_loss: 0.00627) (t_loss: 0.16581) (accu: 0.9827)
[epoch : 38] (l_loss: 0.00300) (t_loss: 0.15853) (accu: 0.9830)
[epoch : 39] (l_loss: 0.00212) (t_loss: 0.15142) (accu: 0.9827)
[epoch : 40] (l_loss: 0.00309) (t_loss: 0.16531) (accu: 0.9816)
[epoch : 41] (l_loss: 0.00342) (t_loss: 0.15445) (accu: 0.9832)
[epoch : 42] (l_loss: 0.00516) (t_loss: 0.15149) (accu: 0.9841)
[epoch : 43] (l_loss: 0.00320) (t_loss: 0.17058) (accu: 0.9818)
[epoch : 44] (l_loss: 0.00218) (t_loss: 0.16040) (accu: 0.9828)
[epoch : 45] (l_loss: 0.00308) (t_loss: 0.18060) (accu: 0.9811)
[epoch : 46] (l_loss: 0.00469) (t_loss: 0.18837) (accu: 0.9820)
[epoch : 47] (l_loss: 0.00467) (t_loss: 0.22855) (accu: 0.9799)
[epoch : 48] (l_loss: 0.00389) (t_loss: 0.17764) (accu: 0.9826)
[epoch : 49] (l_loss: 0.00176) (t_loss: 0.19920) (accu: 0.9809)
[epoch : 50] (l_loss: 0.00413) (t_loss: 0.17359) (accu: 0.9828)
Finish! (Best accu: 0.9841) (Time taken(sec) : 581.10) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (56094 | 210106)         21.07
fc1.weight   :      235200 (49325 | 185875)         20.97
fc2.weight   :        30000 (6291 | 23709)          20.97
fcout.weight :          1000 (478 | 522)            47.80
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (8/22), Remaining weight : 21.07 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.34997) (accu: 0.0960)
[epoch : 1] (l_loss: 0.16656) (t_loss: 0.07549) (accu: 0.9770)
[epoch : 2] (l_loss: 0.04455) (t_loss: 0.06654) (accu: 0.9803)
[epoch : 3] (l_loss: 0.02535) (t_loss: 0.05943) (accu: 0.9824)
[epoch : 4] (l_loss: 0.01474) (t_loss: 0.06459) (accu: 0.9819)
[epoch : 5] (l_loss: 0.01122) (t_loss: 0.06740) (accu: 0.9810)
[epoch : 6] (l_loss: 0.00839) (t_loss: 0.07179) (accu: 0.9830)
[epoch : 7] (l_loss: 0.00868) (t_loss: 0.07672) (accu: 0.9822)
[epoch : 8] (l_loss: 0.00520) (t_loss: 0.07992) (accu: 0.9815)
[epoch : 9] (l_loss: 0.00675) (t_loss: 0.09125) (accu: 0.9795)
[epoch : 10] (l_loss: 0.00577) (t_loss: 0.08896) (accu: 0.9829)
[epoch : 11] (l_loss: 0.00398) (t_loss: 0.09615) (accu: 0.9813)
[epoch : 12] (l_loss: 0.00367) (t_loss: 0.10645) (accu: 0.9800)
[epoch : 13] (l_loss: 0.00769) (t_loss: 0.10378) (accu: 0.9802)
[epoch : 14] (l_loss: 0.00439) (t_loss: 0.10833) (accu: 0.9801)
[epoch : 15] (l_loss: 0.00245) (t_loss: 0.11028) (accu: 0.9811)
[epoch : 16] (l_loss: 0.00340) (t_loss: 0.10813) (accu: 0.9822)
[epoch : 17] (l_loss: 0.00577) (t_loss: 0.11546) (accu: 0.9818)
[epoch : 18] (l_loss: 0.00394) (t_loss: 0.10783) (accu: 0.9806)
[epoch : 19] (l_loss: 0.00394) (t_loss: 0.11619) (accu: 0.9833)
[epoch : 20] (l_loss: 0.00305) (t_loss: 0.12499) (accu: 0.9808)
[epoch : 21] (l_loss: 0.00252) (t_loss: 0.12194) (accu: 0.9813)
[epoch : 22] (l_loss: 0.00206) (t_loss: 0.12071) (accu: 0.9812)
[epoch : 23] (l_loss: 0.00475) (t_loss: 0.12407) (accu: 0.9813)
[epoch : 24] (l_loss: 0.00341) (t_loss: 0.14332) (accu: 0.9814)
[epoch : 25] (l_loss: 0.00564) (t_loss: 0.13506) (accu: 0.9812)
[epoch : 26] (l_loss: 0.00232) (t_loss: 0.13959) (accu: 0.9814)
[epoch : 27] (l_loss: 0.00284) (t_loss: 0.13355) (accu: 0.9818)
[epoch : 28] (l_loss: 0.00462) (t_loss: 0.16781) (accu: 0.9789)
[epoch : 29] (l_loss: 0.00446) (t_loss: 0.13475) (accu: 0.9828)
[epoch : 30] (l_loss: 0.00259) (t_loss: 0.14861) (accu: 0.9818)
[epoch : 31] (l_loss: 0.00137) (t_loss: 0.14598) (accu: 0.9804)
[epoch : 32] (l_loss: 0.00313) (t_loss: 0.15640) (accu: 0.9796)
[epoch : 33] (l_loss: 0.00465) (t_loss: 0.16436) (accu: 0.9805)
[epoch : 34] (l_loss: 0.00123) (t_loss: 0.17672) (accu: 0.9806)
[epoch : 35] (l_loss: 0.00433) (t_loss: 0.18954) (accu: 0.9790)
[epoch : 36] (l_loss: 0.00503) (t_loss: 0.18681) (accu: 0.9794)
[epoch : 37] (l_loss: 0.00224) (t_loss: 0.17498) (accu: 0.9816)
[epoch : 38] (l_loss: 0.00154) (t_loss: 0.17337) (accu: 0.9809)
[epoch : 39] (l_loss: 0.00109) (t_loss: 0.17616) (accu: 0.9821)
[epoch : 40] (l_loss: 0.00420) (t_loss: 0.23497) (accu: 0.9789)
[epoch : 41] (l_loss: 0.00439) (t_loss: 0.17908) (accu: 0.9803)
[epoch : 42] (l_loss: 0.00338) (t_loss: 0.19921) (accu: 0.9800)
[epoch : 43] (l_loss: 0.00213) (t_loss: 0.19103) (accu: 0.9809)
[epoch : 44] (l_loss: 0.00231) (t_loss: 0.19082) (accu: 0.9804)
[epoch : 45] (l_loss: 0.00284) (t_loss: 0.21549) (accu: 0.9804)
[epoch : 46] (l_loss: 0.00166) (t_loss: 0.20904) (accu: 0.9820)
[epoch : 47] (l_loss: 0.00311) (t_loss: 0.21610) (accu: 0.9809)
[epoch : 48] (l_loss: 0.00393) (t_loss: 0.25970) (accu: 0.9788)
[epoch : 49] (l_loss: 0.00399) (t_loss: 0.20986) (accu: 0.9816)
[epoch : 50] (l_loss: 0.00150) (t_loss: 0.19720) (accu: 0.9829)
Finish! (Best accu: 0.9833) (Time taken(sec) : 568.35) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (44923 | 221277)         16.88
fc1.weight   :      235200 (39460 | 195740)         16.78
fc2.weight   :        30000 (5033 | 24967)          16.78
fcout.weight :          1000 (430 | 570)            43.00
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (9/22), Remaining weight : 16.88 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.32830) (accu: 0.1699)
[epoch : 1] (l_loss: 0.17035) (t_loss: 0.07582) (accu: 0.9776)
[epoch : 2] (l_loss: 0.04069) (t_loss: 0.06139) (accu: 0.9814)
[epoch : 3] (l_loss: 0.02272) (t_loss: 0.06281) (accu: 0.9809)
[epoch : 4] (l_loss: 0.01272) (t_loss: 0.08549) (accu: 0.9769)
[epoch : 5] (l_loss: 0.01016) (t_loss: 0.06807) (accu: 0.9824)
[epoch : 6] (l_loss: 0.00659) (t_loss: 0.06270) (accu: 0.9838)
[epoch : 7] (l_loss: 0.00618) (t_loss: 0.08068) (accu: 0.9825)
[epoch : 8] (l_loss: 0.00547) (t_loss: 0.08531) (accu: 0.9815)
[epoch : 9] (l_loss: 0.00461) (t_loss: 0.07455) (accu: 0.9844)
[epoch : 10] (l_loss: 0.00477) (t_loss: 0.08687) (accu: 0.9827)
[epoch : 11] (l_loss: 0.00352) (t_loss: 0.09410) (accu: 0.9809)
[epoch : 12] (l_loss: 0.00486) (t_loss: 0.10028) (accu: 0.9812)
[epoch : 13] (l_loss: 0.00361) (t_loss: 0.10428) (accu: 0.9826)
[epoch : 14] (l_loss: 0.00153) (t_loss: 0.10327) (accu: 0.9838)
[epoch : 15] (l_loss: 0.00382) (t_loss: 0.10992) (accu: 0.9802)
[epoch : 16] (l_loss: 0.00582) (t_loss: 0.12263) (accu: 0.9795)
[epoch : 17] (l_loss: 0.00263) (t_loss: 0.11138) (accu: 0.9824)
[epoch : 18] (l_loss: 0.00135) (t_loss: 0.12117) (accu: 0.9810)
[epoch : 19] (l_loss: 0.00422) (t_loss: 0.12395) (accu: 0.9834)
[epoch : 20] (l_loss: 0.00272) (t_loss: 0.12716) (accu: 0.9805)
[epoch : 21] (l_loss: 0.00227) (t_loss: 0.12936) (accu: 0.9807)
[epoch : 22] (l_loss: 0.00171) (t_loss: 0.14891) (accu: 0.9811)
[epoch : 23] (l_loss: 0.00567) (t_loss: 0.14181) (accu: 0.9815)
[epoch : 24] (l_loss: 0.00350) (t_loss: 0.12665) (accu: 0.9842)
[epoch : 25] (l_loss: 0.00132) (t_loss: 0.12474) (accu: 0.9840)
[epoch : 26] (l_loss: 0.00095) (t_loss: 0.14430) (accu: 0.9815)
[epoch : 27] (l_loss: 0.00416) (t_loss: 0.13834) (accu: 0.9797)
[epoch : 28] (l_loss: 0.00298) (t_loss: 0.15149) (accu: 0.9822)
[epoch : 29] (l_loss: 0.00227) (t_loss: 0.14732) (accu: 0.9808)
[epoch : 30] (l_loss: 0.00452) (t_loss: 0.15497) (accu: 0.9815)
[epoch : 31] (l_loss: 0.00221) (t_loss: 0.15007) (accu: 0.9817)
[epoch : 32] (l_loss: 0.00201) (t_loss: 0.17153) (accu: 0.9795)
[epoch : 33] (l_loss: 0.00217) (t_loss: 0.15595) (accu: 0.9814)
[epoch : 34] (l_loss: 0.00102) (t_loss: 0.16237) (accu: 0.9806)
[epoch : 35] (l_loss: 0.00380) (t_loss: 0.17447) (accu: 0.9819)
[epoch : 36] (l_loss: 0.00142) (t_loss: 0.15810) (accu: 0.9836)
[epoch : 37] (l_loss: 0.00110) (t_loss: 0.17074) (accu: 0.9828)
[epoch : 38] (l_loss: 0.00374) (t_loss: 0.15480) (accu: 0.9837)
[epoch : 39] (l_loss: 0.00126) (t_loss: 0.15391) (accu: 0.9831)
[epoch : 40] (l_loss: 0.00326) (t_loss: 0.17499) (accu: 0.9819)
[epoch : 41] (l_loss: 0.00175) (t_loss: 0.17479) (accu: 0.9821)
[epoch : 42] (l_loss: 0.00118) (t_loss: 0.19123) (accu: 0.9801)
[epoch : 43] (l_loss: 0.00342) (t_loss: 0.18116) (accu: 0.9816)
[epoch : 44] (l_loss: 0.00206) (t_loss: 0.17947) (accu: 0.9818)
[epoch : 45] (l_loss: 0.00052) (t_loss: 0.17649) (accu: 0.9822)
[epoch : 46] (l_loss: 0.00249) (t_loss: 0.18427) (accu: 0.9806)
[epoch : 47] (l_loss: 0.00420) (t_loss: 0.18342) (accu: 0.9818)
[epoch : 48] (l_loss: 0.00131) (t_loss: 0.19552) (accu: 0.9802)
[epoch : 49] (l_loss: 0.00254) (t_loss: 0.17975) (accu: 0.9824)
[epoch : 50] (l_loss: 0.00074) (t_loss: 0.18325) (accu: 0.9823)
Finish! (Best accu: 0.9844) (Time taken(sec) : 570.06) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (35982 | 230218)         13.52
fc1.weight   :      235200 (31568 | 203632)         13.42
fc2.weight   :        30000 (4027 | 25973)          13.42
fcout.weight :          1000 (387 | 613)            38.70
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (10/22), Remaining weight : 13.52 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.25974) (accu: 0.1822)
[epoch : 1] (l_loss: 0.17716) (t_loss: 0.07347) (accu: 0.9764)
[epoch : 2] (l_loss: 0.04090) (t_loss: 0.05848) (accu: 0.9813)
[epoch : 3] (l_loss: 0.02104) (t_loss: 0.06178) (accu: 0.9821)
[epoch : 4] (l_loss: 0.01279) (t_loss: 0.06279) (accu: 0.9835)
[epoch : 5] (l_loss: 0.00781) (t_loss: 0.07411) (accu: 0.9810)
[epoch : 6] (l_loss: 0.00662) (t_loss: 0.08110) (accu: 0.9801)
[epoch : 7] (l_loss: 0.00346) (t_loss: 0.08331) (accu: 0.9797)
[epoch : 8] (l_loss: 0.00347) (t_loss: 0.09401) (accu: 0.9805)
[epoch : 9] (l_loss: 0.00656) (t_loss: 0.09118) (accu: 0.9816)
[epoch : 10] (l_loss: 0.00241) (t_loss: 0.09314) (accu: 0.9825)
[epoch : 11] (l_loss: 0.00265) (t_loss: 0.11207) (accu: 0.9788)
[epoch : 12] (l_loss: 0.00470) (t_loss: 0.08753) (accu: 0.9828)
[epoch : 13] (l_loss: 0.00121) (t_loss: 0.09464) (accu: 0.9823)
[epoch : 14] (l_loss: 0.00050) (t_loss: 0.09401) (accu: 0.9830)
[epoch : 15] (l_loss: 0.00692) (t_loss: 0.09508) (accu: 0.9815)
[epoch : 16] (l_loss: 0.00359) (t_loss: 0.09548) (accu: 0.9837)
[epoch : 17] (l_loss: 0.00051) (t_loss: 0.09103) (accu: 0.9844)
[epoch : 18] (l_loss: 0.00010) (t_loss: 0.09340) (accu: 0.9843)
[epoch : 19] (l_loss: 0.00005) (t_loss: 0.09401) (accu: 0.9849)
[epoch : 20] (l_loss: 0.00003) (t_loss: 0.09588) (accu: 0.9848)
[epoch : 21] (l_loss: 0.00002) (t_loss: 0.09848) (accu: 0.9844)
[epoch : 22] (l_loss: 0.00956) (t_loss: 0.12799) (accu: 0.9809)
[epoch : 23] (l_loss: 0.00292) (t_loss: 0.11382) (accu: 0.9818)
[epoch : 24] (l_loss: 0.00028) (t_loss: 0.10783) (accu: 0.9831)
[epoch : 25] (l_loss: 0.00008) (t_loss: 0.10893) (accu: 0.9834)
[epoch : 26] (l_loss: 0.00004) (t_loss: 0.10979) (accu: 0.9836)
[epoch : 27] (l_loss: 0.00003) (t_loss: 0.11147) (accu: 0.9836)
[epoch : 28] (l_loss: 0.00002) (t_loss: 0.11451) (accu: 0.9834)
[epoch : 29] (l_loss: 0.00002) (t_loss: 0.11552) (accu: 0.9834)
[epoch : 30] (l_loss: 0.00665) (t_loss: 0.15453) (accu: 0.9781)
[epoch : 31] (l_loss: 0.00616) (t_loss: 0.13053) (accu: 0.9828)
[epoch : 32] (l_loss: 0.00091) (t_loss: 0.14030) (accu: 0.9812)
[epoch : 33] (l_loss: 0.00304) (t_loss: 0.13472) (accu: 0.9817)
[epoch : 34] (l_loss: 0.00277) (t_loss: 0.13072) (accu: 0.9816)
[epoch : 35] (l_loss: 0.00143) (t_loss: 0.13846) (accu: 0.9804)
[epoch : 36] (l_loss: 0.00110) (t_loss: 0.14321) (accu: 0.9804)
[epoch : 37] (l_loss: 0.00379) (t_loss: 0.14700) (accu: 0.9804)
[epoch : 38] (l_loss: 0.00188) (t_loss: 0.14716) (accu: 0.9819)
[epoch : 39] (l_loss: 0.00126) (t_loss: 0.15596) (accu: 0.9819)
[epoch : 40] (l_loss: 0.00151) (t_loss: 0.15106) (accu: 0.9820)
[epoch : 41] (l_loss: 0.00389) (t_loss: 0.15394) (accu: 0.9816)
[epoch : 42] (l_loss: 0.00273) (t_loss: 0.14112) (accu: 0.9815)
[epoch : 43] (l_loss: 0.00050) (t_loss: 0.14781) (accu: 0.9821)
[epoch : 44] (l_loss: 0.00285) (t_loss: 0.13939) (accu: 0.9837)
[epoch : 45] (l_loss: 0.00234) (t_loss: 0.14507) (accu: 0.9833)
[epoch : 46] (l_loss: 0.00101) (t_loss: 0.14205) (accu: 0.9825)
[epoch : 47] (l_loss: 0.00018) (t_loss: 0.15273) (accu: 0.9819)
[epoch : 48] (l_loss: 0.00004) (t_loss: 0.15125) (accu: 0.9834)
[epoch : 49] (l_loss: 0.00001) (t_loss: 0.14780) (accu: 0.9830)
[epoch : 50] (l_loss: 0.00000) (t_loss: 0.14750) (accu: 0.9831)
Finish! (Best accu: 0.9849) (Time taken(sec) : 592.66) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (28824 | 237376)         10.83
fc1.weight   :      235200 (25254 | 209946)         10.74
fc2.weight   :        30000 (3221 | 26779)          10.74
fcout.weight :          1000 (349 | 651)            34.90
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (11/22), Remaining weight : 10.83 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.08611) (accu: 0.1886)
[epoch : 1] (l_loss: 0.18050) (t_loss: 0.07022) (accu: 0.9782)
[epoch : 2] (l_loss: 0.03958) (t_loss: 0.06171) (accu: 0.9814)
[epoch : 3] (l_loss: 0.02023) (t_loss: 0.05680) (accu: 0.9819)
[epoch : 4] (l_loss: 0.01174) (t_loss: 0.06346) (accu: 0.9817)
[epoch : 5] (l_loss: 0.00729) (t_loss: 0.07105) (accu: 0.9818)
[epoch : 6] (l_loss: 0.00440) (t_loss: 0.07185) (accu: 0.9827)
[epoch : 7] (l_loss: 0.00445) (t_loss: 0.11458) (accu: 0.9755)
[epoch : 8] (l_loss: 0.00528) (t_loss: 0.07754) (accu: 0.9831)
[epoch : 9] (l_loss: 0.00105) (t_loss: 0.07847) (accu: 0.9843)
[epoch : 10] (l_loss: 0.00169) (t_loss: 0.10367) (accu: 0.9787)
[epoch : 11] (l_loss: 0.00614) (t_loss: 0.08926) (accu: 0.9818)
[epoch : 12] (l_loss: 0.00222) (t_loss: 0.10232) (accu: 0.9812)
[epoch : 13] (l_loss: 0.00176) (t_loss: 0.09842) (accu: 0.9831)
[epoch : 14] (l_loss: 0.00072) (t_loss: 0.09448) (accu: 0.9837)
[epoch : 15] (l_loss: 0.00365) (t_loss: 0.10736) (accu: 0.9823)
[epoch : 16] (l_loss: 0.00310) (t_loss: 0.09752) (accu: 0.9829)
[epoch : 17] (l_loss: 0.00120) (t_loss: 0.10791) (accu: 0.9823)
[epoch : 18] (l_loss: 0.00039) (t_loss: 0.09886) (accu: 0.9841)
[epoch : 19] (l_loss: 0.00006) (t_loss: 0.09903) (accu: 0.9845)
[epoch : 20] (l_loss: 0.00003) (t_loss: 0.10042) (accu: 0.9842)
[epoch : 21] (l_loss: 0.00002) (t_loss: 0.10202) (accu: 0.9843)
[epoch : 22] (l_loss: 0.00002) (t_loss: 0.10354) (accu: 0.9846)
[epoch : 23] (l_loss: 0.00001) (t_loss: 0.10611) (accu: 0.9842)
[epoch : 24] (l_loss: 0.00001) (t_loss: 0.10761) (accu: 0.9845)
[epoch : 25] (l_loss: 0.00791) (t_loss: 0.13018) (accu: 0.9818)
[epoch : 26] (l_loss: 0.00383) (t_loss: 0.11809) (accu: 0.9820)
[epoch : 27] (l_loss: 0.00117) (t_loss: 0.12837) (accu: 0.9807)
[epoch : 28] (l_loss: 0.00045) (t_loss: 0.12078) (accu: 0.9828)
[epoch : 29] (l_loss: 0.00006) (t_loss: 0.11962) (accu: 0.9832)
[epoch : 30] (l_loss: 0.00003) (t_loss: 0.12095) (accu: 0.9834)
[epoch : 31] (l_loss: 0.00002) (t_loss: 0.12140) (accu: 0.9836)
[epoch : 32] (l_loss: 0.00001) (t_loss: 0.12285) (accu: 0.9839)
[epoch : 33] (l_loss: 0.00001) (t_loss: 0.12417) (accu: 0.9837)
[epoch : 34] (l_loss: 0.00001) (t_loss: 0.12514) (accu: 0.9837)
[epoch : 35] (l_loss: 0.00736) (t_loss: 0.16340) (accu: 0.9796)
[epoch : 36] (l_loss: 0.00566) (t_loss: 0.13987) (accu: 0.9820)
[epoch : 37] (l_loss: 0.00121) (t_loss: 0.13407) (accu: 0.9830)
[epoch : 38] (l_loss: 0.00100) (t_loss: 0.15847) (accu: 0.9813)
[epoch : 39] (l_loss: 0.00190) (t_loss: 0.15929) (accu: 0.9805)
[epoch : 40] (l_loss: 0.00245) (t_loss: 0.15752) (accu: 0.9814)
[epoch : 41] (l_loss: 0.00067) (t_loss: 0.15417) (accu: 0.9806)
[epoch : 42] (l_loss: 0.00379) (t_loss: 0.16622) (accu: 0.9822)
[epoch : 43] (l_loss: 0.00289) (t_loss: 0.14676) (accu: 0.9828)
[epoch : 44] (l_loss: 0.00080) (t_loss: 0.15318) (accu: 0.9819)
[epoch : 45] (l_loss: 0.00019) (t_loss: 0.15150) (accu: 0.9818)
[epoch : 46] (l_loss: 0.00005) (t_loss: 0.14572) (accu: 0.9825)
[epoch : 47] (l_loss: 0.00001) (t_loss: 0.14499) (accu: 0.9829)
[epoch : 48] (l_loss: 0.00001) (t_loss: 0.14495) (accu: 0.9829)
[epoch : 49] (l_loss: 0.00000) (t_loss: 0.14501) (accu: 0.9831)
[epoch : 50] (l_loss: 0.00000) (t_loss: 0.14552) (accu: 0.9832)
Finish! (Best accu: 0.9846) (Time taken(sec) : 599.09) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (23095 | 243105)          8.68
fc1.weight   :      235200 (20204 | 214996)          8.59
fc2.weight   :        30000 (2577 | 27423)           8.59
fcout.weight :          1000 (314 | 686)            31.40
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (12/22), Remaining weight : 8.68 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.09147) (accu: 0.2720)
[epoch : 1] (l_loss: 0.18775) (t_loss: 0.07400) (accu: 0.9770)
[epoch : 2] (l_loss: 0.04065) (t_loss: 0.06338) (accu: 0.9791)
[epoch : 3] (l_loss: 0.02006) (t_loss: 0.05706) (accu: 0.9822)
[epoch : 4] (l_loss: 0.01165) (t_loss: 0.06055) (accu: 0.9835)
[epoch : 5] (l_loss: 0.00754) (t_loss: 0.06355) (accu: 0.9832)
[epoch : 6] (l_loss: 0.00482) (t_loss: 0.06842) (accu: 0.9826)
[epoch : 7] (l_loss: 0.00341) (t_loss: 0.07446) (accu: 0.9822)
[epoch : 8] (l_loss: 0.00346) (t_loss: 0.07910) (accu: 0.9821)
[epoch : 9] (l_loss: 0.00262) (t_loss: 0.08939) (accu: 0.9805)
[epoch : 10] (l_loss: 0.00170) (t_loss: 0.08610) (accu: 0.9824)
[epoch : 11] (l_loss: 0.00213) (t_loss: 0.08138) (accu: 0.9836)
[epoch : 12] (l_loss: 0.00320) (t_loss: 0.10420) (accu: 0.9800)
[epoch : 13] (l_loss: 0.00145) (t_loss: 0.10048) (accu: 0.9815)
[epoch : 14] (l_loss: 0.00119) (t_loss: 0.09493) (accu: 0.9824)
[epoch : 15] (l_loss: 0.00161) (t_loss: 0.10222) (accu: 0.9816)
[epoch : 16] (l_loss: 0.00213) (t_loss: 0.11379) (accu: 0.9798)
[epoch : 17] (l_loss: 0.00275) (t_loss: 0.11297) (accu: 0.9810)
[epoch : 18] (l_loss: 0.00083) (t_loss: 0.10446) (accu: 0.9840)
[epoch : 19] (l_loss: 0.00054) (t_loss: 0.10965) (accu: 0.9839)
[epoch : 20] (l_loss: 0.00270) (t_loss: 0.11748) (accu: 0.9833)
[epoch : 21] (l_loss: 0.00246) (t_loss: 0.11587) (accu: 0.9829)
[epoch : 22] (l_loss: 0.00030) (t_loss: 0.11077) (accu: 0.9831)
[epoch : 23] (l_loss: 0.00006) (t_loss: 0.10901) (accu: 0.9845)
[epoch : 24] (l_loss: 0.00003) (t_loss: 0.10931) (accu: 0.9848)
[epoch : 25] (l_loss: 0.00002) (t_loss: 0.11022) (accu: 0.9849)
[epoch : 26] (l_loss: 0.00001) (t_loss: 0.11130) (accu: 0.9850)
[epoch : 27] (l_loss: 0.00001) (t_loss: 0.11268) (accu: 0.9848)
[epoch : 28] (l_loss: 0.00142) (t_loss: 0.18419) (accu: 0.9781)
[epoch : 29] (l_loss: 0.00669) (t_loss: 0.14034) (accu: 0.9817)
[epoch : 30] (l_loss: 0.00116) (t_loss: 0.13115) (accu: 0.9830)
[epoch : 31] (l_loss: 0.00084) (t_loss: 0.12629) (accu: 0.9836)
[epoch : 32] (l_loss: 0.00091) (t_loss: 0.15104) (accu: 0.9801)
[epoch : 33] (l_loss: 0.00204) (t_loss: 0.14155) (accu: 0.9829)
[epoch : 34] (l_loss: 0.00210) (t_loss: 0.14486) (accu: 0.9836)
[epoch : 35] (l_loss: 0.00119) (t_loss: 0.14517) (accu: 0.9825)
[epoch : 36] (l_loss: 0.00034) (t_loss: 0.14496) (accu: 0.9831)
[epoch : 37] (l_loss: 0.00200) (t_loss: 0.14962) (accu: 0.9827)
[epoch : 38] (l_loss: 0.00166) (t_loss: 0.15992) (accu: 0.9816)
[epoch : 39] (l_loss: 0.00209) (t_loss: 0.15126) (accu: 0.9812)
[epoch : 40] (l_loss: 0.00064) (t_loss: 0.14429) (accu: 0.9836)
[epoch : 41] (l_loss: 0.00027) (t_loss: 0.15017) (accu: 0.9827)
[epoch : 42] (l_loss: 0.00108) (t_loss: 0.17558) (accu: 0.9793)
[epoch : 43] (l_loss: 0.00357) (t_loss: 0.15445) (accu: 0.9829)
[epoch : 44] (l_loss: 0.00033) (t_loss: 0.15031) (accu: 0.9833)
[epoch : 45] (l_loss: 0.00021) (t_loss: 0.14761) (accu: 0.9833)
[epoch : 46] (l_loss: 0.00081) (t_loss: 0.16983) (accu: 0.9806)
[epoch : 47] (l_loss: 0.00413) (t_loss: 0.15962) (accu: 0.9813)
[epoch : 48] (l_loss: 0.00062) (t_loss: 0.17185) (accu: 0.9817)
[epoch : 49] (l_loss: 0.00011) (t_loss: 0.16277) (accu: 0.9824)
[epoch : 50] (l_loss: 0.00015) (t_loss: 0.16376) (accu: 0.9819)
Finish! (Best accu: 0.9850) (Time taken(sec) : 583.10) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (18507 | 247693)          6.95
fc1.weight   :      235200 (16163 | 219037)          6.87
fc2.weight   :        30000 (2062 | 27938)           6.87
fcout.weight :          1000 (282 | 718)            28.20
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (13/22), Remaining weight : 6.95 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.15104) (accu: 0.2676)
[epoch : 1] (l_loss: 0.21345) (t_loss: 0.07434) (accu: 0.9777)
[epoch : 2] (l_loss: 0.04519) (t_loss: 0.05836) (accu: 0.9803)
[epoch : 3] (l_loss: 0.02404) (t_loss: 0.05835) (accu: 0.9830)
[epoch : 4] (l_loss: 0.01402) (t_loss: 0.05737) (accu: 0.9833)
[epoch : 5] (l_loss: 0.00826) (t_loss: 0.06682) (accu: 0.9806)
[epoch : 6] (l_loss: 0.00498) (t_loss: 0.06331) (accu: 0.9825)
[epoch : 7] (l_loss: 0.00358) (t_loss: 0.08185) (accu: 0.9801)
[epoch : 8] (l_loss: 0.00278) (t_loss: 0.08079) (accu: 0.9816)
[epoch : 9] (l_loss: 0.00246) (t_loss: 0.08308) (accu: 0.9821)
[epoch : 10] (l_loss: 0.00330) (t_loss: 0.07936) (accu: 0.9831)
[epoch : 11] (l_loss: 0.00053) (t_loss: 0.08692) (accu: 0.9822)
[epoch : 12] (l_loss: 0.00031) (t_loss: 0.08604) (accu: 0.9829)
[epoch : 13] (l_loss: 0.00017) (t_loss: 0.08799) (accu: 0.9837)
[epoch : 14] (l_loss: 0.00507) (t_loss: 0.11220) (accu: 0.9796)
[epoch : 15] (l_loss: 0.00319) (t_loss: 0.10690) (accu: 0.9816)
[epoch : 16] (l_loss: 0.00050) (t_loss: 0.09812) (accu: 0.9831)
[epoch : 17] (l_loss: 0.00017) (t_loss: 0.09752) (accu: 0.9836)
[epoch : 18] (l_loss: 0.00009) (t_loss: 0.09815) (accu: 0.9834)
[epoch : 19] (l_loss: 0.00006) (t_loss: 0.09993) (accu: 0.9835)
[epoch : 20] (l_loss: 0.00005) (t_loss: 0.10320) (accu: 0.9835)
[epoch : 21] (l_loss: 0.00003) (t_loss: 0.10369) (accu: 0.9839)
[epoch : 22] (l_loss: 0.00140) (t_loss: 0.16646) (accu: 0.9742)
[epoch : 23] (l_loss: 0.00772) (t_loss: 0.11957) (accu: 0.9830)
[epoch : 24] (l_loss: 0.00030) (t_loss: 0.11579) (accu: 0.9836)
[epoch : 25] (l_loss: 0.00008) (t_loss: 0.11463) (accu: 0.9840)
[epoch : 26] (l_loss: 0.00005) (t_loss: 0.11568) (accu: 0.9837)
[epoch : 27] (l_loss: 0.00004) (t_loss: 0.11675) (accu: 0.9836)
[epoch : 28] (l_loss: 0.00003) (t_loss: 0.11792) (accu: 0.9837)
[epoch : 29] (l_loss: 0.00002) (t_loss: 0.11977) (accu: 0.9837)
[epoch : 30] (l_loss: 0.00002) (t_loss: 0.12208) (accu: 0.9835)
[epoch : 31] (l_loss: 0.00001) (t_loss: 0.12366) (accu: 0.9832)
[epoch : 32] (l_loss: 0.00001) (t_loss: 0.12608) (accu: 0.9836)
[epoch : 33] (l_loss: 0.00002) (t_loss: 0.13805) (accu: 0.9828)
[epoch : 34] (l_loss: 0.00816) (t_loss: 0.14949) (accu: 0.9802)
[epoch : 35] (l_loss: 0.00094) (t_loss: 0.14280) (accu: 0.9822)
[epoch : 36] (l_loss: 0.00016) (t_loss: 0.13999) (accu: 0.9826)
[epoch : 37] (l_loss: 0.00003) (t_loss: 0.14045) (accu: 0.9832)
[epoch : 38] (l_loss: 0.00002) (t_loss: 0.14058) (accu: 0.9834)
[epoch : 39] (l_loss: 0.00002) (t_loss: 0.14125) (accu: 0.9834)
[epoch : 40] (l_loss: 0.00001) (t_loss: 0.14191) (accu: 0.9838)
[epoch : 41] (l_loss: 0.00001) (t_loss: 0.14255) (accu: 0.9838)
[epoch : 42] (l_loss: 0.00001) (t_loss: 0.14399) (accu: 0.9834)
[epoch : 43] (l_loss: 0.00001) (t_loss: 0.14522) (accu: 0.9835)
[epoch : 44] (l_loss: 0.00000) (t_loss: 0.14753) (accu: 0.9835)
[epoch : 45] (l_loss: 0.00000) (t_loss: 0.14813) (accu: 0.9835)
[epoch : 46] (l_loss: 0.00000) (t_loss: 0.15070) (accu: 0.9835)
[epoch : 47] (l_loss: 0.00000) (t_loss: 0.15397) (accu: 0.9833)
[epoch : 48] (l_loss: 0.00009) (t_loss: 0.19197) (accu: 0.9779)
[epoch : 49] (l_loss: 0.00889) (t_loss: 0.17339) (accu: 0.9804)
[epoch : 50] (l_loss: 0.00036) (t_loss: 0.16626) (accu: 0.9816)
Finish! (Best accu: 0.9840) (Time taken(sec) : 581.17) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (14833 | 251367)          5.57
fc1.weight   :      235200 (12930 | 222270)          5.50
fc2.weight   :        30000 (1649 | 28351)           5.50
fcout.weight :          1000 (254 | 746)            25.40
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (14/22), Remaining weight : 5.57 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.14542) (accu: 0.1960)
[epoch : 1] (l_loss: 0.23145) (t_loss: 0.07959) (accu: 0.9759)
[epoch : 2] (l_loss: 0.04762) (t_loss: 0.05913) (accu: 0.9808)
[epoch : 3] (l_loss: 0.02566) (t_loss: 0.05557) (accu: 0.9827)
[epoch : 4] (l_loss: 0.01524) (t_loss: 0.05472) (accu: 0.9829)
[epoch : 5] (l_loss: 0.00945) (t_loss: 0.06113) (accu: 0.9823)
[epoch : 6] (l_loss: 0.00557) (t_loss: 0.06782) (accu: 0.9812)
[epoch : 7] (l_loss: 0.00364) (t_loss: 0.06823) (accu: 0.9817)
[epoch : 8] (l_loss: 0.00254) (t_loss: 0.08494) (accu: 0.9797)
[epoch : 9] (l_loss: 0.00266) (t_loss: 0.07827) (accu: 0.9829)
[epoch : 10] (l_loss: 0.00155) (t_loss: 0.07811) (accu: 0.9830)
[epoch : 11] (l_loss: 0.00054) (t_loss: 0.08034) (accu: 0.9828)
[epoch : 12] (l_loss: 0.00029) (t_loss: 0.08357) (accu: 0.9836)
[epoch : 13] (l_loss: 0.00439) (t_loss: 0.10583) (accu: 0.9799)
[epoch : 14] (l_loss: 0.00214) (t_loss: 0.09412) (accu: 0.9817)
[epoch : 15] (l_loss: 0.00099) (t_loss: 0.10818) (accu: 0.9794)
[epoch : 16] (l_loss: 0.00043) (t_loss: 0.09561) (accu: 0.9825)
[epoch : 17] (l_loss: 0.00015) (t_loss: 0.10125) (accu: 0.9824)
[epoch : 18] (l_loss: 0.00340) (t_loss: 0.11120) (accu: 0.9820)
[epoch : 19] (l_loss: 0.00180) (t_loss: 0.10177) (accu: 0.9827)
[epoch : 20] (l_loss: 0.00032) (t_loss: 0.10314) (accu: 0.9837)
[epoch : 21] (l_loss: 0.00010) (t_loss: 0.10324) (accu: 0.9839)
[epoch : 22] (l_loss: 0.00006) (t_loss: 0.10426) (accu: 0.9837)
[epoch : 23] (l_loss: 0.00005) (t_loss: 0.10554) (accu: 0.9841)
[epoch : 24] (l_loss: 0.00004) (t_loss: 0.10852) (accu: 0.9834)
[epoch : 25] (l_loss: 0.00003) (t_loss: 0.11031) (accu: 0.9837)
[epoch : 26] (l_loss: 0.00002) (t_loss: 0.11238) (accu: 0.9837)
[epoch : 27] (l_loss: 0.00610) (t_loss: 0.12081) (accu: 0.9828)
[epoch : 28] (l_loss: 0.00156) (t_loss: 0.11711) (accu: 0.9825)
[epoch : 29] (l_loss: 0.00031) (t_loss: 0.11717) (accu: 0.9828)
[epoch : 30] (l_loss: 0.00007) (t_loss: 0.11866) (accu: 0.9831)
[epoch : 31] (l_loss: 0.00004) (t_loss: 0.11860) (accu: 0.9833)
[epoch : 32] (l_loss: 0.00003) (t_loss: 0.11895) (accu: 0.9835)
[epoch : 33] (l_loss: 0.00002) (t_loss: 0.11986) (accu: 0.9834)
[epoch : 34] (l_loss: 0.00002) (t_loss: 0.12168) (accu: 0.9833)
[epoch : 35] (l_loss: 0.00001) (t_loss: 0.12256) (accu: 0.9840)
[epoch : 36] (l_loss: 0.00001) (t_loss: 0.12438) (accu: 0.9833)
[epoch : 37] (l_loss: 0.00001) (t_loss: 0.12625) (accu: 0.9840)
[epoch : 38] (l_loss: 0.00001) (t_loss: 0.12913) (accu: 0.9834)
[epoch : 39] (l_loss: 0.00628) (t_loss: 0.15347) (accu: 0.9808)
[epoch : 40] (l_loss: 0.00185) (t_loss: 0.14433) (accu: 0.9825)
[epoch : 41] (l_loss: 0.00041) (t_loss: 0.14411) (accu: 0.9812)
[epoch : 42] (l_loss: 0.00005) (t_loss: 0.14249) (accu: 0.9818)
[epoch : 43] (l_loss: 0.00002) (t_loss: 0.14213) (accu: 0.9819)
[epoch : 44] (l_loss: 0.00002) (t_loss: 0.14206) (accu: 0.9823)
[epoch : 45] (l_loss: 0.00001) (t_loss: 0.14194) (accu: 0.9820)
[epoch : 46] (l_loss: 0.00001) (t_loss: 0.14215) (accu: 0.9825)
[epoch : 47] (l_loss: 0.00001) (t_loss: 0.14307) (accu: 0.9826)
[epoch : 48] (l_loss: 0.00001) (t_loss: 0.14368) (accu: 0.9826)
[epoch : 49] (l_loss: 0.00000) (t_loss: 0.14500) (accu: 0.9831)
[epoch : 50] (l_loss: 0.00520) (t_loss: 0.18555) (accu: 0.9791)
Finish! (Best accu: 0.9841) (Time taken(sec) : 591.93) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :      266200 (11892 | 254308)          4.47
fc1.weight   :      235200 (10344 | 224856)          4.40
fc2.weight   :        30000 (1319 | 28681)           4.40
fcout.weight :          1000 (229 | 771)            22.90
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (15/22), Remaining weight : 4.47 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.25181) (accu: 0.1903)
[epoch : 1] (l_loss: 0.26872) (t_loss: 0.08931) (accu: 0.9723)
[epoch : 2] (l_loss: 0.05464) (t_loss: 0.06235) (accu: 0.9799)
[epoch : 3] (l_loss: 0.03018) (t_loss: 0.05980) (accu: 0.9810)
[epoch : 4] (l_loss: 0.01844) (t_loss: 0.05948) (accu: 0.9818)
[epoch : 5] (l_loss: 0.01189) (t_loss: 0.06302) (accu: 0.9815)
[epoch : 6] (l_loss: 0.00791) (t_loss: 0.06559) (accu: 0.9816)
[epoch : 7] (l_loss: 0.00490) (t_loss: 0.07060) (accu: 0.9817)
[epoch : 8] (l_loss: 0.00323) (t_loss: 0.07312) (accu: 0.9823)
[epoch : 9] (l_loss: 0.00298) (t_loss: 0.07885) (accu: 0.9824)
[epoch : 10] (l_loss: 0.00194) (t_loss: 0.08598) (accu: 0.9811)
[epoch : 11] (l_loss: 0.00119) (t_loss: 0.08561) (accu: 0.9825)
[epoch : 12] (l_loss: 0.00101) (t_loss: 0.08933) (accu: 0.9831)
[epoch : 13] (l_loss: 0.00308) (t_loss: 0.11112) (accu: 0.9794)
[epoch : 14] (l_loss: 0.00168) (t_loss: 0.10200) (accu: 0.9819)
[epoch : 15] (l_loss: 0.00031) (t_loss: 0.09841) (accu: 0.9815)
[epoch : 16] (l_loss: 0.00018) (t_loss: 0.09941) (accu: 0.9818)
[epoch : 17] (l_loss: 0.00013) (t_loss: 0.10121) (accu: 0.9824)
[epoch : 18] (l_loss: 0.00010) (t_loss: 0.10483) (accu: 0.9827)
[epoch : 19] (l_loss: 0.00008) (t_loss: 0.10720) (accu: 0.9826)
[epoch : 20] (l_loss: 0.00496) (t_loss: 0.11542) (accu: 0.9811)
[epoch : 21] (l_loss: 0.00143) (t_loss: 0.11577) (accu: 0.9816)
[epoch : 22] (l_loss: 0.00086) (t_loss: 0.11550) (accu: 0.9817)
[epoch : 23] (l_loss: 0.00014) (t_loss: 0.11674) (accu: 0.9820)
[epoch : 24] (l_loss: 0.00007) (t_loss: 0.11717) (accu: 0.9820)
[epoch : 25] (l_loss: 0.00005) (t_loss: 0.11802) (accu: 0.9819)
[epoch : 26] (l_loss: 0.00004) (t_loss: 0.11896) (accu: 0.9824)
[epoch : 27] (l_loss: 0.00003) (t_loss: 0.12089) (accu: 0.9826)
[epoch : 28] (l_loss: 0.00002) (t_loss: 0.12433) (accu: 0.9824)
[epoch : 29] (l_loss: 0.00002) (t_loss: 0.12605) (accu: 0.9830)
[epoch : 30] (l_loss: 0.00220) (t_loss: 0.16741) (accu: 0.9789)
[epoch : 31] (l_loss: 0.00322) (t_loss: 0.14194) (accu: 0.9806)
[epoch : 32] (l_loss: 0.00024) (t_loss: 0.13810) (accu: 0.9819)
[epoch : 33] (l_loss: 0.00015) (t_loss: 0.13678) (accu: 0.9814)
[epoch : 34] (l_loss: 0.00003) (t_loss: 0.13719) (accu: 0.9814)
[epoch : 35] (l_loss: 0.00002) (t_loss: 0.13761) (accu: 0.9819)
[epoch : 36] (l_loss: 0.00002) (t_loss: 0.13821) (accu: 0.9819)
[epoch : 37] (l_loss: 0.00001) (t_loss: 0.13913) (accu: 0.9820)
[epoch : 38] (l_loss: 0.00001) (t_loss: 0.14004) (accu: 0.9820)
[epoch : 39] (l_loss: 0.00001) (t_loss: 0.14189) (accu: 0.9823)
[epoch : 40] (l_loss: 0.00001) (t_loss: 0.14557) (accu: 0.9817)
[epoch : 41] (l_loss: 0.00001) (t_loss: 0.14422) (accu: 0.9819)
[epoch : 42] (l_loss: 0.00423) (t_loss: 0.20318) (accu: 0.9771)
[epoch : 43] (l_loss: 0.00161) (t_loss: 0.17538) (accu: 0.9793)
[epoch : 44] (l_loss: 0.00044) (t_loss: 0.16236) (accu: 0.9806)
[epoch : 45] (l_loss: 0.00012) (t_loss: 0.16229) (accu: 0.9806)
[epoch : 46] (l_loss: 0.00017) (t_loss: 0.16229) (accu: 0.9811)
[epoch : 47] (l_loss: 0.00149) (t_loss: 0.19482) (accu: 0.9785)
[epoch : 48] (l_loss: 0.00200) (t_loss: 0.18226) (accu: 0.9788)
[epoch : 49] (l_loss: 0.00142) (t_loss: 0.17977) (accu: 0.9805)
[epoch : 50] (l_loss: 0.00031) (t_loss: 0.17172) (accu: 0.9802)
Finish! (Best accu: 0.9831) (Time taken(sec) : 574.41) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (9537 | 256663)          3.58
fc1.weight   :       235200 (8275 | 226925)          3.52
fc2.weight   :        30000 (1056 | 28944)           3.52
fcout.weight :          1000 (206 | 794)            20.60
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (16/22), Remaining weight : 3.58 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.26809) (accu: 0.1709)
[epoch : 1] (l_loss: 0.30292) (t_loss: 0.09795) (accu: 0.9714)
[epoch : 2] (l_loss: 0.06218) (t_loss: 0.06962) (accu: 0.9781)
[epoch : 3] (l_loss: 0.03596) (t_loss: 0.06047) (accu: 0.9808)
[epoch : 4] (l_loss: 0.02319) (t_loss: 0.05899) (accu: 0.9806)
[epoch : 5] (l_loss: 0.01569) (t_loss: 0.05936) (accu: 0.9809)
[epoch : 6] (l_loss: 0.01113) (t_loss: 0.06477) (accu: 0.9806)
[epoch : 7] (l_loss: 0.00729) (t_loss: 0.06870) (accu: 0.9799)
[epoch : 8] (l_loss: 0.00492) (t_loss: 0.06981) (accu: 0.9819)
[epoch : 9] (l_loss: 0.00348) (t_loss: 0.07914) (accu: 0.9786)
[epoch : 10] (l_loss: 0.00250) (t_loss: 0.07985) (accu: 0.9806)
[epoch : 11] (l_loss: 0.00261) (t_loss: 0.08632) (accu: 0.9802)
[epoch : 12] (l_loss: 0.00123) (t_loss: 0.09333) (accu: 0.9794)
[epoch : 13] (l_loss: 0.00077) (t_loss: 0.09025) (accu: 0.9815)
[epoch : 14] (l_loss: 0.00221) (t_loss: 0.10763) (accu: 0.9788)
[epoch : 15] (l_loss: 0.00175) (t_loss: 0.09792) (accu: 0.9818)
[epoch : 16] (l_loss: 0.00064) (t_loss: 0.09952) (accu: 0.9807)
[epoch : 17] (l_loss: 0.00025) (t_loss: 0.10109) (accu: 0.9819)
[epoch : 18] (l_loss: 0.00017) (t_loss: 0.10377) (accu: 0.9817)
[epoch : 19] (l_loss: 0.00015) (t_loss: 0.10854) (accu: 0.9816)
[epoch : 20] (l_loss: 0.00253) (t_loss: 0.12622) (accu: 0.9790)
[epoch : 21] (l_loss: 0.00265) (t_loss: 0.11503) (accu: 0.9799)
[epoch : 22] (l_loss: 0.00043) (t_loss: 0.11260) (accu: 0.9806)
[epoch : 23] (l_loss: 0.00014) (t_loss: 0.11302) (accu: 0.9809)
[epoch : 24] (l_loss: 0.00009) (t_loss: 0.11455) (accu: 0.9814)
[epoch : 25] (l_loss: 0.00007) (t_loss: 0.11500) (accu: 0.9817)
[epoch : 26] (l_loss: 0.00006) (t_loss: 0.11845) (accu: 0.9812)
[epoch : 27] (l_loss: 0.00005) (t_loss: 0.12118) (accu: 0.9814)
[epoch : 28] (l_loss: 0.00397) (t_loss: 0.12661) (accu: 0.9794)
[epoch : 29] (l_loss: 0.00137) (t_loss: 0.12968) (accu: 0.9820)
[epoch : 30] (l_loss: 0.00039) (t_loss: 0.13010) (accu: 0.9816)
[epoch : 31] (l_loss: 0.00020) (t_loss: 0.12816) (accu: 0.9819)
[epoch : 32] (l_loss: 0.00006) (t_loss: 0.12831) (accu: 0.9816)
[epoch : 33] (l_loss: 0.00004) (t_loss: 0.12951) (accu: 0.9820)
[epoch : 34] (l_loss: 0.00003) (t_loss: 0.12963) (accu: 0.9827)
[epoch : 35] (l_loss: 0.00003) (t_loss: 0.13144) (accu: 0.9827)
[epoch : 36] (l_loss: 0.00002) (t_loss: 0.13316) (accu: 0.9824)
[epoch : 37] (l_loss: 0.00002) (t_loss: 0.13596) (accu: 0.9822)
[epoch : 38] (l_loss: 0.00425) (t_loss: 0.15382) (accu: 0.9813)
[epoch : 39] (l_loss: 0.00068) (t_loss: 0.14195) (accu: 0.9811)
[epoch : 40] (l_loss: 0.00015) (t_loss: 0.14341) (accu: 0.9817)
[epoch : 41] (l_loss: 0.00005) (t_loss: 0.14098) (accu: 0.9809)
[epoch : 42] (l_loss: 0.00003) (t_loss: 0.14222) (accu: 0.9810)
[epoch : 43] (l_loss: 0.00002) (t_loss: 0.14238) (accu: 0.9812)
[epoch : 44] (l_loss: 0.00002) (t_loss: 0.14423) (accu: 0.9815)
[epoch : 45] (l_loss: 0.00001) (t_loss: 0.14438) (accu: 0.9816)
[epoch : 46] (l_loss: 0.00001) (t_loss: 0.14685) (accu: 0.9818)
[epoch : 47] (l_loss: 0.00001) (t_loss: 0.14841) (accu: 0.9817)
[epoch : 48] (l_loss: 0.00273) (t_loss: 0.14900) (accu: 0.9809)
[epoch : 49] (l_loss: 0.00248) (t_loss: 0.15563) (accu: 0.9809)
[epoch : 50] (l_loss: 0.00046) (t_loss: 0.14969) (accu: 0.9816)
Finish! (Best accu: 0.9827) (Time taken(sec) : 587.70) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (7649 | 258551)          2.87
fc1.weight   :       235200 (6620 | 228580)          2.81
fc2.weight   :        30000 (844 | 29156)            2.81
fcout.weight :          1000 (185 | 815)            18.50
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (17/22), Remaining weight : 2.87 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.33478) (accu: 0.1336)
[epoch : 1] (l_loss: 0.33982) (t_loss: 0.10637) (accu: 0.9693)
[epoch : 2] (l_loss: 0.07078) (t_loss: 0.07550) (accu: 0.9771)
[epoch : 3] (l_loss: 0.04273) (t_loss: 0.06532) (accu: 0.9798)
[epoch : 4] (l_loss: 0.02857) (t_loss: 0.06081) (accu: 0.9804)
[epoch : 5] (l_loss: 0.01982) (t_loss: 0.06463) (accu: 0.9791)
[epoch : 6] (l_loss: 0.01432) (t_loss: 0.06650) (accu: 0.9802)
[epoch : 7] (l_loss: 0.01007) (t_loss: 0.06858) (accu: 0.9804)
[epoch : 8] (l_loss: 0.00727) (t_loss: 0.07188) (accu: 0.9803)
[epoch : 9] (l_loss: 0.00527) (t_loss: 0.07502) (accu: 0.9805)
[epoch : 10] (l_loss: 0.00395) (t_loss: 0.08270) (accu: 0.9793)
[epoch : 11] (l_loss: 0.00270) (t_loss: 0.08087) (accu: 0.9809)
[epoch : 12] (l_loss: 0.00229) (t_loss: 0.08625) (accu: 0.9799)
[epoch : 13] (l_loss: 0.00174) (t_loss: 0.09470) (accu: 0.9802)
[epoch : 14] (l_loss: 0.00204) (t_loss: 0.09691) (accu: 0.9805)
[epoch : 15] (l_loss: 0.00128) (t_loss: 0.10660) (accu: 0.9803)
[epoch : 16] (l_loss: 0.00164) (t_loss: 0.10367) (accu: 0.9807)
[epoch : 17] (l_loss: 0.00068) (t_loss: 0.10607) (accu: 0.9807)
[epoch : 18] (l_loss: 0.00034) (t_loss: 0.10847) (accu: 0.9809)
[epoch : 19] (l_loss: 0.00034) (t_loss: 0.11829) (accu: 0.9784)
[epoch : 20] (l_loss: 0.00300) (t_loss: 0.11520) (accu: 0.9788)
[epoch : 21] (l_loss: 0.00091) (t_loss: 0.12270) (accu: 0.9788)
[epoch : 22] (l_loss: 0.00029) (t_loss: 0.11921) (accu: 0.9799)
[epoch : 23] (l_loss: 0.00017) (t_loss: 0.12094) (accu: 0.9790)
[epoch : 24] (l_loss: 0.00012) (t_loss: 0.12152) (accu: 0.9798)
[epoch : 25] (l_loss: 0.00024) (t_loss: 0.14397) (accu: 0.9785)
[epoch : 26] (l_loss: 0.00295) (t_loss: 0.12862) (accu: 0.9797)
[epoch : 27] (l_loss: 0.00040) (t_loss: 0.12805) (accu: 0.9800)
[epoch : 28] (l_loss: 0.00020) (t_loss: 0.12728) (accu: 0.9798)
[epoch : 29] (l_loss: 0.00008) (t_loss: 0.12873) (accu: 0.9799)
[epoch : 30] (l_loss: 0.00006) (t_loss: 0.13150) (accu: 0.9798)
[epoch : 31] (l_loss: 0.00005) (t_loss: 0.13251) (accu: 0.9796)
[epoch : 32] (l_loss: 0.00026) (t_loss: 0.16912) (accu: 0.9775)
[epoch : 33] (l_loss: 0.00342) (t_loss: 0.14766) (accu: 0.9791)
[epoch : 34] (l_loss: 0.00033) (t_loss: 0.15148) (accu: 0.9787)
[epoch : 35] (l_loss: 0.00009) (t_loss: 0.14812) (accu: 0.9785)
[epoch : 36] (l_loss: 0.00016) (t_loss: 0.14982) (accu: 0.9792)
[epoch : 37] (l_loss: 0.00005) (t_loss: 0.14878) (accu: 0.9790)
[epoch : 38] (l_loss: 0.00003) (t_loss: 0.14868) (accu: 0.9793)
[epoch : 39] (l_loss: 0.00003) (t_loss: 0.14940) (accu: 0.9795)
[epoch : 40] (l_loss: 0.00002) (t_loss: 0.15338) (accu: 0.9792)
[epoch : 41] (l_loss: 0.00345) (t_loss: 0.17664) (accu: 0.9782)
[epoch : 42] (l_loss: 0.00051) (t_loss: 0.17009) (accu: 0.9784)
[epoch : 43] (l_loss: 0.00011) (t_loss: 0.16862) (accu: 0.9792)
[epoch : 44] (l_loss: 0.00013) (t_loss: 0.16523) (accu: 0.9790)
[epoch : 45] (l_loss: 0.00004) (t_loss: 0.16497) (accu: 0.9790)
[epoch : 46] (l_loss: 0.00002) (t_loss: 0.16562) (accu: 0.9789)
[epoch : 47] (l_loss: 0.00002) (t_loss: 0.16576) (accu: 0.9791)
[epoch : 48] (l_loss: 0.00002) (t_loss: 0.16623) (accu: 0.9792)
[epoch : 49] (l_loss: 0.00001) (t_loss: 0.16908) (accu: 0.9791)
[epoch : 50] (l_loss: 0.00001) (t_loss: 0.16873) (accu: 0.9793)
Finish! (Best accu: 0.9809) (Time taken(sec) : 586.35) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (6139 | 260061)          2.31
fc1.weight   :       235200 (5296 | 229904)          2.25
fc2.weight   :        30000 (676 | 29324)            2.25
fcout.weight :          1000 (167 | 833)            16.70
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (18/22), Remaining weight : 2.31 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.43596) (accu: 0.1077)
[epoch : 1] (l_loss: 0.38439) (t_loss: 0.11935) (accu: 0.9654)
[epoch : 2] (l_loss: 0.08074) (t_loss: 0.08621) (accu: 0.9740)
[epoch : 3] (l_loss: 0.05156) (t_loss: 0.07160) (accu: 0.9774)
[epoch : 4] (l_loss: 0.03666) (t_loss: 0.06946) (accu: 0.9780)
[epoch : 5] (l_loss: 0.02675) (t_loss: 0.06677) (accu: 0.9790)
[epoch : 6] (l_loss: 0.01983) (t_loss: 0.07067) (accu: 0.9784)
[epoch : 7] (l_loss: 0.01502) (t_loss: 0.07014) (accu: 0.9794)
[epoch : 8] (l_loss: 0.01127) (t_loss: 0.07414) (accu: 0.9788)
[epoch : 9] (l_loss: 0.00850) (t_loss: 0.08024) (accu: 0.9771)
[epoch : 10] (l_loss: 0.00641) (t_loss: 0.08256) (accu: 0.9788)
[epoch : 11] (l_loss: 0.00488) (t_loss: 0.08561) (accu: 0.9785)
[epoch : 12] (l_loss: 0.00379) (t_loss: 0.09248) (accu: 0.9780)
[epoch : 13] (l_loss: 0.00294) (t_loss: 0.09694) (accu: 0.9782)
[epoch : 14] (l_loss: 0.00250) (t_loss: 0.10383) (accu: 0.9777)
[epoch : 15] (l_loss: 0.00190) (t_loss: 0.10899) (accu: 0.9766)
[epoch : 16] (l_loss: 0.00130) (t_loss: 0.11145) (accu: 0.9784)
[epoch : 17] (l_loss: 0.00185) (t_loss: 0.11775) (accu: 0.9771)
[epoch : 18] (l_loss: 0.00130) (t_loss: 0.12381) (accu: 0.9772)
[epoch : 19] (l_loss: 0.00095) (t_loss: 0.11950) (accu: 0.9789)
[epoch : 20] (l_loss: 0.00046) (t_loss: 0.12115) (accu: 0.9785)
[epoch : 21] (l_loss: 0.00046) (t_loss: 0.12953) (accu: 0.9776)
[epoch : 22] (l_loss: 0.00255) (t_loss: 0.14163) (accu: 0.9771)
[epoch : 23] (l_loss: 0.00088) (t_loss: 0.13850) (accu: 0.9777)
[epoch : 24] (l_loss: 0.00038) (t_loss: 0.13838) (accu: 0.9776)
[epoch : 25] (l_loss: 0.00021) (t_loss: 0.13636) (accu: 0.9782)
[epoch : 26] (l_loss: 0.00014) (t_loss: 0.13945) (accu: 0.9780)
[epoch : 27] (l_loss: 0.00013) (t_loss: 0.14288) (accu: 0.9787)
[epoch : 28] (l_loss: 0.00109) (t_loss: 0.17484) (accu: 0.9744)
[epoch : 29] (l_loss: 0.00322) (t_loss: 0.15274) (accu: 0.9791)
[epoch : 30] (l_loss: 0.00040) (t_loss: 0.14722) (accu: 0.9796)
[epoch : 31] (l_loss: 0.00014) (t_loss: 0.14827) (accu: 0.9796)
[epoch : 32] (l_loss: 0.00010) (t_loss: 0.14932) (accu: 0.9795)
[epoch : 33] (l_loss: 0.00008) (t_loss: 0.15157) (accu: 0.9792)
[epoch : 34] (l_loss: 0.00006) (t_loss: 0.15398) (accu: 0.9790)
[epoch : 35] (l_loss: 0.00006) (t_loss: 0.15665) (accu: 0.9787)
[epoch : 36] (l_loss: 0.00006) (t_loss: 0.16134) (accu: 0.9783)
[epoch : 37] (l_loss: 0.00402) (t_loss: 0.17848) (accu: 0.9765)
[epoch : 38] (l_loss: 0.00050) (t_loss: 0.17002) (accu: 0.9770)
[epoch : 39] (l_loss: 0.00012) (t_loss: 0.16773) (accu: 0.9780)
[epoch : 40] (l_loss: 0.00007) (t_loss: 0.16710) (accu: 0.9783)
[epoch : 41] (l_loss: 0.00005) (t_loss: 0.16832) (accu: 0.9785)
[epoch : 42] (l_loss: 0.00004) (t_loss: 0.16991) (accu: 0.9787)
[epoch : 43] (l_loss: 0.00003) (t_loss: 0.17090) (accu: 0.9788)
[epoch : 44] (l_loss: 0.00003) (t_loss: 0.17192) (accu: 0.9791)
[epoch : 45] (l_loss: 0.00002) (t_loss: 0.17650) (accu: 0.9787)
[epoch : 46] (l_loss: 0.00061) (t_loss: 0.21855) (accu: 0.9756)
[epoch : 47] (l_loss: 0.00303) (t_loss: 0.18445) (accu: 0.9786)
[epoch : 48] (l_loss: 0.00030) (t_loss: 0.18249) (accu: 0.9791)
[epoch : 49] (l_loss: 0.00009) (t_loss: 0.18258) (accu: 0.9795)
[epoch : 50] (l_loss: 0.00004) (t_loss: 0.18437) (accu: 0.9795)
Finish! (Best accu: 0.9796) (Time taken(sec) : 610.82) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (4927 | 261273)          1.85
fc1.weight   :       235200 (4237 | 230963)          1.80
fc2.weight   :        30000 (540 | 29460)            1.80
fcout.weight :          1000 (150 | 850)            15.00
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (19/22), Remaining weight : 1.85 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.38041) (accu: 0.1032)
[epoch : 1] (l_loss: 0.44476) (t_loss: 0.13366) (accu: 0.9629)
[epoch : 2] (l_loss: 0.10077) (t_loss: 0.09839) (accu: 0.9708)
[epoch : 3] (l_loss: 0.06827) (t_loss: 0.08458) (accu: 0.9742)
[epoch : 4] (l_loss: 0.05134) (t_loss: 0.07767) (accu: 0.9759)
[epoch : 5] (l_loss: 0.04049) (t_loss: 0.07470) (accu: 0.9760)
[epoch : 6] (l_loss: 0.03240) (t_loss: 0.07780) (accu: 0.9759)
[epoch : 7] (l_loss: 0.02649) (t_loss: 0.07528) (accu: 0.9779)
[epoch : 8] (l_loss: 0.02175) (t_loss: 0.07863) (accu: 0.9767)
[epoch : 9] (l_loss: 0.01789) (t_loss: 0.08372) (accu: 0.9764)
[epoch : 10] (l_loss: 0.01505) (t_loss: 0.08521) (accu: 0.9768)
[epoch : 11] (l_loss: 0.01281) (t_loss: 0.08743) (accu: 0.9755)
[epoch : 12] (l_loss: 0.01044) (t_loss: 0.09127) (accu: 0.9771)
[epoch : 13] (l_loss: 0.00910) (t_loss: 0.09335) (accu: 0.9772)
[epoch : 14] (l_loss: 0.00771) (t_loss: 0.10022) (accu: 0.9770)
[epoch : 15] (l_loss: 0.00644) (t_loss: 0.10112) (accu: 0.9776)
[epoch : 16] (l_loss: 0.00555) (t_loss: 0.10476) (accu: 0.9766)
[epoch : 17] (l_loss: 0.00462) (t_loss: 0.11049) (accu: 0.9764)
[epoch : 18] (l_loss: 0.00432) (t_loss: 0.11453) (accu: 0.9766)
[epoch : 19] (l_loss: 0.00375) (t_loss: 0.12067) (accu: 0.9759)
[epoch : 20] (l_loss: 0.00326) (t_loss: 0.12728) (accu: 0.9757)
[epoch : 21] (l_loss: 0.00247) (t_loss: 0.12855) (accu: 0.9768)
[epoch : 22] (l_loss: 0.00213) (t_loss: 0.12999) (accu: 0.9762)
[epoch : 23] (l_loss: 0.00265) (t_loss: 0.14163) (accu: 0.9760)
[epoch : 24] (l_loss: 0.00209) (t_loss: 0.13714) (accu: 0.9762)
[epoch : 25] (l_loss: 0.00125) (t_loss: 0.13883) (accu: 0.9762)
[epoch : 26] (l_loss: 0.00138) (t_loss: 0.15163) (accu: 0.9763)
[epoch : 27] (l_loss: 0.00184) (t_loss: 0.15332) (accu: 0.9763)
[epoch : 28] (l_loss: 0.00148) (t_loss: 0.15597) (accu: 0.9764)
[epoch : 29] (l_loss: 0.00107) (t_loss: 0.15342) (accu: 0.9760)
[epoch : 30] (l_loss: 0.00119) (t_loss: 0.15827) (accu: 0.9771)
[epoch : 31] (l_loss: 0.00166) (t_loss: 0.16586) (accu: 0.9761)
[epoch : 32] (l_loss: 0.00072) (t_loss: 0.17132) (accu: 0.9763)
[epoch : 33] (l_loss: 0.00049) (t_loss: 0.16491) (accu: 0.9763)
[epoch : 34] (l_loss: 0.00035) (t_loss: 0.16705) (accu: 0.9766)
[epoch : 35] (l_loss: 0.00252) (t_loss: 0.18695) (accu: 0.9752)
[epoch : 36] (l_loss: 0.00102) (t_loss: 0.17786) (accu: 0.9770)
[epoch : 37] (l_loss: 0.00030) (t_loss: 0.17634) (accu: 0.9767)
[epoch : 38] (l_loss: 0.00020) (t_loss: 0.17759) (accu: 0.9771)
[epoch : 39] (l_loss: 0.00018) (t_loss: 0.17784) (accu: 0.9766)
[epoch : 40] (l_loss: 0.00136) (t_loss: 0.19503) (accu: 0.9750)
[epoch : 41] (l_loss: 0.00198) (t_loss: 0.20308) (accu: 0.9748)
[epoch : 42] (l_loss: 0.00059) (t_loss: 0.19354) (accu: 0.9770)
[epoch : 43] (l_loss: 0.00020) (t_loss: 0.19143) (accu: 0.9772)
[epoch : 44] (l_loss: 0.00013) (t_loss: 0.19259) (accu: 0.9770)
[epoch : 45] (l_loss: 0.00011) (t_loss: 0.19335) (accu: 0.9771)
[epoch : 46] (l_loss: 0.00010) (t_loss: 0.19674) (accu: 0.9763)
[epoch : 47] (l_loss: 0.00121) (t_loss: 0.21878) (accu: 0.9754)
[epoch : 48] (l_loss: 0.00219) (t_loss: 0.21361) (accu: 0.9758)
[epoch : 49] (l_loss: 0.00029) (t_loss: 0.20684) (accu: 0.9770)
[epoch : 50] (l_loss: 0.00012) (t_loss: 0.20571) (accu: 0.9774)
Finish! (Best accu: 0.9779) (Time taken(sec) : 597.91) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (3957 | 262243)          1.49
fc1.weight   :       235200 (3390 | 231810)          1.44
fc2.weight   :        30000 (432 | 29568)            1.44
fcout.weight :          1000 (135 | 865)            13.50
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (20/22), Remaining weight : 1.49 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.38054) (accu: 0.1032)
[epoch : 1] (l_loss: 0.49787) (t_loss: 0.14802) (accu: 0.9581)
[epoch : 2] (l_loss: 0.11524) (t_loss: 0.10705) (accu: 0.9681)
[epoch : 3] (l_loss: 0.08222) (t_loss: 0.09451) (accu: 0.9713)
[epoch : 4] (l_loss: 0.06550) (t_loss: 0.08804) (accu: 0.9733)
[epoch : 5] (l_loss: 0.05431) (t_loss: 0.08469) (accu: 0.9738)
[epoch : 6] (l_loss: 0.04643) (t_loss: 0.08447) (accu: 0.9740)
[epoch : 7] (l_loss: 0.04052) (t_loss: 0.08309) (accu: 0.9753)
[epoch : 8] (l_loss: 0.03548) (t_loss: 0.08283) (accu: 0.9760)
[epoch : 9] (l_loss: 0.03127) (t_loss: 0.08303) (accu: 0.9753)
[epoch : 10] (l_loss: 0.02787) (t_loss: 0.08594) (accu: 0.9767)
[epoch : 11] (l_loss: 0.02496) (t_loss: 0.08743) (accu: 0.9754)
[epoch : 12] (l_loss: 0.02256) (t_loss: 0.08988) (accu: 0.9755)
[epoch : 13] (l_loss: 0.02042) (t_loss: 0.09264) (accu: 0.9756)
[epoch : 14] (l_loss: 0.01856) (t_loss: 0.09468) (accu: 0.9752)
[epoch : 15] (l_loss: 0.01706) (t_loss: 0.09714) (accu: 0.9759)
[epoch : 16] (l_loss: 0.01520) (t_loss: 0.09928) (accu: 0.9753)
[epoch : 17] (l_loss: 0.01389) (t_loss: 0.10400) (accu: 0.9750)
[epoch : 18] (l_loss: 0.01268) (t_loss: 0.10581) (accu: 0.9745)
[epoch : 19] (l_loss: 0.01153) (t_loss: 0.11478) (accu: 0.9743)
[epoch : 20] (l_loss: 0.01054) (t_loss: 0.11223) (accu: 0.9745)
[epoch : 21] (l_loss: 0.00991) (t_loss: 0.11754) (accu: 0.9746)
[epoch : 22] (l_loss: 0.00891) (t_loss: 0.12073) (accu: 0.9744)
[epoch : 23] (l_loss: 0.00847) (t_loss: 0.12592) (accu: 0.9744)
[epoch : 24] (l_loss: 0.00741) (t_loss: 0.12788) (accu: 0.9744)
[epoch : 25] (l_loss: 0.00683) (t_loss: 0.12953) (accu: 0.9742)
[epoch : 26] (l_loss: 0.00669) (t_loss: 0.12957) (accu: 0.9755)
[epoch : 27] (l_loss: 0.00603) (t_loss: 0.13794) (accu: 0.9747)
[epoch : 28] (l_loss: 0.00572) (t_loss: 0.14034) (accu: 0.9756)
[epoch : 29] (l_loss: 0.00499) (t_loss: 0.14320) (accu: 0.9749)
[epoch : 30] (l_loss: 0.00480) (t_loss: 0.15022) (accu: 0.9742)
[epoch : 31] (l_loss: 0.00457) (t_loss: 0.15240) (accu: 0.9746)
[epoch : 32] (l_loss: 0.00416) (t_loss: 0.15291) (accu: 0.9759)
[epoch : 33] (l_loss: 0.00399) (t_loss: 0.15840) (accu: 0.9748)
[epoch : 34] (l_loss: 0.00343) (t_loss: 0.16097) (accu: 0.9746)
[epoch : 35] (l_loss: 0.00333) (t_loss: 0.16614) (accu: 0.9746)
[epoch : 36] (l_loss: 0.00330) (t_loss: 0.16676) (accu: 0.9750)
[epoch : 37] (l_loss: 0.00284) (t_loss: 0.17229) (accu: 0.9748)
[epoch : 38] (l_loss: 0.00301) (t_loss: 0.17732) (accu: 0.9741)
[epoch : 39] (l_loss: 0.00250) (t_loss: 0.17902) (accu: 0.9747)
[epoch : 40] (l_loss: 0.00231) (t_loss: 0.18476) (accu: 0.9746)
[epoch : 41] (l_loss: 0.00210) (t_loss: 0.19148) (accu: 0.9750)
[epoch : 42] (l_loss: 0.00266) (t_loss: 0.19366) (accu: 0.9737)
[epoch : 43] (l_loss: 0.00228) (t_loss: 0.19421) (accu: 0.9745)
[epoch : 44] (l_loss: 0.00189) (t_loss: 0.19449) (accu: 0.9748)
[epoch : 45] (l_loss: 0.00119) (t_loss: 0.19833) (accu: 0.9747)
[epoch : 46] (l_loss: 0.00198) (t_loss: 0.20511) (accu: 0.9742)
[epoch : 47] (l_loss: 0.00178) (t_loss: 0.21112) (accu: 0.9731)
[epoch : 48] (l_loss: 0.00210) (t_loss: 0.21069) (accu: 0.9742)
[epoch : 49] (l_loss: 0.00134) (t_loss: 0.21241) (accu: 0.9744)
[epoch : 50] (l_loss: 0.00138) (t_loss: 0.21423) (accu: 0.9737)
Finish! (Best accu: 0.9767) (Time taken(sec) : 592.70) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (3180 | 263020)          1.19
fc1.weight   :       235200 (2712 | 232488)          1.15
fc2.weight   :        30000 (346 | 29654)            1.15
fcout.weight :          1000 (122 | 878)            12.20
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (21/22), Remaining weight : 1.19 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.40918) (accu: 0.1032)
[epoch : 1] (l_loss: 0.58484) (t_loss: 0.17736) (accu: 0.9492)
[epoch : 2] (l_loss: 0.14284) (t_loss: 0.12992) (accu: 0.9625)
[epoch : 3] (l_loss: 0.10424) (t_loss: 0.11076) (accu: 0.9647)
[epoch : 4] (l_loss: 0.08491) (t_loss: 0.10159) (accu: 0.9693)
[epoch : 5] (l_loss: 0.07292) (t_loss: 0.09498) (accu: 0.9707)
[epoch : 6] (l_loss: 0.06432) (t_loss: 0.09178) (accu: 0.9723)
[epoch : 7] (l_loss: 0.05762) (t_loss: 0.09267) (accu: 0.9720)
[epoch : 8] (l_loss: 0.05242) (t_loss: 0.09134) (accu: 0.9732)
[epoch : 9] (l_loss: 0.04778) (t_loss: 0.09121) (accu: 0.9735)
[epoch : 10] (l_loss: 0.04371) (t_loss: 0.09284) (accu: 0.9734)
[epoch : 11] (l_loss: 0.04032) (t_loss: 0.09046) (accu: 0.9761)
[epoch : 12] (l_loss: 0.03761) (t_loss: 0.09143) (accu: 0.9763)
[epoch : 13] (l_loss: 0.03505) (t_loss: 0.09229) (accu: 0.9762)
[epoch : 14] (l_loss: 0.03287) (t_loss: 0.09485) (accu: 0.9761)
[epoch : 15] (l_loss: 0.03101) (t_loss: 0.09570) (accu: 0.9749)
[epoch : 16] (l_loss: 0.02905) (t_loss: 0.09979) (accu: 0.9758)
[epoch : 17] (l_loss: 0.02774) (t_loss: 0.09978) (accu: 0.9752)
[epoch : 18] (l_loss: 0.02627) (t_loss: 0.10322) (accu: 0.9756)
[epoch : 19] (l_loss: 0.02519) (t_loss: 0.10507) (accu: 0.9753)
[epoch : 20] (l_loss: 0.02392) (t_loss: 0.10534) (accu: 0.9760)
[epoch : 21] (l_loss: 0.02302) (t_loss: 0.11055) (accu: 0.9759)
[epoch : 22] (l_loss: 0.02209) (t_loss: 0.11343) (accu: 0.9756)
[epoch : 23] (l_loss: 0.02121) (t_loss: 0.11424) (accu: 0.9747)
[epoch : 24] (l_loss: 0.02044) (t_loss: 0.11581) (accu: 0.9749)
[epoch : 25] (l_loss: 0.01956) (t_loss: 0.11807) (accu: 0.9733)
[epoch : 26] (l_loss: 0.01877) (t_loss: 0.12194) (accu: 0.9746)
[epoch : 27] (l_loss: 0.01848) (t_loss: 0.12381) (accu: 0.9743)
[epoch : 28] (l_loss: 0.01783) (t_loss: 0.12534) (accu: 0.9746)
[epoch : 29] (l_loss: 0.01740) (t_loss: 0.12760) (accu: 0.9756)
[epoch : 30] (l_loss: 0.01659) (t_loss: 0.12905) (accu: 0.9745)
[epoch : 31] (l_loss: 0.01623) (t_loss: 0.13402) (accu: 0.9737)
[epoch : 32] (l_loss: 0.01549) (t_loss: 0.13270) (accu: 0.9746)
[epoch : 33] (l_loss: 0.01533) (t_loss: 0.13650) (accu: 0.9742)
[epoch : 34] (l_loss: 0.01495) (t_loss: 0.13756) (accu: 0.9739)
[epoch : 35] (l_loss: 0.01409) (t_loss: 0.14387) (accu: 0.9741)
[epoch : 36] (l_loss: 0.01405) (t_loss: 0.14613) (accu: 0.9731)
[epoch : 37] (l_loss: 0.01368) (t_loss: 0.14333) (accu: 0.9742)
[epoch : 38] (l_loss: 0.01307) (t_loss: 0.15039) (accu: 0.9736)
[epoch : 39] (l_loss: 0.01305) (t_loss: 0.15059) (accu: 0.9736)
[epoch : 40] (l_loss: 0.01248) (t_loss: 0.14881) (accu: 0.9748)
[epoch : 41] (l_loss: 0.01202) (t_loss: 0.15528) (accu: 0.9732)
[epoch : 42] (l_loss: 0.01186) (t_loss: 0.15479) (accu: 0.9747)
[epoch : 43] (l_loss: 0.01145) (t_loss: 0.15815) (accu: 0.9743)
[epoch : 44] (l_loss: 0.01133) (t_loss: 0.16442) (accu: 0.9737)
[epoch : 45] (l_loss: 0.01064) (t_loss: 0.16476) (accu: 0.9739)
[epoch : 46] (l_loss: 0.01044) (t_loss: 0.16872) (accu: 0.9734)
[epoch : 47] (l_loss: 0.01043) (t_loss: 0.16648) (accu: 0.9737)
[epoch : 48] (l_loss: 0.01015) (t_loss: 0.16988) (accu: 0.9737)
[epoch : 49] (l_loss: 0.00980) (t_loss: 0.17587) (accu: 0.9726)
[epoch : 50] (l_loss: 0.00953) (t_loss: 0.17305) (accu: 0.9731)
Finish! (Best accu: 0.9763) (Time taken(sec) : 607.19) 


------------------------------------------------------------
   Layer                     Weight                Ratio(%)
all.weight   :       266200 (2555 | 263645)          0.96
fc1.weight   :       235200 (2169 | 233031)          0.92
fc2.weight   :        30000 (277 | 29723)            0.92
fcout.weight :          1000 (109 | 891)            10.90
------------------------------------------------------------
Learning start! [Test_Iter : (1/5), Prune_iter : (22/22), Remaining weight : 0.96 %]
[epoch : 0] (l_loss: 0.00000) (t_loss: 2.40593) (accu: 0.1032)
[epoch : 1] (l_loss: 0.65041) (t_loss: 0.19594) (accu: 0.9469)
[epoch : 2] (l_loss: 0.15494) (t_loss: 0.13729) (accu: 0.9597)
[epoch : 3] (l_loss: 0.11235) (t_loss: 0.11766) (accu: 0.9645)
[epoch : 4] (l_loss: 0.09244) (t_loss: 0.10783) (accu: 0.9682)
[epoch : 5] (l_loss: 0.08012) (t_loss: 0.10394) (accu: 0.9692)
[epoch : 6] (l_loss: 0.07151) (t_loss: 0.10195) (accu: 0.9691)
[epoch : 7] (l_loss: 0.06474) (t_loss: 0.09851) (accu: 0.9711)
[epoch : 8] (l_loss: 0.05941) (t_loss: 0.10251) (accu: 0.9708)
[epoch : 9] (l_loss: 0.05516) (t_loss: 0.10047) (accu: 0.9718)
[epoch : 10] (l_loss: 0.05157) (t_loss: 0.10309) (accu: 0.9716)
[epoch : 11] (l_loss: 0.04927) (t_loss: 0.10161) (accu: 0.9727)
[epoch : 12] (l_loss: 0.04686) (t_loss: 0.10434) (accu: 0.9718)
[epoch : 13] (l_loss: 0.04490) (t_loss: 0.10544) (accu: 0.9721)
[epoch : 14] (l_loss: 0.04292) (t_loss: 0.10764) (accu: 0.9705)
[epoch : 15] (l_loss: 0.04139) (t_loss: 0.10811) (accu: 0.9714)
[epoch : 16] (l_loss: 0.03994) (t_loss: 0.10806) (accu: 0.9708)
[epoch : 17] (l_loss: 0.03876) (t_loss: 0.11243) (accu: 0.9718)
[epoch : 18] (l_loss: 0.03752) (t_loss: 0.11261) (accu: 0.9706)
[epoch : 19] (l_loss: 0.03669) (t_loss: 0.11414) (accu: 0.9713)
[epoch : 20] (l_loss: 0.03552) (t_loss: 0.11886) (accu: 0.9704)
[epoch : 21] (l_loss: 0.03458) (t_loss: 0.11740) (accu: 0.9698)
[epoch : 22] (l_loss: 0.03368) (t_loss: 0.12166) (accu: 0.9700)
[epoch : 23] (l_loss: 0.03298) (t_loss: 0.12067) (accu: 0.9708)
[epoch : 24] (l_loss: 0.03193) (t_loss: 0.12216) (accu: 0.9709)
[epoch : 25] (l_loss: 0.03165) (t_loss: 0.12715) (accu: 0.9714)
[epoch : 26] (l_loss: 0.03117) (t_loss: 0.12700) (accu: 0.9710)
[epoch : 27] (l_loss: 0.03036) (t_loss: 0.12855) (accu: 0.9715)
[epoch : 28] (l_loss: 0.03024) (t_loss: 0.13112) (accu: 0.9703)
[epoch : 29] (l_loss: 0.02917) (t_loss: 0.13113) (accu: 0.9697)
[epoch : 30] (l_loss: 0.02923) (t_loss: 0.13299) (accu: 0.9698)
[epoch : 31] (l_loss: 0.02829) (t_loss: 0.13754) (accu: 0.9699)
[epoch : 32] (l_loss: 0.02820) (t_loss: 0.13629) (accu: 0.9701)
[epoch : 33] (l_loss: 0.02785) (t_loss: 0.13780) (accu: 0.9701)
[epoch : 34] (l_loss: 0.02737) (t_loss: 0.13973) (accu: 0.9704)
[epoch : 35] (l_loss: 0.02706) (t_loss: 0.14340) (accu: 0.9694)
[epoch : 36] (l_loss: 0.02668) (t_loss: 0.14313) (accu: 0.9696)
[epoch : 37] (l_loss: 0.02607) (t_loss: 0.14555) (accu: 0.9699)
[epoch : 38] (l_loss: 0.02576) (t_loss: 0.14498) (accu: 0.9698)
[epoch : 39] (l_loss: 0.02583) (t_loss: 0.14703) (accu: 0.9695)
[epoch : 40] (l_loss: 0.02536) (t_loss: 0.15014) (accu: 0.9695)
[epoch : 41] (l_loss: 0.02487) (t_loss: 0.15521) (accu: 0.9688)
[epoch : 42] (l_loss: 0.02464) (t_loss: 0.15448) (accu: 0.9700)
[epoch : 43] (l_loss: 0.02450) (t_loss: 0.15398) (accu: 0.9696)
