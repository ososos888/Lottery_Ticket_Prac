{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import visdom\n",
    "import copy\n",
    "import torch.nn.utils.prune as prune\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "# custom librarys (model, parameters...)\n",
    "import custom.utils as cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(55)\n",
    "torch.cuda.manual_seed_all(55)\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices  2\n",
      "Current cuda device  1\n",
      "GeForce RTX 2080 Ti\n",
      "cpu와 cuda 중 다음 기기로 학습함: cuda:1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "GPU_NUM = 1\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(device))\n",
    "\n",
    "print(\"cpu와 cuda 중 다음 기기로 학습함:\", device, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#switch = 0\n",
    "best_accu = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'LeNet300'\n",
    "#model_type = 'Conv6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = cu.parameters()\n",
    "\n",
    "if model_type == 'LeNet300':\n",
    "    model = cu.LeNet300().to(device)\n",
    "elif model_type == 'Conv6':\n",
    "    model = cu.Conv6().to(device)\n",
    "    \n",
    "param.type(model_type)    \n",
    "model_init = copy.deepcopy(model)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0003\n",
      "epochs: 50\n",
      "batch_size: 60\n",
      "weight_decay: 0.003\n",
      "iteration: 0\n",
      "prune_per_c: 0.15\n",
      "prune_per_f: 0.2\n",
      "prune_per_o: 0.1\n",
      "noi: 12\n",
      "trainset: Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../CIFAR10/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.243, 0.261))\n",
      "           )\n",
      "valset: Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../CIFAR10/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.243, 0.261))\n",
      "           )\n",
      "testset: Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ../CIFAR10/\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.243, 0.261))\n",
      "           )\n",
      "train_loader: <torch.utils.data.dataloader.DataLoader object at 0x7f647b8aa2d0>\n",
      "val_loader: <torch.utils.data.dataloader.DataLoader object at 0x7f647b8aa250>\n",
      "test_loader: <torch.utils.data.dataloader.DataLoader object at 0x7f647b8aa550>\n",
      "validation_ratio: 0.1\n",
      "classes: ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
     ]
    }
   ],
   "source": [
    "# parameter check\n",
    "print('\\n'.join(\"%s: %s\" % item for item in param.__dict__.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('\\n'.join(\"%s: %s\" % item for item in sorted(param.__dict__.items(), key=lambda i: i[0])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('\\n'.join(\"%s: %s\" % item for item in sorted(param.__dict__.items(), key=lambda i: i[0])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('\\n'.join(\"%s: %s\" % item for item in param.__dict__.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(param.train_loader.sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "# visdom setting\n",
    "vis = visdom.Visdom()\n",
    "vis.close(env=\"main\")\n",
    "\n",
    "Tracker_type = \"Accuracy_Tracker\"\n",
    "title = model_type + \"_\" + Tracker_type\n",
    "\n",
    "# make plot\n",
    "vis_plt = vis.line(X=torch.Tensor(1).zero_(), Y=torch.Tensor(1).zero_(), \n",
    "                    opts=dict(title = title,\n",
    "                              legend=['100.0'],\n",
    "                             showlegend=True,\n",
    "                              xtickmin = 0,\n",
    "                              xtickmax = 20000,\n",
    "                              ytickmin = 0.95,\n",
    "                              ytickmax = 0.99\n",
    "                             )\n",
    "                   )\n",
    "\n",
    "def visdom_plot(loss_plot, loss_value, num, name):\n",
    "    vis.line(X = num,\n",
    "            Y = loss_value,\n",
    "            win = loss_plot,\n",
    "            name = name,\n",
    "            update = 'append'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "param.epochs = 1\n",
    "#param.noi = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(param.train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(param.val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameter\n",
    "lr = 0.0012\n",
    "#epochs = 50\n",
    "#epochs = 20\n",
    "epochs = 30\n",
    "batch_size = 60\n",
    "weight_decay = 1.2e-3\n",
    "iteration = 0\n",
    "remaining_weight = 1\n",
    "prune_per = 0.2\n",
    "# number of iteration\n",
    "noi = 11\n",
    "\n",
    "switch = 0\n",
    "best_accu = []\n",
    "# 마지막 layer의 Pruning rate는 기존의 1/2\n",
    "# prune_per_ll = prune_per/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cp_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "mnist_train = dsets.MNIST(root='../MNIST_data/',\n",
    "                         train=True,\n",
    "                         transform=transforms,\n",
    "                         download=True)\n",
    "mnist_test = dsets.MNIST(root='../MNIST_data/',\n",
    "                        train=False,\n",
    "                        transform=transforms,\n",
    "                        download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
    "                                         shuffle=False,\n",
    "                                         drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test, prune function\n",
    "def train(model, dataloader, optimizer, criterion, cp_mask):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, label) in enumerate(dataloader):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        # 0-weight 학습 방지\n",
    "        \"\"\"\n",
    "        if cp_mask:\n",
    "            i = 0\n",
    "            for name, p in model.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    p.grad.data *= cp_mask[i]\n",
    "                    i += 1\n",
    "        \"\"\"            \n",
    "        optimizer.step()\n",
    "        running_loss += loss / len(dataloader)\n",
    "    return running_loss\n",
    "\n",
    "def test(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in dataloader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            #test_loss += F.nll_loss(outputs, label, reduction='sum').item() # sum up batch loss\n",
    "            loss = criterion(outputs, label)\n",
    "            #predicted = outputs.data.max(1, keepdim=True)[1]\n",
    "            #correct += predicted.eq(label.data.view_as(predicted)).sum().item()\n",
    "            \n",
    "            test_loss += loss / len(dataloader)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "        #accuracy =  correct / len(dataloader)\n",
    "        # 로더 -> 배치 개수 로더.dataset -> 전체 길이, \n",
    "    return (correct/total), test_loss\n",
    "\n",
    "# prune function\n",
    "# pruning mask 생성 -> mask 복사 -> init값 복사 -> prune 진행\n",
    "def weight_init(model1, model2, c_rate, f_rate, o_rate):\n",
    "    # layer별로 지정된 rate만큼 prune mask 생성\n",
    "    for name, module in model1.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.l1_unstructured(module, name = 'weight', amount = c_rate)\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if name != 'fc3':\n",
    "                prune.l1_unstructured(module, name = 'weight', amount = f_rate)\n",
    "            else:\n",
    "                prune.l1_unstructured(module, name = 'weight', amount = o_rate)\n",
    "                        \n",
    "    # mask 복사\n",
    "    cp_mask = []\n",
    "    for name, mask in model1.named_buffers():\n",
    "        cp_mask.append(mask)\n",
    "    \n",
    "    # init 값을 model에 복사\n",
    "    for name, p in model1.named_parameters():\n",
    "        if 'weight_orig' in name:\n",
    "            for name2, p2 in model2.named_parameters():\n",
    "                if name[0:len(name) - 5] in name2:\n",
    "                    p.data = copy.deepcopy(p2.data)\n",
    "        if 'bias_orig' in name:\n",
    "            for name2, p2 in model2.named_parameters():\n",
    "                if name[0:len(name) - 5] in name2:\n",
    "                    p.data = copy.deepcopy(p2.data)\n",
    "                    \n",
    "    # prune 진행\n",
    "    for name, module in model1.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.remove(module, name = 'weight')\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            prune.remove(module, name = 'weight')\n",
    "            \n",
    "    # copy된 mask return\n",
    "    return cp_mask\n",
    "\n",
    "# weight count function\n",
    "# dict type['name' : [all, non_zero, zero, ratio]]\n",
    "def weight_counter(model):\n",
    "    layer_weight = {'all.weight':[0, 0, 0, 0]}\n",
    "    \n",
    "    for name, p in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            remain, pruned = (p != 0).sum().item(), (p == 0).sum().item()\n",
    "            layer_weight[name] = [remain+pruned, remain, pruned, round((remain/(remain+pruned))*100, 2)]\n",
    "            \n",
    "    for i in layer_weight.keys():\n",
    "        for j in range(0, 3):\n",
    "            layer_weight['all.weight'][j] += layer_weight[i][j]\n",
    "    layer_weight['all.weight'][3] = round(layer_weight['all.weight'][1]/layer_weight['all.weight'][0]*100, 2)\n",
    "\n",
    "    print(\"Layer\".center(12), \"Weight\".center(39), \"Ratio(%)\".rjust(7), sep='')\n",
    "    for i in layer_weight.keys():\n",
    "        print(\"%s\" % i.ljust(13), \":\",\n",
    "              (\"%s (%s | %s)\" % (layer_weight[i][0], layer_weight[i][1], layer_weight[i][2])).center(36),\n",
    "              (\"%.2f\" % layer_weight[i][3]).rjust(7),\n",
    "              sep=''\n",
    "             )\n",
    "        \n",
    "    return layer_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Layer                     Weight                Ratio(%)\n",
      "all.weight   :       2261184 (2261184 | 0)         100.00\n",
      "conv1.weight :          1728 (1728 | 0)            100.00\n",
      "conv2.weight :         36864 (36864 | 0)           100.00\n",
      "conv3.weight :         73728 (73728 | 0)           100.00\n",
      "conv4.weight :        147456 (147456 | 0)          100.00\n",
      "conv5.weight :        294912 (294912 | 0)          100.00\n",
      "conv6.weight :        589824 (589824 | 0)          100.00\n",
      "fc1.weight   :       1048576 (1048576 | 0)         100.00\n",
      "fc2.weight   :         65536 (65536 | 0)           100.00\n",
      "fc3.weight   :          2560 (2560 | 0)            100.00\n"
     ]
    }
   ],
   "source": [
    "aaa = weight_counter(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z[0] = [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z[i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z[i] = aaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z[1] =aaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z[0]['all.weight'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z[i] += [aaa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight_counter2(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    layer_weight = {'all.weight':[0, 0, 0, 0]}\n",
    "    for name, p in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            remain, pruned = (p != 0).sum().item(), (p == 0).sum().item()\n",
    "            layer_weight[name] = [remain+pruned, remain, pruned, round((remain/remain+pruned), 1)]\n",
    "    for i in layer_weight.keys():\n",
    "        for j in range(0, 3):\n",
    "            layer_weight['all.weight'][j] += layer_weight[i][j]\n",
    "    layer_weight['all.weight'][3] = round(layer_weight['all.weight'][1]/layer_weight['all.weight'][0], 1)\n",
    "\n",
    "    print(\"Layer\".center(12), \"Weight\".center(38), \"Ratio\".rjust(7), sep='')\n",
    "    for i in layer_weight.keys():\n",
    "        print(\"%s\" % i.ljust(13), \":\",\n",
    "              (\"%s (%s | %s)\" % (layer_weight[i][0], layer_weight[i][1], layer_weight[i][2])).center(35),\n",
    "              (\"%.2f\" % layer_weight[i][3]).rjust(7),\n",
    "              sep=''\n",
    "             )\n",
    "    return layer_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "abc = weight_counter(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(len(param.train_loader), len(param.val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def test(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, label in dataloader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            outputs = model(data)\n",
    "            \n",
    "            predicted = torch.argmax(outputs.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "            accuracy = (correct/total)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay = 1.2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#EPS = 1e-6\n",
    "# number of weight\n",
    "a = ((model.fc1.weight != 0).sum(dim=1)).sum(dim=0) + ((model.fc2.weight != 0).sum(dim=1)).sum(dim=0) + ((model.fc3.weight != 0).sum(dim=1)).sum(dim=0)\n",
    "#b = ((model.fc1.weight == 0).sum(dim=1)).sum(dim=0) + ((model.fc2.weight == 0).sum(dim=1)).sum(dim=0) + ((model.fc3.weight == 0).sum(dim=1)).sum(dim=0)\n",
    "b = ((model.fc1.weight == 0).sum(dim=1)).sum(dim=0) + ((model.fc2.weight == 0).sum(dim=1)).sum(dim=0) + ((model.fc3.weight == 0).sum(dim=1)).sum(dim=0)\n",
    "\n",
    "now = (a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def calc_now(model):\n",
    "    fc1_1 = ((model.fc1.weight != 0).sum(dim=1)).sum(dim=0).item()\n",
    "    fc1_0 = ((model.fc1.weight == 0).sum(dim=1)).sum(dim=0).item()\n",
    "    fc1 = fc1_1 + fc1_0\n",
    "    fc1_p = fc1_0 / fc1_1\n",
    "    fc2_1 = ((model.fc2.weight != 0).sum(dim=1)).sum(dim=0).item()\n",
    "    fc2_0 = ((model.fc2.weight == 0).sum(dim=1)).sum(dim=0).item()\n",
    "    fc2 = fc2_1 + fc2_0\n",
    "    fc3_1 = ((model.fc3.weight != 0).sum(dim=1)).sum(dim=0).item()\n",
    "    fc3_0 = ((model.fc3.weight == 0).sum(dim=1)).sum(dim=0).item()\n",
    "    fc3 = fc3_1 + fc3_0\n",
    "    #print(fc1, fc2, fc3, fc1+fc2+fc3, fc1_1 + fc2_1 + fc3_1 ,fc1_0 + fc2_0 + fc3_0)\n",
    "    print(\"Remaining weight %.1f %%\" %(((fc1_1+fc2_1+fc3_1)/(fc1+fc2+fc3))*100))\n",
    "    print('total weight :',\n",
    "        '%d' % (fc1+fc2+fc3),\n",
    "         '(%d |' % (fc1_1+fc2_1+fc3_1),\n",
    "         '%d)' % (fc1_0+fc2_0+fc3_0)\n",
    "         )\n",
    "    print('fc1 :',\n",
    "        '%d' % fc1,\n",
    "         '(%d |' % fc1_1,\n",
    "         '%d)' % fc1_0\n",
    "         )\n",
    "    print('fc2 :',\n",
    "        '%d' % fc2,\n",
    "         '(%d |' % fc2_1,\n",
    "         '%d)' % fc2_0\n",
    "         )\n",
    "    print('fc3 :',\n",
    "        '%d' % fc3,\n",
    "         '(%d |' % fc3_1,\n",
    "         '%d)' % fc3_0\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight_init(model, model_init, 1 - weight_remaining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight_init(model, model_init, 1 - weight_remaining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(model.fc3.weight_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        prune.l1_unstructured(module, name = 'weight', amount = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # init 값 복사\n",
    "for name, p in model.named_parameters():\n",
    "     if 'weight_orig' in name:\n",
    "        for name2, p2 in model_init.named_parameters():\n",
    "            if name[0:len(name) - 5] in name2:\n",
    "                p.data = copy.deepcopy(p2.data)\n",
    "                break\n",
    "    if 'bias_orig' in name:\n",
    "        for name2, p2 in modelinit.named_parameters():\n",
    "            if name[0:len(name) - 5] in name2:\n",
    "                p.data = copy.deepcopy(p2.data)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        prune.remove(module, name = 'weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(model.fc3.weight[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(model_init.fc3.weight[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.prune_per_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.bias', 'conv1.weight', 'conv2.bias', 'conv2.weight', 'conv3.bias', 'conv3.weight', 'conv4.bias', 'conv4.weight', 'conv5.bias', 'conv5.weight', 'conv6.bias', 'conv6.weight', 'fc1.bias', 'fc1.weight', 'fc2.bias', 'fc2.weight', 'fc3.bias', 'fc3.weight'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.conv1.weight[0]\n",
    "for name, p in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(model.conv1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for name, p in model.named_modules():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i = 0\n",
    "for name, p in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(p.grad.data * cp_mask[i])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model2[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for name, p in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.conv1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cp_mask[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(p[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j = 0\n",
    "\n",
    "for name, p in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        p.register_hook(lambda grad: grad.mul_(cp_mask[j]))\n",
    "        print(name, len(cp_mask[j]))\n",
    "        j += 1\n",
    "        #print(p[0], cp_mask[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = cu.parameters()\n",
    "\n",
    "if model_type == 'LeNet300':\n",
    "    model = cu.LeNet300().to(device)\n",
    "elif model_type == 'Conv6':\n",
    "    model = cu.Conv6().to(device)\n",
    "    \n",
    "param.type(model_type)    \n",
    "model_init = copy.deepcopy(model)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Layer                     Weight                Ratio(%)\n",
      "all.weight   :        266200 (266200 | 0)          100.00\n",
      "fc1.weight   :        235200 (235200 | 0)          100.00\n",
      "fc2.weight   :         30000 (30000 | 0)           100.00\n",
      "fc3.weight   :          1000 (1000 | 0)            100.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'all.weight': [266200, 266200, 0, 100.0],\n",
       " 'fc1.weight': [235200, 235200, 0, 100.0],\n",
       " 'fc2.weight': [30000, 30000, 0, 100.0],\n",
       " 'fc3.weight': [1000, 1000, 0, 100.0]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_counter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "cp_mask = torch.FloatTensor()\n",
    "cp_mask = weight_init(model, model_init, \n",
    "                           (0.5),\n",
    "                           (0.5),\n",
    "                           (0.5)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000, -0.0000,  0.0000,  ..., -0.0003, -0.0006, -0.0008],\n",
       "        [ 0.0028, -0.0000,  0.0028,  ...,  0.0000, -0.0000,  0.0041],\n",
       "        [-0.0000, -0.0020, -0.0046,  ..., -0.0029, -0.0000, -0.0000],\n",
       "        ...,\n",
       "        [-0.0037, -0.0024, -0.0036,  ..., -0.0000,  0.0035,  0.0033],\n",
       "        [ 0.0000,  0.0000, -0.0021,  ..., -0.0040, -0.0039, -0.0000],\n",
       "        [-0.0039,  0.0043,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       device='cuda:1', requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1., 0., 1.,  ..., 1., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 1., 0., 0.]], device='cuda:1'),\n",
       " tensor([[1., 0., 0.,  ..., 1., 1., 0.],\n",
       "         [1., 0., 1.,  ..., 1., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 1., 0., 1.],\n",
       "         ...,\n",
       "         [0., 1., 1.,  ..., 1., 0., 1.],\n",
       "         [1., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:1'),\n",
       " tensor([[0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
       "          1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
       "          1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "          0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
       "          0., 1., 1., 0., 1., 1., 1., 1., 0., 1.],\n",
       "         [1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
       "          1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "          1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "          1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "          1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 1., 1., 0., 1., 0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
       "          0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
       "          0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "          1., 1., 1., 1., 1., 0., 0., 1., 1., 1.],\n",
       "         [1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "          1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
       "          1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
       "          1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
       "          0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "          1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
       "          1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "          1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
       "          0., 0., 0., 1., 0., 0., 1., 1., 1., 0.],\n",
       "         [1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
       "          0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
       "          1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "          1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
       "          0., 1., 0., 0., 1., 1., 1., 1., 1., 0.],\n",
       "         [0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
       "          0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
       "          1., 0., 1., 0., 1., 1., 1., 0., 0., 1.],\n",
       "         [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "          1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
       "          0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
       "          0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
       "          0., 0., 0., 1., 1., 1., 1., 0., 0., 1.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
       "          1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
       "          1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
       "          1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "         [1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
       "          1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
       "          1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
       "          0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
       "          0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "          1., 1., 0., 0., 0., 1., 0., 1., 0., 1.]], device='cuda:1')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "a = cp_mask[i]\n",
    "print(a[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = param.lr, weight_decay = param.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = train(model, param.train_loader, optimizer, criterion, cp_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight\n",
      "fc1.bias\n",
      "fc2.weight\n",
      "fc2.bias\n",
      "fc3.weight\n",
      "fc3.bias\n"
     ]
    }
   ],
   "source": [
    "for name, i in model.named_parameters():\n",
    "    print(name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "100\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for name, i in model.named_parameters():\n",
    "    #print(name, i)\n",
    "    if 'fc1.weight' in name:\n",
    "        print(len(cp_mask[0]))\n",
    "        i.register_hook(lambda grad: grad * cp_mask[0])\n",
    "    elif 'fc2.weight' in name:\n",
    "        print(len(cp_mask[1]))\n",
    "        i.register_hook(lambda grad: grad * cp_mask[1])\n",
    "    elif 'fc3.weight' in name:\n",
    "        print(len(cp_mask[2]))\n",
    "        i.register_hook(lambda grad: grad * cp_mask[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for name, i in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(len(a))\n",
    "        i.register_hook(lambda grad:grad.mul_(cp_mask[count]))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-7a1b813c96b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "cp_mask = torch.tensor(cp_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.FloatTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-5886eb6874a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcp_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'state_dict'"
     ]
    }
   ],
   "source": [
    "cp_mask.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc1.weight.register_hook(lambda grad: grad * cp_mask[0])\n",
    "model.fc2.weight.register_hook(lambda grad: grad * cp_mask[1])\n",
    "model.fc3.weight.register_hook(lambda grad: grad * cp_mask[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f647b409450>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = model.fc3.weight.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0516,  0.0000,  0.0000,  0.0000,  0.0923,  0.0685,  0.0551,  0.0000,\n",
       "          0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0674, -0.0621,\n",
       "         -0.0839, -0.0829,  0.0839, -0.0896,  0.0872, -0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0597, -0.0000,  0.0771,  0.0513, -0.0852,  0.0990,\n",
       "         -0.0000,  0.0589, -0.0000, -0.0889, -0.0000,  0.0000, -0.0000, -0.0000,\n",
       "          0.0780, -0.0000,  0.0628,  0.0927, -0.0846,  0.0000, -0.0783, -0.0729,\n",
       "         -0.0000,  0.0000,  0.0513, -0.0834,  0.0904,  0.0688, -0.0000,  0.0696,\n",
       "         -0.0597, -0.0995, -0.0000, -0.0000,  0.0708, -0.0896, -0.0633, -0.0000,\n",
       "          0.0000,  0.0661,  0.0777,  0.0967, -0.0000, -0.0909, -0.0000, -0.0000,\n",
       "          0.0846, -0.0000,  0.0791,  0.0534, -0.0000,  0.0000, -0.0000, -0.0827,\n",
       "         -0.0931, -0.0810,  0.0000,  0.0000, -0.0912, -0.0633, -0.0000,  0.0806,\n",
       "         -0.0000,  0.0000,  0.0000, -0.0917,  0.0511,  0.0000,  0.0795,  0.0556,\n",
       "         -0.0000,  0.0000,  0.0897,  0.0939],\n",
       "        [-0.0885,  0.0914, -0.0623,  0.0774,  0.0000,  0.0000, -0.0862,  0.0000,\n",
       "          0.0769, -0.0000, -0.0000,  0.0947,  0.0000, -0.0000,  0.0000,  0.0826,\n",
       "          0.0000,  0.0796,  0.0000, -0.0000, -0.0533, -0.0000,  0.0886,  0.0000,\n",
       "         -0.0605, -0.0000, -0.0689, -0.0000,  0.0000, -0.0000,  0.0613,  0.0000,\n",
       "         -0.0000,  0.0806,  0.0750, -0.0000,  0.0923,  0.0000,  0.0645, -0.0000,\n",
       "         -0.0908,  0.0866, -0.0000, -0.0849,  0.0000, -0.0651,  0.0000, -0.0000,\n",
       "          0.0898, -0.0839, -0.0569, -0.0000, -0.0875, -0.0000, -0.0000, -0.0000,\n",
       "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0995, -0.0560,  0.0000,\n",
       "         -0.0856, -0.0000, -0.0715,  0.0733,  0.0954,  0.0000, -0.0877, -0.0642,\n",
       "          0.0992,  0.0726, -0.0000,  0.0682,  0.0678, -0.0000,  0.0806,  0.0782,\n",
       "          0.0792,  0.0800,  0.0000,  0.0862, -0.0679, -0.0913,  0.0000,  0.0824,\n",
       "          0.0545,  0.0000, -0.0626,  0.0945, -0.0912, -0.0755,  0.0983,  0.0615,\n",
       "         -0.0000,  0.0000, -0.0000, -0.0531],\n",
       "        [ 0.0000, -0.0836, -0.0787,  0.0762, -0.0000,  0.0969,  0.0871, -0.0883,\n",
       "          0.0000, -0.0000,  0.0000, -0.0000, -0.0906,  0.0000, -0.0000, -0.0855,\n",
       "         -0.0000, -0.0000, -0.0996, -0.0854, -0.0961, -0.0000,  0.0513,  0.0000,\n",
       "         -0.0000, -0.0717, -0.0000,  0.0000, -0.0000,  0.0820, -0.0000, -0.0734,\n",
       "         -0.0705,  0.0949,  0.0000,  0.0000,  0.0543,  0.0000, -0.0000,  0.0584,\n",
       "          0.0000,  0.0000, -0.0938, -0.0000, -0.0000, -0.0000, -0.0000,  0.0883,\n",
       "         -0.0000,  0.0785,  0.0000,  0.0911, -0.0000,  0.0000,  0.0000, -0.0528,\n",
       "          0.0733,  0.0000, -0.0690, -0.0000,  0.0000,  0.0897,  0.0000,  0.0767,\n",
       "         -0.0000,  0.0719,  0.0953,  0.0930, -0.0000,  0.0000, -0.0000,  0.0000,\n",
       "          0.0704,  0.0000,  0.0988, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
       "         -0.0000, -0.0000, -0.0557,  0.0000,  0.0000,  0.0695,  0.0000,  0.0973,\n",
       "          0.0000,  0.0895, -0.0000,  0.0866,  0.0965, -0.0000,  0.0000, -0.0873,\n",
       "         -0.0000,  0.0000,  0.0526,  0.0000],\n",
       "        [ 0.0000, -0.0744, -0.0688, -0.0000,  0.0706, -0.0000,  0.0767, -0.0795,\n",
       "         -0.0786, -0.0865, -0.0000, -0.0857, -0.0887,  0.0000, -0.0692, -0.0000,\n",
       "         -0.0698,  0.0000, -0.0674,  0.0000,  0.0776,  0.0000, -0.0000, -0.0000,\n",
       "         -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0775, -0.0781, -0.0699,\n",
       "         -0.0519, -0.0000, -0.0750,  0.0545, -0.0857,  0.0000, -0.0000, -0.0798,\n",
       "          0.0000,  0.0743,  0.0000,  0.0000, -0.0600, -0.0000,  0.0929,  0.0000,\n",
       "          0.0000, -0.0966, -0.0833,  0.0958,  0.0000, -0.0677,  0.0000, -0.0678,\n",
       "         -0.0698, -0.0929, -0.0000, -0.0662,  0.0844,  0.0000,  0.0000, -0.0000,\n",
       "          0.0000, -0.0862,  0.0872,  0.0586,  0.0000,  0.0000, -0.0582,  0.0578,\n",
       "          0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0708, -0.0744,\n",
       "          0.0790,  0.0778, -0.0749, -0.0513, -0.0000,  0.0000, -0.0000, -0.0766,\n",
       "          0.0000,  0.0966, -0.0782, -0.0000, -0.0724,  0.0733, -0.0000,  0.0559,\n",
       "          0.0000, -0.0870, -0.0000,  0.0000],\n",
       "        [-0.0803, -0.0000, -0.0882, -0.0893,  0.0910, -0.0000, -0.0000,  0.0000,\n",
       "         -0.0000, -0.0842,  0.0000, -0.0786,  0.0000, -0.0000,  0.0000,  0.0000,\n",
       "         -0.0926, -0.0758,  0.0987, -0.0000, -0.0540, -0.0000, -0.0661,  0.0809,\n",
       "          0.0968, -0.0810, -0.0000, -0.0000, -0.0000,  0.0668, -0.0000,  0.0000,\n",
       "          0.0643,  0.0684, -0.0000, -0.0623, -0.0000,  0.0697, -0.0000, -0.0511,\n",
       "          0.0679,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.0000,  0.0000,  0.0543, -0.0000,  0.0710, -0.0000, -0.0651,  0.0000,\n",
       "         -0.0935,  0.0000,  0.0713,  0.0735,  0.0000,  0.0844,  0.0000, -0.0000,\n",
       "         -0.0000, -0.0996,  0.0000, -0.0000, -0.0000,  0.0955,  0.0571, -0.0741,\n",
       "         -0.0846, -0.0000,  0.0000,  0.0954, -0.0000,  0.0572, -0.0000, -0.0000,\n",
       "          0.0000, -0.0681,  0.0814,  0.0531,  0.0920, -0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0906, -0.0000, -0.0698,  0.0000, -0.0554,  0.0710,  0.0000,\n",
       "          0.0630,  0.0000, -0.0997,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0908, -0.0546,  0.0835,  0.0000,  0.0816, -0.0873,\n",
       "          0.0000,  0.0861, -0.0876, -0.0974,  0.0000, -0.0787, -0.0838, -0.0509,\n",
       "         -0.0621,  0.0000,  0.0630, -0.0914, -0.0000,  0.0718,  0.0954,  0.0000,\n",
       "         -0.0629,  0.0000, -0.0583, -0.0876,  0.0000, -0.0713, -0.0000, -0.0859,\n",
       "         -0.0836,  0.0712, -0.0632,  0.0674, -0.0828, -0.0000,  0.0601, -0.0907,\n",
       "         -0.0968, -0.0000, -0.0000, -0.0000, -0.0000, -0.0721,  0.0000, -0.0000,\n",
       "         -0.0000, -0.0852, -0.0000, -0.0000,  0.0808,  0.0645, -0.0556,  0.0000,\n",
       "          0.0879,  0.0956, -0.0918,  0.0000,  0.0797, -0.0583,  0.0915, -0.0000,\n",
       "         -0.0000, -0.0935,  0.0542, -0.0991, -0.0000,  0.0868,  0.0629,  0.0000,\n",
       "          0.0000,  0.0878,  0.0757,  0.0000,  0.0000, -0.0000,  0.0750,  0.0936,\n",
       "          0.0888,  0.0000, -0.0000,  0.0946, -0.0740, -0.0000, -0.0000, -0.0000,\n",
       "          0.0825, -0.0523, -0.0000, -0.0000, -0.0000,  0.0533,  0.0836, -0.0834,\n",
       "         -0.0796,  0.0524,  0.0580,  0.0000],\n",
       "        [-0.0000, -0.0693, -0.0000,  0.0517,  0.0000,  0.0000,  0.0556, -0.0703,\n",
       "          0.0000,  0.0000,  0.0770,  0.0955, -0.0521, -0.0000,  0.0520,  0.0000,\n",
       "         -0.0000, -0.0000,  0.0000,  0.0544, -0.0000,  0.0618,  0.0751,  0.0000,\n",
       "         -0.0000, -0.0817, -0.0968,  0.0732,  0.0850,  0.0663,  0.0000, -0.0539,\n",
       "          0.0000,  0.0934,  0.0834, -0.0000,  0.0000,  0.0000, -0.0000, -0.0804,\n",
       "          0.0000,  0.0000,  0.0987,  0.0512, -0.0000, -0.0764,  0.0858,  0.0809,\n",
       "         -0.0638, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
       "         -0.0715,  0.0773, -0.0979,  0.0580, -0.0000, -0.0779,  0.0000,  0.0000,\n",
       "         -0.0803, -0.0000, -0.0513, -0.0570,  0.0662,  0.0000, -0.0000, -0.0000,\n",
       "          0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0841,  0.0697,  0.0800,\n",
       "         -0.0000, -0.0000, -0.0000, -0.0912,  0.0771, -0.0000, -0.0000,  0.0000,\n",
       "          0.0000, -0.0563,  0.0715, -0.0000, -0.0695,  0.0000,  0.0000,  0.0000,\n",
       "          0.0959, -0.0000, -0.0000, -0.0633],\n",
       "        [ 0.0000,  0.0908, -0.0574, -0.0000, -0.0000, -0.0609,  0.0733, -0.0000,\n",
       "          0.0680,  0.0000, -0.0948,  0.0000, -0.0776,  0.0000,  0.0543, -0.0000,\n",
       "         -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0840,  0.0857,  0.0557,\n",
       "         -0.0000, -0.0000,  0.0956, -0.0814,  0.0000, -0.0955, -0.0970,  0.0917,\n",
       "         -0.0889, -0.0000, -0.0904,  0.0000, -0.0000, -0.0751, -0.0760,  0.0620,\n",
       "         -0.0922,  0.0545, -0.0000, -0.0000,  0.0792,  0.0986,  0.0000, -0.0881,\n",
       "          0.0555, -0.0000,  0.0515,  0.0893, -0.0000, -0.0000,  0.0000,  0.0000,\n",
       "         -0.0000, -0.0686, -0.0915, -0.0000,  0.0678, -0.0936,  0.0654,  0.0854,\n",
       "         -0.0000, -0.0610,  0.0956,  0.0538, -0.0996,  0.0000,  0.0000,  0.0000,\n",
       "          0.0549, -0.0000,  0.0000,  0.0000,  0.0000, -0.0692, -0.0742, -0.0000,\n",
       "          0.0000,  0.0968,  0.0873, -0.0650, -0.0000,  0.0000, -0.0000, -0.0684,\n",
       "         -0.0000,  0.0919,  0.0000, -0.0712, -0.0766, -0.0000,  0.0000,  0.0526,\n",
       "          0.0000,  0.0634, -0.0696,  0.0801],\n",
       "        [ 0.0000, -0.0700, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0949,\n",
       "          0.0863, -0.0580,  0.0623, -0.0851,  0.0000,  0.0890,  0.0765,  0.0921,\n",
       "          0.0000, -0.0719, -0.0719, -0.0667,  0.0000, -0.0000, -0.0902, -0.0000,\n",
       "         -0.0000, -0.0000, -0.0729, -0.0000, -0.0510,  0.0000, -0.0000,  0.0000,\n",
       "         -0.0822, -0.0000,  0.0978,  0.0000,  0.0700,  0.0000,  0.0000,  0.0791,\n",
       "          0.0933, -0.0684,  0.0954,  0.0000,  0.0869, -0.0000,  0.0000,  0.0666,\n",
       "         -0.0630,  0.0000,  0.0881, -0.0000, -0.0000, -0.0561,  0.0947,  0.0000,\n",
       "          0.0000,  0.0692,  0.0000,  0.0000, -0.0000, -0.0000,  0.0512, -0.0000,\n",
       "          0.0749,  0.0983, -0.0000, -0.0000,  0.0757, -0.0608, -0.0000, -0.0000,\n",
       "          0.0994,  0.0801, -0.0000,  0.0806,  0.0000, -0.0747, -0.0747,  0.0580,\n",
       "         -0.0000,  0.0000, -0.0000, -0.0947,  0.0000,  0.0527,  0.0960,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0730,  0.0000, -0.0000,  0.0000,\n",
       "         -0.0000, -0.0952, -0.0743,  0.0731],\n",
       "        [ 0.0635, -0.0000, -0.0000, -0.0562, -0.0000,  0.0000,  0.0000, -0.0735,\n",
       "          0.0941,  0.0608, -0.0000,  0.0609, -0.0602,  0.0545,  0.0897, -0.0846,\n",
       "         -0.0000,  0.0981, -0.0000,  0.0000, -0.0729,  0.0000, -0.0831,  0.0671,\n",
       "          0.0000,  0.0000, -0.0000, -0.0853, -0.0000, -0.0000, -0.0684, -0.0000,\n",
       "         -0.0000, -0.0996,  0.0723, -0.0634,  0.0000, -0.0000, -0.0000,  0.0000,\n",
       "         -0.0000,  0.0000, -0.0850, -0.0854,  0.0554,  0.0627, -0.0000,  0.0951,\n",
       "         -0.0000, -0.0925, -0.0973,  0.0000,  0.0772, -0.0000, -0.0743, -0.0711,\n",
       "         -0.0000, -0.0632, -0.0962, -0.0000, -0.0679, -0.0756, -0.0000, -0.0705,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0979,  0.0000, -0.0000,\n",
       "          0.0596,  0.0537,  0.0968, -0.0853,  0.0000,  0.0560, -0.0749, -0.0832,\n",
       "         -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0927,  0.0956,  0.0826,\n",
       "          0.0000,  0.0000,  0.0000, -0.0000, -0.0687, -0.0000,  0.0000, -0.0747,\n",
       "          0.0910,  0.0543,  0.0572, -0.0544]], device='cuda:1')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = param.lr, weight_decay = param.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0., 1., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.]], device='cuda:1'), tensor([[0., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.]], device='cuda:1'), tensor([[1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
      "         1., 0., 1., 1., 0., 0., 1., 1., 0., 1.],\n",
      "        [1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 1., 1., 1., 0., 1.],\n",
      "        [0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "         0., 0., 1., 1., 0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
      "         0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
      "         1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
      "         0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
      "         0., 0., 0., 1., 0., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
      "         0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
      "         1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 1., 1.],\n",
      "        [1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "         0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "         1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 1., 1., 0., 1., 1., 0.]], device='cuda:1')]\n"
     ]
    }
   ],
   "source": [
    "print(cp_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000, -0.0470, -0.0452,  ..., -0.0340, -0.0000, -0.0000],\n",
       "        [-0.0000,  0.0000, -0.0345,  ..., -0.0000, -0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0454,  ..., -0.0000, -0.0000, -0.0000],\n",
       "        ...,\n",
       "        [-0.0000,  0.0402, -0.0000,  ...,  0.0000, -0.0000, -0.0539],\n",
       "        [ 0.0000,  0.0544, -0.0000,  ...,  0.0360, -0.0329, -0.0534],\n",
       "        [ 0.0302, -0.0513, -0.0000,  ...,  0.0000, -0.0000, -0.0000]],\n",
       "       device='cuda:1', requires_grad=True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0000, -0.0470, -0.0452,  ..., -0.0340, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000, -0.0345,  ..., -0.0000, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0454,  ..., -0.0000, -0.0000, -0.0000],\n",
      "        ...,\n",
      "        [-0.0000,  0.0402, -0.0000,  ...,  0.0000, -0.0000, -0.0539],\n",
      "        [ 0.0000,  0.0544, -0.0000,  ...,  0.0360, -0.0329, -0.0534],\n",
      "        [ 0.0302, -0.0513, -0.0000,  ...,  0.0000, -0.0000, -0.0000]],\n",
      "       device='cuda:1', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, p in model.named_parameters():\n",
    "    if name =='fc2.weight':\n",
    "        print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[0., 1., 1.,  ..., 0., 1., 0.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.]], device='cuda:1')\n",
      "1\n",
      "tensor([[1., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 0.,  ..., 1., 1., 1.]], device='cuda:1')\n",
      "2\n",
      "tensor([[0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
      "         1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
      "         0., 0., 1., 1., 0., 1., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
      "         1., 0., 0., 1., 1., 0., 0., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "         0., 1., 0., 1., 1., 0., 1., 0., 1., 0.],\n",
      "        [1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
      "         1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
      "         1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 1., 0.],\n",
      "        [0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
      "         1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 1., 0., 1.]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "for name, p in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(j)\n",
    "        #p.register_hook(lambda grad: grad.mul_(cp_mask[j]))\n",
    "        #print(name)\n",
    "        #print(p[0], cp_mask[j][0])\n",
    "        print(cp_mask[j])\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "tensor([[[ 0.1292,  0.1685,  0.0939],\n",
      "         [ 0.0260,  0.1417, -0.0091],\n",
      "         [-0.1594,  0.1055,  0.1800]],\n",
      "\n",
      "        [[-0.1203, -0.1542, -0.1689],\n",
      "         [ 0.1283,  0.1091,  0.0083],\n",
      "         [ 0.1156,  0.0912, -0.0722]],\n",
      "\n",
      "        [[-0.0412, -0.0720, -0.1287],\n",
      "         [ 0.1364,  0.0321,  0.0139],\n",
      "         [-0.0617, -0.0857, -0.0816]]], device='cuda:1',\n",
      "       grad_fn=<SelectBackward>) tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]], device='cuda:1')\n",
      "conv2.weight\n",
      "tensor([[[-0.0229, -0.0392, -0.0019],\n",
      "         [ 0.0119, -0.0340, -0.0226],\n",
      "         [-0.0115, -0.0108,  0.0122]],\n",
      "\n",
      "        [[ 0.0262, -0.0175,  0.0302],\n",
      "         [-0.0141,  0.0091,  0.0263],\n",
      "         [-0.0217,  0.0252,  0.0401]],\n",
      "\n",
      "        [[ 0.0354, -0.0386,  0.0379],\n",
      "         [-0.0086,  0.0101, -0.0118],\n",
      "         [ 0.0361,  0.0321,  0.0376]],\n",
      "\n",
      "        [[ 0.0175,  0.0268,  0.0070],\n",
      "         [ 0.0292, -0.0244,  0.0021],\n",
      "         [-0.0070, -0.0061, -0.0411]],\n",
      "\n",
      "        [[ 0.0069,  0.0108,  0.0200],\n",
      "         [-0.0108,  0.0058, -0.0016],\n",
      "         [-0.0212, -0.0386, -0.0283]],\n",
      "\n",
      "        [[ 0.0063, -0.0208, -0.0343],\n",
      "         [-0.0195,  0.0355, -0.0410],\n",
      "         [-0.0177, -0.0372, -0.0118]],\n",
      "\n",
      "        [[-0.0130,  0.0134, -0.0035],\n",
      "         [ 0.0287,  0.0313, -0.0130],\n",
      "         [ 0.0344, -0.0201,  0.0044]],\n",
      "\n",
      "        [[ 0.0201, -0.0256,  0.0337],\n",
      "         [ 0.0215,  0.0230, -0.0107],\n",
      "         [-0.0061, -0.0110, -0.0404]],\n",
      "\n",
      "        [[-0.0323, -0.0071, -0.0282],\n",
      "         [ 0.0133, -0.0284, -0.0018],\n",
      "         [-0.0382,  0.0095, -0.0121]],\n",
      "\n",
      "        [[-0.0366, -0.0179, -0.0142],\n",
      "         [-0.0071,  0.0082,  0.0020],\n",
      "         [-0.0406,  0.0128, -0.0218]],\n",
      "\n",
      "        [[-0.0416,  0.0038, -0.0412],\n",
      "         [-0.0320, -0.0212, -0.0266],\n",
      "         [ 0.0346, -0.0217, -0.0190]],\n",
      "\n",
      "        [[-0.0204, -0.0400,  0.0360],\n",
      "         [ 0.0101,  0.0319, -0.0153],\n",
      "         [-0.0213,  0.0166, -0.0134]],\n",
      "\n",
      "        [[ 0.0007,  0.0354, -0.0349],\n",
      "         [-0.0044, -0.0033, -0.0171],\n",
      "         [ 0.0033, -0.0279, -0.0250]],\n",
      "\n",
      "        [[-0.0033, -0.0051,  0.0345],\n",
      "         [ 0.0414,  0.0227, -0.0408],\n",
      "         [ 0.0265,  0.0300, -0.0137]],\n",
      "\n",
      "        [[-0.0089,  0.0096, -0.0278],\n",
      "         [ 0.0302,  0.0403, -0.0021],\n",
      "         [-0.0171, -0.0221, -0.0172]],\n",
      "\n",
      "        [[-0.0085,  0.0050, -0.0231],\n",
      "         [-0.0130, -0.0009,  0.0407],\n",
      "         [-0.0143, -0.0081,  0.0099]],\n",
      "\n",
      "        [[-0.0371,  0.0124,  0.0023],\n",
      "         [ 0.0092,  0.0384,  0.0140],\n",
      "         [ 0.0116,  0.0209,  0.0363]],\n",
      "\n",
      "        [[ 0.0305,  0.0272, -0.0313],\n",
      "         [-0.0166, -0.0054, -0.0010],\n",
      "         [-0.0123,  0.0220,  0.0407]],\n",
      "\n",
      "        [[-0.0037, -0.0178,  0.0105],\n",
      "         [-0.0181, -0.0139, -0.0230],\n",
      "         [ 0.0349,  0.0134,  0.0113]],\n",
      "\n",
      "        [[ 0.0013,  0.0118, -0.0226],\n",
      "         [ 0.0296, -0.0236, -0.0125],\n",
      "         [-0.0340, -0.0068,  0.0093]],\n",
      "\n",
      "        [[ 0.0160,  0.0312, -0.0410],\n",
      "         [-0.0244, -0.0150,  0.0258],\n",
      "         [-0.0067, -0.0320, -0.0157]],\n",
      "\n",
      "        [[-0.0406,  0.0301, -0.0229],\n",
      "         [ 0.0368, -0.0009, -0.0289],\n",
      "         [ 0.0092, -0.0347, -0.0161]],\n",
      "\n",
      "        [[-0.0146,  0.0104, -0.0098],\n",
      "         [ 0.0117,  0.0023,  0.0046],\n",
      "         [-0.0059,  0.0062,  0.0381]],\n",
      "\n",
      "        [[ 0.0095, -0.0136,  0.0412],\n",
      "         [ 0.0308,  0.0370, -0.0352],\n",
      "         [-0.0382, -0.0330,  0.0156]],\n",
      "\n",
      "        [[-0.0305,  0.0363, -0.0244],\n",
      "         [-0.0310, -0.0149,  0.0027],\n",
      "         [-0.0372, -0.0056,  0.0109]],\n",
      "\n",
      "        [[-0.0359, -0.0149, -0.0304],\n",
      "         [ 0.0126, -0.0185, -0.0296],\n",
      "         [-0.0269,  0.0137,  0.0176]],\n",
      "\n",
      "        [[ 0.0197, -0.0110, -0.0119],\n",
      "         [ 0.0354,  0.0396,  0.0158],\n",
      "         [ 0.0204, -0.0128,  0.0378]],\n",
      "\n",
      "        [[ 0.0171, -0.0287,  0.0183],\n",
      "         [-0.0091, -0.0032,  0.0139],\n",
      "         [ 0.0039, -0.0270,  0.0328]],\n",
      "\n",
      "        [[-0.0051,  0.0339,  0.0179],\n",
      "         [-0.0018,  0.0157,  0.0058],\n",
      "         [-0.0153, -0.0223, -0.0172]],\n",
      "\n",
      "        [[ 0.0071, -0.0414,  0.0025],\n",
      "         [-0.0076, -0.0285, -0.0362],\n",
      "         [-0.0328,  0.0199, -0.0243]],\n",
      "\n",
      "        [[ 0.0329, -0.0122, -0.0282],\n",
      "         [-0.0004,  0.0066, -0.0297],\n",
      "         [-0.0412,  0.0374, -0.0006]],\n",
      "\n",
      "        [[-0.0184,  0.0124, -0.0361],\n",
      "         [ 0.0321,  0.0368, -0.0232],\n",
      "         [-0.0407, -0.0234, -0.0205]],\n",
      "\n",
      "        [[-0.0018, -0.0304, -0.0253],\n",
      "         [-0.0374, -0.0190,  0.0347],\n",
      "         [ 0.0043,  0.0074, -0.0015]],\n",
      "\n",
      "        [[ 0.0078, -0.0405,  0.0056],\n",
      "         [ 0.0295,  0.0121, -0.0035],\n",
      "         [ 0.0358,  0.0345,  0.0045]],\n",
      "\n",
      "        [[-0.0226, -0.0062,  0.0248],\n",
      "         [ 0.0087, -0.0171, -0.0003],\n",
      "         [ 0.0050,  0.0287, -0.0040]],\n",
      "\n",
      "        [[-0.0112,  0.0329, -0.0360],\n",
      "         [-0.0189, -0.0149, -0.0124],\n",
      "         [-0.0129, -0.0046,  0.0354]],\n",
      "\n",
      "        [[-0.0417,  0.0128, -0.0334],\n",
      "         [-0.0135,  0.0171, -0.0166],\n",
      "         [-0.0214, -0.0219,  0.0223]],\n",
      "\n",
      "        [[-0.0228, -0.0261,  0.0110],\n",
      "         [ 0.0027, -0.0381,  0.0080],\n",
      "         [-0.0257, -0.0060, -0.0156]],\n",
      "\n",
      "        [[ 0.0320,  0.0348,  0.0337],\n",
      "         [ 0.0013,  0.0149, -0.0235],\n",
      "         [ 0.0102, -0.0170,  0.0045]],\n",
      "\n",
      "        [[-0.0362, -0.0330,  0.0412],\n",
      "         [-0.0143,  0.0142, -0.0197],\n",
      "         [-0.0177, -0.0374, -0.0024]],\n",
      "\n",
      "        [[ 0.0320,  0.0054, -0.0411],\n",
      "         [-0.0365,  0.0027, -0.0046],\n",
      "         [-0.0263,  0.0387,  0.0066]],\n",
      "\n",
      "        [[ 0.0197,  0.0045, -0.0331],\n",
      "         [-0.0052,  0.0304, -0.0138],\n",
      "         [-0.0079, -0.0274, -0.0090]],\n",
      "\n",
      "        [[ 0.0061,  0.0334,  0.0090],\n",
      "         [-0.0387,  0.0067,  0.0094],\n",
      "         [ 0.0373,  0.0292,  0.0409]],\n",
      "\n",
      "        [[ 0.0302,  0.0179, -0.0091],\n",
      "         [-0.0064, -0.0415,  0.0262],\n",
      "         [-0.0040,  0.0332,  0.0028]],\n",
      "\n",
      "        [[ 0.0146,  0.0028, -0.0283],\n",
      "         [-0.0352, -0.0297,  0.0245],\n",
      "         [-0.0348,  0.0355, -0.0168]],\n",
      "\n",
      "        [[-0.0200, -0.0218,  0.0361],\n",
      "         [ 0.0008,  0.0214, -0.0024],\n",
      "         [ 0.0241,  0.0390, -0.0169]],\n",
      "\n",
      "        [[ 0.0401,  0.0177,  0.0314],\n",
      "         [ 0.0101, -0.0357,  0.0118],\n",
      "         [ 0.0142,  0.0143,  0.0345]],\n",
      "\n",
      "        [[ 0.0292,  0.0370, -0.0119],\n",
      "         [ 0.0147, -0.0137, -0.0386],\n",
      "         [ 0.0036,  0.0334, -0.0287]],\n",
      "\n",
      "        [[ 0.0284, -0.0372, -0.0088],\n",
      "         [-0.0011,  0.0115,  0.0294],\n",
      "         [-0.0055,  0.0151,  0.0344]],\n",
      "\n",
      "        [[-0.0352, -0.0361,  0.0381],\n",
      "         [ 0.0156, -0.0209, -0.0185],\n",
      "         [-0.0215,  0.0024,  0.0179]],\n",
      "\n",
      "        [[-0.0206, -0.0032, -0.0198],\n",
      "         [-0.0357, -0.0105, -0.0248],\n",
      "         [ 0.0283, -0.0139,  0.0022]],\n",
      "\n",
      "        [[-0.0038, -0.0373, -0.0297],\n",
      "         [ 0.0073, -0.0175, -0.0036],\n",
      "         [ 0.0183, -0.0033, -0.0153]],\n",
      "\n",
      "        [[-0.0386,  0.0376, -0.0077],\n",
      "         [ 0.0122,  0.0196, -0.0148],\n",
      "         [-0.0169, -0.0286,  0.0029]],\n",
      "\n",
      "        [[-0.0084, -0.0093, -0.0323],\n",
      "         [-0.0015,  0.0094, -0.0335],\n",
      "         [-0.0336, -0.0291, -0.0179]],\n",
      "\n",
      "        [[-0.0128,  0.0147, -0.0334],\n",
      "         [ 0.0057,  0.0066,  0.0224],\n",
      "         [ 0.0136,  0.0203,  0.0045]],\n",
      "\n",
      "        [[ 0.0190,  0.0393, -0.0268],\n",
      "         [-0.0234,  0.0416, -0.0080],\n",
      "         [-0.0352, -0.0361,  0.0261]],\n",
      "\n",
      "        [[-0.0006, -0.0210, -0.0338],\n",
      "         [ 0.0073, -0.0310, -0.0193],\n",
      "         [ 0.0163,  0.0243, -0.0144]],\n",
      "\n",
      "        [[ 0.0141,  0.0345, -0.0114],\n",
      "         [ 0.0102,  0.0252, -0.0399],\n",
      "         [-0.0138, -0.0388, -0.0095]],\n",
      "\n",
      "        [[-0.0320,  0.0297,  0.0319],\n",
      "         [-0.0365, -0.0269, -0.0212],\n",
      "         [ 0.0077, -0.0058,  0.0200]],\n",
      "\n",
      "        [[-0.0284, -0.0375,  0.0053],\n",
      "         [ 0.0224,  0.0085,  0.0105],\n",
      "         [ 0.0014, -0.0308, -0.0333]],\n",
      "\n",
      "        [[-0.0352,  0.0243,  0.0271],\n",
      "         [ 0.0037,  0.0319, -0.0205],\n",
      "         [-0.0360,  0.0355,  0.0316]],\n",
      "\n",
      "        [[-0.0217, -0.0290, -0.0280],\n",
      "         [-0.0274, -0.0181,  0.0211],\n",
      "         [-0.0137,  0.0082,  0.0048]],\n",
      "\n",
      "        [[ 0.0258, -0.0305, -0.0175],\n",
      "         [-0.0162, -0.0173,  0.0246],\n",
      "         [-0.0108, -0.0144,  0.0241]],\n",
      "\n",
      "        [[ 0.0395,  0.0357, -0.0134],\n",
      "         [-0.0395, -0.0079, -0.0327],\n",
      "         [-0.0028,  0.0077, -0.0095]]], device='cuda:1',\n",
      "       grad_fn=<SelectBackward>) tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]], device='cuda:1')\n",
      "conv3.weight\n",
      "tensor([[[ 1.6734e-02, -2.5660e-03, -3.1099e-02],\n",
      "         [ 1.8047e-02, -3.1531e-02,  4.7285e-03],\n",
      "         [ 1.5785e-02,  1.4123e-02, -1.2999e-02]],\n",
      "\n",
      "        [[ 3.8152e-02,  1.0525e-02,  1.1439e-02],\n",
      "         [-1.6884e-02, -5.5244e-03, -1.2339e-02],\n",
      "         [ 4.0195e-02,  3.7767e-02,  1.7668e-02]],\n",
      "\n",
      "        [[ 1.0388e-02,  2.0975e-02,  7.8803e-03],\n",
      "         [-1.7235e-02, -4.0629e-02,  3.9224e-02],\n",
      "         [-2.4277e-02,  3.4180e-02,  2.7356e-02]],\n",
      "\n",
      "        [[-3.9908e-02, -4.0198e-02, -1.8350e-02],\n",
      "         [ 1.3115e-02, -1.4908e-02, -1.6274e-02],\n",
      "         [-1.9917e-02,  4.5749e-03, -1.4967e-02]],\n",
      "\n",
      "        [[ 1.1670e-02, -2.7749e-02, -8.8128e-03],\n",
      "         [ 3.8670e-02, -3.2861e-02,  3.2968e-02],\n",
      "         [-2.4644e-02, -2.6355e-02, -1.1428e-02]],\n",
      "\n",
      "        [[-2.8052e-03,  1.2966e-02,  1.2739e-02],\n",
      "         [-1.8192e-02, -1.8207e-02,  7.1700e-03],\n",
      "         [ 6.8681e-03, -3.4242e-02,  3.2540e-02]],\n",
      "\n",
      "        [[ 4.3609e-03,  3.5061e-03, -1.1440e-02],\n",
      "         [ 6.4264e-03, -4.2102e-03,  4.0326e-02],\n",
      "         [-2.6352e-02, -3.4367e-02, -7.3771e-03]],\n",
      "\n",
      "        [[-3.6095e-02,  1.9568e-02,  2.5651e-02],\n",
      "         [ 3.8638e-02, -1.7301e-03, -2.1148e-02],\n",
      "         [ 4.1242e-02,  1.8489e-02,  3.2186e-02]],\n",
      "\n",
      "        [[-3.0271e-02, -7.2153e-03,  2.5299e-02],\n",
      "         [-1.5215e-02,  2.7639e-02,  1.4063e-02],\n",
      "         [ 3.9321e-02,  1.5485e-02, -3.5764e-02]],\n",
      "\n",
      "        [[ 3.9539e-03, -2.2471e-03,  4.1345e-02],\n",
      "         [ 3.1807e-03, -8.1931e-04, -2.1074e-02],\n",
      "         [-3.4966e-02, -3.9481e-02,  4.0977e-02]],\n",
      "\n",
      "        [[-3.8812e-02, -3.4203e-02, -3.6466e-02],\n",
      "         [ 2.8966e-02,  4.0589e-02, -9.9433e-03],\n",
      "         [-3.8075e-02,  6.9652e-03, -3.8217e-02]],\n",
      "\n",
      "        [[ 1.2903e-02,  2.0002e-02, -2.6200e-03],\n",
      "         [ 5.6519e-03, -2.7861e-02,  1.6498e-02],\n",
      "         [-3.0079e-04,  1.6931e-02, -1.9456e-02]],\n",
      "\n",
      "        [[ 3.8013e-02,  2.0435e-02,  2.9736e-02],\n",
      "         [ 2.4484e-02, -6.3658e-04,  3.4345e-02],\n",
      "         [-8.7191e-03, -2.8814e-02,  5.0292e-03]],\n",
      "\n",
      "        [[ 7.1347e-03, -2.0739e-02,  1.6403e-02],\n",
      "         [-1.6124e-02, -6.9023e-03, -6.7163e-03],\n",
      "         [ 1.6239e-03, -9.7391e-03,  6.6851e-03]],\n",
      "\n",
      "        [[ 3.4814e-02,  2.0097e-02,  3.3146e-02],\n",
      "         [-1.2026e-02, -2.1699e-02, -3.0999e-02],\n",
      "         [-1.4594e-02, -2.8887e-02, -1.2512e-02]],\n",
      "\n",
      "        [[ 1.0453e-02, -2.2831e-02, -1.4390e-02],\n",
      "         [ 4.0195e-02,  1.6097e-03,  4.3098e-03],\n",
      "         [-1.1949e-03,  2.1533e-02, -2.2687e-02]],\n",
      "\n",
      "        [[ 6.2647e-04,  3.6366e-02, -3.0371e-02],\n",
      "         [-3.8975e-02, -2.7340e-02,  4.1205e-02],\n",
      "         [ 2.1948e-02, -2.2371e-03, -2.3718e-02]],\n",
      "\n",
      "        [[ 1.9549e-02,  1.5333e-02, -2.0247e-02],\n",
      "         [ 1.6739e-02,  8.5610e-03, -3.1690e-03],\n",
      "         [-2.9496e-02, -1.3749e-02, -3.4809e-02]],\n",
      "\n",
      "        [[ 1.0212e-02, -3.6733e-02,  6.8641e-03],\n",
      "         [ 5.7730e-03,  3.3691e-02,  3.8022e-02],\n",
      "         [ 8.2808e-03,  3.8035e-02,  3.5680e-02]],\n",
      "\n",
      "        [[-1.8307e-02,  3.6146e-02,  8.9843e-03],\n",
      "         [ 7.9689e-03,  3.5596e-02, -9.0262e-03],\n",
      "         [-2.7267e-03, -1.6603e-02,  4.0824e-03]],\n",
      "\n",
      "        [[-9.7574e-03, -5.2697e-04, -2.9235e-02],\n",
      "         [-2.0484e-02,  2.5840e-02,  2.5040e-03],\n",
      "         [ 2.5114e-02, -1.4783e-03, -7.6085e-03]],\n",
      "\n",
      "        [[-2.2583e-02,  2.8980e-02,  4.0880e-02],\n",
      "         [ 3.4195e-02, -6.0653e-03,  1.6087e-02],\n",
      "         [ 2.7653e-02, -4.3684e-03,  1.2253e-02]],\n",
      "\n",
      "        [[-8.6570e-03, -1.2887e-02, -1.4586e-03],\n",
      "         [ 2.8611e-02,  2.8619e-02, -2.6022e-02],\n",
      "         [-3.3472e-02, -1.1920e-03,  7.8798e-03]],\n",
      "\n",
      "        [[-2.5455e-02, -3.6882e-02, -1.4245e-02],\n",
      "         [-2.7062e-02, -6.6159e-03, -2.4623e-02],\n",
      "         [ 2.8340e-02,  2.5390e-02,  1.7219e-02]],\n",
      "\n",
      "        [[ 2.0590e-02, -1.4360e-02, -2.5034e-02],\n",
      "         [ 6.0935e-03, -3.0299e-02, -5.1968e-04],\n",
      "         [ 6.7069e-03, -2.9876e-02, -5.7279e-03]],\n",
      "\n",
      "        [[-2.0855e-02,  2.0175e-02, -4.0734e-02],\n",
      "         [-1.5252e-02, -1.4015e-02, -2.2945e-02],\n",
      "         [ 6.3252e-03, -3.0394e-02, -2.3495e-02]],\n",
      "\n",
      "        [[-2.4285e-02,  2.2141e-02, -1.0683e-02],\n",
      "         [ 1.6078e-03,  1.9961e-02,  1.5279e-02],\n",
      "         [ 8.9675e-03, -3.3373e-02, -3.8317e-02]],\n",
      "\n",
      "        [[ 2.7785e-02, -8.0616e-04,  3.0922e-02],\n",
      "         [ 1.0538e-02,  3.2623e-03,  1.2661e-03],\n",
      "         [-3.5105e-03,  3.2168e-02,  3.4218e-02]],\n",
      "\n",
      "        [[ 1.9960e-02, -1.9821e-02, -3.0147e-03],\n",
      "         [-1.5867e-02,  1.7085e-02, -1.0465e-02],\n",
      "         [-2.3288e-02,  2.1803e-02,  3.5438e-02]],\n",
      "\n",
      "        [[-3.6808e-02, -3.3464e-02, -3.0057e-02],\n",
      "         [-2.8499e-02, -2.7519e-03, -2.4002e-02],\n",
      "         [-1.8864e-02,  1.8093e-02, -3.5766e-03]],\n",
      "\n",
      "        [[ 2.4148e-02,  3.0591e-02,  5.2145e-03],\n",
      "         [ 8.8083e-03, -1.0727e-02,  2.0524e-02],\n",
      "         [ 1.2237e-02,  7.8414e-03,  1.0353e-03]],\n",
      "\n",
      "        [[ 1.0322e-02, -3.6120e-02, -2.5936e-02],\n",
      "         [ 6.4916e-03,  3.1492e-02,  4.0610e-02],\n",
      "         [ 2.9701e-02,  4.1388e-04,  6.7509e-03]],\n",
      "\n",
      "        [[ 9.4838e-03,  2.9129e-02, -2.9490e-02],\n",
      "         [-2.3988e-02,  1.2937e-03,  3.2002e-02],\n",
      "         [ 1.1465e-02,  3.5292e-02, -4.1657e-02]],\n",
      "\n",
      "        [[-1.2716e-02,  6.7553e-04, -1.4255e-02],\n",
      "         [-2.0789e-02,  3.5971e-02,  3.4888e-02],\n",
      "         [ 7.5259e-03,  3.4132e-02, -1.8505e-02]],\n",
      "\n",
      "        [[-1.6615e-02, -3.3311e-03, -5.5145e-05],\n",
      "         [ 3.4025e-02, -1.3672e-02,  1.0031e-02],\n",
      "         [ 6.8171e-03, -2.3968e-02, -2.3372e-02]],\n",
      "\n",
      "        [[ 3.1675e-02, -2.1811e-02, -3.1075e-02],\n",
      "         [ 3.9038e-02, -4.5594e-03,  3.2682e-02],\n",
      "         [ 2.6064e-02,  3.5732e-02, -3.2669e-02]],\n",
      "\n",
      "        [[ 4.7642e-03,  4.0773e-02, -1.9553e-02],\n",
      "         [ 1.1350e-02,  2.6818e-02, -3.2520e-02],\n",
      "         [ 6.3012e-03,  3.2435e-02,  9.6430e-03]],\n",
      "\n",
      "        [[ 4.0830e-02, -2.9497e-04, -1.8047e-02],\n",
      "         [-1.6107e-02, -1.5258e-02, -2.9971e-02],\n",
      "         [ 1.5916e-02, -3.5227e-02,  3.4533e-02]],\n",
      "\n",
      "        [[-3.9425e-02,  5.2301e-03,  2.5317e-02],\n",
      "         [ 5.4509e-03,  1.2977e-02, -1.8972e-02],\n",
      "         [ 4.0680e-02,  3.4382e-02, -2.8477e-02]],\n",
      "\n",
      "        [[-2.3161e-04,  1.7187e-02, -4.0644e-02],\n",
      "         [-4.9223e-03, -3.5324e-02,  8.9393e-03],\n",
      "         [ 2.8554e-02,  2.7982e-02, -2.4542e-02]],\n",
      "\n",
      "        [[-2.1443e-02, -3.4857e-02, -2.3403e-02],\n",
      "         [ 2.6130e-02, -2.2821e-02, -2.6888e-02],\n",
      "         [-3.7733e-02,  1.5056e-03, -1.0454e-02]],\n",
      "\n",
      "        [[-3.2210e-02, -2.7777e-02,  3.9935e-03],\n",
      "         [-3.7547e-02, -1.9896e-02, -3.7579e-02],\n",
      "         [-4.3584e-03,  2.9330e-02, -2.9460e-02]],\n",
      "\n",
      "        [[ 3.0084e-02, -1.9371e-02, -3.5411e-02],\n",
      "         [-1.1470e-02, -8.4886e-03,  3.0722e-02],\n",
      "         [-1.1563e-03, -1.6641e-02, -7.9382e-03]],\n",
      "\n",
      "        [[ 1.3672e-02,  2.1095e-02, -6.8688e-03],\n",
      "         [-3.3863e-02, -7.0981e-03,  1.5084e-02],\n",
      "         [-2.3591e-02, -1.2273e-02,  3.9705e-02]],\n",
      "\n",
      "        [[ 4.1224e-02, -3.9014e-02,  1.4642e-02],\n",
      "         [-2.3174e-02,  1.7915e-02,  3.5107e-02],\n",
      "         [-2.0506e-02, -7.9060e-03, -2.2960e-02]],\n",
      "\n",
      "        [[ 1.3025e-02, -1.2716e-02,  3.6476e-02],\n",
      "         [ 2.3943e-02, -3.0602e-02,  2.8954e-02],\n",
      "         [ 1.1539e-02,  3.7310e-02, -3.6147e-02]],\n",
      "\n",
      "        [[-1.4932e-02, -2.8272e-02, -2.6666e-02],\n",
      "         [-1.8046e-02, -1.7591e-02, -1.7189e-02],\n",
      "         [ 3.6754e-02, -1.6653e-02, -8.3292e-03]],\n",
      "\n",
      "        [[-2.1637e-03,  1.1914e-02, -1.2378e-02],\n",
      "         [-3.5862e-02, -3.6459e-02,  2.5521e-02],\n",
      "         [-8.7723e-03, -3.5929e-03,  1.2230e-03]],\n",
      "\n",
      "        [[-3.2535e-02,  3.3951e-02, -3.4070e-02],\n",
      "         [ 3.2864e-03,  3.2525e-02, -3.6165e-02],\n",
      "         [ 4.0954e-03, -2.7700e-02,  3.1855e-02]],\n",
      "\n",
      "        [[ 2.0745e-02, -1.1632e-02, -2.7204e-03],\n",
      "         [-2.8967e-02,  3.9344e-02, -2.9773e-02],\n",
      "         [-2.3208e-02,  7.2899e-03, -3.4015e-02]],\n",
      "\n",
      "        [[ 1.1959e-02, -3.9839e-02, -3.9329e-02],\n",
      "         [-2.1599e-02,  2.2992e-02, -9.9331e-03],\n",
      "         [ 3.0920e-03, -2.2642e-02,  9.0407e-04]],\n",
      "\n",
      "        [[ 2.3905e-02,  2.5891e-02,  3.2354e-02],\n",
      "         [-1.2105e-02,  2.0703e-02,  2.0816e-02],\n",
      "         [-2.9350e-02,  2.5038e-02,  2.7503e-02]],\n",
      "\n",
      "        [[ 1.3713e-02, -2.6428e-03, -2.5241e-02],\n",
      "         [-3.4001e-02,  3.5367e-02,  4.1395e-02],\n",
      "         [-3.9172e-02,  1.3047e-02,  3.3305e-02]],\n",
      "\n",
      "        [[ 3.0108e-02,  3.2648e-02, -2.2088e-04],\n",
      "         [ 1.1889e-03,  1.7494e-02,  1.1741e-02],\n",
      "         [-1.3923e-02,  9.5431e-03, -2.6840e-02]],\n",
      "\n",
      "        [[ 9.3205e-03,  2.5591e-02,  3.4885e-02],\n",
      "         [-3.7193e-02,  2.1437e-02, -2.6342e-02],\n",
      "         [-2.1472e-02,  2.1675e-02,  1.4929e-02]],\n",
      "\n",
      "        [[-1.4996e-02, -1.6949e-02, -1.4370e-02],\n",
      "         [-1.1488e-03, -1.6009e-02, -1.3721e-02],\n",
      "         [ 1.3120e-02, -2.2624e-03, -2.9015e-02]],\n",
      "\n",
      "        [[-8.9419e-03,  3.4102e-02, -2.3204e-04],\n",
      "         [-4.8700e-03,  3.2755e-02, -7.6670e-03],\n",
      "         [-1.1305e-02, -3.4534e-02, -7.8662e-03]],\n",
      "\n",
      "        [[ 2.8859e-02, -1.4493e-02,  1.9623e-02],\n",
      "         [ 1.6249e-02,  2.2989e-02, -3.5838e-02],\n",
      "         [-3.8632e-02,  3.5215e-02, -3.7244e-02]],\n",
      "\n",
      "        [[-3.0659e-02, -3.6437e-02, -2.7809e-02],\n",
      "         [-3.9735e-02, -3.3201e-02, -4.3309e-03],\n",
      "         [ 4.7950e-03,  3.4310e-02, -4.1519e-02]],\n",
      "\n",
      "        [[-2.7253e-02,  3.2119e-02, -3.2085e-02],\n",
      "         [ 5.5993e-03, -2.4614e-02, -4.0009e-02],\n",
      "         [ 1.1717e-02, -3.9515e-02,  1.9874e-02]],\n",
      "\n",
      "        [[-1.4216e-02, -1.3754e-02, -2.9585e-02],\n",
      "         [-1.7109e-02,  2.9286e-02,  2.7472e-02],\n",
      "         [-4.0122e-02,  5.4885e-03, -8.5759e-03]],\n",
      "\n",
      "        [[ 2.9331e-02,  3.3986e-02, -1.8017e-02],\n",
      "         [-2.1166e-02,  3.4231e-02, -1.7899e-02],\n",
      "         [ 3.9185e-02,  3.5607e-02,  2.7913e-03]],\n",
      "\n",
      "        [[-2.0908e-02,  4.1006e-02,  2.4061e-02],\n",
      "         [ 1.7951e-02,  1.6805e-02,  3.8484e-02],\n",
      "         [-1.2983e-02, -3.1882e-02,  3.7589e-02]],\n",
      "\n",
      "        [[-2.9316e-02, -2.5492e-02,  4.0484e-02],\n",
      "         [ 1.4098e-02, -3.8585e-02, -1.2730e-02],\n",
      "         [ 2.2780e-02,  3.2000e-02,  2.9357e-02]]], device='cuda:1',\n",
      "       grad_fn=<SelectBackward>) tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]], device='cuda:1')\n",
      "conv4.weight\n",
      "tensor([[[ 0.0028,  0.0270, -0.0072],\n",
      "         [ 0.0222, -0.0148,  0.0102],\n",
      "         [-0.0274,  0.0074, -0.0194]],\n",
      "\n",
      "        [[ 0.0057, -0.0281,  0.0237],\n",
      "         [ 0.0227,  0.0052, -0.0282],\n",
      "         [-0.0195, -0.0118, -0.0167]],\n",
      "\n",
      "        [[ 0.0173, -0.0190, -0.0091],\n",
      "         [-0.0270,  0.0062,  0.0208],\n",
      "         [-0.0254,  0.0183,  0.0082]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0190, -0.0217, -0.0133],\n",
      "         [ 0.0050, -0.0184, -0.0230],\n",
      "         [-0.0280, -0.0005,  0.0141]],\n",
      "\n",
      "        [[-0.0197, -0.0098, -0.0014],\n",
      "         [ 0.0269, -0.0245,  0.0004],\n",
      "         [-0.0239,  0.0007,  0.0070]],\n",
      "\n",
      "        [[ 0.0035, -0.0216, -0.0290],\n",
      "         [-0.0020,  0.0067, -0.0010],\n",
      "         [ 0.0239, -0.0277, -0.0069]]], device='cuda:1',\n",
      "       grad_fn=<SelectBackward>) tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]], device='cuda:1')\n",
      "conv5.weight\n",
      "tensor([[[-1.2016e-02, -1.4507e-03, -2.3181e-02],\n",
      "         [ 1.1656e-03,  1.5498e-02, -5.8093e-03],\n",
      "         [ 1.5733e-02,  2.5823e-02,  1.5401e-02]],\n",
      "\n",
      "        [[ 1.9216e-02, -1.6412e-02,  2.3946e-02],\n",
      "         [-7.4186e-03,  2.2209e-03, -1.3817e-02],\n",
      "         [ 1.2253e-02,  2.0699e-02,  4.0323e-03]],\n",
      "\n",
      "        [[-1.2841e-02,  2.8661e-02,  2.7112e-02],\n",
      "         [-2.9431e-02, -1.4246e-03, -1.0743e-02],\n",
      "         [-1.7480e-02,  2.3809e-02, -1.8044e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.1411e-02, -7.5597e-03,  2.1980e-02],\n",
      "         [ 2.5489e-02, -2.5378e-02,  1.4815e-02],\n",
      "         [ 2.4381e-02, -3.3570e-05,  1.2554e-02]],\n",
      "\n",
      "        [[-1.9455e-02, -3.0541e-03,  2.2170e-02],\n",
      "         [-4.0948e-03,  5.7898e-04, -1.7857e-02],\n",
      "         [-5.4698e-04, -1.7797e-02, -2.8705e-03]],\n",
      "\n",
      "        [[ 2.6522e-02, -2.2320e-02, -1.7303e-02],\n",
      "         [ 2.0974e-02,  1.3127e-02, -2.1876e-02],\n",
      "         [ 9.3378e-03, -1.1860e-02,  2.4110e-02]]], device='cuda:1',\n",
      "       grad_fn=<SelectBackward>) tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]], device='cuda:1')\n",
      "conv6.weight\n",
      "tensor([[[ 0.0142,  0.0059,  0.0082],\n",
      "         [-0.0060, -0.0153, -0.0074],\n",
      "         [ 0.0125,  0.0084,  0.0128]],\n",
      "\n",
      "        [[-0.0077,  0.0010, -0.0033],\n",
      "         [ 0.0011,  0.0016,  0.0172],\n",
      "         [ 0.0103,  0.0165, -0.0074]],\n",
      "\n",
      "        [[ 0.0018,  0.0033,  0.0196],\n",
      "         [ 0.0092, -0.0130,  0.0073],\n",
      "         [ 0.0164, -0.0200, -0.0129]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0071, -0.0178,  0.0208],\n",
      "         [ 0.0183, -0.0109,  0.0178],\n",
      "         [ 0.0109, -0.0079, -0.0013]],\n",
      "\n",
      "        [[ 0.0038, -0.0048, -0.0078],\n",
      "         [ 0.0028,  0.0100, -0.0141],\n",
      "         [ 0.0064, -0.0012, -0.0036]],\n",
      "\n",
      "        [[-0.0161, -0.0112, -0.0092],\n",
      "         [-0.0201, -0.0003, -0.0162],\n",
      "         [ 0.0038, -0.0035, -0.0035]]], device='cuda:1',\n",
      "       grad_fn=<SelectBackward>) tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]], device='cuda:1')\n",
      "fc1.weight\n",
      "tensor([ 0.0091,  0.0102,  0.0046,  ..., -0.0121,  0.0016, -0.0099],\n",
      "       device='cuda:1', grad_fn=<SelectBackward>) tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:1')\n",
      "fc2.weight\n",
      "tensor([ 0.0551,  0.0433,  0.0307, -0.0368,  0.0016,  0.0330,  0.0388, -0.0443,\n",
      "        -0.0262, -0.0502, -0.0161,  0.0174, -0.0074,  0.0087, -0.0287, -0.0124,\n",
      "        -0.0427, -0.0238, -0.0224, -0.0583, -0.0298, -0.0204, -0.0051,  0.0034,\n",
      "         0.0428, -0.0073,  0.0318,  0.0116, -0.0531, -0.0580,  0.0355, -0.0263,\n",
      "         0.0220, -0.0602, -0.0313,  0.0068,  0.0459,  0.0564,  0.0109, -0.0192,\n",
      "        -0.0371,  0.0234, -0.0014,  0.0280,  0.0278, -0.0504,  0.0462,  0.0572,\n",
      "        -0.0248, -0.0146, -0.0556, -0.0454, -0.0398, -0.0440,  0.0280, -0.0602,\n",
      "         0.0451, -0.0482, -0.0599,  0.0618,  0.0102,  0.0093,  0.0266, -0.0253,\n",
      "         0.0270, -0.0323,  0.0064, -0.0616, -0.0432,  0.0598,  0.0599, -0.0552,\n",
      "        -0.0381, -0.0048,  0.0615,  0.0156, -0.0400,  0.0332,  0.0054,  0.0408,\n",
      "         0.0035, -0.0234,  0.0020, -0.0601, -0.0565,  0.0446, -0.0542,  0.0059,\n",
      "        -0.0306,  0.0187,  0.0352,  0.0179, -0.0081,  0.0232, -0.0447,  0.0338,\n",
      "         0.0244, -0.0458,  0.0547,  0.0181,  0.0266, -0.0055,  0.0270,  0.0417,\n",
      "        -0.0137, -0.0216,  0.0583,  0.0336,  0.0416, -0.0527,  0.0061,  0.0323,\n",
      "        -0.0275, -0.0074,  0.0543, -0.0471, -0.0220,  0.0280,  0.0064,  0.0614,\n",
      "        -0.0391, -0.0461, -0.0019,  0.0609, -0.0218,  0.0162,  0.0301, -0.0616,\n",
      "        -0.0429, -0.0078, -0.0223,  0.0102, -0.0400, -0.0359, -0.0519, -0.0397,\n",
      "        -0.0479, -0.0352, -0.0397,  0.0426,  0.0547,  0.0484, -0.0519,  0.0126,\n",
      "        -0.0287, -0.0528,  0.0202,  0.0104, -0.0365, -0.0210, -0.0268, -0.0360,\n",
      "        -0.0178,  0.0558, -0.0493, -0.0424, -0.0254, -0.0323, -0.0284,  0.0140,\n",
      "        -0.0528, -0.0392,  0.0480, -0.0222, -0.0528, -0.0204, -0.0621, -0.0187,\n",
      "         0.0138, -0.0079,  0.0367,  0.0171, -0.0576, -0.0132,  0.0602,  0.0249,\n",
      "         0.0216, -0.0203, -0.0018, -0.0363,  0.0020,  0.0596,  0.0129,  0.0269,\n",
      "         0.0079,  0.0588, -0.0099,  0.0465,  0.0487,  0.0400, -0.0611, -0.0304,\n",
      "        -0.0359, -0.0385, -0.0110, -0.0382, -0.0533,  0.0061,  0.0334, -0.0188,\n",
      "        -0.0304, -0.0322, -0.0309,  0.0084, -0.0478, -0.0418, -0.0022,  0.0406,\n",
      "         0.0164, -0.0067, -0.0344,  0.0402,  0.0272,  0.0502,  0.0313,  0.0170,\n",
      "        -0.0563, -0.0433, -0.0485, -0.0086,  0.0550,  0.0058,  0.0152,  0.0484,\n",
      "        -0.0586, -0.0535, -0.0046, -0.0314, -0.0184, -0.0203,  0.0323,  0.0408,\n",
      "         0.0562, -0.0289, -0.0379, -0.0032, -0.0604, -0.0599, -0.0555, -0.0431,\n",
      "         0.0297, -0.0251, -0.0587,  0.0289,  0.0271, -0.0363, -0.0526, -0.0101,\n",
      "         0.0366, -0.0513,  0.0152,  0.0291, -0.0505, -0.0466,  0.0082,  0.0225],\n",
      "       device='cuda:1', grad_fn=<SelectBackward>) tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.], device='cuda:1')\n",
      "fc3.weight\n",
      "tensor([ 7.6890e-03, -3.6649e-02,  5.9590e-02, -4.2926e-02,  2.7237e-02,\n",
      "         5.2710e-02, -5.4603e-02,  3.5584e-02,  2.7941e-02, -3.2613e-02,\n",
      "        -5.8250e-02, -4.6459e-02,  9.4622e-03,  4.9968e-02,  5.2741e-02,\n",
      "        -7.3652e-03, -4.9151e-02, -3.7955e-02,  1.2158e-03,  5.4745e-02,\n",
      "        -1.7738e-02, -4.9666e-02,  3.2432e-02,  2.1323e-02, -1.7187e-02,\n",
      "        -3.6458e-02,  5.1150e-02, -6.1049e-03, -1.0101e-02,  1.0226e-02,\n",
      "        -3.4342e-02,  9.4833e-04, -4.0037e-02, -2.6993e-02, -2.1734e-02,\n",
      "        -1.0343e-02,  4.8679e-02, -1.1301e-02, -4.4951e-02, -2.4664e-03,\n",
      "        -1.6751e-02,  4.2464e-02,  5.2488e-02, -8.3954e-03,  5.9923e-02,\n",
      "         5.4813e-02, -3.3796e-02,  4.9552e-02, -2.6248e-02,  3.5886e-02,\n",
      "        -3.0951e-02,  4.2183e-03, -3.6124e-02, -6.3934e-04, -3.2179e-02,\n",
      "        -4.8573e-02, -3.5609e-02, -3.6832e-03,  2.8826e-02,  2.1156e-03,\n",
      "         1.3734e-04, -4.0242e-02,  3.0561e-02,  3.5835e-02,  1.3900e-03,\n",
      "         5.0367e-02,  4.4298e-02,  5.2146e-02, -1.8378e-02, -2.0506e-02,\n",
      "        -4.2609e-02, -3.6456e-02, -3.8613e-02, -1.2653e-02,  3.1574e-02,\n",
      "         1.2124e-03, -5.7803e-02, -4.4135e-02,  2.6971e-02, -3.7627e-02,\n",
      "         3.4280e-02, -1.2124e-02, -3.1103e-02,  2.9689e-02,  6.3315e-03,\n",
      "        -4.4978e-02,  1.2964e-02, -4.9616e-04,  2.0574e-02,  3.0878e-02,\n",
      "        -3.7234e-02,  4.0831e-02,  5.1146e-02, -1.1536e-02, -1.3133e-02,\n",
      "        -5.0238e-02,  4.8414e-02, -2.7973e-02,  5.7297e-02,  4.1277e-03,\n",
      "        -3.0485e-02, -2.1339e-02, -3.3587e-03, -4.6966e-02, -4.7902e-02,\n",
      "         3.2706e-02,  6.7365e-03,  2.4687e-02,  4.1024e-02,  3.2961e-02,\n",
      "        -5.6521e-02,  2.4689e-02,  3.5997e-02, -6.1706e-02,  4.8570e-02,\n",
      "         6.0467e-02, -2.7125e-02,  4.0221e-02,  2.1056e-02,  8.9198e-04,\n",
      "        -5.7183e-02, -4.7279e-02,  2.2876e-02,  2.0316e-02, -6.0155e-02,\n",
      "        -3.7070e-02,  1.3705e-02, -6.7007e-03, -2.7594e-02,  5.3302e-02,\n",
      "         3.0736e-02,  1.7539e-02, -7.4909e-03, -3.7125e-02,  1.4840e-02,\n",
      "         4.5410e-02,  3.7304e-02, -5.8316e-02,  2.9167e-02,  3.5933e-02,\n",
      "         4.5516e-05,  4.3370e-02, -5.4465e-02, -5.8635e-02, -2.6638e-03,\n",
      "         1.2085e-02, -4.4353e-02,  1.7780e-02,  2.8165e-02,  2.3354e-02,\n",
      "        -4.3221e-02, -6.6973e-03,  2.6887e-02,  4.3619e-02, -3.1245e-02,\n",
      "        -1.0859e-02,  4.0054e-02, -3.5586e-02, -3.7115e-02,  5.9401e-03,\n",
      "        -2.2450e-03, -2.2020e-02, -4.6015e-02,  5.6977e-02, -3.6759e-02,\n",
      "        -1.6392e-02,  5.6549e-02,  2.9449e-02,  4.1007e-02,  5.2173e-02,\n",
      "         2.9320e-02, -3.1283e-02, -1.1905e-02,  1.3729e-02,  2.7350e-02,\n",
      "         3.3365e-02,  3.2488e-02,  5.8925e-02,  4.8815e-02, -1.5883e-02,\n",
      "         2.6175e-02, -7.1794e-04,  2.0912e-02,  3.8410e-02, -1.8847e-02,\n",
      "        -1.3168e-02,  4.9705e-02,  3.6469e-02,  3.8061e-02,  4.6797e-02,\n",
      "         4.9893e-02,  3.0479e-02,  3.2981e-03,  2.4908e-02,  6.8775e-03,\n",
      "         2.0704e-02,  3.0280e-02, -3.7123e-03,  4.9393e-02,  2.4270e-04,\n",
      "        -1.7000e-02,  4.3215e-02,  1.1996e-02, -1.8140e-02, -5.8204e-02,\n",
      "         5.4902e-02,  6.8356e-03, -5.1845e-02, -6.2011e-02, -9.7137e-03,\n",
      "        -1.5128e-02, -1.6150e-02, -4.1919e-02,  2.5920e-02, -1.2524e-02,\n",
      "        -3.9993e-02, -2.9169e-02,  3.7464e-02,  6.1591e-02, -8.6735e-03,\n",
      "         9.3563e-03, -2.3211e-02,  2.8340e-02,  2.3612e-02, -4.2650e-02,\n",
      "        -2.4926e-03, -2.6484e-02, -4.0729e-02, -5.5216e-02,  3.3929e-02,\n",
      "        -3.2489e-02,  5.5096e-02,  5.6050e-02,  2.9803e-02,  4.2099e-02,\n",
      "        -4.7161e-02, -5.6837e-02,  3.7843e-02, -5.3196e-02, -5.9665e-02,\n",
      "         2.9696e-02, -5.6538e-02,  2.7176e-02, -3.0918e-02, -4.0334e-02,\n",
      "        -6.0490e-02,  3.8251e-02,  4.6406e-02, -2.7976e-02,  1.2265e-02,\n",
      "         4.1669e-02, -4.0490e-02,  5.2583e-02,  9.0696e-03,  5.9778e-02,\n",
      "         6.0885e-02], device='cuda:1', grad_fn=<SelectBackward>) tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.], device='cuda:1')\n",
      "Learning start!\n",
      "\n",
      "   Layer                     Weight                Ratio(%)\n",
      "all.weight   :       2261184 (2261184 | 0)         100.00\n",
      "conv1.weight :          1728 (1728 | 0)            100.00\n",
      "conv2.weight :         36864 (36864 | 0)           100.00\n",
      "conv3.weight :         73728 (73728 | 0)           100.00\n",
      "conv4.weight :        147456 (147456 | 0)          100.00\n",
      "conv5.weight :        294912 (294912 | 0)          100.00\n",
      "conv6.weight :        589824 (589824 | 0)          100.00\n",
      "fc1.weight   :       1048576 (1048576 | 0)         100.00\n",
      "fc2.weight   :         65536 (65536 | 0)           100.00\n",
      "fc3.weight   :          2560 (2560 | 0)            100.00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273c4b74612140099d72a82976a680d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch : 0] (r_loss: x.xxxxx) (t_loss: x.xxxxx) (accu: 0.1000)\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f410194d0467>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m              )\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# model training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcp_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# val_set이 있을 경우 val_set을 통해 loss, accu를 구한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-045d67efb9b0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, cp_mask)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m# 0-weight 학습 방지\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \"\"\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(param.noi):\n",
    "    best_accu.append(0)\n",
    "    best_accu[i] = [0, 0, 0]\n",
    "    cp_mask = []\n",
    "\n",
    "    # pruning 및 mask 복사\n",
    "    # layer별 prune rate를 입력\n",
    "    cp_mask = weight_init(model, model_init, \n",
    "                           (1 - ((1-param.prune_per_c) ** i)),\n",
    "                           (1 - ((1-param.prune_per_f) ** i)),\n",
    "                           (1 - ((1-param.prune_per_o) ** i))\n",
    "                          )\n",
    "    #model2[0].weight.register_hook(lambda grad: grad * gradient_mask)\n",
    "    j = 0\n",
    "    for name, p in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            p.register_hook(lambda grad: grad.mul_(cp_mask[j]))\n",
    "            print(name)\n",
    "            print(p[0], cp_mask[j][0])\n",
    "            j += 1\n",
    "    \n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr = param.lr, weight_decay = param.weight_decay)\n",
    "    print(\"Learning start!\\n\")\n",
    "    # weight 개수 계산 및 저장\n",
    "    \n",
    "    #iteration 횟수 = i\n",
    "    \n",
    "    weight_counts = weight_counter(model)\n",
    "    \n",
    "    #print(model.conv1.weight[0])\n",
    "    #print(model.fc3.weight[0])\n",
    "    \n",
    "    \n",
    "    remaining_weight = weight_counts['all.weight'][3]\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    for epoch in tqdm(range(param.epochs)):\n",
    "        # epoch가 0일때 정확도 계산\n",
    "        if epoch == 0:\n",
    "            accuracy, test_loss = test(model, param.test_loader, criterion)\n",
    "            visdom_plot(vis_plt,torch.Tensor([accuracy]), torch.Tensor([0]),\n",
    "                        str(remaining_weight)\n",
    "                       )\n",
    "            print('[epoch : %d]' % (epoch),\n",
    "             '(r_loss: x.xxxxx)',\n",
    "             '(t_loss: x.xxxxx)',\n",
    "             '(accu: %.4f)' % (accuracy)\n",
    "             )\n",
    "        # model training    \n",
    "        running_loss = train(model, param.train_loader, optimizer, criterion, cp_mask)\n",
    "        \n",
    "        # val_set이 있을 경우 val_set을 통해 loss, accu를 구한다.\n",
    "        if param.valset == 'empty':\n",
    "            accuracy, test_loss = test(model, param.test_loader, criterion)\n",
    "        else:\n",
    "            accuracy, test_loss = test(model, param.val_loader, criterion)\n",
    "        \n",
    "        # visdom plot\n",
    "        visdom_plot(vis_plt, torch.Tensor([accuracy]), torch.Tensor([(epoch+1) * 1000]),\n",
    "                    str(remaining_weight)\n",
    "                   )\n",
    "        \n",
    "        # best accuracy list (weight_remain, epoch, accuracy)\n",
    "        if best_accu[i][2] <= accuracy:\n",
    "            best_accu[i] = [remaining_weight, epoch, accuracy]\n",
    "        \n",
    "        print('[epoch : %d]' % (epoch+1),\n",
    "             '(r_loss: %.5f)' % (running_loss),\n",
    "             '(t_loss: %.5f)' % (test_loss),\n",
    "             '(accu: %.4f)' % (accuracy)\n",
    "             )\n",
    "    stop_time = timeit.default_timer()\n",
    "    #print(model.fc3.weight[0][0])\n",
    "    #print(model_init.fc3.weight[0][0])\n",
    "    \n",
    "    #print(model.fc3.weight[0])\n",
    "    \n",
    "    #print(model.conv1.weight[0])\n",
    "    #print(model.fc3.weight[0])\n",
    "    \n",
    "    print(\"Finish!\",\n",
    "          \"(Best accu: %.4f)\" % best_accu[i][2],\n",
    "          \"(Time taken(sec) : %.2f)\" % (stop_time - start_time),\n",
    "          \"\\n\\n\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.conv1.weight[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 2)\n",
    ")\n",
    "print(model2[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model2[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for name, p in model2.named_modules():\n",
    "        print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if x > 0:\n",
    "\tvalue = 10\n",
    "else:\n",
    "\tvalue = 20\n",
    "\n",
    "value = 10 if x > 0 else 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gradient mask\n",
    "gradient_mask = torch.zeros(2, 2)\n",
    "gradient_mask[0, 0] = 1.0\n",
    "#model2[0].weight.register_hook(lambda grad: grad.mul_(gradient_mask))\n",
    "model2[0].weight.register_hook(lambda grad: grad * gradient_mask)\n",
    "print(model2[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2[0].weight[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, p in model2.named_parameters():\n",
    "    print(name)\n",
    "    if 'weight' in name:\n",
    "        p.register_hook(lambda grad: grad * 0 if p != 0 else grad)\n",
    "    #for i in range(len(p)):\n",
    "        #for j in range(len([0][p])):\n",
    "            #print(p[i][j])    \n",
    "    #print(p != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, p in model2.named_modules():\n",
    "    print(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model2.parameters(), lr=1.0, weight_decay = 0.003)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "x = torch.randn(batch_size, 2)\n",
    "target = torch.randint(0, 2, (batch_size,))\n",
    "\n",
    "optimizer.zero_grad()\n",
    "output = model2(x)\n",
    "loss = criterion(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad 생성\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient: ', model2[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weight before training\n",
    "#w0 = model2[0].weight.detach().clone()\n",
    "\n",
    "# Single training iteration\n",
    "optimizer.step()\n",
    "\n",
    "# Compare weight update\n",
    "w1 = model2[0].weight.detach().clone()\n",
    "print('Weights updated ', w0!=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#name, all, non_zero, zero, per\n",
    "def weight_counter(model):\n",
    "    layer_weight = []\n",
    "    all_weight = ['All_weight',0 ,0 ,0, 0]\n",
    "    for name, p in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            none_zero_w = (p != 0).sum().item()\n",
    "            zero_w = (p == 0).sum().item()\n",
    "            all_w = none_zero_w + zero_w\n",
    "\n",
    "            all_weight[1] += all_w\n",
    "            all_weight[2] += none_zero_w\n",
    "            all_weight[3] += zero_w\n",
    "\n",
    "            layer_weight.append([name, all_w, none_zero_w, zero_w, round(none_zero_w/all_w,4)])\n",
    "\n",
    "    all_weight[4] = round((all_weight[2]/all_weight[1]), 4)\n",
    "    layer_weight.insert(0, all_weight)\n",
    "    for i in range(len(layer_weight)):\n",
    "        print(layer_weight[i])\n",
    "    \n",
    "    return layer_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_counter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.conv1.weight != 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.conv1.weight != 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.conv1.weight == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.fc1.weight != 0).sum(dim=1).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.fc1.weight != 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def calc_now(model):\n",
    "    fc1_1 = ((model.fc1.weight != 0).sum(dim=1)).sum(dim=0).item()\n",
    "    fc1_0 = ((model.fc1.weight == 0).sum(dim=1)).sum(dim=0).item()\n",
    "    fc1 = fc1_1 + fc1_0\n",
    "    fc1_p = fc1_0 / fc1_1\n",
    "    fc2_1 = ((model.fc2.weight != 0).sum(dim=1)).sum(dim=0).item()\n",
    "    fc2_0 = ((model.fc2.weight == 0).sum(dim=1)).sum(dim=0).item()\n",
    "    fc2 = fc2_1 + fc2_0\n",
    "    fc3_1 = ((model.fc3.weight != 0).sum(dim=1)).sum(dim=0).item()\n",
    "    fc3_0 = ((model.fc3.weight == 0).sum(dim=1)).sum(dim=0).item()\n",
    "    fc3 = fc3_1 + fc3_0\n",
    "    #print(fc1, fc2, fc3, fc1+fc2+fc3, fc1_1 + fc2_1 + fc3_1 ,fc1_0 + fc2_0 + fc3_0)\n",
    "    print(\"Remaining weight %.1f %%\" %(((fc1_1+fc2_1+fc3_1)/(fc1+fc2+fc3))*100))\n",
    "    print('total weight :',\n",
    "        '%d' % (fc1+fc2+fc3),\n",
    "         '(%d |' % (fc1_1+fc2_1+fc3_1),\n",
    "         '%d)' % (fc1_0+fc2_0+fc3_0)\n",
    "         )\n",
    "    print('fc1 :',\n",
    "        '%d' % fc1,\n",
    "         '(%d |' % fc1_1,\n",
    "         '%d)' % fc1_0\n",
    "         )\n",
    "    print('fc2 :',\n",
    "        '%d' % fc2,\n",
    "         '(%d |' % fc2_1,\n",
    "         '%d)' % fc2_0\n",
    "         )\n",
    "    print('fc3 :',\n",
    "        '%d' % fc3,\n",
    "         '(%d |' % fc3_1,\n",
    "         '%d)' % fc3_0\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model1.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        prune.remove(module, name = 'weight')\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        prune.remove(module, name = 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.fc3.weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Maximum accuracy per weight remaining\")\n",
    "for i in range(len(best_accu)):\n",
    "    print(\"Remaining weight %.1f %% \" % (best_accu[i][0] * 100),\n",
    "         \"Epoch %d\" % best_accu[i][1],\n",
    "         \"Accu %.4f %%\" % best_accu[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model2 = cu.LeNet300().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model2.state_dict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "param.type(\"LeNet300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model2.fc3.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cp_mask = weight_init(model, model_init,\n",
    "                              0,\n",
    "                              0,\n",
    "                              0\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model2.fc3.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class abc:\n",
    "    def __init__(self):\n",
    "        self.a = 1\n",
    "        self.b = 2\n",
    "        d = 3\n",
    "        c = self.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aa = abc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aa.d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(aa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for name, p in model.named_parameters():\n",
    "    EPS = 1e-6\n",
    "    if 'weight' in name:\n",
    "        tensor = p.data.cpu().numpy()\n",
    "        grad_tensor = p.grad.data.cpu().numpy()\n",
    "        grad_tensor = np.where(tensor < EPS, 0, grad_tensor)\n",
    "        p.grad.data = torch.from_numpy(grad_tensor).to(device)\n",
    "        print(p.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 숫자 60000\n",
    "배치 길이 60\n",
    "배치 개수 1000\n",
    "epoch = 50\n",
    "\n",
    "이터레이션 횟수 50000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
