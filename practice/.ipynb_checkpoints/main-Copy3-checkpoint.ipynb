{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import visdom\n",
    "import copy\n",
    "import torch.nn.utils.prune as prune\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import timeit\n",
    "import sys\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import argparse\n",
    "\n",
    "# custom librarys (model, parameters...) Lottery_Ticket_Prac/custom/utils.py\n",
    "import custom.utils as cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# random seed for test\n",
    "torch.manual_seed(55)\n",
    "torch.cuda.manual_seed_all(55)\n",
    "torch.backends.cudnn.enabled = False\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    # 인자값을 받을 수 있는 인스턴스 생성\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    #입력받을 인자값 등록\n",
    "    parser.add_argument(\"--testname\", default = \"Test\", type=str, help=\"Check your test file name\")\n",
    "    parser.add_argument(\"--epochs\",default=30, type=int, help=\"Epoch\")\n",
    "    parser.add_argument(\"--lr\",default=1.2e-3, type=float, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--batch_size\", default=60, type=int, help=\"Batch size\")\n",
    "    parser.add_argument(\"--weight_decay\", default=0, type=float, help=\"Weight_decay\")\n",
    "    parser.add_argument(\"--test_iters\", default=2, type=int, help=\"Test iterations\")\n",
    "    parser.add_argument(\"--prune_iters\", default=2, type=int, help=\"Pruning iterations\")\n",
    "    parser.add_argument(\"--prune_per_conv\", default=1, type=float, help=\"Prune percentage of convoultion layer\")\n",
    "    parser.add_argument(\"--prune_per_linear\", default=0.2, type=float, help=\"Prune percentage of linear layer\")\n",
    "    parser.add_argument(\"--prune_per_out\", default=0.1, type=float, help=\"Prune percentage of out layer\")\n",
    "    parser.add_argument(\"--dataset\", default=\"mnist\", type=str, help=\"mnist | cifar10\")\n",
    "    parser.add_argument(\"--validation_ratio\", default = (1/12), type=float, help=\"Validation ratio\")\n",
    "    parser.add_argument(\"--arch_type\", default=\"Lenet300_100\", type=str, help=\"Lenet300_100, Lenet250_75, Lenet200_50\")\n",
    "    parser.add_argument(\"--test_type\", default=\"test_accu\", type=str, help=\"If you want to use validation set, enter val_accu\")\n",
    "\n",
    "    # 입력받은 인자값을 args에 저장\n",
    "    args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test, prune, util function\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, label) in enumerate(dataloader):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss / len(dataloader)\n",
    "    return running_loss\n",
    "\n",
    "def test(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in dataloader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            test_loss += loss / len(dataloader)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "        # 로더 -> 배치 개수 로더.dataset -> 전체 길이, \n",
    "    return (correct/total), test_loss\n",
    "\n",
    "# prune function\n",
    "# pruning mask 생성 -> mask 복사 -> weight initialize -> prune 진행\n",
    "def weight_prune(prune_iter):\n",
    "    conv_rate = (1 - ((1-args.prune_per_conv) ** prune_iter))\n",
    "    fc_rate = (1 - ((1-args.prune_per_linear) ** prune_iter))\n",
    "    out_rate = (1 - ((1-args.prune_per_out) ** prune_iter))\n",
    "    # make prune mask\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.l1_unstructured(module, name = 'weight', amount = conv_rate)\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if 'out' in name:\n",
    "                prune.l1_unstructured(module, name = 'weight', amount = fc_rate)\n",
    "            else:\n",
    "                prune.l1_unstructured(module, name = 'weight', amount = out_rate)\n",
    "            \n",
    "    # mask copy   \n",
    "    cpd_mask = {}\n",
    "    for name, mask in model.named_buffers():\n",
    "        cpd_mask[name] = mask\n",
    "    \n",
    "    # going prune\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.remove(module, name = 'weight')\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            prune.remove(module, name = 'weight')                \n",
    "    # return copied mask\n",
    "    return cpd_mask\n",
    "\n",
    "def weight_init():\n",
    "    # 웨이트 초기화\n",
    "    for name_model, param_model in model.named_parameters():\n",
    "        for name_init, param_init in model_init.named_parameters():\n",
    "            if name_model in name_init:\n",
    "                param_model.data = copy.deepcopy(param_init.data)\n",
    "                break\n",
    "    # mask 적용\n",
    "    for name_model, param_model in model.named_parameters():\n",
    "        for name_mask in cpd_mask:\n",
    "            if name_model in name_mask:\n",
    "                param_model.data = param_model.data.mul_(cpd_mask[name_mask])\n",
    "                break\n",
    "                \n",
    "    # gradient hook (freeze zero-weight)\n",
    "    for name_model, module in model.named_modules():\n",
    "        for name_mask in cpd_mask:\n",
    "            if name_model != \"\" and name_model in name_mask:\n",
    "                hook = module.weight.register_hook(lambda grad, name_mask=name_mask : grad.mul_(cpd_mask[name_mask]))\n",
    "        \n",
    "    optimizer = optim.Adam(model.parameters(), lr = args.lr, weight_decay = args.weight_decay)\n",
    "    \n",
    "    return optimizer, hook\n",
    "\n",
    "# visdom plot append \n",
    "def visdom_plot(loss_plot, num, loss_value, name):\n",
    "    vis.line(X = num,\n",
    "            Y = loss_value,\n",
    "            win = loss_plot,\n",
    "            name = str(name),\n",
    "            update = 'append'\n",
    "            )\n",
    "    \n",
    "def result_plot():\n",
    "    x = []\n",
    "    for i in range(sum_epoch+1):\n",
    "        x.append(i*1000)\n",
    "\n",
    "    for name in test_result['Average of trials']:\n",
    "        visdom_plot(vis_plt, torch.Tensor(x), torch.Tensor(test_result['Average of trials'][name][2]),\n",
    "                            name)\n",
    "\n",
    "# weight count function\n",
    "# dict type ['Layer name' : [all, non_zero, zero, ratio]]\n",
    "def weight_counter(model):\n",
    "    layer_weight = {'all.weight':[0, 0, 0, 0]}\n",
    "    \n",
    "    for name, p in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            remain, pruned = (p != 0).sum().item(), (p == 0).sum().item()\n",
    "            layer_weight[name] = [remain+pruned, remain, pruned, round((remain/(remain+pruned))*100, 2)]\n",
    "            \n",
    "    for i in layer_weight.keys():\n",
    "        for j in range(0, 3):\n",
    "            layer_weight['all.weight'][j] += layer_weight[i][j]\n",
    "    layer_weight['all.weight'][3] = round(layer_weight['all.weight'][1]/layer_weight['all.weight'][0]*100, 2)\n",
    "    print(\"------------------------------------------------------------\\n\",\n",
    "          \"Layer\".center(12), \"Weight\".center(39), \"Ratio(%)\".rjust(7), sep='')\n",
    "    for i in layer_weight.keys():\n",
    "        \n",
    "        print(\"%s\" % i.ljust(13), \":\",\n",
    "              (\"%s (%s | %s)\" % (layer_weight[i][0], layer_weight[i][1], layer_weight[i][2])).center(36),\n",
    "              (\"%.2f\" % layer_weight[i][3]).rjust(7),\n",
    "              sep=''\n",
    "             )\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    return layer_weight\n",
    "\n",
    "# print best accuracy in each iteration\n",
    "def best_accuracy(best_accu):\n",
    "    print(\"Maximum accuracy per weight remaining\")\n",
    "    \"\"\"\n",
    "    for i in range(len(best_accu)):\n",
    "        print(\"Remaining weight %.1f %% \" % (best_accu[i][0] * 100),\n",
    "             \"Epoch %d\" % best_accu[i][1],\n",
    "             \"Accu %.4f\" % best_accu[i][2])\n",
    "    \"\"\"\n",
    "    for name in best_accu:\n",
    "        print(\"Remaining weight %s %% \" % name,\n",
    "             \"Epoch %d\" % best_accu[name][0],\n",
    "             \"Accu %.4f\" % best_accu[name][1])\n",
    "        \n",
    "# initial accuracy\n",
    "def zero_accu(model, dataloader, criterion, remaining_weight, vis_plt):\n",
    "    accuracy, test_loss = test(model, dataloader, criterion)\n",
    "    #visdom_plot(vis_plt,torch.Tensor([accuracy]), torch.Tensor([0]), str(remaining_weight))\n",
    "    print('[epoch : 0] (l_loss: x.xxxxx) (t_loss: %.5f) (accu: %.4f)' % (test_loss, accuracy))\n",
    "    return accuracy, test_loss\n",
    "\n",
    "def append_result_data(running_loss, test_loss, accuracy):\n",
    "    result_data[0].append(running_loss)\n",
    "    result_data[1].append(test_loss)\n",
    "    result_data[2].append(accuracy)\n",
    "    \n",
    "def wcount():\n",
    "    # [전체, 남은, 비율]\n",
    "    fulllist = []\n",
    "    for i in range (args.prune_iters):\n",
    "        weight = [0, 0, 0]\n",
    "        for name, p in model.named_parameters():\n",
    "            if 'conv' in name:\n",
    "                if 'weight' in name:\n",
    "                    a = (p != 0).sum().item() + (p == 0).sum().item()\n",
    "                    weight[0] += a\n",
    "                    weight[1] += int(a * (((1-args.prune_per_conv) ** i)))\n",
    "            elif 'fc' in name:\n",
    "                if 'weight' in name:\n",
    "                    if 'out' in name:\n",
    "                        a = (p != 0).sum().item() + (p == 0).sum().item()\n",
    "                        weight[0] += a\n",
    "                        weight[1] += int(a * (((1-args.prune_per_out) ** i)))\n",
    "                    else:\n",
    "                        a = (p != 0).sum().item() + (p == 0).sum().item()\n",
    "                        weight[0] += a\n",
    "                        weight[1] += int(a * (((1-args.prune_per_linear) ** i)))\n",
    "        fulllist.append(round(weight[1]/weight[0] * 100, 2))\n",
    "    return fulllist\n",
    "\n",
    "def result_dict():\n",
    "    result = {}\n",
    "    weightper = wcount()\n",
    "    for i in range(args.test_iters):\n",
    "        result[(i+1)] = {}\n",
    "        #for j in range(len(weightper)):\n",
    "            #result[(i+1)][weightper[j]] = {}\n",
    "            #for z in range(sum_epoch):\n",
    "                #result[(i+1)][weightper[j]][z] = {}\n",
    "    return result\n",
    "\n",
    "def average_calc():\n",
    "    test_result['Average of trials'] = {}\n",
    "    for weight_per in test_result[1]:\n",
    "        test_result['Average of trials'][weight_per] = [[],[],[]]\n",
    "        for i in range(3):\n",
    "            for j in range(sum_epoch+1):\n",
    "                test_result['Average of trials'][weight_per][i].append(0)\n",
    "        for i in range(3):\n",
    "            for j in range(1, args.test_iters+1):\n",
    "                for k in range(sum_epoch+1):\n",
    "                    test_result['Average of trials'][weight_per][i][k] += test_result[j][weight_per][i][k]\n",
    "            for z in range(sum_epoch+1):\n",
    "                test_result['Average of trials'][weight_per][i][z] /= (args.test_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices : 2\n",
      "Current cuda device : 1 (GeForce RTX 2080 Ti))\n",
      "cpu와 cuda 중 다음 기기로 학습함: cuda:1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cuda setting. GPU_NUM = 사용할 GPU의 번호\n",
    "GPU_NUM = 1\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "print ('Available devices :', torch.cuda.device_count())\n",
    "print ('Current cuda device : %d (%s))' % (torch.cuda.current_device(), torch.cuda.get_device_name(device)))\n",
    "print(\"cpu와 cuda 중 다음 기기로 학습함:\", device, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-981db9bfa6f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# 설정한 비율만큼 분할 시의 data 갯수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_ratio\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m# shuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ])\n",
    "\n",
    "trainset = dsets.MNIST(root='../MNIST_data/',\n",
    "                         train=True,\n",
    "                         transform = transform,\n",
    "                         download=True)\n",
    "testset = dsets.MNIST(root='../MNIST_data/',\n",
    "                        train=False,\n",
    "                        transform = transform,\n",
    "                        download=True)\n",
    "valset = dsets.MNIST('../MNIST_data/',\n",
    "                         train=True,\n",
    "                         transform = transform,\n",
    "                         download=True)\n",
    "\n",
    "# validation set 분류\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "# 설정한 비율만큼 분할 시의 data 갯수\n",
    "split = int(np.floor(args.validation_ratio * num_train))\n",
    "# shuffle\n",
    "np.random.shuffle(indices)\n",
    "# data 분할\n",
    "train_idx, val_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = trainset,\n",
    "                                          batch_size = args.batch_size,\n",
    "                                          sampler = train_sampler,\n",
    "                                          drop_last = True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset = valset,\n",
    "                                          batch_size = args.batch_size,\n",
    "                                          sampler = val_sampler,\n",
    "                                          drop_last = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = testset,\n",
    "                                          shuffle = False,\n",
    "                                          drop_last = True)\n",
    "class Lenet300_100(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lenet300, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28, 300, bias = True)\n",
    "        self.fc2 = nn.Linear(300, 100, bias = True)\n",
    "        self.fcout = nn.Linear(100, 10, bias = True)\n",
    "        \n",
    "        init.xavier_normal_(self.fc1.weight)\n",
    "        init.xavier_normal_(self.fc2.weight)\n",
    "        init.xavier_normal_(self.fcout.weight)\n",
    "        #init.normal_(self.fc1.bias)\n",
    "        #init.normal_(self.fc2.bias)\n",
    "        #init.normal_(self.fcout.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fcout(x)\n",
    "        return x\n",
    "\n",
    "model = Lenet300().to(device)\n",
    "model_init = copy.deepcopy(model)\n",
    "\n",
    "trained_weights = {}\n",
    "test_result = result_dict()\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "# parameter check\n",
    "#print('\\n'.join(\"%s: %s\" % item for item in __dict__.items()),'\\n\\n')\n",
    "print('Model structure\\n',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-561009c6644b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtest_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mprune_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"시작 전\"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mcpd_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_prune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprune_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "for test_iter in range(args.test_iters):\n",
    "    for prune_iter in range(args.prune_iters):\n",
    "        print(\"시작 전\" ,model.fcout.weight[3])\n",
    "        cpd_mask = weight_prune(prune_iter)\n",
    "        optimizer, hook = weight_init()\n",
    "        start_t = timeit.default_timer()\n",
    "        weight_counter(model)\n",
    "        print(\"Prune 후\" ,model.fcout.weight[3],'\\n\\n')\n",
    "        for epoch in range(args.epochs):\n",
    "\n",
    "            running_loss = train(model, train_loader, optimizer, criterion)\n",
    "            accuracy, test_loss = test(model, test_loader, criterion)\n",
    "            print('[epoch : %d] (l_loss: %.5f) (t_loss: %.5f) (accu: %.4f)' %\n",
    "                      ((epoch+1), (running_loss), (test_loss), (accuracy)))\n",
    "        stop_t = timeit.default_timer()\n",
    "        hook.remove()\n",
    "        print(\"Finish! (Time taken(sec) : %.2f) \\n\\n\" %\n",
    "              ((stop_t - start_t)))\n",
    "        weight_counter(model)\n",
    "        print(\"끝\" ,model.fcout.weight[3],'\\n\\n')\n",
    "#result_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--testname TESTNAME] [--lr LR]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--test_iters TEST_ITERS]\n",
      "                             [--prune_iters PRUNE_ITERS]\n",
      "                             [--prune_per_conv PRUNE_PER_CONV]\n",
      "                             [--prune_per_linear PRUNE_PER_LINEAR]\n",
      "                             [--prune_per_out PRUNE_PER_OUT]\n",
      "                             [--dataset DATASET]\n",
      "                             [--validation_ratio VALIDATION_RATIO]\n",
      "                             [--arch_type ARCH_TYPE] [--test_type TEST_TYPE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/jinhyuk/.local/share/jupyter/runtime/kernel-7876cd84-54d6-4dc0-a004-2b67a7218da5.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
