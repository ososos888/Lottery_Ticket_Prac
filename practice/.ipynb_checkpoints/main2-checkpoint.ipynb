{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import visdom\n",
    "import copy\n",
    "import torch.nn.utils.prune as prune\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import timeit\n",
    "import sys\n",
    "import os\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import argparse\n",
    "from model import Lenet300_100, Lenet250_75, Lenet200_50\n",
    "\n",
    "# custom librarys (model, parameters...) Lottery_Ticket_Prac/custom/utils.py\n",
    "import custom.utils as cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# random seed for test\\ntorch.manual_seed(55)\\ntorch.cuda.manual_seed_all(55)\\ntorch.backends.cudnn.enabled = False\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# random seed for test\n",
    "torch.manual_seed(55)\n",
    "torch.cuda.manual_seed_all(55)\n",
    "torch.backends.cudnn.enabled = False\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda setting. GPU_NUM = 사용할 GPU의 번호\n",
    "GPU_NUM = 1\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "print ('Available devices :', torch.cuda.device_count())\n",
    "print ('Current cuda device : %d (%s))' % (torch.cuda.current_device(), torch.cuda.get_device_name(device)))\n",
    "print(\"cpu와 cuda 중 다음 기기로 학습함:\", device, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--testname TESTNAME] [--epochs EPOCHS]\n",
      "                             [--lr LR] [--batch_size BATCH_SIZE]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--test_iters TEST_ITERS]\n",
      "                             [--prune_iters PRUNE_ITERS]\n",
      "                             [--prune_per_conv PRUNE_PER_CONV]\n",
      "                             [--prune_per_linear PRUNE_PER_LINEAR]\n",
      "                             [--prune_per_out PRUNE_PER_OUT]\n",
      "                             [--dataset DATASET]\n",
      "                             [--validation_ratio VALIDATION_RATIO]\n",
      "                             [--arch_type ARCH_TYPE] [--test_type TEST_TYPE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/jinhyuk/.local/share/jupyter/runtime/kernel-eca29539-8c9e-4bcd-b680-e818d4a5d190.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinhyuk/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    # 인자값을 받을 수 있는 인스턴스 생성\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    #입력받을 인자값 등록\n",
    "    parser.add_argument(\"--testname\", default = \"Lenet300test21\", type=str, help=\"Check your test file name\")\n",
    "    parser.add_argument(\"--epochs\",default=25, type=int, help=\"Epoch\")\n",
    "    parser.add_argument(\"--lr\",default=1.2e-3, type=float, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--batch_size\", default=60, type=int, help=\"Batch size\")\n",
    "    parser.add_argument(\"--weight_decay\", default=0, type=float, help=\"Weight_decay\")\n",
    "    parser.add_argument(\"--test_iters\", default=2, type=int, help=\"Test iterations\")\n",
    "    parser.add_argument(\"--prune_iters\", default=21, type=int, help=\"Pruning iterations\")\n",
    "    parser.add_argument(\"--prune_per_conv\", default=1, type=float, help=\"Prune percentage of convoultion layer\")\n",
    "    parser.add_argument(\"--prune_per_linear\", default=0.2, type=float, help=\"Prune percentage of linear layer\")\n",
    "    parser.add_argument(\"--prune_per_out\", default=0.1, type=float, help=\"Prune percentage of out layer\")\n",
    "    parser.add_argument(\"--dataset\", default=\"mnist\", type=str, help=\"mnist | cifar10\")\n",
    "    parser.add_argument(\"--validation_ratio\", default = (1/12), type=float, help=\"Validation ratio\")\n",
    "    parser.add_argument(\"--model_arch\", default=\"Lenet300_100\", type=str, help=\"Lenet300_100, Lenet250_75, Lenet200_50\")\n",
    "    parser.add_argument(\"--test_type\", default=\"test_accu\", type=str, help=\"If you want to use validation set, enter val_accu\")\n",
    "\n",
    "    # 입력받은 인자값을 args에 저장\n",
    "    args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test, prune, util function\n",
    "# Import model function. return model\n",
    "def import_model():\n",
    "    if args.model_arch == \"Lenet300_100\":\n",
    "        model = Lenet300_100.Lenet().to(device)\n",
    "    elif args.model_arch == \"Lenet250_75\":\n",
    "        model = Lenet300_100.Lenet().to(device)\n",
    "    elif args.model_arch == \"Lenet200_50\":\n",
    "        model = Lenet300_100.Lenet().to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Data loader function. return dataloader(train, validation, test)\n",
    "def data_loader():\n",
    "    if args.dataset == \"mnist\":\n",
    "        # mnist dataset\n",
    "        transform = transforms.Compose([\n",
    "                        transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])\n",
    "\n",
    "        trainset = dsets.MNIST(root='../MNIST_data/',\n",
    "                                 train=True,\n",
    "                                 transform = transform,\n",
    "                                 download=True)\n",
    "        testset = dsets.MNIST(root='../MNIST_data/',\n",
    "                                train=False,\n",
    "                                transform = transform,\n",
    "                                download=True)\n",
    "        valset = dsets.MNIST('../MNIST_data/',\n",
    "                                 train=True,\n",
    "                                 transform = transform,\n",
    "                                 download=True)\n",
    "\n",
    "        # validation set 분류\n",
    "        num_train = len(trainset)\n",
    "        indices = list(range(num_train))\n",
    "        # 설정한 비율만큼 분할 시의 data 갯수\n",
    "        split = int(np.floor(args.validation_ratio * num_train))\n",
    "        # shuffle\n",
    "        np.random.shuffle(indices)\n",
    "        # data 분할\n",
    "        train_idx, val_idx = indices[split:], indices[:split]\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(dataset = trainset,\n",
    "                                                  batch_size = args.batch_size,\n",
    "                                                  sampler = train_sampler,\n",
    "                                                  drop_last = True)\n",
    "\n",
    "        val_loader = torch.utils.data.DataLoader(dataset = valset,\n",
    "                                                  batch_size = args.batch_size,\n",
    "                                                  sampler = val_sampler,\n",
    "                                                  drop_last = True)\n",
    "\n",
    "        test_loader = torch.utils.data.DataLoader(dataset = testset,\n",
    "                                                  shuffle = False,\n",
    "                                                  drop_last = True)\n",
    "    #elif:\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "    \n",
    "    \n",
    "# model training function    \n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, label) in enumerate(dataloader):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss / len(dataloader)\n",
    "    return running_loss\n",
    "\n",
    "# model test function return accuracy n loss\n",
    "def test(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in dataloader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            test_loss += loss / len(dataloader)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "        # 로더 -> 배치 개수 로더.dataset -> 전체 길이, \n",
    "    return (correct/total), test_loss\n",
    "\n",
    "# prune function. weight pruning n copied mask\n",
    "def weight_prune(prune_iter):\n",
    "    conv_rate = (1 - ((1-args.prune_per_conv) ** prune_iter))\n",
    "    fc_rate = (1 - ((1-args.prune_per_linear) ** prune_iter))\n",
    "    out_rate = (1 - ((1-args.prune_per_out) ** prune_iter))\n",
    "    # make prune mask\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.l1_unstructured(module, name = 'weight', amount = conv_rate)\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if 'out' in name:\n",
    "                prune.l1_unstructured(module, name = 'weight', amount = out_rate)\n",
    "            else:\n",
    "                prune.l1_unstructured(module, name = 'weight', amount = fc_rate)\n",
    "            \n",
    "    # mask copy   \n",
    "    cpd_mask = {}\n",
    "    for name, mask in model.named_buffers():\n",
    "        cpd_mask[name] = mask\n",
    "    \n",
    "    # going prune\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.remove(module, name = 'weight')\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            prune.remove(module, name = 'weight')                \n",
    "\n",
    "    return cpd_mask\n",
    "\n",
    "# weight initialize and apply mask function. \n",
    "def weight_init_apply():\n",
    "    # weight initialize to first model\n",
    "    for name_model, param_model in model.named_parameters():\n",
    "        for name_init, param_init in model_init.named_parameters():\n",
    "            if name_model in name_init:\n",
    "                param_model.data = copy.deepcopy(param_init.data)\n",
    "                break\n",
    "                \n",
    "    # apply prune mask\n",
    "    for name_model, param_model in model.named_parameters():\n",
    "        for name_mask in cpd_mask:\n",
    "            if name_model in name_mask:\n",
    "                param_model.data = param_model.data.mul_(cpd_mask[name_mask])\n",
    "                break\n",
    "                \n",
    "    # gradient hook (freeze zero-weight)\n",
    "    for name_model, module in model.named_modules():\n",
    "        for name_mask in cpd_mask:\n",
    "            if name_model != \"\" and name_model in name_mask:\n",
    "                hook = module.weight.register_hook(lambda grad, name_mask=name_mask : grad.mul_(cpd_mask[name_mask]))\n",
    "        \n",
    "    optimizer = optim.Adam(model.parameters(), lr = args.lr, weight_decay = args.weight_decay)\n",
    "    \n",
    "    return optimizer, hook\n",
    "\n",
    "# visdom setting\n",
    "def visdom_window():\n",
    "    vis = visdom.Visdom()\n",
    "    vis.close(env=\"main\")\n",
    "\n",
    "    Tracker_type = \"Accuracy_Tracker\"\n",
    "    title = [args.testname, Tracker_type]\n",
    "    title = \"_\",join(title)\n",
    "\n",
    "    # make plot\n",
    "    vis_plt = vis.line(X=torch.Tensor(1).zero_(), Y=torch.Tensor(1).zero_(), \n",
    "                        opts=dict(title = title,\n",
    "                                  legend=['100.0'],\n",
    "                                  showlegend=True,\n",
    "                                  xtickmin = 0,\n",
    "                                  xtickmax = 20000,\n",
    "                                  ytickmin = 0.94,\n",
    "                                  ytickmax = 0.99\n",
    "                                 )\n",
    "                       )    \n",
    "# visdom plot (append)\n",
    "def visdom_plot(loss_plot, num, loss_value, name):\n",
    "    vis.line(X = num,\n",
    "            Y = loss_value,\n",
    "            win = loss_plot,\n",
    "            name = str(name),\n",
    "            update = 'append'\n",
    "            )\n",
    "def result_plot():\n",
    "    x = []\n",
    "    for i in range(sum_epoch+1):\n",
    "        x.append(i*1000)\n",
    "\n",
    "    for name in test_result['Average of trials']:\n",
    "        visdom_plot(vis_plt, torch.Tensor(x), torch.Tensor(test_result['Average of trials'][name][2]),\n",
    "                            name)\n",
    "\n",
    "# weight count function\n",
    "# dict type ['Layer name' : [all, non_zero, zero, ratio]]\n",
    "def weight_counter(model):\n",
    "    layer_weight = {'all.weight':[0, 0, 0, 0]}\n",
    "    \n",
    "    for name, p in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            remain, pruned = (p != 0).sum().item(), (p == 0).sum().item()\n",
    "            layer_weight[name] = [remain+pruned, remain, pruned, round((remain/(remain+pruned))*100, 2)]\n",
    "            \n",
    "    for i in layer_weight.keys():\n",
    "        for j in range(0, 3):\n",
    "            layer_weight['all.weight'][j] += layer_weight[i][j]\n",
    "    layer_weight['all.weight'][3] = round(layer_weight['all.weight'][1]/layer_weight['all.weight'][0]*100, 2)\n",
    "    print(\"------------------------------------------------------------\\n\",\n",
    "          \"Layer\".center(12), \"Weight\".center(39), \"Ratio(%)\".rjust(7), sep='')\n",
    "    for i in layer_weight.keys():\n",
    "        \n",
    "        print(\"%s\" % i.ljust(13), \":\",\n",
    "              (\"%s (%s | %s)\" % (layer_weight[i][0], layer_weight[i][1], layer_weight[i][2])).center(36),\n",
    "              (\"%.2f\" % layer_weight[i][3]).rjust(7),\n",
    "              sep=''\n",
    "             )\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    return layer_weight\n",
    "\n",
    "# print best accuracy in each iteration\n",
    "def best_accuracy(best_accu):\n",
    "    print(\"Maximum accuracy per weight remaining\")\n",
    "    \"\"\"\n",
    "    for i in range(len(best_accu)):\n",
    "        print(\"Remaining weight %.1f %% \" % (best_accu[i][0] * 100),\n",
    "             \"Epoch %d\" % best_accu[i][1],\n",
    "             \"Accu %.4f\" % best_accu[i][2])\n",
    "    \"\"\"\n",
    "    for name in best_accu:\n",
    "        print(\"Remaining weight %s %% \" % name,\n",
    "             \"Epoch %d\" % best_accu[name][0],\n",
    "             \"Accu %.4f\" % best_accu[name][1])\n",
    "        \n",
    "# initial accuracy\n",
    "def zero_accu(model, dataloader, criterion, remaining_weight, vis_plt):\n",
    "    accuracy, test_loss = test(model, dataloader, criterion)\n",
    "    #visdom_plot(vis_plt,torch.Tensor([accuracy]), torch.Tensor([0]), str(remaining_weight))\n",
    "    print('[epoch : 0] (l_loss: x.xxxxx) (t_loss: %.5f) (accu: %.4f)' % (test_loss, accuracy))\n",
    "    return accuracy, test_loss\n",
    "\n",
    "def append_result_data(running_loss, test_loss, accuracy):\n",
    "    result_data[0].append(running_loss)\n",
    "    result_data[1].append(test_loss)\n",
    "    result_data[2].append(accuracy)\n",
    "    \n",
    "def wcount():\n",
    "    # [all, remain, per]\n",
    "    fulllist = []\n",
    "    for i in range (args.prune_iters):\n",
    "        weight = [0, 0, 0]\n",
    "        for name, p in model.named_parameters():\n",
    "            if 'conv' in name:\n",
    "                if 'weight' in name:\n",
    "                    a = (p != 0).sum().item() + (p == 0).sum().item()\n",
    "                    weight[0] += a\n",
    "                    weight[1] += int(a * (((1-args.prune_per_conv) ** i)))\n",
    "            elif 'fc' in name:\n",
    "                if 'weight' in name:\n",
    "                    if 'out' in name:\n",
    "                        a = (p != 0).sum().item() + (p == 0).sum().item()\n",
    "                        weight[0] += a\n",
    "                        weight[1] += int(a * (((1-args.prune_per_out) ** i)))\n",
    "                    else:\n",
    "                        a = (p != 0).sum().item() + (p == 0).sum().item()\n",
    "                        weight[0] += a\n",
    "                        weight[1] += int(a * (((1-args.prune_per_linear) ** i)))\n",
    "        fulllist.append(round(weight[1]/weight[0] * 100, 2))\n",
    "    return fulllist\n",
    "\n",
    "def result_dict():\n",
    "    result = {}\n",
    "    weightper = wcount()\n",
    "    for i in range(args.test_iters):\n",
    "        result[(i+1)] = {}\n",
    "        #for j in range(len(weightper)):\n",
    "            #result[(i+1)][weightper[j]] = {}\n",
    "            #for z in range(sum_epoch):\n",
    "                #result[(i+1)][weightper[j]][z] = {}\n",
    "    return result\n",
    "\n",
    "def average_calc():\n",
    "    test_result['Average of trials'] = {}\n",
    "    for weight_per in test_result[1]:\n",
    "        test_result['Average of trials'][weight_per] = [[],[],[]]\n",
    "        for i in range(3):\n",
    "            for j in range(sum_epoch+1):\n",
    "                test_result['Average of trials'][weight_per][i].append(0)\n",
    "        for i in range(3):\n",
    "            for j in range(1, args.test_iters+1):\n",
    "                for k in range(sum_epoch+1):\n",
    "                    test_result['Average of trials'][weight_per][i][k] += test_result[j][weight_per][i][k]\n",
    "            for z in range(sum_epoch+1):\n",
    "                test_result['Average of trials'][weight_per][i][z] /= (args.test_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_result/1111_result.txt test_result/1111_AccuData\n"
     ]
    }
   ],
   "source": [
    "# Filename n location\n",
    "FolderLocation = \"test_result\"\n",
    "FName_result, FName_accu = args.testname.split(), args.testname.split()\n",
    "FName_result.append('result.txt'), FName_accu.append('AccuData')\n",
    "FName_result, FName_accu = os.path.join(FolderLocation, \"_\".join(FName_result)), os.path.join(FolderLocation, \"_\".join(FName_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selcet model's number\n",
      " (1 : Lenet_300_100)\n",
      " (2 : Lenet_250_75)\n",
      " (3 : Lenet_200_50)\n",
      " (4 : Conv6)\n",
      "\n",
      "1\n",
      "Selected model : Lenet_300_100\n",
      "Enter a file name \n",
      "Lenet300test18\n",
      "File name is [Lenet300test18]. Are you sure? [1 : Yes, 2: No]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "FolderLocation = \"test_result\"\n",
    "FileName = os.path.join(a, testname.append('_result.txt'))\n",
    "temp = sys.stdout\n",
    "sys.stdout = open(FileName,'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = import_model()\n",
    "model_init = copy.deepcopy(model)\n",
    "train_loader, val_loader, test_loader = data_loader()\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "test_result = result_dict()\n",
    "# parameter check\n",
    "#print('\\n'.join(\"%s: %s\" % item for item in __dict__.items()),'\\n\\n')\n",
    "print(args)\n",
    "\n",
    "sys.stdout.close()\n",
    "sys.stdout = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "visdom_window():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "결과 저장\n",
    "iter - prune - epoch 별 loss 및 정확도\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning start!\n",
      "Test_Iter (1/3) Start!\n",
      "Learning start! [Prune_iter : (1/20), Remaining weight : 100.0 %]\n",
      "Learning start! [Prune_iter : (2/20), Remaining weight : 80.04 %]\n",
      "Learning start! [Prune_iter : (3/20), Remaining weight : 64.06 %]\n"
     ]
    }
   ],
   "source": [
    "print(\"Learning start!\")\n",
    "for test_iter in range(1, args.test_iters+1):\n",
    "    print(\"Test_Iter (%d/%d) Start!\" % (test_iter, args.test_iters))\n",
    "    temp = sys.stdout\n",
    "    sys.stdout = open(FileName,'a')\n",
    "\n",
    "    print(\"=====================================================================\",\n",
    "          \"\\n\\nTest_Iter (%d/%d)\" % (test_iter, args.test_iters))\n",
    "    \n",
    "    best_accu = {}\n",
    "\n",
    "    for prune_iter in range(args.prune_iters):\n",
    "        result_data = [[],[],[]]\n",
    "        \n",
    "        if prune_iter != 0:\n",
    "            cpd_mask = weight_prune(prune_iter)\n",
    "            optimizer, hook = weight_init()\n",
    "        else:\n",
    "            model = copy.deepcopy(model_init)\n",
    "            optimizer = optim.Adam(model.parameters(), lr = args.lr, weight_decay = args.weight_decay)\n",
    "            #print(model.fcout.weight)\n",
    "        \n",
    "        # prune 진행 후 남은 weight 수 확인\n",
    "        weight_counts = weight_counter(model)\n",
    "        # 총 weight 중 남은 weight의 수 저장 (visdom plot시 사용하기 위함)\n",
    "        remaining_weight = weight_counts['all.weight'][3]\n",
    "        \n",
    "        sys.stdout.close()\n",
    "        sys.stdout = temp\n",
    "        print(\"Learning start! [Prune_iter : (%d/%d), Remaining weight : %s %%]\" % (prune_iter+1 , args.prune_iters, remaining_weight))\n",
    "        temp = sys.stdout\n",
    "        sys.stdout = open(FileName,'a')\n",
    "        \n",
    "        print(\"Learning start! [Prune_iter : (%d/%d), Remaining weight : %s %%]\" % (prune_iter+1 , args.prune_iters, remaining_weight))\n",
    "        # 시작 시간 check\n",
    "        #print(model.fcout.weight[0])\n",
    "        # initial accuracy 확인 및 plot\n",
    "        accuracy, test_loss = zero_accu(model, test_loader, criterion, remaining_weight, vis_plt)\n",
    "        \n",
    "        append_result_data(0, test_loss.item(), accuracy)\n",
    "        \n",
    "        best_accu[remaining_weight] = [0, 0]\n",
    "        \n",
    "        start_t = timeit.default_timer()\n",
    "        \n",
    "        for epoch in range(args.epochs):\n",
    "            # model training, return training loss    \n",
    "            running_loss = train(model, train_loader, optimizer, criterion)\n",
    "            # val_set이 있을 경우 val_set을 통해 loss, accu를 구한다.\n",
    "            \n",
    "            if args.test_type == 'test_accu':\n",
    "                accuracy, test_loss = test(model, test_loader, criterion)\n",
    "            else:\n",
    "                accuracy, test_loss = test(model, val_loader, criterion)\n",
    "            \n",
    "            append_result_data(running_loss.item(), test_loss.item(), accuracy)\n",
    "            \n",
    "            # Appending best accuracy in list (weight_remain, epoch, accuracy)\n",
    "            if best_accu[remaining_weight][1] <= accuracy:\n",
    "                best_accu[remaining_weight] = [epoch, accuracy]\n",
    "\n",
    "            print('[epoch : %d] (l_loss: %.5f) (t_loss: %.5f) (accu: %.4f)' %\n",
    "                  ((epoch+1), (running_loss), (test_loss), (accuracy)))\n",
    "        if prune_iter != 0:\n",
    "            hook.remove()\n",
    "        stop_t = timeit.default_timer()\n",
    "\n",
    "        #print(model.fcout.weight[0])\n",
    "        \n",
    "        print(\"Finish! (Best accu: %.4f) (Time taken(sec) : %.2f) \\n\\n\" %\n",
    "              ((best_accu[remaining_weight][1]), (stop_t - start_t)))\n",
    "        test_result[test_iter][remaining_weight] = result_data\n",
    "    #test_result[i+1][remaining_weight] = result_data    \n",
    "    # iteration별 최고 정확도 확인\n",
    "    best_accuracy(best_accu)\n",
    "    \n",
    "    sys.stdout.close()\n",
    "    sys.stdout = temp\n",
    "    \n",
    "    \n",
    "    print(\"Test_Iter (%d/%d) Finish!\" % (test_iter, args.test_iters))\n",
    "\n",
    "average_calc()\n",
    "result_plot()\n",
    "\n",
    "# Save test_result, weights dictionary\n",
    "dic_FileName = \"test_result/\" + args.testname + \"_result_data\"\n",
    "\n",
    "torch.save(test_result, dic_FileName)\n",
    "\n",
    "\"\"\"\n",
    "with open(dic_FileName, 'wb') as f:\n",
    "    pickle.dump(test_result, f)\n",
    "with open(dic_FileName2, 'wb') as f2:\n",
    "    pickle.dump(trained_weights, f2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average_calc()\n",
    "result_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.fcout.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = sys.stdout\n",
    "sys.stdout = open(FileName,'a')\n",
    "\n",
    "print(\"Average test data\")\n",
    "for name in test_result['Average of trials']:\n",
    "    print(\"Remaining weight %.2f %%\" % name)\n",
    "    print(\"Epoch Train_loss  Test_loss  Accuracy\")\n",
    "    for i in range(args.epochs+1):\n",
    "        print('%d     %.6f    %.6f   %.4f' % (\n",
    "            i,\n",
    "            test_result['Average of trials'][name][0][i],\n",
    "            test_result['Average of trials'][name][1][i],\n",
    "            test_result['Average of trials'][name][2][i]))\n",
    "\n",
    "sys.stdout.close()\n",
    "sys.stdout = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "test_result_ = torch.load(dic_FileName)\n",
    "trained_weights_ = torch.load(dic_FileName2)\n",
    "print(test_result_)\n",
    "print(trained_weights_)\n",
    "\n",
    "with open(dic_fileName, 'rb') as fin:\n",
    "    test_result_ = pickle.load(fin)\n",
    "with open(dic_fileName2, 'rb') as fin2:\n",
    "    trained_weights_ = pickle.load(fin2)\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
