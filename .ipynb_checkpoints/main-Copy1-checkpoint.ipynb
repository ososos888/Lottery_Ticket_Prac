{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import visdom\n",
    "import copy\n",
    "import torch.nn.utils.prune as prune\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "# custom librarys (model, parameters...) Lottery_Ticket_Prac/custom/utils.py\n",
    "import custom.utils as cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(55)\n",
    "torch.cuda.manual_seed_all(55)\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices  2\n",
      "Current cuda device  1\n",
      "GeForce RTX 2080 Ti\n",
      "cpu와 cuda 중 다음 기기로 학습함: cuda:1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cuda setting\n",
    "GPU_NUM = 1\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(device))\n",
    "\n",
    "print(\"cpu와 cuda 중 다음 기기로 학습함:\", device, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: LeNet300\n",
      "lr: 0.0012\n",
      "epochs: 50\n",
      "batch_size: 60\n",
      "weight_decay: 0.0012\n",
      "iteration: 0\n",
      "prune_per_c: 1\n",
      "prune_per_f: 0.2\n",
      "prune_per_o: 0.1\n",
      "noi: 12\n",
      "trainset: Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ../MNIST_data/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
      "           )\n",
      "valset: empty\n",
      "testset: Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ../MNIST_data/\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
      "           )\n",
      "train_loader: <torch.utils.data.dataloader.DataLoader object at 0x7f80d416bed0>\n",
      "val_loader: empty\n",
      "test_loader: <torch.utils.data.dataloader.DataLoader object at 0x7f80d416be10>\n"
     ]
    }
   ],
   "source": [
    "# set model type\n",
    "model_type = 'LeNet300'\n",
    "#model_type = 'Conv6'\n",
    "\n",
    "best_accu = []\n",
    "\n",
    "# model, parameter get\n",
    "param = cu.parameters()\n",
    "\n",
    "if model_type == 'LeNet300':\n",
    "    model = cu.LeNet300().to(device)\n",
    "elif model_type == 'Conv6':\n",
    "    model = cu.Conv6().to(device)\n",
    "#elif ...\n",
    "    \n",
    "param.type(model_type)    \n",
    "model_init = copy.deepcopy(model)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# parameter check\n",
    "print('\\n'.join(\"%s: %s\" % item for item in param.__dict__.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "# visdom setting\n",
    "vis = visdom.Visdom()\n",
    "vis.close(env=\"main\")\n",
    "\n",
    "Tracker_type = \"Accuracy_Tracker\"\n",
    "title = model_type + \"_\" + Tracker_type\n",
    "\n",
    "# make plot\n",
    "vis_plt = vis.line(X=torch.Tensor(1).zero_(), Y=torch.Tensor(1).zero_(), \n",
    "                    opts=dict(title = title,\n",
    "                              legend=['100.0'],\n",
    "                             showlegend=True,\n",
    "                              xtickmin = 0,\n",
    "                              xtickmax = 20000,\n",
    "                              ytickmin = 0.95,\n",
    "                              ytickmax = 0.99\n",
    "                             )\n",
    "                   )\n",
    "\n",
    "def visdom_plot(loss_plot, num, loss_value, name):\n",
    "    vis.line(X = num,\n",
    "            Y = loss_value,\n",
    "            win = loss_plot,\n",
    "            name = name,\n",
    "            update = 'append'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change parameter for test (class에 직접 접근하여 변경)\n",
    "param.epochs = 5\n",
    "param.noi = 5\n",
    "\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test, prune function\n",
    "def train(model, dataloader, optimizer, criterion, cp_mask):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, label) in enumerate(dataloader):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss / len(dataloader)\n",
    "    return running_loss\n",
    "\n",
    "def test(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in dataloader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            test_loss += loss / len(dataloader)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "        # 로더 -> 배치 개수 로더.dataset -> 전체 길이, \n",
    "    return (correct/total), test_loss\n",
    "\n",
    "# prune function\n",
    "# pruning mask 생성 -> mask 복사 -> weight initialize -> prune 진행\n",
    "def weight_init(model1, model2, c_rate, f_rate, o_rate):\n",
    "    # layer별로 지정된 rate만큼 prune mask 생성\n",
    "    for name, module in model1.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.l1_unstructured(module, name = 'weight', amount = c_rate)\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if name != 'fc3':\n",
    "                prune.l1_unstructured(module, name = 'weight', amount = f_rate)\n",
    "            else:\n",
    "                prune.l1_unstructured(module, name = 'weight', amount = o_rate)\n",
    "                        \n",
    "    # mask 복사\n",
    "    cp_mask = {}\n",
    "    for name, mask in model1.named_buffers():\n",
    "        cp_mask[name[:(len(name)-12)]] = mask\n",
    "    \n",
    "    # weight initialize\n",
    "    for name, p in model1.named_parameters():\n",
    "        if 'weight_orig' in name:\n",
    "            for name2, p2 in model2.named_parameters():\n",
    "                if name[0:len(name) - 5] in name2:\n",
    "                    p.data = copy.deepcopy(p2.data)\n",
    "        if 'bias_orig' in name:\n",
    "            for name2, p2 in model2.named_parameters():\n",
    "                if name[0:len(name) - 5] in name2:\n",
    "                    p.data = copy.deepcopy(p2.data)\n",
    "                    \n",
    "    # prune 진행\n",
    "    for name, module in model1.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.remove(module, name = 'weight')\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            prune.remove(module, name = 'weight')            \n",
    "    \n",
    "    # gradient hook\n",
    "    for name, module in model.named_modules():\n",
    "        if 'fc' in name:\n",
    "            module.weight.register_hook(lambda grad, name=name : grad.mul_(cp_mask[name]))\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr = param.lr, weight_decay = param.weight_decay)\n",
    "    \n",
    "    \n",
    "    # copy된 mask return\n",
    "    return cp_mask, optimizer\n",
    "\n",
    "# weight count function\n",
    "# dict type ['Layer name' : [all, non_zero, zero, ratio]]\n",
    "def weight_counter(model):\n",
    "    layer_weight = {'all.weight':[0, 0, 0, 0]}\n",
    "    \n",
    "    for name, p in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            remain, pruned = (p != 0).sum().item(), (p == 0).sum().item()\n",
    "            layer_weight[name] = [remain+pruned, remain, pruned, round((remain/(remain+pruned))*100, 2)]\n",
    "            \n",
    "    for i in layer_weight.keys():\n",
    "        for j in range(0, 3):\n",
    "            layer_weight['all.weight'][j] += layer_weight[i][j]\n",
    "    layer_weight['all.weight'][3] = round(layer_weight['all.weight'][1]/layer_weight['all.weight'][0]*100, 2)\n",
    "    #print(\"-----------------------------------------------------\")\n",
    "    print(\"------------------------------------------------------------\\n\",\n",
    "          \"Layer\".center(12), \"Weight\".center(39), \"Ratio(%)\".rjust(7), sep='')\n",
    "    for i in layer_weight.keys():\n",
    "        \n",
    "        print(\"%s\" % i.ljust(13), \":\",\n",
    "              (\"%s (%s | %s)\" % (layer_weight[i][0], layer_weight[i][1], layer_weight[i][2])).center(36),\n",
    "              (\"%.2f\" % layer_weight[i][3]).rjust(7),\n",
    "              sep=''\n",
    "             )\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    return layer_weight\n",
    "\n",
    "# print best accuracy in each iteration\n",
    "def best_accuracy(best_accu):\n",
    "    print(\"Maximum accuracy per weight remaining\")\n",
    "    for i in range(len(best_accu)):\n",
    "        print(\"Remaining weight %.1f %% \" % (best_accu[i][0] * 100),\n",
    "             \"Epoch %d\" % best_accu[i][1],\n",
    "             \"Accu %.4f %%\" % best_accu[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "   Layer                     Weight                Ratio(%)\n",
      "all.weight   :        266200 (266200 | 0)          100.00\n",
      "fc1.weight   :        235200 (235200 | 0)          100.00\n",
      "fc2.weight   :         30000 (30000 | 0)           100.00\n",
      "fc3.weight   :          1000 (1000 | 0)            100.00\n",
      "------------------------------------------------------------\n",
      "\n",
      " Learning start! (Round : 1, Remaining weight : 100.0 %) \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef2e6f8d8a3449e84f8490bd50e49ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch : 0] (r_loss: x.xxxxx) (t_loss: x.xxxxx) (accu: 0.0735)\n",
      "[epoch : 1] (r_loss: 0.20805) (t_loss: 0.14887) (accu: 0.9502)\n",
      "[epoch : 2] (r_loss: 0.10718) (t_loss: 0.11349) (accu: 0.9627)\n",
      "[epoch : 3] (r_loss: 0.09354) (t_loss: 0.10031) (accu: 0.9679)\n",
      "[epoch : 4] (r_loss: 0.08629) (t_loss: 0.09715) (accu: 0.9680)\n",
      "[epoch : 5] (r_loss: 0.08111) (t_loss: 0.08932) (accu: 0.9716)\n",
      "\n",
      "Finish! (Best accu: 0.9716) (Time taken(sec) : 58.78) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "   Layer                     Weight                Ratio(%)\n",
      "all.weight   :      266200 (213060 | 53140)         80.04\n",
      "fc1.weight   :      235200 (188160 | 47040)         80.00\n",
      "fc2.weight   :        30000 (24000 | 6000)          80.00\n",
      "fc3.weight   :          1000 (900 | 100)            90.00\n",
      "------------------------------------------------------------\n",
      "\n",
      " Learning start! (Round : 2, Remaining weight : 80.04 %) \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e318bb95d554155800d878955d37091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch : 0] (r_loss: x.xxxxx) (t_loss: x.xxxxx) (accu: 0.1287)\n",
      "[epoch : 1] (r_loss: 0.19947) (t_loss: 0.11859) (accu: 0.9628)\n",
      "[epoch : 2] (r_loss: 0.10571) (t_loss: 0.10958) (accu: 0.9642)\n",
      "[epoch : 3] (r_loss: 0.08939) (t_loss: 0.12707) (accu: 0.9587)\n",
      "[epoch : 4] (r_loss: 0.08335) (t_loss: 0.09968) (accu: 0.9676)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b0288a839e89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m              )\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# model training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcp_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# val_set이 있을 경우 val_set을 통해 loss, accu를 구한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-b7dbeff5af5a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, cp_mask)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrunning_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(param.noi):\n",
    "    best_accu.append(0)\n",
    "    best_accu[i] = [0, 0, 0]\n",
    "    cp_mask = []\n",
    "    # pruning weight, mask 복사, optimizer 재설정\n",
    "    # layer별 prune rate를 입력\n",
    "    cp_mask, optimizer = weight_init(model, model_init, \n",
    "                           (1 - ((1-param.prune_per_c) ** i)),\n",
    "                           (1 - ((1-param.prune_per_f) ** i)),\n",
    "                           (1 - ((1-param.prune_per_o) ** i))\n",
    "                          )\n",
    "    #print(model.fc1.weight[0][300:325])\n",
    "    # prune 진행 후 남은 weight 수 확인\n",
    "    weight_counts = weight_counter(model)\n",
    "    # 총 weight 중 남은 weight의 수 저장 (visdom plot시 사용하기 위함)\n",
    "    remaining_weight = weight_counts['all.weight'][3]\n",
    "    print(\"\\n Learning start! (Round : %d, Remaining weight : %s %%) \\n\" % (i+1 , remaining_weight))\n",
    "    # 시작 시간 check\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    for epoch in tqdm(range(param.epochs)):\n",
    "        # 최초 정확도 확인\n",
    "        if epoch == 0:\n",
    "            accuracy, test_loss = test(model, param.test_loader, criterion)\n",
    "            visdom_plot(vis_plt,torch.Tensor([accuracy]), torch.Tensor([0]),\n",
    "                        str(remaining_weight)\n",
    "                       )\n",
    "            print('[epoch : %d]' % (epoch),\n",
    "             '(r_loss: x.xxxxx)',\n",
    "             '(t_loss: x.xxxxx)',\n",
    "             '(accu: %.4f)' % (accuracy)\n",
    "             )\n",
    "        # model training    \n",
    "        running_loss = train(model, param.train_loader, optimizer, criterion, cp_mask)\n",
    "        \n",
    "        # val_set이 있을 경우 val_set을 통해 loss, accu를 구한다.\n",
    "        if param.valset == 'empty':\n",
    "            accuracy, test_loss = test(model, param.test_loader, criterion)\n",
    "        else:\n",
    "            accuracy, test_loss = test(model, param.val_loader, criterion)\n",
    "        \n",
    "        # visdom plot (plot window, x-axis, y-axis, label name)\n",
    "        visdom_plot(vis_plt, torch.Tensor([(epoch+1) * 1000]), torch.Tensor([accuracy]),\n",
    "                    str(remaining_weight)\n",
    "                   )\n",
    "        \n",
    "        # best accuracy list (weight_remain, epoch, accuracy)\n",
    "        if best_accu[i][2] <= accuracy:\n",
    "            best_accu[i] = [remaining_weight, epoch, accuracy]\n",
    "        \n",
    "        print('[epoch : %d]' % (epoch+1),\n",
    "             '(r_loss: %.5f)' % (running_loss),\n",
    "             '(t_loss: %.5f)' % (test_loss),\n",
    "             '(accu: %.4f)' % (accuracy)\n",
    "             )\n",
    "        \n",
    "    stop_time = timeit.default_timer()    \n",
    "    #print(model.fc1.weight[0][300:325])\n",
    "    print(\"Finish!\",\n",
    "          \"(Best accu: %.4f)\" % best_accu[i][2],\n",
    "          \"(Time taken(sec) : %.2f)\" % (stop_time - start_time),\n",
    "          \"\\n\\n\\n\\n\\n\\n\\n\")\n",
    "\n",
    "# iteration별 최고 정확도 확인\n",
    "best_accuracy(best_accu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
