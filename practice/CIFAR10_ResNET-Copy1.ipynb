{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import visdom\n",
    "import subprocess\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "# visdom setting\n",
    "vis = visdom.Visdom()\n",
    "vis.close(env=\"main\")\n",
    "\n",
    "# make plot\n",
    "loss_plt = vis.line(Y=torch.Tensor(1).zero_(),\n",
    "                    opts=dict(title = 'VGG_Loss_Tracker',\n",
    "                              legend=['T_loss', 'V_loss'],\n",
    "                             showlegend=True\n",
    "                             )\n",
    "                   )\n",
    "\n",
    "def loss_tracker(loss_plot, loss_value, num, name):\n",
    "    vis.line(X = num,\n",
    "            Y = loss_value,\n",
    "            win = loss_plot,\n",
    "            name = name,\n",
    "            update = 'append'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "torch.manual_seed(555)\n",
    "torch.cuda.manual_seed_all(555)\n",
    "np.random.seed(555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "설정된 학습용 기기 : cuda:1\n"
     ]
    }
   ],
   "source": [
    "# CUDA 설정\n",
    "\n",
    "GPU_NUM = 1\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#device = torch.device('cuda')\n",
    "print(\"설정된 학습용 기기 :\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "lr = 0.1\n",
    "epochs = 30\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data 전처리\n",
    "transform = transforms.Compose([transforms.ToTensor()]\n",
    "                              )\n",
    "# Data load\n",
    "trainset = dsets.CIFAR10('../CIFAR10/',\n",
    "                         train=True,\n",
    "                         transform = transform,\n",
    "                         download=False)\n",
    "\n",
    "# transforms.Normalize\n",
    "train_data_mean = trainset.data.mean( axis=(0,1,2) )\n",
    "train_data_std = trainset.data.std( axis=(0,1,2) )\n",
    "\n",
    "# 각 pixel은 0~255값을 가지므로 이를 나누어 정규화한다.\n",
    "train_data_mean /= 255\n",
    "train_data_std /= 255\n",
    "\n",
    "transform_test = transforms.Compose([transforms.Resize(40),\n",
    "                                     transforms.RandomCrop(32),\n",
    "                                    transforms.ToTensor(),\n",
    "                               transforms.Normalize(train_data_mean, train_data_std)\n",
    "                               ])\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize(train_data_mean, train_data_std)\n",
    "                               ])\n",
    "\n",
    "# Normalize 된 dataset으로 reload\n",
    "trainset = dsets.CIFAR10('../CIFAR10/',\n",
    "                         train=True,\n",
    "                         transform = transform_test,\n",
    "                         download=False)\n",
    "\n",
    "valset = dsets.CIFAR10('../CIFAR10/',\n",
    "                         train=True,\n",
    "                         transform = transform,\n",
    "                         download=False)\n",
    "\n",
    "testset = dsets.CIFAR10('../CIFAR10/',\n",
    "                         train=False,\n",
    "                         transform = transform,\n",
    "                         download=False)\n",
    "\n",
    "# validation set 분류\n",
    "validation_ratio = 0.15\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "# 설정한 비율만큼 분할 시의 data 갯수\n",
    "split = int(np.floor(validation_ratio * num_train))\n",
    "# shuffle\n",
    "np.random.shuffle(indices)\n",
    "# data 분할\n",
    "train_idx, val_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = trainset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          sampler = train_sampler,\n",
    "                                          drop_last = True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset = valset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          sampler = val_sampler,\n",
    "                                          drop_last = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = testset,\n",
    "                                          batch_size = 4,\n",
    "                                          shuffle = False,\n",
    "                                          drop_last = True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog',\n",
    "           'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model output channel 수를 맞춰준다\n",
    "model = models.resnet18()\n",
    "model.fc = nn.Linear(512, 10)\n",
    "model.to(device)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\na = torch.Tensor(1,3,32,32).to(device)\\nout = model(a)\\nprint(out)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model's output channel check\n",
    "'''\n",
    "a = torch.Tensor(1,3,32,32).to(device)\n",
    "out = model(a)\n",
    "print(out)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# validation loss 함수\\ndef loss_eval(model, criterion, DataLoader, total_batch):\\n    with torch.no_grad():\\n        model.eval()\\n        running_loss = 0.0\\n        for i, data in enumerate(DataLoader, 0):\\n            inputs, labels = data\\n            inputs, labels = inputs.to(device), labels.to(device)\\n            \\n            outputs = model(inputs)\\n            loss = criterion(outputs, labels) \\n            running_loss += loss / total_batch\\n        return running_loss    \\n        \\n# accuracy 계산 함수\\ndef accu_eval(DataLoader):\\n    with torch.no_grad():\\n        model.eval()\\n        correct = 0\\n        total = 0\\n        for i, data in enumerate(DataLoader, 0):\\n            inputs, labels = data\\n            inputs, labels = inputs.to(device), labels.to(device)\\n            outputs = model(inputs)\\n            \\n            _, predicted = torch.max(outputs.data, 1)\\n            total += labels.size(0)\\n            correct += (predicted == labels).sum().item()\\n        return correct, total'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model 훈련 함수\n",
    "def train(model, optimizer, criterion, DataLoader, total_batch):\n",
    "    model.train()    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (data, label) in enumerate(DataLoader):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "                \n",
    "        \"\"\"\n",
    "        if batch_idx == 0:\n",
    "            print(data, label)\n",
    "        \"\"\"\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss / total_batch\n",
    "    return running_loss \n",
    "\n",
    "\"\"\"\n",
    "def train(model, dataloader, optimizer, criterion, cp_mask):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    #EPS = 1e-6\n",
    "    for batch_idx, (data, label) in enumerate(dataloader):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "\n",
    "        if cp_mask:\n",
    "            # 0-weight 학습 방지 code\n",
    "            i = 0\n",
    "            for name, p in model.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    p.grad.data *= cp_mask[i]\n",
    "                    i += 1\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss = loss / len(dataloader)\n",
    "    return running_loss\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"# validation loss 함수\n",
    "def loss_eval(model, criterion, DataLoader, total_batch):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(DataLoader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels) \n",
    "            running_loss += loss / total_batch\n",
    "        return running_loss    \n",
    "        \n",
    "# accuracy 계산 함수\n",
    "def accu_eval(DataLoader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(DataLoader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        return correct, total\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    test_loss = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, label) in enumerate(dataloader):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            outputs = model(data)\n",
    "            #test_loss += F.nll_loss(outputs, label, reduction='sum').item() # sum up batch loss\n",
    "            loss = criterion(outputs, label)\n",
    "            predicted = outputs.data.max(1, keepdim=True)[1]\n",
    "            correct += predicted.eq(label.data.view_as(predicted)).sum().item()\n",
    "            \n",
    "            test_loss += loss / len(dataloader)\n",
    "        #test_loss /= len(test_loader.dataset)\n",
    "            \n",
    "        accuracy = correct / len(dataloader)\n",
    "\n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def test(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in dataloader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            outputs = model(data)\n",
    "            #test_loss += F.nll_loss(outputs, label, reduction='sum').item() # sum up batch loss\n",
    "            loss = criterion(outputs, label)\n",
    "            predicted = outputs.data.max(1, keepdim=True)[1]\n",
    "            correct += predicted.eq(label.data.view_as(predicted)).sum().item()\n",
    "            \n",
    "        test_loss = loss / len(dataloader)\n",
    "        accuracy =  correct / len(dataloader.dataset)\n",
    "        # 로더 -> 배치 개수 로더.dataset -> 전체 길이, \n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "# 특정 가중치값이 커질수록 오버피팅이 발생할 가능성이 높아지므로\n",
    "# 이를 해소하기 위해 특정값을 손실함수에 더해주는 것 (가중치 업데이트 쏠림 방지)\n",
    "optimizer = optim.SGD(model.parameters(), lr = lr, momentum=0.9, weight_decay=3e-4)\n",
    "lr_sche = optim.lr_scheduler.StepLR(optimizer, step_size=13, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Start!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinhyuk/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch : 1] (T_loss: 2.12849)  (V_loss: 1.624769)  (Val Accuract : 49 %)\n",
      "[epoch : 2] (T_loss: 1.56727)  (V_loss: 1.447676)  (Val Accuract : 61 %)\n",
      "[epoch : 3] (T_loss: 1.36953)  (V_loss: 1.371854)  (Val Accuract : 64 %)\n",
      "[epoch : 4] (T_loss: 1.23542)  (V_loss: 1.231578)  (Val Accuract : 72 %)\n",
      "[epoch : 5] (T_loss: 1.11280)  (V_loss: 1.133960)  (Val Accuract : 76 %)\n",
      "[epoch : 6] (T_loss: 1.02569)  (V_loss: 1.154103)  (Val Accuract : 76 %)\n",
      "[epoch : 7] (T_loss: 0.96215)  (V_loss: 1.056794)  (Val Accuract : 81 %)\n",
      "[epoch : 8] (T_loss: 0.90450)  (V_loss: 1.068637)  (Val Accuract : 81 %)\n",
      "[epoch : 9] (T_loss: 0.86479)  (V_loss: 1.042346)  (Val Accuract : 83 %)\n",
      "[epoch : 10] (T_loss: 0.82380)  (V_loss: 0.954766)  (Val Accuract : 85 %)\n",
      "[epoch : 11] (T_loss: 0.79894)  (V_loss: 1.142243)  (Val Accuract : 79 %)\n",
      "[epoch : 12] (T_loss: 0.77304)  (V_loss: 1.011878)  (Val Accuract : 85 %)\n",
      "[epoch : 13] (T_loss: 0.63049)  (V_loss: 0.863099)  (Val Accuract : 90 %)\n",
      "[epoch : 14] (T_loss: 0.59470)  (V_loss: 0.853834)  (Val Accuract : 91 %)\n",
      "[epoch : 15] (T_loss: 0.57941)  (V_loss: 0.835953)  (Val Accuract : 92 %)\n",
      "[epoch : 16] (T_loss: 0.56103)  (V_loss: 0.823209)  (Val Accuract : 93 %)\n",
      "[epoch : 17] (T_loss: 0.56106)  (V_loss: 0.860529)  (Val Accuract : 91 %)\n",
      "[epoch : 18] (T_loss: 0.54537)  (V_loss: 0.905810)  (Val Accuract : 89 %)\n",
      "[epoch : 19] (T_loss: 0.53293)  (V_loss: 0.871767)  (Val Accuract : 91 %)\n",
      "[epoch : 20] (T_loss: 0.51990)  (V_loss: 0.892966)  (Val Accuract : 91 %)\n",
      "[epoch : 21] (T_loss: 0.52073)  (V_loss: 0.858272)  (Val Accuract : 91 %)\n",
      "[epoch : 22] (T_loss: 0.50566)  (V_loss: 0.861220)  (Val Accuract : 92 %)\n",
      "[epoch : 23] (T_loss: 0.49521)  (V_loss: 0.944041)  (Val Accuract : 90 %)\n",
      "[epoch : 24] (T_loss: 0.48275)  (V_loss: 0.839872)  (Val Accuract : 92 %)\n",
      "[epoch : 25] (T_loss: 0.47682)  (V_loss: 0.810626)  (Val Accuract : 94 %)\n",
      "[epoch : 26] (T_loss: 0.36122)  (V_loss: 0.833994)  (Val Accuract : 95 %)\n",
      "[epoch : 27] (T_loss: 0.32449)  (V_loss: 0.860869)  (Val Accuract : 94 %)\n",
      "[epoch : 28] (T_loss: 0.31580)  (V_loss: 0.871894)  (Val Accuract : 94 %)\n",
      "[epoch : 29] (T_loss: 0.30102)  (V_loss: 0.834883)  (Val Accuract : 95 %)\n",
      "[epoch : 30] (T_loss: 0.29778)  (V_loss: 0.817539)  (Val Accuract : 96 %)\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "t_batch = len(train_loader)\n",
    "v_batch = len(val_loader)\n",
    "print('Learning Start!')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    lr_sche.step()\n",
    "    # model training\n",
    "    t_running_loss = train(model, optimizer, criterion, train_loader, t_batch)\n",
    "\n",
    "    # validation loss\n",
    "    #v_running_loss = loss_eval(model, criterion, val_loader, v_batch)\n",
    "\n",
    "    # validation accuracy\n",
    "    #correct, total = accu_eval(val_loader)\n",
    "    \n",
    "    correct, v_running_loss = test(model, val_loader, criterion)\n",
    "    \n",
    "    # Plot & print\n",
    "    loss_tracker(loss_plt, torch.Tensor([t_running_loss]), torch.Tensor([epoch]), 'T_loss')\n",
    "    loss_tracker(loss_plt, torch.Tensor([v_running_loss]), torch.Tensor([epoch]), 'V_loss')\n",
    "\n",
    "    print('[epoch : %d] (T_loss: %.5f) ' % (epoch + 1, t_running_loss),\n",
    "          '(V_loss: %5f) ' % (v_running_loss),\n",
    "          #'(Val Accuract : %d %%)' % (100 * correct / total)\n",
    "          '(Val Accuract : %d %%)' % (correct)\n",
    "         )\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accu_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c6de34b2563a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccu_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy (testset) : %.3f %%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accu_eval' is not defined"
     ]
    }
   ],
   "source": [
    "# model test\n",
    "correct, total = accu_eval(test_loader)\n",
    "print('Accuracy (testset) : %.3f %%' % (100*correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[epoch : 1] (T_loss: 2.11550)  (V_loss: 1.610088)  (Val Accuract : 40 %)\n",
    "[epoch : 2] (T_loss: 1.48724)  (V_loss: 1.418673)  (Val Accuract : 47 %)\n",
    "[epoch : 3] (T_loss: 1.32180)  (V_loss: 1.354464)  (Val Accuract : 51 %)\n",
    "[epoch : 4] (T_loss: 1.18753)  (V_loss: 1.269082)  (Val Accuract : 55 %)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
